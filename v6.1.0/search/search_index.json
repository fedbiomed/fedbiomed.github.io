{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Collaborative Learning in Healthcare","text":""},{"location":"#open-transparent-and-trusted-collaborative-learning-for-real-world-healthcare-applications","title":"Open, Transparent and Trusted Collaborative Learning for Real-world Healthcare Applications","text":"<p>What is Fed-BioMed?</p> <p>Fed-BioMed is an open-source research and development initiative aiming at translating collaborative learning into real-world medical research applications.</p> <p>Fed-BioMed provides:</p> <ul> <li>A demonstrated framework for deploying collaborative learning in hospital networks,</li> <li>Easy deployment of state-of-the art federated learning and federated analytics methods, </li> <li>User-friendly tools for data managment and client participation to collaborative learning,</li> <li>A framework-agnostic environment for easily deploying machine learning methods, </li> <li>Clear solutions compliant with data providers' privacy, and nodes governance requirements.</li> </ul> <p>Fed-BioMed is an ongoing initiative, and the code is available on GitHub.</p>"},{"location":"#licensing","title":"Licensing","text":"<p>Fed-BioMed is released under the Apache License 2.0. For further details please see the LICENSE file in the Fed-BioMed source code repository.</p>"},{"location":"#join-us","title":"Join us!","text":"<ul> <li>If you want to be part of Fed-BioMed contact <code>fedbiomed _at_ inria _dot_ fr</code></li> </ul>"},{"location":"developer/","title":"Developer documentation","text":""},{"location":"developer/#developer-guidelines","title":"Developer guidelines","text":"<ul> <li>Usages and tools</li> <li>Continuous integration</li> </ul>"},{"location":"developer/ci/","title":"Developer info on continuous integration","text":"<p>Continuous integration uses GitHub Actions. </p>"},{"location":"developer/ci/#events-that-trigger-ci-tests","title":"Events that trigger CI tests","text":"<p>CI tests are triggered automatically by GitHub on a:</p> <ul> <li>pull request to <code>develop</code> or <code>master</code> branch</li> <li>push in <code>develop</code> or <code>master</code> branches (eg: after a merge, pushing a fix directly to this branch)</li> </ul> <p>The pull request can not be completed before CI pipeline succeeds</p> <ul> <li>pushing a fix to the branch with the open pull request re-triggers the CI test</li> <li>CI test can also be manually triggered form <code>Pull Requests</code> &gt; <code>Check</code> &gt; <code>Re-run all checks</code> or directly from <code>Action</code> tab. </li> </ul> <p>CI pipeline currently contains :</p> <ul> <li> <p>running unit tests</p> <ul> <li>update conda envs for <code>researcher</code></li> <li>run unit tests</li> </ul> </li> <li> <p>running a simplenet + federated average training, on a few batches of a MNIST dataset, with 2 nodes. For that, CI launches <code>./scripts/run_test_mnist</code> (an also be launched on localhost)</p> <ul> <li>update conda env for <code>node</code> (rely on unit tests for others)</li> <li>activate conda and environments, launch nodes.</li> <li>convert with <code>jupyter nbconvert</code> the notebook <code>./notebooks/101_getting-started.ipynb</code> to the python script <code>./notebooks/101_getting-started.py</code></li> <li>launch the <code>fedbiomed</code> script <code>./notebooks/101_getting-started.py</code></li> <li>succeed if the script completes without failure.</li> </ul> </li> <li> <p>running test build process for documentation </p> </li> </ul> <p>Execution exceptions</p> <p>CI build tests are run if a file related to the build is changed. For example, if the changes (difference between base and feature branch) in a pull request are only made in the gui directory or docs, the CI action for unit tests will be skipped. Please see the exceptions in <code>.gihub/workflows/*.yml</code></p>"},{"location":"developer/ci/#displaying-outputs-and-results","title":"Displaying Outputs and Results","text":"<p>To view CI test output and logs:</p> <ul> <li>view the pull request in github (select <code>Pull requests</code> in top bar, then select your pull request).</li> <li>click on the <code>Checks</code> at the top bar of the pull request and select the <code>Check</code> that you want to display.</li> <li>Click on the jobs to see its console output. </li> </ul>"},{"location":"developer/ci/#unit-tests-coverage","title":"Unit tests coverage","text":"<p>Unit tests coverage reports are published on Codecov platform for each branch/pull request. The report contains overall test coverage for the branch and detailed coverage rates file by file. </p> <ul> <li>Once a GitHub workflow/pipeline is executed for unit-test Codecov with automatically add a comment to the pull request that shows:<ul> <li>Overall test coverage</li> <li>The difference code coverage between base and feature branch </li> </ul> </li> </ul> <p>To access reports on Codecov please go Fed-BioMed Codecov dashboard or go to your pull request,click on <code>Checks</code> at the top of the pull request view and click on <code>View this Pull Request on Codecov</code></p>"},{"location":"developer/ci/#ci-and-github-actions-configuration","title":"CI and GitHub Actions Configuration","text":"<p>GitHub actions are configured using <code>yml</code> files for each workflow. Workflow files can contain multiple jobs and multiple steps for each job. Please go <code>.github/workflow</code> directory to display all workflows for CI. </p> <p>The <code>name</code> value in each <code>yml</code> file corresponds to the name of the workflows that are displayed in <code>Actions</code> page of the Fed-BioMed repository. The <code>name</code> value under each <code>job</code> corresponds to each <code>Checks</code> in pull requests.</p> <p>Please see GitHub actions documentation for more information. </p>"},{"location":"developer/ci/#ci-slaves","title":"CI slaves","text":"<p>CI slaves are located on <code>ci.inria.fr</code>. To be able to add extra configuration and installation you have to connect with your account on <code>ci.inria.fr</code>. You need to be approved by one member of the Fed-BioMed CI project or to be a member of Inria to be able get an account on <code>ci.inria.fr</code>. You can request the Fed-BioMed team to become a member of the Fed-BioMed CI project.</p>"},{"location":"developer/definition-of-done/","title":"Definition of Done for Fed-BioMed","text":"<p>v1.2 - 2023-10-16</p> <p>The Definition of Done is a set of items that must be completed and quality measures that must be met, before a task or a user story can be considered complete. The DoD gives the team a shared understanding of the work that was completed. </p>"},{"location":"developer/definition-of-done/#validate-ci","title":"Validate CI","text":"<ul> <li>Pass CI build tests </li> <li>Make sure documentation test build process is passed. Changes in docstring and documentation impacts documentation build process.  </li> </ul>"},{"location":"developer/definition-of-done/#review-of-the-code","title":"Review of the code","text":"<p>The reviewer can question any aspect of the increment in coherence with Usage and Tools, exchange with the developer (good practice : leave a github trace of the exchanges), approve it or not.</p> <ul> <li>Be specific in the pull request about what to review (critical code or properties of the code).</li> <li>Coding style: inspire from PEP-8.</li> <li>Understand the code and try to detect bugs.</li> <li>Remove detected bugs.</li> </ul>"},{"location":"developer/definition-of-done/#documentation","title":"Documentation","text":""},{"location":"developer/definition-of-done/#code","title":"Code","text":"<ul> <li>Comment critical or difficult points in the code; obvious lines (eg: tests) are excluded from comments.</li> <li>Add <code>FIXME</code> or <code>TODO</code> tags for any detected bugs or improvements that are technically beyond the scope of the pull request.</li> <li>Write minimal comments in the code (docstring) for a function or a class: parameters and typing, return, purpose of the class or the function.</li> </ul>"},{"location":"developer/definition-of-done/#userdeveloper-docs","title":"User/Developer Docs","text":"<ul> <li>Add API reference in <code>docs/developer/api</code> if there is a new module introduced.</li> <li>Write minimal documentation for scripts (separate README file) or notebooks (inside the notebook).</li> <li>Update/Add documentation in <code>docs</code> if there is a new feature or change in API that impacts the content or examples in documentation.</li> </ul>"},{"location":"developer/definition-of-done/#write-unit-test-for-the-code","title":"Write unit-test for the code","text":"<ul> <li>Focus on pure unit tests, no integration tests for now.</li> <li>Be clever : put reasonable effort on writing tests. Current target of unit tests is to reach 100% coverage of code, with reasonably clever functional coverage. When tests are too complicated to write, don't write them (until we consider code is mature enough to have hgher tests requirements).</li> <li>Add unit test when correcting a bug.</li> </ul> <p>Please refer to the guide of unit testing practices before starting to write or to modify the unit tests in Fed-BioMed. </p>"},{"location":"developer/definition-of-done/#post-merge-actions","title":"Post-merge actions","text":"<p>After merging:</p> <ul> <li>close the pull request</li> <li>update the issue</li> <li>if the pull request terminates the issue, mark the issue as <code>done</code> and close it.</li> </ul>"},{"location":"developer/development-environment/","title":"Configuring Development Environment","text":"<p>This article is written to guide developers to create development environment. This guide can also be used to build Fed-BioMed software from its source.</p> <p>This article will guide you to:</p> <ul> <li>Clone Fed-BioMed source repository.</li> <li>Setup a recommended tool to manage different Python versions.</li> <li>Setup some optional tools to manage isolated python environments such as <code>conda</code> and <code>virtualenv</code>.</li> <li>Setup pdm to manage dependencies and build operations, and it usage.</li> </ul>"},{"location":"developer/development-environment/#clone-the-repository","title":"Clone the repository","text":"<p>The first step is to clone the repository please go to Fed-BioMed GitHub Repository and clone it your local environment.</p> <pre><code>git clone git@github.com:fedbiomed/fedbiomed.git &lt;path-to-clone&gt;\ncd &lt;path-to-clone&gt;\n</code></pre>"},{"location":"developer/development-environment/#compatible-python-version","title":"Compatible Python version","text":"<p>Before you start using Fed-BioMed or developing on Fed-BioMed please make sure that required Python version is installed on your machine. Please go to <code>pyproject.toml</code> file located in the root of Fed-BioMed clone, and check required Python version interval.</p> <p>Tool for different Python version</p> <p>You can use the tool <code>pyenv</code> to be able to install specific Python versions into your local environment <pre><code>pyenv install 3.10\npyenv global 3.10\n</code></pre></p> <p>The code above will install latest Python 3.10, and activate this version globally for your machine.</p>"},{"location":"developer/development-environment/#install-recommended-tools-to-manage-development-environment","title":"Install recommended tools to manage development environment","text":""},{"location":"developer/development-environment/#use-a-virtual-environments-to-manage-dependencies","title":"Use a virtual environments to manage dependencies","text":"<p>Using virtual environments will allow you to isolate your Fed-BioMed development environment from your other projects. There, you have of course several options. You can use build-in python <code>venv</code> or <code>conda</code> or any other compatible Python virtual environment tools.</p> <p>While <code>conda</code> also allows installing a specified Python version, <code>venv</code> requires that Python and Pip are already installed. Therefore, if <code>conda</code> is used, additional tools like <code>pyenv</code> are not necessary for managing Python versions. However, if <code>venv</code> is preferred, using tools like <code>pyenv</code> to install the required Python version is recommended.</p>"},{"location":"developer/development-environment/#case-venv","title":"Case: venv","text":"<p>The advantage of <code>venv</code> is that it keeps all project dependencies within your project folder and is part of <code>python</code> standard library. Please go to Fed-BioMed project root (Fed-BioMed) clone and execute following command.</p> <pre><code>cd &lt;path-to-fedbiomed-clone&gt;\npython -m venv ./dev-env\n</code></pre> <p>The command above will generate <code>dev-env</code> folder in the project folder where all the project dependencies are kept.</p> <p>To activate the environment you can execute the following command from project root directory.</p> <pre><code>source dev-env/bin/activate\n</code></pre> <p>You can also use default naming convention <code>.venv</code> for virtual environment name.</p>"},{"location":"developer/development-environment/#case-conda","title":"Case: Conda","text":"<p><code>conda</code> can install Python version directly in the environment that is created. The command below will create a virtual environment and install the specified Python version.</p> <pre><code>conda create --name fedbiomed-dev python=3.10\nconda activate fedbiomed-dev\n</code></pre>"},{"location":"developer/development-environment/#install-pdm","title":"Install Pdm","text":"<p>Once virtual environments are set <code>PDM</code> package manager can be installed via <code>PyPI</code>. It is also possible to install <code>PDM</code> globally for the Python version that is activated or base <code>conda</code> environment if <code>conda</code> is preferred as virtual environment manager.</p> <pre><code>pip install pdm\n</code></pre> <p>The command above will install PDM in the active <code>conda</code> environment or in active <code>pyenv</code> if it is used for managing Python versions.</p>"},{"location":"developer/development-environment/#nodejs-and-yarn","title":"NodeJS and yarn","text":"<p>Fed-BioMed Node GUI uses the ReactJS library for web development. The ReactJS application for the Fed-BioMed Node GUI requires <code>Node.js</code> and <code>Yarn</code> to be installed, both for development and to enable automatic building during the creation of the Fed-BioMed package.</p> <p>Therefore, it is essential to have Node.js and Yarn installed on your system. Please follow the installation instructions for your operating system to install Node.js and Yarn.</p>"},{"location":"developer/development-environment/#using-pdm","title":"Using PDM","text":"<p>PDM is a tool to manage dependencies of a Python project, and build the python package:</p> <ul> <li>from <code>pyproject.toml</code> containing package source configuration, that is to say explicit instructions for build settings and dependencies</li> <li>from <code>pdm.lock</code> containing package locked cache version, that is to say a precise resolution of the configuration in <code>pyproject.toml</code> Both are located in the root directory of the project.</li> </ul> <p>Once PDM is installed, dependencies can be installed through the following command. It uses the existing <code>pdm.lock</code> to install dependencies for all Fed-BioMed components:</p> <pre><code>pdm install\n</code></pre> <p>This command will automatically detect the environment that used currently and install all dependencies in it. For example, if conda env is activated and there is no <code>.venv</code> folder in root directory of the project, all dependencies will  be installed within the <code>conda</code> environment. However, if another virtual environment is activated, or if <code>.venv</code> folder is existing PDM will install dependencies within that environment.</p> <p>If there is no virtual environment activated pdm will create a default virtual environment using python's <code>virtualenv</code> in <code>.venv</code></p> <p>To install dependencies for only some Fed-BioMed component (from <code>researcher</code>, <code>gui</code>, <code>node</code>) use:</p> <pre><code>pdm install -G researcher\n</code></pre>"},{"location":"developer/development-environment/#re-generate-pdmlock","title":"Re-generate <code>pdm.lock</code>","text":"<p>For some reason you may want to remove and re-generate a new <code>pdm.lock</code>.</p> <p>If there is no <code>pdm.lock</code> lock file when running <code>pdm install</code>, it first resolves all the dependencies defined in <code>pyproject.toml</code> and create a <code>pdm.lock</code> lock file.</p> <p>If running <code>pdm lock</code> it only generates a new lock file, but does not perform the installation.</p> <p>Important</p> <p>By default, the optional packages are not included in generated the <code>pdm.lock</code> file. These optional packages contain dependencies specific to each Fed-BioMed component. Use <code>pdm install -G :all</code> or <code>pdm install -G researcher,node,gui</code> or <code>pdm lock -G :all</code> to generate lockfile with dependencies for all components.</p> <p>There are also other installation options such selection of specific groups or extra dependencies. Please see <code>pdm install --help</code> and <code>pyproject.toml</code> file for more information.</p>"},{"location":"developer/development-environment/#what-about-instant-changes-in-source-code","title":"What about instant changes in source code?","text":"<p>PDM automatically adds package source to your environment that makes <code>fedbiomed</code> module accessible from Python interpreter. It also handles instant changes in the source code. If a new module is added or a module is edited PDM will automatically make changes available within the environment currently used. It means there is no need to install or rebuild <code>fedbiomed</code> package.</p> <p>Important</p> <p>You should never install <code>fedbiomed</code> from <code>pip</code> in your development environment. This may interrupt to access current source code changes in your development environment.</p>"},{"location":"developer/development-environment/#add-new-module","title":"Add new module","text":"<p>New packages can be added through <code>pyproject.toml</code>, or using the command <code>pdm add</code>. Please see <code>pdm add --help</code> for detailed usage instructions.</p> <p>Once a new package added via <code>pyproject.toml</code> it may be required to run <code>pdm lock -G :all</code> to resolve the dependencies, then, execute <code>pdm install</code> to update packages and install missing ones.</p>"},{"location":"developer/development-environment/#for-more","title":"For more","text":"<p>Please visit <code>pdm</code> documentation for more information and usage details.</p>"},{"location":"developer/development-environment/#post-actions","title":"Post actions","text":"<p>To verify your installation please run <code>pytest tests</code> and <code>tox -r</code> to make sure there is no missing module in your environment.</p>"},{"location":"developer/development-environment/#developmentdebugging-for-gui","title":"Development/Debugging for GUI","text":"<p>If you want to customize or work on user interface for debugging purposes, it is always better to use ReactJS in development mode, otherwise building GUI after every update will take a lot of time. To launch user interface in development mode first you need to start Flask server. This can be easily done with the previous start command. Currently, Flask server always get started on development mode.  To enable debug mode you should add <code>--debug</code> flag to the start command.</p> <pre><code># data folder defaults to `/path/to/my-node/data`\nfedbiomed node -p /path/to/my-node gui start --debug\n\n# Or use an alternate data path \n# fedbiomed node -p /path/to/my-node gui start --data-folder /alternate/data-path --debug\n</code></pre> <p>Important: Please do not change Flask port and host while starting it for development purposes. Because React (UI) will be calling <code>localhost:8484/api</code> endpoint in development mode.</p> <p>The command above will serve the web application and the API services. It means that on the URL <code>localhost:8484</code> you will be able to see the user interface. This user interface won't be updated automatically because it is already built. To have dynamic update for user interface you can start React with <code>yarn start</code>.</p> <pre><code># use the python environment for [development](../docs/developer/development-environment.md)\ncd ${FEDBIOMED_DIR}/gui/ui\nyarn start\n</code></pre> <p>After that if you go <code>localhost:3000</code> you will see same user interface is up and running for development.  When you change the source codes in <code>${FEDBIOMED_DIR}/gui/ui/src</code> it will get dynamically updated on <code>localhost:3000</code>.</p> <p>Since Flask is already started in debug mode, you can do your development/update/changes for server side (Flask) in <code>${FEDBIOMED_DIR}/gui/server</code>. React part (ui) on development mode will call API endpoint from <code>localhost:8484</code>, this is why first you should start Flask server first.</p> <p>After development/debugging is done. To update changes in built GUI, you need rebuild the React app. Afterwards, you will be able to see changes on the <code>localhost:8484</code> URL which serve built UI files.</p> <pre><code>yarn build\nfedbiomed node gui start\n</code></pre>"},{"location":"developer/development-environment/#troubleshooting","title":"Troubleshooting","text":"<p>You may encounter some common issues during installation or after the installation due to some missing packages. Please visit troubleshooting dedicated page for common issues.</p>"},{"location":"developer/development-environment/#error-on-macosubuntu-regarding-pyenv-usage","title":"Error on MacOS/Ubuntu regarding <code>pyenv</code> usage:","text":"<ul> <li> <p><code>_lzma</code> module not found</p> <ul> <li> <p>If you are using MacOS and installing Python versions through <code>pyenv</code> you may have some missing packages in your environment. <code>ModuleNotFoundError: No module named '_lzma'</code> is one of them. If you faced this error please install <code>brew install xz</code>, and reinstall python version <code>pyenv install &lt;version&gt;</code>.</p> </li> <li> <p>If you are using Ubuntu or Debian based OS, and install python through <code>pyenv</code>, and get <code>ModuleNotFoundError: No module named '_lzma'</code>     This could be fixed by using     <pre><code>sudo apt install liblzma-dev\n</code></pre></p> </li> <li>Other issues</li> <li>For other issues, <code>pyenv</code> comes with an utility <code>pyenv-doctor</code>, made for checking <code>pyenv</code> installation and dependencies, and can be run with: <code>pyenv doctor</code>. It will try to find the issue with your installation and proposes appropriate solutions to your issue.</li> </ul> <p>More troubleshooting for <code>pyenv</code> can be found here.</p> </li> </ul>"},{"location":"developer/development-environment/#building-fed-biomed-takes-too-long","title":"Building Fed-BioMed Takes too Long","text":"<p>Some static files located in the root Fed-BioMed source directory (e.g., notebooks, tests, etc.) are also included in the final distribution. Therefore, having large data files or artifacts left from operations for testing and development purposes can increase the loading time. Please ensure that such data files are cleared before building the Fed-BioMed package to reduce build time.</p>"},{"location":"developer/messaging/","title":"RPC Protocol and Messages","text":"<p>The messages/payloads between Fed-BioMed components are defined and typed on the application and the communication (gRPC) layers. The application layer messages are typed and defined in order to use within the application, and the messages for communication are the protobuff object that is used for sending the data from one end-point to another. After a client or the server receives a protobuf it immediately converts it to corresponding dataclass defined in the application layer. Therefore, new messages introduced by developer should be defined in <code>fedbiomed.common.message</code> module and should inherit from <code>fedbiomed.common.message</code>. However, while some message types does not require to be modified or created as protocol buffers, some does. For example, application messages such as <code>TrainRequest</code>, <code>TrainReply</code>, <code>SerachRequest</code> are typed generally using single protobuf as <code>TaskResponse</code>, but some messages are defined in protocol files explicitly as it is in <code>fedbiomed.message.module</code> (e.g <code>Log</code> and <code>Scalar</code>). Following sections will give the details of the messages used in communication and application layers. </p>"},{"location":"developer/messaging/#rpc-protocol-buffers","title":"RPC protocol buffers","text":"<p>Researcher component is the RPC server that provides RPC services for the nodes. The services and the corresponding message types for services are defined in <code>fedbiomed/transport/protocols/researcher.proto</code>. The service and message instances are generated automatically after compiling the protocol files. It is mandatory to define a message for each service and use corresponding Python instance as input or output value of services implemented in the software. </p>"},{"location":"developer/messaging/#compiling-proto-files","title":"Compiling proto files","text":"<p>Protocol files should be compiled to be able to create corresponding message and service objects in Python. Compilation can be done by executing <code>{FEDBIOMED_DIR}/scripts/protoc.py</code>. The script will generate RPC service and protocol buffer for Python in the directory <code>{FEDBIOMED_DIR}/fedbiomed/transport/protocols</code>. </p>"},{"location":"developer/messaging/#example-protocol-messages-and-corresponding-data-classes-in-the-application-layer","title":"Example protocol messages and corresponding data classes in the application layer","text":"<p>In <code>researcher.proto</code>, the message <code>TaskResponse</code> and <code>TaskResult</code> is a general format representing the bytes of messages. These messages are converted to corresponding <code>Message</code> dataclass of Fed-BioMed after they are received by node or researcher server (e.g <code>fedbiomed.common.message.TaskResponse</code>). <code>TaskResponse</code> or <code>TaskResult</code> can wrap the message related to tasks as bytes. For example, The <code>bytes_</code> field of <code>TaskResult</code> can contain bytes of <code>TrainReply</code> or <code>SearchReply</code>. The reason that these messages are typed as bytes is that they can be big message that needs to be sent as stream. Please see the following train request example to understand the workflow in message serialization;</p> <ol> <li>Researcher creates <code>fedbiomed.common.message.TrainRequest</code> message to assign train task to the nodes. </li> <li>The node executes <code>GetTask</code> service of researcher.</li> <li>Researcher service takes the <code>TrainRequest</code>.     3.1 serializes as bytes.     3.2 creates <code>fedbiomed.common.message.TaskResponse</code> from bytes      3.3 converts <code>TaskResponse</code> dataclass to <code>TaskResponse</code> protobuf using method <code>fedbiomed.common.message.TaskResponse.to_proto()</code></li> <li>send <code>TaskResponse</code> to the node. </li> </ol> <p>However, in <code>researcher.proto</code> some messages are typed and defined explicitly. For example, the message <code>TrainRequest</code> is the input value for the service <code>GetTaskUnary</code> that is executed by the nodes. <code>TrainRequest</code> ptoobuf has also corresponding message type defined in <code>fedbiomed.common.message</code> as <code>TaskRequest</code>. Please see the implemented dataclass for <code>TaskRequest</code> in <code>message.py</code> module.</p> <p><pre><code>@dataclass\nclass TaskRequest(ProtoSerializableMessage, RequiresProtocolVersion):\n    \"\"\"Task request message from node to researcher\"\"\"\n    __PROTO_TYPE__ = r_pb2.TaskRequest\n    node: str\n</code></pre> The messages that have corresponding protocol buffer in communication layer should define <code>__PROTO_TYPE__</code> attribute to acknowledge which protobuf will be used for this message class to be able to send it through RPC call. Thanks to this declaration the message can be converted to proto using <code>.to_proto()</code> or protobuf can be converted to python dataclass using <code>.form_proto(proto)</code> methods.  </p>"},{"location":"developer/testing-in-fedbiomed/","title":"Testing in Fed-BioMed","text":""},{"location":"developer/testing-in-fedbiomed/#unit-tests","title":"Unit Tests","text":"<p>Unit tests are designed to test smaller code units that make up larger solutions. These tests focus on isolated parts of the system and should not depend on external components such as third-party libraries or database connections. Meaning that third party library shouldn't cause failure in tests or shouldn't be tested.  </p> <p>When writing unit tests, it is important to consider the following aspects:</p> <ul> <li>Reliance on file system operations.</li> <li>Mocking remote API calls or database connections.</li> <li>Minimizing dependence on third-party libraries.</li> <li>Keeping the tests isolated from the overall system.</li> <li>Focusing on testing smaller unit functions within the code.</li> </ul> <p>In unit-testing the focus is to find the bug that can be found at small software pieces in isolation. There is no definitive answer as to whether the code should be completely isolated from the system in unit tests. In some cases, it may be beneficial to mock objects such as databases, depending on the complexity and ease of mocking.</p> <p>Isolation from system means: Isolate from </p> <ul> <li>the database </li> <li>third party dependencies</li> <li>In complex cases: modules/classes within the library </li> <li>file system</li> <li>API calls</li> </ul> <p>It is important that test cases should be simple and focus on testing a single method/function. Unit tests should not test the integration of different modules.</p> <p>Please find the following guidelines to pay attention to while writing unit tests:</p> <ol> <li>Mock file system operations.</li> <li>Tests should not depend on the system.</li> <li>Tests should focus on smaller units and only validate the result of the function being tested.</li> <li>Test the result of the method/function without getting lost in implementation details.</li> <li>Make sure abstract classes are tested separately by mocking abstract methods.</li> <li>Test the methods that inherit from base/abstract classes, but not the ones that are already tested in the base/abstract class.</li> <li>Pay attention to testing the functionality of the wrapper class instead of testing the integration of wrapped classes.</li> <li>Try to use Fake classes as little as possible. Follow the guide in section 8 to decide if a Fake class is required.</li> <li>Mock complex classes to reduce complexity.</li> <li>Do not mock the methods of the class being tested unless you have to.</li> <li>Instead of mocking lower layer API mock the higher-level instance that is used by the class being tested.</li> <li>Write separate test cases for private methods if the method is complex and complicated to test through public methods</li> <li>Focus on what the function does while testing methods that receive generic data types as input.</li> <li>For methods that can take different data types as arguments, do not write additional tests with different data types if the function doesn't explicitly perform specific actions for those types.</li> <li>If your tests are difficult to write or complex, rethink the design of the class you're testing, and ensure that your implementation respects the SOLID principles.</li> <li>Please add a new unit tests if a new bug is discovered. </li> </ol>"},{"location":"developer/testing-in-fedbiomed/#1-file-system-operations","title":"1. File system operations","text":"<ul> <li>Tests should not fail due to file system errors. Therefore, use mocking for write/read operations.</li> <li>Tests should trust that the module used for write/read operations has been tested and works correctly, as long as correct input values are provided. Therefore, tests should only verify that <code>write</code> or <code>read</code> methods are called with the correct arguments.</li> </ul>"},{"location":"developer/testing-in-fedbiomed/#2-system-dependent-tests","title":"2. System-dependent tests","text":"<ul> <li>Avoid testing data that depends on the system, such as process time, file size, or permissions.</li> <li>System-dependent tests also include testing GPU or CPU functionality.<ul> <li>Avoid writing tests that compare the number of processes or CPU count.</li> <li>If there are functions that require testing GPU functionality, implement those tests as integration tests.</li> </ul> </li> </ul>"},{"location":"developer/testing-in-fedbiomed/#3-focus-on-smaller-units","title":"3. Focus on smaller units","text":"<ul> <li>Each test function should test a single method of a class, function, or module.</li> <li>Test functions can execute other methods of the class being tested or other functions. However, the purpose should be to prepare the test state rather than testing those executions.</li> <li>If the method being tested executes private members of the class, please follow the section Private methods of a class.</li> </ul>"},{"location":"developer/testing-in-fedbiomed/#4-focus-on-intention-more-than-implementation","title":"4. Focus on intention more than implementation","text":"<ul> <li>Focus on the expected execution outcome of the method being tested.</li> <li>Validate the result of the function/method rather than the underlying implementation details.</li> <li>The purpose of the test should be to verify if the outcome is as expected given the inputs.</li> </ul>"},{"location":"developer/testing-in-fedbiomed/#5-testing-mix-in-baseabstract-class","title":"5. Testing Mix-in Base/Abstract Class","text":"<ul> <li>Test abstract/base classes by mocking abstract methods: <pre><code>self.abstract_methods_patcher = patch.multiple(AbstractClass, __abstractmethods__=set())\n</code></pre></li> <li>Test the remaining methods.</li> <li>If there are remaining private methods, please test them explicitly rather than testing through the class that implements the abstract/base class. This avoids duplicating tests among the classes that inherit from the abstract/base class being tested.</li> </ul>"},{"location":"developer/testing-in-fedbiomed/#6-testing-classes-that-implement-baseabstract-classes","title":"6. Testing classes that implement Base/Abstract classes","text":"<ul> <li>Following item 5, avoid testing the methods that are already tested in the TestCase of the abstract/base class.</li> <li>Test the methods that are overwritten by the class being tested.</li> <li>When testing extended methods, please follow the principles in Item 3 - Focus on smaller units.</li> </ul>"},{"location":"developer/testing-in-fedbiomed/#7-testing-wrappers","title":"7. Testing Wrappers","text":""},{"location":"developer/testing-in-fedbiomed/#71-wrappers-that-wrap-objects-with-a-common-interface","title":"7.1 Wrappers that wrap objects with a COMMON interface","text":"<ul> <li>Focus on wrapper methods rather than the wrapped objects. As objects that can be wrapped by the wrapper have a common interface, use a single object instead of applying the same test for each of them.</li> <li>If the wrapped object is complex and requires different states, consider creating a fake object, and follow the section Using Fake objects/classes to determine if a fake object is necessary.</li> </ul>"},{"location":"developer/testing-in-fedbiomed/#72-wrappers-that-wrap-objects-with-different-interfaces","title":"7.2 Wrappers that wrap objects with DIFFERENT interfaces","text":"<ul> <li>If the wrapper wraps different objects with different interfaces (methods), you can write tests for each wrapped object.</li> <li>Follow the same logic as in \"7.1\" for mocking or using fake objects.</li> </ul>"},{"location":"developer/testing-in-fedbiomed/#8-using-fake-or-stub-objectsclasses","title":"8. Using Fake or Stub objects/classes","text":"<ul> <li>Fake or Stub objects require coherency between the real implementation and the fake one. Therefore, avoid using fake objects as much as possible because they can be difficult to maintain.</li> <li>Please answer the following questions before deciding to create a fake class:<ul> <li> <ol> <li>Is the object passed to a function or class as an argument?</li> </ol> </li> <li> <ol> <li>Is the object dependent on the system or external actions (e.g., database)?</li> </ol> </li> <li> <ol> <li>Does the object have too many dependencies on other modules of the library?</li> </ol> </li> </ul> </li> <li>If 1. and 2. or 3. are \"yes,\" feel free to create a fake object.</li> <li>If 1. is \"no\" and the others are \"yes,\" consider using mocking instead of fake or stub objects.</li> </ul>"},{"location":"developer/testing-in-fedbiomed/#9-using-mocked-objects","title":"9. Using Mocked objects","text":"<ul> <li>Mocking helps isolate the test from the environment and easily test different scenarios of a single function.</li> <li>Pay attention to using the <code>spec</code>, <code>spec_set</code>, or <code>auto_spec</code> functionalities of <code>unittest.mock.Mock</code>. These functionalities help mock objects that respect the specifications of the real class, so the mock will be updated if an API of the real class changes.</li> <li> <p>Using too many mock objects may not be a good idea because:</p> <ul> <li>It may increase the complexity of the test function.</li> <li>It may isolate the test too much from the remaining parts of the software, resulting in invalid tests.</li> </ul> </li> <li> <p>Before deciding to mock an object, please answer the following questions:</p> <ul> <li> <ol> <li>Is the object instantiated or used directly within the module or class being tested?</li> </ol> </li> <li> <ol> <li>Is the object dependent on the system or external actions (e.g., database, network etc)?</li> </ol> </li> <li> <ol> <li>Does the object have too many dependencies on other modules of the library?</li> </ol> </li> <li> <ol> <li>Does the method being tested require many results or states of the object that needs to be mocked?</li> </ol> </li> <li> <ol> <li>Is mocking saving you a lot of time and reducing complexity?</li> </ol> </li> </ul> </li> <li> <p>If 1. is no and any other condition is yes, consider using a fake class.</p> </li> <li>If 1. is yes and any other condition is yes, consider using mocking. </li> <li>If mocking changes the intention of the test, try to find another solution besides mocking or using fake class.</li> </ul>"},{"location":"developer/testing-in-fedbiomed/#10-avoid-mocking-the-methods-of-the-class-being-tested","title":"10. Avoid mocking the methods of the class being tested","text":"<ul> <li>Please avoid mocking public/private methods of the class being tested.</li> <li>You can mock in some cases if you think that you have to mock it to reduce complexity. However, the need for mocking can be due to some design issues. Therefore, please make sure that the class being tested, or the test case is designed well. </li> </ul>"},{"location":"developer/testing-in-fedbiomed/#11-leveling-mocks","title":"11. Leveling Mocks","text":"<ul> <li>The classes being tested may require instantiating other instances that contain methods that need to be mocked, such as a database connection or file read/write operation. In such scenarios, instead of mocking the built-in read/write methods, mock the higher-level instance that is used by the class being tested.</li> </ul>"},{"location":"developer/testing-in-fedbiomed/#12-private-methods-of-a-class","title":"12. Private methods of a class","text":"<ul> <li>Write separate tests for private methods if it has complicated state preparation.</li> <li>For some cases, if a private method is not complex and easy to cover every line of code, you can test them through public method. However, there is no need to test for the second time through another public method if it is already tested once. </li> </ul>"},{"location":"developer/testing-in-fedbiomed/#13-generic-classes-that-accept-different-data-types-eg-dict","title":"13. Generic Classes that Accept Different Data Types (e.g., Dict)","text":"<ul> <li>When testing generic classes that can accept different data types, such as dictionaries containing different types of data, consider the following:<ul> <li>Test different data types if the behavior of the class depends on the specific data types.</li> <li>If the class uses another class to manage different data types, and that class has been tested, there is no need to test all possible combinations of data types.</li> </ul> </li> </ul>"},{"location":"developer/testing-in-fedbiomed/#14-testing-different-methods-that-accept-different-data-types-for-a-single-argument","title":"14. Testing Different Methods that Accept Different Data Types for a Single Argument","text":"<ul> <li>Test each data type separately (e.g., float, int) to cover all possible scenarios.</li> <li>IMPORTANT: If the behavior of the method is consistent across different data types, there is no need to test all combinations.</li> </ul>"},{"location":"developer/testing-in-fedbiomed/#15-having-complex-unit-test-cases-in-the-end","title":"15. Having complex unit test cases in the end","text":"<ul> <li>If unit tests require too much mocking and complexity, it is better to reconsider the design chosen for the class being tested. Following SOLID principles while designing a class or a module can help.<ul> <li>Single Responsibility Principle<ul> <li>A class should have only one reason to change.</li> </ul> </li> <li>Open-Closed Principle<ul> <li>Classes should be open for extension and closed to modification. Adding new functionality should not require modifying the existing code for the class, as it introduces potential bugs.</li> </ul> </li> <li>Liskov Substitution Principle<ul> <li>Subclasses should be substitutable for their base classes.</li> </ul> </li> <li>Interface Segregation Principle<ul> <li>Specific interfaces are better than one general-purpose interface.</li> </ul> </li> <li>Dependency Inversion Principle<ul> <li>Classes should depend on interfaces or abstract classes instead of concrete classes and functions.</li> </ul> </li> </ul> </li> </ul>"},{"location":"developer/testing-in-fedbiomed/#16-learn-from-bugs","title":"16. Learn from bugs","text":"<ul> <li>When a bug is discovered please consider adding a unit tests for it. However, if the bug can not be tested using unit tests please consider adding integration or end-to-end test. </li> </ul>"},{"location":"developer/testing-in-fedbiomed/#integration-testing","title":"Integration Testing","text":"<p>Integration testing involves testing the interaction between software modules. In the context of Fed-BioMed, integration testing can be performed on various integrated modules such as nodes, rounds, data loaders, training plans, models, optimizers, command-line interfaces (CLIs), dataset managers, model registration/approval components, and certificate registration components. The selection of integrated modules for testing depends on the logical integration of the system. </p> <ul> <li>Use database</li> <li>Write and read files from file system</li> <li>Rely on 3rd party libraries </li> </ul> <p>The environment and the API for integration tests are still in working process.</p>"},{"location":"developer/testing-in-fedbiomed/#end-to-end-testing","title":"End-to-End Testing","text":"<p>End-to-end testing focuses on testing the entire system or application as a whole, ensuring that all components work together correctly. In the context of Fed-BioMed, end-to-end testing would involve testing the complete workflow and functionalities of the collaborative learning framework, including data collection, model training, aggregation, and evaluation.</p>"},{"location":"developer/usage_and_tools/","title":"Developer usages and tools","text":""},{"location":"developer/usage_and_tools/#introduction","title":"Introduction","text":"<p>The purpose of this guide is to explicit the coding rules and conventions used for this project and explain the use of some of our tools.</p> <p>This guide is dedicated to all Fed-BioMed developers: Contributors, Reviewers, Team Developers, Core Developers.</p> <p>Some aspects of this guide may change in the future, stay alert for such changes.</p>"},{"location":"developer/usage_and_tools/#code","title":"Code","text":""},{"location":"developer/usage_and_tools/#coding-environment","title":"Coding environment","text":"<p>Except for some bash tools and scripts, the python language is used for most parts of the code.</p> <p>conda is used to ease the installation of python and the necessary packages.</p>"},{"location":"developer/usage_and_tools/#coding-style","title":"Coding style","text":"<p>We try to stick as close as possible to python coding style as described here</p> <p>We do not enforce coding style validation at each commit. In the future, we may implement some of the tools described here</p>"},{"location":"developer/usage_and_tools/#coding-rules","title":"Coding rules","text":"<p>Project specific coding rules come in addition to general coding style. Their goal is to favour code homogeneity within the project. They are meant to evolve during the project when needed. They also include advice for managing branches (commits, pushes, merges vs rebase).</p>"},{"location":"developer/usage_and_tools/#license","title":"License","text":"<p>Project code files should begin with these comment lines to help trace their origin: <pre><code># This file is originally part of Fed-BioMed\n# SPDX-License-Identifier: Apache-2.0\n</code></pre></p> <p>Code files can be reused from another project with a compatible non-contaminating license. They shall retain the original license and copyright mentions. The <code>CREDIT.md</code> file and <code>credit/</code> directory shall be completed and updated accordingly.</p>"},{"location":"developer/usage_and_tools/#authors","title":"Authors","text":"<p>Project does not mention authors in the code files. Developers can add themselves to <code>AUTHORS.md</code>.</p>"},{"location":"developer/usage_and_tools/#repositories","title":"Repositories","text":""},{"location":"developer/usage_and_tools/#framework","title":"Framework","text":"<p>The framework is contained in one git repository with 2 functional parts:</p> <ul> <li> <p>node: the library and tools to run on each node</p> </li> <li> <p>researcher: the library and tools to run on researcher's side</p> </li> </ul>"},{"location":"developer/usage_and_tools/#documentation","title":"Documentation","text":"<p>The documentation is contained in the repository under <code>docs</code> directory that is used for building the web site. The static files that are obtained after building documentation are kept in the repository <code>fedbiomed/fedbiomed.github.io</code> to serve for web.</p> <p>Fed-BioMed documentation page is configured to be built and published once there is new version tag released.  Publish process is launched as GitHub workflow job where the documentation is built and pushed to public repository <code>fedbiomed/fedbiomed.github.io</code>.</p>"},{"location":"developer/usage_and_tools/#events-for-documentation-build","title":"Events for documentation build","text":"<p>There are two events that trigger documentation publishing:</p> <ul> <li> <p><code>Publish MASTER fedbiomed/fedbiomed.github.io</code> when pushing a new commit to master</p> <p>The documentation website contains static pages such as the home page, about us, and support (main pages). These pages are separate from the documentation versioning process since they can be updated without requiring a new version to be published. As a result, whenever a new commit is pushed to the master branch, the GitHub workflow named <code>Publish MASTER fedbiomed/fedbiomed.github.io</code> is triggered. This workflow, located at <code>.github/workflows/doc-github-io-main-build.yml</code>, is responsible for publishing the changes made to the main pages.</p> </li> <li> <p><code>Publish NEW TAG in fedbiomed/fedbiomed.github.io</code> when pushing a new version tag</p> <p>The documentation-related pages located in the directories <code>getting-started</code>, <code>developer</code>, <code>tutorials</code>, and <code>user-guide</code> are built whenever a new version tag is pushed. The name of the workflow is <code>Publish NEW TAG in fedbiomed/fedbiomed.github.io</code> and the workflow file is located at <code>.github/workflows/doc-github-io-version-build.yml</code>.</p> </li> </ul>"},{"location":"developer/usage_and_tools/#process-flow-for-documentation-deployment","title":"Process flow for documentation deployment","text":"<ul> <li>The workflow file checks out the pushed commit or tag.</li> <li>It clones the <code>fedbiomed/fedbiomed.github.io</code> repository, which stores all the web static files.</li> <li>The documentation is built, and the artifacts are copied into the cloned folder of <code>fedbiomed/fedbiomed.github.io</code>.</li> <li>Changes are committed and pushed to <code>fedbiomed/fedbiomed.github.io</code>.</li> <li>The push event triggers the deployment job in the <code>fedbiomed/fedbiomed.github.io</code> repository.</li> </ul>"},{"location":"developer/usage_and_tools/#manual-build-for-testing","title":"Manual build for testing","text":"<p>Use the following command to build and serve documentation page on <code>http::/localhost:8000</code>. This allows you to test/verify changes in <code>docs</code> and also in doc-strings.</p> <pre><code># use the [development environment](./development-environment.md)\ncd ${FEDBIOMED_DIR}\n./scripts/docs/fedbiomed_doc.sh serve\n</code></pre> <p>Please see usage for additional options.</p> <pre><code>cd ${FEDBIOMED_DIR}\n./scripts/docs/fedbiomed_doc.sh --help\n</code></pre>"},{"location":"developer/usage_and_tools/#roles-and-accesses","title":"Roles and accesses","text":"<p>Current roles in Fed-BioMed development process are:</p> <ul> <li>Fed-BioMed Users: people using Fed-BioMed for research and/or deployment in collaborative learning applications, and reporting issues.</li> <li>Fed-BioMed Contributors: developers proposing their changes to the Fed-BioMed code and documentation via pull requests.</li> <li>Fed-BioMed Reviewers: developers reviewing the pull requests.<ul> <li>Reviewers can be Contributors, Team Developers or Core Developers.</li> </ul> </li> <li>Fed-BioMed Team Developers: developers recurrently proposing changes to the Fed-BioMed code and documentation via pull requests, and working in coordinated manner with other Team Developers<ul> <li>Currently, Team Developers are chosen by the existing Team Developers among the volunteer Contributors.</li> </ul> </li> <li>Fed-BioMed Core Developers: developers coordinating the coding of components and documentation of Fed-BioMed, design of extensions and modifications the API.<ul> <li>Currently, Core Developers also give final approval and merge the pull requests</li> <li>and new Core Developers are chosen by the existing Core Developers among the Team Developers.</li> </ul> </li> </ul> <p>In terms of mapping to accounts and roles on GitHub Fed-BioMed repository and organization:</p> <ul> <li>Users and Contributors have no specific access to the Fed-BioMed repository, they are not member of the Fed-BioMed GitHub organization</li> <li>Reviewers and Team Developers receive the github repository write access.</li> <li>Core Developers receive the github repository maintain access.</li> </ul> <p>Fed-BioMed developers/users access are personal and shall not be shared with someone else.</p> <p>Reviewers, Team Developers and Core Developers receive:</p> <ul> <li>registration as members of the GitHub Fed-BioMed organization, and membership in the Developers team (plus CoreDevelopers team for Core Cevelopers) of the organization</li> <li>invitation to Fed-BioMed developer Discord server</li> <li>registration in Fed-BioMed developer mailing lists (     discussion list <code>fedbiomed-developers _at_ inria _dot_ fr</code>,     development notifications list <code>fedbiomed-notifications _at_ inria _dot_ fr</code>)</li> <li>invitation to Fed-BioMed technical team shared files zone on <code>mybox.inria.fr</code></li> </ul> <p>Current list of Core Developers listed by alphabetical order:</p> <ul> <li>Yannick Bouillard</li> <li>Sergen Cansiz</li> <li>Francesco Cremonesi</li> <li>Marco Lorenzi</li> <li>Marc Vesin</li> </ul>"},{"location":"developer/usage_and_tools/#lifecycle","title":"Lifecycle","text":""},{"location":"developer/usage_and_tools/#gitflow-paradigm","title":"Gitflow paradigm","text":"<p>The gitflow paradigm must be followed when creating new development branches and for code release ( see here or here)</p>"},{"location":"developer/usage_and_tools/#release-next-release","title":"Release, next release","text":"<p>Creating a release or integrating a feature to the next release is the responsibility of Core Developers.</p> <p>As we use the gitflow paradigm, the <code>master</code> branch of each repository contains the releases. Next release is integrated under <code>develop</code>.</p> <p>In other words, the <code>master</code> and <code>develop</code> branches are protected and only writable by Core Developers.</p>"},{"location":"developer/usage_and_tools/#pull-request","title":"Pull request","text":"<p>New features are developed in a <code>feature</code> branch (refer to gitflow paradigm).</p> <p>Branch name for developing new features should start with <code>feature/</code> and make them easily linkable with the corresponding issue. For example if the branch is related to issue 123, name it <code>feature/123-my-short-explanation</code>.</p> <p>When the feature is ready, the Developer creates a pull request (PR) via GitHub. Be sure to request merging to the <code>develop</code> branch.</p> <p>The Core Developers team then assigns the pull request one Core Developer (Assignee PR field in GitHub) and one Reviewer (Reviewer PR field in GitHub). The Assignee and the Reviewer can be the same physical person, but they both shall be different people from the developer of the feature.</p> <p>The Reviewer then does a technical review of the pull request evaluating:</p> <ul> <li>the functional correctness of the feature (eg match with implemented algorithm or formula)</li> <li>the maturity of the feature implementation including conformance to the definition of done (DoD).</li> <li>the absence of technical regression introduced by the feature</li> <li>the technical coherence of the implementation of the feature with the existing code base</li> </ul> <p>The Reviewer marks the PR as Approved in GitHub once it is technically ready to be merged.</p> <p>The Assignee assesses:</p> <ul> <li>the interest of the feature in relation with the project goal and roadmap</li> <li>the absence of functional conflict introduced by the feature</li> <li>the valid timeline for merging the feature (if any dependency with other features)</li> </ul> <p>The Assignee merges the PR if it meets these requirements and it is Approved. If the merging needs to be delayed for some reason, the Assignee gives the final approval for merging with its condition/timeline as a comment of the PR.</p> <p>Once a branch is merged (or stalled , abandoned) it is usually deleted. If there is some reason to keep it, it should then be renamed to something starting with <code>attic/</code> (eg <code>attic/short-description-of-branch</code>).</p>"},{"location":"developer/usage_and_tools/#organization-and-scrum","title":"Organization and Scrum","text":"<ul> <li> <p>The development team is in charge of implementing Fed-BioMed's project goal and roadmap. It carries the bulk of the development effort, coordinating the work of Reviewers, Team Developers, Core Developers.</p> <p>It works as an agile team inspiring from Scrum and loosely implementing it. Development team's work is usually organized in sprints.</p> <p>Reviewers, Team Developers and Core Developers are welcome to the team meetings (daily meeting, sprint review, sprint retrospective) in the developer Discord lounge.</p> <p>Core Developers are invited to sprint planning meetings. Reviewers and Team Developers can be invited to sprint planning meetings depending on their involvement in current actions.</p> <p>Participating to the meetings is recommended in periods when one is actively taking part in a major development action where interaction is needed with other team members.</p> </li> <li> <p>External developers are autonomous developers (Contributors) working at their own pace. This typically fits primarily for punctual contribution, work on some specific function, PoC, etc. External developers are encouraged to interact with the development team to ensure coherence of their planned contributions with the rest of the development activity.</p> </li> </ul>"},{"location":"developer/usage_and_tools/#product-backlog","title":"Product backlog","text":"<p>Product backlog is a Scrum artifact composed of the product goal and product backlog entries. Each product backlog entry can contain a functional requirement, a user story, a task, etc.</p> <p>The current product goal content is:</p> <ol> <li>priority 1 : translating Collaborative Learning to real world healthcare applications</li> <li>priority 2 : as an open source software initiative, developing of the community, welcoming other contributions</li> <li>priority 3 : supporting initiatives that use Fed-BioMed</li> <li>priority 4 : experimenting new research and technologies</li> </ol> <p>Product backlog entries are:</p> <ul> <li>all milestones except those with a [PoC] mark starting their title</li> <li>issues with a product backlog label</li> </ul> <p>Product backlog is modified by the product owner only or with explicit validation of the product owner.</p> <p>Modifications of the product backlog include:</p> <ul> <li>adding new entries (issues/milestones) to the product backlog</li> <li>during sprint planning, moving issues from the product backlog to the new sprint's sprint backlog (they are selected for next sprint)</li> <li>during sprint planning, moving back uncomplete issues from the previous sprint's sprint backlog to the product backlog (they won't be continued during next sprint)</li> <li>moving product backlog issues to attic (they are now considered obsolete)</li> <li>closing product backlog milestones</li> </ul> <p>Note: product backlog entries and sprint backlog entries can mention \"priority 1\", etc. in their description to explicitely link to a product goal priority.</p>"},{"location":"developer/usage_and_tools/#sprint-backlog","title":"Sprint backlog","text":"<p>Sprint backlog is a Scrum artifact composed of a sprint goal (why) and product backlog elements selected for the sprint.</p> <p>Sprint backlog is a plan by and for the developers in order to achieve the sprint goal.</p> <p>Sprint backlog entries are:</p> <ul> <li>issues with a sprint backlog label</li> </ul> <p>Sprint backlog is created by the development team during the sprint planning. It can be updated and refined during the sprint (new issues, tasks and functional requirements rewriting) in accordance with the sprint goal.</p> <p>During the sprint planning, all uncomplete entries remaining from the previous sprint's sprint backlog can be:</p> <ul> <li>kept in the sprint backlog (they will be continued during next sprint)</li> <li>moved back to the product backlog (they won't be continued during next sprint)</li> <li>moved to the attic (they are now considered obsolete) and closed</li> </ul> <p>During the sprint planning, all complete entries from the previous sprint's sprint backlog:</p> <ul> <li>should already be closed (if not, close them) and marked with done label</li> <li>are removed from the sprint backlog</li> </ul>"},{"location":"developer/usage_and_tools/#proof-of-concepts","title":"Proof of concepts","text":"<p>Proof of concept (PoC) are used to experiment new scientific or technical explorations in Fed-BioMed: PoCs are not bound in time or attached to a sprint. They are closed when they complete or after being stalled for several months.</p> <p>Proof of concepts are:</p> <ul> <li>all milestones with a [PoC] mark starting their title</li> </ul> <p>PoCs code is not integrated to the next release (no merge). PoCs are not committed to code quality practices (eg: meeting the DoD). When a PoC completes, it may be decided that the PoC functionality:</p> <ul> <li>will not be implemented: close the PoC milestone</li> <li>will be implemented: convert the PoC milestone to a product backlog milestone</li> </ul> <p>PoC use branches starting with <code>poc/</code> eg <code>poc/my-short-poc-description</code>.</p> <p>PoC is not a Scrum notion.</p>"},{"location":"developer/usage_and_tools/#milestones-and-issues","title":"Milestones and issues","text":"<p>GitHub milestones and issues are used to keep track of product backlog, sprint backlog and other product items (bugs, proposals, user requests).</p>"},{"location":"developer/usage_and_tools/#milestones","title":"Milestones","text":"<p>Milestones are used to describe mid-term (eg: multi-months) major goals of the project (tasks, functional requirements, user stories).</p> <p>A milestone is:</p> <ul> <li>either a proof of concept (PoC)</li> <li>or a product backlog entry</li> </ul>"},{"location":"developer/usage_and_tools/#issues","title":"Issues","text":"<p>Issues are used to describe smaller goals of the project (tasks, functional requirements, user stories)</p> <p>An open issue has exactly one type amongst:</p> <ul> <li>needs-triage for new user issues</li> <li>user support</li> <li>a candidate</li> <li>a product backlog entry</li> <li>a sprint backlog entry</li> </ul> <p>An issue:</p> <ul> <li>can be created by a user. It must then be labelled as needs-triage</li> <li>can be created by an individual developer. It must then label as a candidate.</li> <li>can be moved from needs-triage to user support or candidate by a team developer. The team developer ensures it contains necessary information and is explicit enough. Team developer can also add misc labels.</li> <li>can be moved to the product backlog by the product owner or with explicit validation of the product owner</li> <li>can be moved to the sprint backlog during sprint planning by the team developers</li> <li>is closed and marked done when it is completed. If it belongs to the sprint backlog, it should keep this label until the end of the current sprint.</li> </ul> <p>A closed issue has exactly one type amongst:</p> <ul> <li>done (and not anymore in the sprint backlog)</li> <li>attic</li> </ul> <p>An issue can be labelled as attic and closed when it is considered obsolete or not relevant. It then loses its open issue type label (needs-triage, candidate, product backlog, sprint backlog) except for user support issues (make it easy to browse past user support issues).</p>"},{"location":"developer/usage_and_tools/#labels","title":"Labels","text":"<p>Zero or more labels are associated to an issue. We sort labels in several categories:</p>"},{"location":"developer/usage_and_tools/#type-labels","title":"type labels:","text":"<ul> <li>needs-triage : A user submits a work request to the team (extension proposal, bug, other request)</li> <li>user support : A developer validates a user-submitted issue to be a request for support or for information (if it is a bug report or a work request, then it should rather be a labelled as candidate)</li> <li>candidate : An individual developer submits a work request to the team (extension proposal, bug, other request) or validates a user-submitted issue</li> <li>product backlog : The product owner adds an entry to the product backlog</li> <li>sprint backlog : The development team adds an entry to the sprint backlog</li> <li>attic : The entry is not completed, but is now considered obsolete and closed</li> <li>done: The entry is completed, closed, and not anymore in the sprint backlog</li> </ul>"},{"location":"developer/usage_and_tools/#status-labels","title":"status labels:","text":"<p>All sprint backlog issues have one status label. Other issues only have a status label when they are active (eg: a developer not participating to a sprint, a developer working during intersprint).</p> <ul> <li>todo : Issue not started yet (but intention to start soon)</li> <li>doing : Issue implementation in progress</li> <li>in review : Issue implementation is finished, a pull request open and is ready for review (or under review)</li> <li>done : Issue is completed, it meets the DoD and was merged to the next release integration branch, but it still belongs to the sprint backlog</li> </ul>"},{"location":"developer/usage_and_tools/#misc-labels","title":"misc labels","text":"<p>These are optional label that give additional information on an issue.</p> <ul> <li>bug : This issue is about reporting and resolving a suspected bug</li> <li>improvement : This label indicates that the created issue relates to the improvement of an existing feature.</li> <li>new feature : This issue proposes to introduce a new functionality. This probably should not be set in the same issue as a bug or user support</li> <li>documentation : Documentation related issue</li> <li>good first issue : Nice to pick for a new contributor</li> </ul> <p>Note: some previously existing tags are now removed - postponed, feature, improvement, intersprint</p>"},{"location":"developer/usage_and_tools/#example","title":"Example","text":"<ul> <li> <p>an issue with labels sprint backlog + todo + bug means that this issue is in the current sprint's backlog, that it is not yet started, and that it solves a bug.</p> </li> <li> <p>summary :</p> </li> </ul> <p></p>"},{"location":"developer/usage_and_tools/#other-tools","title":"Other tools","text":"<ul> <li>project file repository (Mybox Fed-BioMed-tech)</li> </ul>"},{"location":"developer/api/common/certificate_manager/","title":"Certificate Manager","text":""},{"location":"developer/api/common/certificate_manager/#fedbiomed.common.certificate_manager-attributes","title":"Attributes","text":""},{"location":"developer/api/common/certificate_manager/#fedbiomed.common.certificate_manager-classes","title":"Classes","text":""},{"location":"developer/api/common/certificate_manager/#fedbiomed.common.certificate_manager.CertificateManager","title":"CertificateManager","text":"<pre><code>CertificateManager(db_path=None)\n</code></pre> <p>Certificate manager to manage certificates of parties</p> Attrs <p>_db: TinyDB database to store certificates</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <p>The name of the DB file to connect through TinyDB</p> required Source code in <code>fedbiomed/common/certificate_manager.py</code> <pre><code>def __init__(self, db_path: str = None):\n    \"\"\"Constructs certificate manager\n\n    Args:\n        db: The name of the DB file to connect through TinyDB\n    \"\"\"\n\n    self._db: Union[Table, None] = None\n    self._query: Query = Query()\n\n    if db_path is not None:\n        self.set_db(db_path)\n</code></pre>"},{"location":"developer/api/common/certificate_manager/#fedbiomed.common.certificate_manager.CertificateManager-functions","title":"Functions","text":""},{"location":"developer/api/common/certificate_manager/#fedbiomed.common.certificate_manager.CertificateManager.delete","title":"delete","text":"<pre><code>delete(party_id)\n</code></pre> <p>Deletes given party from table</p> <p>Parameters:</p> Name Type Description Default <code>party_id</code> <p>Party id</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The document IDs of deleted certificates</p> Source code in <code>fedbiomed/common/certificate_manager.py</code> <pre><code>def delete(self, party_id) -&gt; List[int]:\n    \"\"\"Deletes given party from table\n\n    Args:\n        party_id: Party id\n\n    Returns:\n        The document IDs of deleted certificates\n    \"\"\"\n\n    return self._db.remove(self._query.party_id == party_id)\n</code></pre>"},{"location":"developer/api/common/certificate_manager/#fedbiomed.common.certificate_manager.CertificateManager.generate_self_signed_ssl_certificate","title":"generate_self_signed_ssl_certificate  <code>staticmethod</code>","text":"<pre><code>generate_self_signed_ssl_certificate(certificate_folder, certificate_name='FBM_', component_id='unknown', subject=None)\n</code></pre> <p>Creates self-signed certificates</p> <p>Parameters:</p> Name Type Description Default <code>certificate_folder</code> <p>The path where certificate files <code>.pem</code> and <code>.key</code> will be saved. Path should be absolute.</p> required <code>certificate_name</code> <code>str</code> <p>Name of the certificate file.</p> <code>'FBM_'</code> <code>component_id</code> <code>str</code> <p>ID of the component</p> <code>'unknown'</code> <p>Returns:</p> Name Type Description <code>private_key</code> <code>str</code> <p>Private key file</p> <code>public_key</code> <code>str</code> <p>Certificate file</p> <p>Raises:</p> Type Description <code>FedbiomedCertificateError</code> <p>If certificate directory is invalid or an error occurs while writing certificate files in given path.</p> <p>Certificate files</p> <pre><code>Certificate files will be saved in the given directory as\n`certificates.key` for private key `certificate.pem` for public key.\n</code></pre> Source code in <code>fedbiomed/common/certificate_manager.py</code> <pre><code>@staticmethod\ndef generate_self_signed_ssl_certificate(\n    certificate_folder,\n    certificate_name: str = \"FBM_\",\n    component_id: str = \"unknown\",\n    subject: Optional[Dict[str, str]] = None,\n) -&gt; Tuple[str, str]:\n    \"\"\"Creates self-signed certificates\n\n    Args:\n        certificate_folder: The path where certificate files `.pem` and `.key`\n            will be saved. Path should be absolute.\n        certificate_name: Name of the certificate file.\n        component_id: ID of the component\n\n    Returns:\n        private_key: Private key file\n        public_key: Certificate file\n\n    Raises:\n        FedbiomedCertificateError: If certificate directory is invalid or an error\n            occurs while writing certificate files in given path.\n\n    !!! info \"Certificate files\"\n            Certificate files will be saved in the given directory as\n            `certificates.key` for private key `certificate.pem` for public key.\n    \"\"\"\n    subject = subject or {}\n\n    if not os.path.abspath(certificate_folder):\n        raise FedbiomedCertificateError(\n            f\"{ErrorNumbers.FB619.value}: Certificate path should be absolute: \"\n            f\"{certificate_folder}\"\n        )\n\n    if not os.path.isdir(certificate_folder):\n        raise FedbiomedCertificateError(\n            f\"{ErrorNumbers.FB619.value}: Certificate path is not valid: {certificate_folder}\"\n        )\n\n    pkey = crypto.PKey()\n    pkey.generate_key(crypto.TYPE_RSA, 2048)\n\n    cn = subject.get(\"CommonName\", \"*\")\n    on = subject.get(\"OrganizationName\", component_id)\n\n    x509 = crypto.X509()\n    subject = x509.get_subject()\n    subject.commonName = cn\n    subject.organizationName = on\n    x509.set_issuer(subject)\n\n    extensions = []\n    try:\n        if ipaddress.ip_address(cn):\n            extensions.append(\n                # TODO: X509Extension is deprecated, update with newer version\n                crypto.X509Extension(\n                    type_name=b\"subjectAltName\",\n                    critical=False,\n                    value=f\"IP:{cn}\".encode(\"ASCII\"),\n                )\n            )\n    except ValueError:\n        pass\n    if extensions:\n        x509.add_extensions(extensions)\n\n    x509.gmtime_adj_notBefore(0)\n    x509.gmtime_adj_notAfter(5 * 365 * 24 * 60 * 60)\n    x509.set_pubkey(pkey)\n    x509.set_serial_number(random.randrange(100000))\n    x509.set_version(2)\n    x509.sign(pkey, \"SHA256\")\n\n    # Certificate names\n    key_file = os.path.join(certificate_folder, f\"{certificate_name}.key\")\n    pem_file = os.path.join(certificate_folder, f\"{certificate_name}.pem\")\n\n    try:\n        with open(key_file, \"wb\") as f:\n            f.write(crypto.dump_privatekey(crypto.FILETYPE_PEM, pkey))\n            f.close()\n    except Exception as e:\n        raise FedbiomedCertificateError(\n            f\"{ErrorNumbers.FB619.value}: Can not write public key: {e}\"\n        ) from e\n\n    try:\n        with open(pem_file, \"wb\") as f:\n            f.write(crypto.dump_certificate(crypto.FILETYPE_PEM, x509))\n            f.close()\n    except Exception as e:\n        raise FedbiomedCertificateError(\n            f\"{ErrorNumbers.FB619.value}: Can not write public key: {e}\"\n        ) from e\n\n    return key_file, pem_file\n</code></pre>"},{"location":"developer/api/common/certificate_manager/#fedbiomed.common.certificate_manager.CertificateManager.get","title":"get","text":"<pre><code>get(party_id)\n</code></pre> <p>Gets certificate/public key  of given party</p> <p>Parameters:</p> Name Type Description Default <code>party_id</code> <code>str</code> <p>ID of the party which certificate will be retrieved from DB</p> required <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>Certificate, dict like TinyDB document</p> Source code in <code>fedbiomed/common/certificate_manager.py</code> <pre><code>def get(self, party_id: str) -&gt; Union[dict, None]:\n    \"\"\"Gets certificate/public key  of given party\n\n    Args:\n        party_id: ID of the party which certificate will be retrieved from DB\n\n    Returns:\n        Certificate, dict like TinyDB document\n    \"\"\"\n\n    v = self._db.get(self._query.party_id == party_id)\n    return v\n</code></pre>"},{"location":"developer/api/common/certificate_manager/#fedbiomed.common.certificate_manager.CertificateManager.insert","title":"insert","text":"<pre><code>insert(certificate, party_id, component, upsert=False)\n</code></pre> <p>Inserts new certificate</p> <p>Parameters:</p> Name Type Description Default <code>certificate</code> <code>str</code> <p>Public-key for the FL parties</p> required <code>party_id</code> <code>str</code> <p>ID of the party</p> required <code>component</code> <code>str</code> <p>Node or researcher,</p> required <code>ip</code> <p>IP of the component which the certificate will be registered</p> required <code>port</code> <p>Port of the component which the certificate will be registered</p> required <code>upsert</code> <code>bool</code> <p>Update document with new data if it is existing</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[int, list[int]]</code> <p>Document ID of inserted certificate</p> <p>Raises:</p> Type Description <code>FedbiomedCertificateError</code> <p>party is already registered</p> Source code in <code>fedbiomed/common/certificate_manager.py</code> <pre><code>def insert(\n    self,\n    certificate: str,\n    party_id: str,\n    component: str,\n    upsert: bool = False,\n) -&gt; Union[int, list[int]]:\n    \"\"\"Inserts new certificate\n\n    Args:\n        certificate: Public-key for the FL parties\n        party_id: ID of the party\n        component: Node or researcher,\n        ip: IP of the component which the certificate will be registered\n        port: Port of the component which the certificate will be registered\n        upsert: Update document with new data if it is existing\n\n    Returns:\n        Document ID of inserted certificate\n\n    Raises:\n        FedbiomedCertificateError: party is already registered\n    \"\"\"\n    certificate_ = self.get(party_id=party_id)\n    if not certificate_:\n        return self._db.insert(\n            {\n                \"certificate\": certificate,\n                \"party_id\": party_id,\n                \"component\": component,\n            }\n        )\n\n    if upsert:\n        return self._db.upsert(\n            {\n                \"certificate\": certificate,\n                \"component\": component,\n                \"party_id\": party_id,\n            },\n            self._query.party_id == party_id,\n        )\n    raise FedbiomedCertificateError(\n        f\"{ErrorNumbers.FB619.value}: Party {party_id} already registered. \"\n        \"Please use `upsert=True` or '--upsert' option through CLI\"\n    )\n</code></pre>"},{"location":"developer/api/common/certificate_manager/#fedbiomed.common.certificate_manager.CertificateManager.list","title":"list","text":"<pre><code>list(verbose=False)\n</code></pre> <p>Lists registered certificates.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>Prints list of registered certificates in tabular format</p> <code>False</code> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List of certificate objects registered in DB</p> Source code in <code>fedbiomed/common/certificate_manager.py</code> <pre><code>def list(self, verbose: bool = False) -&gt; List[dict]:\n    \"\"\"Lists registered certificates.\n\n    Args:\n        verbose: Prints list of registered certificates in tabular format\n\n    Returns:\n        List of certificate objects registered in DB\n    \"\"\"\n    certificates = self._db.all()\n\n    if verbose:\n        to_print = copy.deepcopy(certificates)\n        for doc in to_print:\n            doc.pop(\"certificate\")\n\n        print(tabulate(to_print, headers=\"keys\"))\n\n    return certificates\n</code></pre>"},{"location":"developer/api/common/certificate_manager/#fedbiomed.common.certificate_manager.CertificateManager.register_certificate","title":"register_certificate","text":"<pre><code>register_certificate(certificate_path, party_id, upsert=False)\n</code></pre> <p>Registers certificate</p> <p>Parameters:</p> Name Type Description Default <code>certificate_path</code> <code>str</code> <p>Path where certificate/key file stored</p> required <code>party_id</code> <code>str</code> <p>ID of the FL party which the certificate will be registered</p> required <code>upsert</code> <code>bool</code> <p>If <code>True</code> overwrites existing certificate for specified party.  If <code>False</code> and the certificate for the specified party already existing it raises error.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[int, List[int]]</code> <p>The document ID of registered certificated.</p> <p>Raises:</p> Type Description <code>FedbiomedCertificateError</code> <p>If certificate file is not existing in file system</p> Source code in <code>fedbiomed/common/certificate_manager.py</code> <pre><code>def register_certificate(\n    self,\n    certificate_path: str,\n    party_id: str,\n    upsert: bool = False,\n) -&gt; Union[int, List[int]]:\n    \"\"\"Registers certificate\n\n    Args:\n        certificate_path: Path where certificate/key file stored\n        party_id: ID of the FL party which the certificate will be registered\n        upsert: If `True` overwrites existing certificate for specified party.  If `False`\n            and the certificate for the specified party already existing it raises error.\n\n    Returns:\n        The document ID of registered certificated.\n\n    Raises:\n        FedbiomedCertificateError: If certificate file is not existing in file system\n    \"\"\"\n\n    if not os.path.isfile(certificate_path):\n        raise FedbiomedCertificateError(\n            f\"{ErrorNumbers.FB619.value}: Certificate path does not represents a file.\"\n        )\n\n    # Read certificate content\n    certificate_content = read_file(certificate_path)\n\n    # Save certificate in database\n    component = (\n        ComponentType.NODE.name\n        if party_id.startswith(NODE_PREFIX)\n        else ComponentType.RESEARCHER.name\n    )\n\n    return self.insert(\n        certificate=certificate_content,\n        party_id=party_id,\n        component=component,\n        upsert=upsert,\n    )\n</code></pre>"},{"location":"developer/api/common/certificate_manager/#fedbiomed.common.certificate_manager.CertificateManager.set_db","title":"set_db","text":"<pre><code>set_db(db_path)\n</code></pre> <p>Sets database</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>The path of DB file where <code>Certificates</code> table are stored</p> required Source code in <code>fedbiomed/common/certificate_manager.py</code> <pre><code>def set_db(self, db_path: str) -&gt; None:\n    \"\"\"Sets database\n\n    Args:\n        db_path: The path of DB file where `Certificates` table are stored\n    \"\"\"\n    db = TinyDB(db_path)\n    db.table_class = DBTable\n    self._db: Table = db.table(\"Certificates\")\n</code></pre>"},{"location":"developer/api/common/certificate_manager/#fedbiomed.common.certificate_manager-functions","title":"Functions","text":""},{"location":"developer/api/common/certificate_manager/#fedbiomed.common.certificate_manager.generate_certificate","title":"generate_certificate","text":"<pre><code>generate_certificate(root, component_id, prefix=None, subject=None)\n</code></pre> <p>Generates certificates</p> <p>Parameters:</p> Name Type Description Default <code>component_id</code> <p>ID of the component for which the certificate will be generated</p> required <p>Returns:</p> Name Type Description <code>key_file</code> <code>str</code> <p>The path where private key file is saved</p> <code>pem_file</code> <code>str</code> <p>The path where public key file is saved</p> <p>Raises:</p> Type Description <code>FedbiomedEnvironError</code> <p>If certificate directory for the component has already <code>certificate.pem</code> or <code>certificate.key</code> files generated.</p> Source code in <code>fedbiomed/common/certificate_manager.py</code> <pre><code>def generate_certificate(\n    root,\n    component_id,\n    prefix: Optional[str] = None,\n    subject: Optional[Dict[str, str]] = None,\n) -&gt; Tuple[str, str]:\n    \"\"\"Generates certificates\n\n    Args:\n        component_id: ID of the component for which the certificate will be generated\n\n    Returns:\n        key_file: The path where private key file is saved\n        pem_file: The path where public key file is saved\n\n    Raises:\n        FedbiomedEnvironError: If certificate directory for the component has already\n            `certificate.pem` or `certificate.key` files generated.\n    \"\"\"\n\n    certificate_path = os.path.join(root, CERTS_FOLDER_NAME)\n\n    if os.path.isdir(certificate_path) and (\n        os.path.isfile(os.path.join(certificate_path, \"certificate.key\"))\n        or os.path.isfile(os.path.join(certificate_path, \"certificate.pem\"))\n    ):\n        raise ValueError(\n            f\"Certificate generation is aborted. Directory {certificate_path} has already \"\n            f\"certificates. Please remove those files to regenerate\"\n        )\n\n    os.makedirs(certificate_path, exist_ok=True)\n\n    key_file, pem_file = CertificateManager.generate_self_signed_ssl_certificate(\n        certificate_folder=certificate_path,\n        certificate_name=prefix if prefix else \"\",\n        component_id=component_id,\n        subject=subject,\n    )\n\n    return key_file, pem_file\n</code></pre>"},{"location":"developer/api/common/cli/","title":"CLI","text":"<p>Common CLI Modules</p> <p>This module includes common CLI methods and parser extension</p>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli-attributes","title":"Attributes","text":""},{"location":"developer/api/common/cli/#fedbiomed.common.cli.BOLD","title":"BOLD  <code>module-attribute</code>","text":"<pre><code>BOLD = '\\x1b[1m'\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.GRN","title":"GRN  <code>module-attribute</code>","text":"<pre><code>GRN = '\\x1b[1;32m'\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.NC","title":"NC  <code>module-attribute</code>","text":"<pre><code>NC = '\\x1b[0m'\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.RED","title":"RED  <code>module-attribute</code>","text":"<pre><code>RED = '\\x1b[1;31m'\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.YLW","title":"YLW  <code>module-attribute</code>","text":"<pre><code>YLW = '\\x1b[1;33m'\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli-classes","title":"Classes","text":""},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CLIArgumentParser","title":"CLIArgumentParser","text":"<pre><code>CLIArgumentParser(subparser, parser=None)\n</code></pre> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def __init__(self, subparser: argparse.ArgumentParser, parser = None):\n\n    self._subparser = subparser\n    # Parser that is going to be add using subparser\n    self._parser = None\n\n    self._main_parser = parser\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CLIArgumentParser-functions","title":"Functions","text":""},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CLIArgumentParser.default","title":"default","text":"<pre><code>default(args=None)\n</code></pre> <p>Default function for subparser command</p> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def default(self, args: argparse.Namespace = None) -&gt; None:\n    \"\"\"Default function for subparser command\"\"\"\n\n    self._parser.print_help()\n\n    return None\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI","title":"CommonCLI","text":"<pre><code>CommonCLI()\n</code></pre> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._parser: argparse.ArgumentParser = argparse.ArgumentParser(\n        prog=\"fedbiomed\", formatter_class=argparse.RawTextHelpFormatter\n    )\n\n    self._subparsers = self._parser.add_subparsers()\n    self._certificate_manager: CertificateManager = CertificateManager()\n    self._description: str = \"\"\n    self._args = None\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI-attributes","title":"Attributes","text":""},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI.arguments","title":"arguments  <code>property</code>","text":"<pre><code>arguments\n</code></pre> <p>Gets global parser arguments</p> <p>Returns:</p> Type Description <code>Namespace</code> <p>Parser arguments</p>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI.description","title":"description  <code>property</code> <code>writable</code>","text":"<pre><code>description\n</code></pre> <p>Gets description of CLI</p> <p>Returns:</p> Type Description <code>str</code> <p>Description (Intro) for the CLI</p>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI.parser","title":"parser  <code>property</code>","text":"<pre><code>parser\n</code></pre> <p>Gets parser for CLI</p> <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>Main argument parser object</p>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI.subparsers","title":"subparsers  <code>property</code>","text":"<pre><code>subparsers\n</code></pre> <p>Gets subparsers of common cli</p> <p>Returns:</p> Type Description <p>Subparsers of CLI parser</p>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI-functions","title":"Functions","text":""},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI.config_action","title":"config_action  <code>staticmethod</code>","text":"<pre><code>config_action(this, component)\n</code></pre> <p>Returns CLI argument action for config file name</p> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>@staticmethod\ndef config_action(this: \"CommonCLI\", component: ComponentType):\n    \"\"\"Returns CLI argument action for config file name\"\"\"\n    return ComponentDirectoryAction\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI.error","title":"error  <code>staticmethod</code>","text":"<pre><code>error(message)\n</code></pre> <p>Prints given error message</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message</p> required Source code in <code>fedbiomed/common/cli.py</code> <pre><code>@staticmethod\ndef error(message: str) -&gt; None:\n    \"\"\"Prints given error message\n\n    Args:\n        message: Error message\n    \"\"\"\n    print(f\"{RED}ERROR:{NC}\")\n    print(f\"{BOLD}{message}{NC}\")\n    logger.critical(message)\n    sys.exit(1)\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initializes parser classes and common parser for child classes.</p> <p>This parser classes will be added by child classes.</p> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def initialize(self):\n    \"\"\"Initializes parser classes and common parser for child classes.\n\n    This parser classes will be added by child classes.\n    \"\"\"\n\n    self._parser.add_argument(\n        \"-y\",\n        action=\"store_true\"\n    )\n\n    for arg_parser in self._arg_parsers_classes:\n        p = arg_parser(self._subparsers, self._parser)\n        p.initialize()\n        self._arg_parsers.update({arg_parser.__name__: p})\n\n    self.initialize_certificate_parser()\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI.initialize_certificate_parser","title":"initialize_certificate_parser","text":"<pre><code>initialize_certificate_parser()\n</code></pre> <p>Common arguments</p> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def initialize_certificate_parser(self):\n    \"\"\"Common arguments\"\"\"\n\n    # Add certificate sub parser (sub-command)\n    certificate_parser = self._subparsers.add_parser(\n        \"certificate\",\n        help=\"Command to manage certificates in node and researcher components. \"\n        \"Please see 'certificate --help' for more information.\",\n        prog=\"fedbiomed [ node | researcher ] [--path [COMPONENT_DIRECTORY]] certificate\",\n    )\n\n    def print_help(args):\n        certificate_parser.print_help()\n\n    certificate_parser.set_defaults(func=print_help)\n\n    # Create sub parser under `certificate` command\n    certificate_sub_parsers = certificate_parser.add_subparsers(\n        description=\"Commands that can be used with the option `certificate`\",\n        title=\"Subcommands\",\n    )\n\n    register_parser = certificate_sub_parsers.add_parser(\n        \"register\",\n        help=\"Register certificate of specified party. Please run 'fedbiomed' \"\n            \"[COMPONENT SPECIFICATION] certificate register --help'\",\n    )  # command register\n\n    list_parser = certificate_sub_parsers.add_parser(\n        \"list\", help=\"Lists registered certificates\"\n    )  # command list\n    delete_parser = certificate_sub_parsers.add_parser(\n        \"delete\", help=\"Deletes specified certificate from database\"\n    )  # command delete\n\n    # Command `certificate generate`\n    generate = certificate_sub_parsers.add_parser(\n        \"generate\",\n        help=\"Generates certificate for given component/party if files don't exist yet. \"\n        \"Uses an alternate directory if '--path DIRECTORY' is given.\"\n        \" If files already exist, overwrite existing certificate.\\n\"\n        \"Certificate are here refering to the public certificate and its associated private key \"\n        \"(the latter should remain secret and not shared to other parties).\"\n    )\n\n    # Command `certificate generate`\n    prepare = certificate_sub_parsers.add_parser(\n        \"registration-instructions\",\n        help=\"Prepares certificate of current component to send other FL participant\"\n             \" through trusted channel.\",\n    )\n\n    register_parser.set_defaults(func=self._register_certificate)\n    list_parser.set_defaults(func=self._list_certificates)\n    delete_parser.set_defaults(func=self._delete_certificate)\n    generate.set_defaults(func=self._generate_certificate)\n    prepare.set_defaults(func=self._prepare_certificate_for_registration)\n\n    # Add arguments\n    register_parser.add_argument(\n        \"-pk\",\n        \"--public-key\",\n        metavar=\"PUBLIC_KEY\",\n        type=str,\n        nargs=\"?\",\n        required=True,\n        help=\"Certificate/key that will be registered\",\n    )\n\n    register_parser.add_argument(\n        \"-pi\",\n        \"--party-id\",\n        metavar=\"PUBLIC_ID\",\n        type=str,\n        nargs=\"?\",\n        required=True,\n        help=\"ID of the party to which the certificate is to be registered (component ID).\",\n    )\n\n    register_parser.add_argument(\n        \"--upsert\",\n        action=\"store_true\",\n        help=\"Updates if certificate of given party id is already existing.\",\n    )\n\n    generate.add_argument(\n        \"--path\",\n        type=str,\n        nargs=\"?\",\n        required=False,\n        help=\"The path to the RESEARCHER|NODE component, in which certificate will be saved.\"\n        \" By default it will overwrite existing certificate.\",\n    )\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI.initialize_magic_dev_environment_parsers","title":"initialize_magic_dev_environment_parsers","text":"<pre><code>initialize_magic_dev_environment_parsers()\n</code></pre> <p>Initializes argument parser for the option to create development environment.</p> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def initialize_magic_dev_environment_parsers(self) -&gt; None:\n    \"\"\"Initializes argument parser for the option to create development environment.\"\"\"\n    magic = self._subparsers.add_parser(\n        \"certificate-dev-setup\",\n        description=\"Prepares development environment by registering certificates \"\n                    \"of each component created in a single clone of Fed-BioMed. Parses \"\n                    \"configuration files ends with '.ini' that are created in 'etc' \"\n                    \"directory. This setup requires to have one 'researcher' and \"\n                    \"at least 2 nodes.\",\n        help=\"Prepares development environment by registering certificates of each \"\n             \"component created in a single clone of Fed-BioMed.\",\n    )\n    magic.set_defaults(func=self._create_magic_dev_environment)\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI.initialize_optional","title":"initialize_optional","text":"<pre><code>initialize_optional()\n</code></pre> <p>Initializes optional subparser</p> <p>Optional subparsers are not going to be visible for the CLI that are inherited from CommonCLI class as long as <code>intialize_optional</code> method is not executed.</p> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def initialize_optional(self):\n    \"\"\"Initializes optional subparser\n\n    Optional subparsers are not going to be visible for the CLI that are\n    inherited from CommonCLI class as long as `intialize_optional` method\n    is not executed.\n    \"\"\"\n\n    self.initialize_magic_dev_environment_parsers()\n    self.initialize_version()\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI.initialize_version","title":"initialize_version","text":"<pre><code>initialize_version()\n</code></pre> <p>Initializes argument parser for common options.</p> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def initialize_version(self):\n    \"\"\"Initializes argument parser for common options.\"\"\"\n    self._parser.add_argument(\n        \"--version\",\n        \"-v\",\n        action='version',\n        version=str(__version__),\n        help=\"Print software version\",\n    )\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI.parse_args","title":"parse_args","text":"<pre><code>parse_args(args_=None)\n</code></pre> <p>Parse arguments after adding the arguments</p> <p>Attention</p> <pre><code>Please make sure this method is called after all necessary arguments are set\n</code></pre> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def parse_args(self, args_=None):\n    \"\"\"Parse arguments after adding the arguments\n\n    !!! warning \"Attention\"\n            Please make sure this method is called after all necessary arguments are set\n    \"\"\"\n    args, unknown_args = self._parser.parse_known_args(args_)\n    if hasattr(args, \"func\"):\n        specs = get_method_spec(args.func)\n        if specs:\n            # If default function has 2 arguments\n            if len(specs) &gt; 1:\n                return args.func(args, unknown_args)\n\n            # Run parser_args to raise error for unrecognized arguments\n            if unknown_args:\n                args = self._parser.parse_args(args_)\n            args.func(args)\n        else:\n            # Raise for unrecognized arguments\n            if unknown_args:\n                self._parser.parse_args(args_)\n            args.func()\n    else:\n        self._parser.print_help()\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.CommonCLI.success","title":"success  <code>staticmethod</code>","text":"<pre><code>success(message)\n</code></pre> <p>Prints given message with success tag</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Message to print as successful operation</p> required Source code in <code>fedbiomed/common/cli.py</code> <pre><code>@staticmethod\ndef success(message: str) -&gt; None:\n    \"\"\"Prints given message with success tag\n\n    Args:\n        message: Message to print as successful operation\n    \"\"\"\n    print(f\"{GRN}Operation successful! {NC}\")\n    print(f\"{BOLD}{message}{NC}\")\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.ComponentDirectoryAction","title":"ComponentDirectoryAction","text":"<pre><code>ComponentDirectoryAction(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Action</code></p> <p>Action for the argument config</p> <p>This action class gets the config file name and set config object before executing any command.</p> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n    # Sets config by default if option string for config is not present.\n    # The default is defined by the argument parser.\n    if (\n        not set(self.option_strings).intersection(set(sys.argv)) and\n        not set([\"--help\", \"-h\"]).intersection(set(sys.argv)) and\n        len(sys.argv) &gt; 2\n    ):\n        self._create_config(self.default)\n\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli.ComponentDirectoryAction-functions","title":"Functions","text":""},{"location":"developer/api/common/cli/#fedbiomed.common.cli.ComponentDirectoryAction.set_component","title":"set_component  <code>abstractmethod</code>","text":"<pre><code>set_component(component_dir)\n</code></pre> <p>Implements configuration import</p> <p>Parameters:</p> Name Type Description Default <code>component_dir</code> <code>str</code> <p>Name of the config file for the component</p> required Source code in <code>fedbiomed/common/cli.py</code> <pre><code>@abstractmethod\ndef set_component(self, component_dir: str) -&gt; None:\n    \"\"\"Implements configuration import\n\n    Args:\n        component_dir: Name of the config file for the component\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/cli/#fedbiomed.common.cli-functions","title":"Functions","text":""},{"location":"developer/api/common/config/","title":"Config","text":""},{"location":"developer/api/common/config/#fedbiomed.common.config-attributes","title":"Attributes","text":""},{"location":"developer/api/common/config/#fedbiomed.common.config-classes","title":"Classes","text":""},{"location":"developer/api/common/config/#fedbiomed.common.config.Component","title":"Component","text":"<pre><code>Component()\n</code></pre> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def __init__(self):\n    \"\"\"Test\"\"\"\n    self._reference = '.fedbiomed'\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Component-attributes","title":"Attributes","text":""},{"location":"developer/api/common/config/#fedbiomed.common.config.Component.config_cls","title":"config_cls  <code>instance-attribute</code>","text":"<pre><code>config_cls\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Component-functions","title":"Functions","text":""},{"location":"developer/api/common/config/#fedbiomed.common.config.Component.initiate","title":"initiate","text":"<pre><code>initiate(root=None)\n</code></pre> <p>Creates or initiates existing component</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def initiate(self, root: Optional[str] = None) -&gt; Union[\"NodeConfig\", \"ResearcherConfig\"] :\n    \"\"\"Creates or initiates existing component\"\"\"\n\n    if not root:\n        root = os.path.join(os.getcwd(), self._default_component_name)\n\n    reference = self.validate(root)\n    config = self.config_cls(root)\n\n    if not os.path.isfile(reference):\n        create_fedbiomed_setup_folders(root)\n        with open(os.path.join(root, '.fedbiomed'), 'w', encoding='UTF-8') as file_:\n            file_.write(self.config_cls.COMPONENT_TYPE)\n        config.generate()\n        config.write()\n    else:\n        config.read()\n\n    return config\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Component.is_component_existing","title":"is_component_existing","text":"<pre><code>is_component_existing(component_dir)\n</code></pre> <p>Checks if component existing in the given root directory</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if any component is instantiated in the given directory</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def is_component_existing(self, component_dir: str) -&gt; bool:\n    \"\"\"Checks if component existing in the given root directory\n\n    Returns:\n        True if any component is instantiated in the given directory\n    \"\"\"\n    ref = os.path.join(component_dir, self._reference)\n    if os.path.isdir(component_dir):\n        if docker_special_case(component_dir):\n            return False\n\n        if os.listdir(component_dir) and not os.path.isfile(ref):\n            raise ValueError(\n                f\"Cannot create component. Path {component_dir} \"\n                \"is not empty for Fed-BioMed component initialization. Please \"\n                f\"remove folder {component_dir} or specify another path\"\n            )\n\n    # Special case for docker container mounted folders\n    # empty .fedbiomed is required to keep it\n    if os.path.isfile(ref) and not read_file(ref):\n        return False\n\n    return os.path.isfile(ref)\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Component.validate","title":"validate","text":"<pre><code>validate(root)\n</code></pre> <p>Validates given root folder is a component can be instantiated</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <p>Root directory that Fed-BioMed component will be instantiated.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Full path to reference file</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def validate(self, root) -&gt; str:\n    \"\"\"Validates given root folder is a component can be instantiated\n\n    Args:\n        root: Root directory that Fed-BioMed component will be instantiated.\n\n    Returns:\n        Full path to reference file\n    \"\"\"\n\n    iscomp = self.is_component_existing(root)\n    ref = os.path.join(root, self._reference)\n\n    if iscomp:\n        comp_type = read_file(ref)\n        if comp_type != self.config_cls.COMPONENT_TYPE:\n            raise ValueError(\n                f'Component directory has already been initilazed for component type {comp_type}'\n                ' can not overwrite or reuse it for component type '\n                f'{self.config_cls.COMPONENT_TYPE}')\n\n    return ref\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config","title":"Config","text":"<pre><code>Config(root)\n</code></pre> <p>Base Config class</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>str</code> <p>Root directory of the component.</p> <code>name</code> <code>str</code> <p>Config name (e.g config.ini or config-n1.ini).</p> <code>path</code> <code>str</code> <p>Absolute path to configuration.</p> <code>vars</code> <code>Dict[str, Any]</code> <p>A dictionary that contains configuration related variables. Such as dynamic paths that relies of component root etc.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory for the component</p> required Source code in <code>fedbiomed/common/config.py</code> <pre><code>def __init__(\n    self, root: str\n) -&gt; None:\n    \"\"\"Initializes configuration\n\n    Args:\n        root: Root directory for the component\n    \"\"\"\n    self._cfg = configparser.ConfigParser()\n    self.load(root)\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config-attributes","title":"Attributes","text":""},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.root","title":"root  <code>instance-attribute</code>","text":"<pre><code>root\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.vars","title":"vars  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>vars = {}\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config-functions","title":"Functions","text":""},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.COMPONENT_TYPE","title":"COMPONENT_TYPE  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>COMPONENT_TYPE()\n</code></pre> <p>Abstract attribute to oblige defining component type</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>@classmethod\n@abstractmethod\ndef COMPONENT_TYPE(cls):  # pylint: disable=C0103\n    \"\"\"Abstract attribute to oblige defining component type\"\"\"\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.add_parameters","title":"add_parameters  <code>abstractmethod</code>","text":"<pre><code>add_parameters()\n</code></pre> <p>\"Component specific argument creation</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>@abstractmethod\ndef add_parameters(self):\n    \"\"\" \"Component specific argument creation\"\"\"\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.generate","title":"generate","text":"<pre><code>generate(id=None)\n</code></pre> <p>\"Generate configuration file</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <p>Overwrites existing configration file</p> required <code>id</code> <code>Optional[str]</code> <p>Component ID</p> <code>None</code> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def generate(\n    self,\n    id: Optional[str] = None\n) -&gt; None:\n    \"\"\" \"Generate configuration file\n\n    Args:\n        force: Overwrites existing configration file\n        id: Component ID\n    \"\"\"\n\n    # Check if configuration is already existing\n    if not self.is_config_existing():\n        # Create default section\n        component_id = id if id else f\"{self.COMPONENT_TYPE}_{uuid.uuid4()}\"\n\n        self._cfg[\"default\"] = {\n            \"id\": component_id,\n            \"component\": self.COMPONENT_TYPE,\n            \"version\": str(self._CONFIG_VERSION),\n        }\n\n        db_path = os.path.join(\n            self.root, VAR_FOLDER_NAME, f\"{DB_PREFIX}{component_id}.json\"\n        )\n        self._cfg[\"default\"][\"db\"] = os.path.relpath(\n            db_path, os.path.join(self.root, CONFIG_FOLDER_NAME)\n        )\n\n        # Calls child class add_parameterss\n        self.add_parameters()\n    else:\n        self.read()\n\n    self._update_vars()\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.get","title":"get","text":"<pre><code>get(section, key, **kwargs)\n</code></pre> <p>Returns value for given key and section</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def get(self, section, key, **kwargs) -&gt; str:\n    \"\"\"Returns value for given key and section\"\"\"\n\n    return self._get(section, key, **kwargs)\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.getbool","title":"getbool","text":"<pre><code>getbool(section, key, **kwargs)\n</code></pre> <p>Gets boolean value from config</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def getbool(self, section, key, **kwargs) -&gt; bool:\n    \"\"\"Gets boolean value from config\"\"\"\n\n    return self._get(section, key, **kwargs).lower() in ('true', '1')\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.is_config_existing","title":"is_config_existing","text":"<pre><code>is_config_existing()\n</code></pre> <p>Checks if config file exists</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if config file is already existing</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def is_config_existing(self) -&gt; bool:\n    \"\"\"Checks if config file exists\n\n    Returns:\n        True if config file is already existing\n    \"\"\"\n\n    return os.path.isfile(self.config_path)\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.load","title":"load","text":"<pre><code>load(root)\n</code></pre> <p>Load configuration from given name and root</p> <p>This implementation allows to load configuration after Config class is instantiated.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where component files will be saved configuration file.</p> required Source code in <code>fedbiomed/common/config.py</code> <pre><code>def load(\n    self,\n    root: str,\n) -&gt; None:\n    \"\"\"Load configuration from given name and root\n\n    This implementation allows to load configuration after Config class\n    is instantiated.\n\n    Args:\n        root: Root directory where component files will be saved\n            configuration file.\n    \"\"\"\n\n    self.root = root\n    self.config_path = os.path.join(self.root, 'etc', self._CONFIG_FILE_NAME)\n    self.generate()\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.read","title":"read","text":"<pre><code>read()\n</code></pre> <p>Reads configuration file that is already existing in given path</p> <p>Raises version compatibility error</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def read(self) -&gt; bool:\n    \"\"\"Reads configuration file that is already existing in given path\n\n    Raises version compatibility error\n    \"\"\"\n    self._cfg.read(self.config_path)\n\n    # Validate config version\n    raise_for_version_compatibility(\n        self._cfg[\"default\"][\"version\"],\n        self._CONFIG_VERSION,\n        f\"Configuration file {self.config_path}: found version %s expected version %s\",\n    )\n\n    return True\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.sections","title":"sections","text":"<pre><code>sections()\n</code></pre> <p>Returns sections of the config</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def sections(self) -&gt; list:\n    \"\"\"Returns sections of the config\"\"\"\n\n    return self._cfg.sections()\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.set","title":"set","text":"<pre><code>set(section, key, value)\n</code></pre> <p>Sets config section values</p> <p>Parameters:</p> Name Type Description Default <code>section</code> <p>the name of the config file section as defined by the <code>ini</code> standard</p> required <code>key</code> <p>the name of the attribute to be set</p> required <code>value</code> <p>the value of the attribute to be set</p> required <p>Returns:</p> Name Type Description <code>value</code> <code>None</code> <p>the value of the attribute that was just set</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def set(self, section, key, value) -&gt; None:\n    \"\"\"Sets config section values\n\n    Args:\n        section: the name of the config file section as defined by the `ini` standard\n        key: the name of the attribute to be set\n        value: the value of the attribute to be set\n\n    Returns:\n        value: the value of the attribute that was just set\n    \"\"\"\n    self._cfg.set(section, key, value)\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config.Config.write","title":"write","text":"<pre><code>write()\n</code></pre> <p>Writes config file</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def write(self):\n    \"\"\"Writes config file\"\"\"\n\n    try:\n        with open(self.config_path, \"w\", encoding=\"UTF-8\") as f:\n            self._cfg.write(f)\n    except configparser.Error as exp:\n        raise FedbiomedConfigurationError(\n            f\"{ErrorNumbers.FB600.value}: cannot save config file:  {self.path}\"\n        ) from exp\n</code></pre>"},{"location":"developer/api/common/config/#fedbiomed.common.config-functions","title":"Functions","text":""},{"location":"developer/api/common/config/#fedbiomed.common.config.docker_special_case","title":"docker_special_case","text":"<pre><code>docker_special_case(component_path)\n</code></pre> <p>Special case for docker containers.</p> <p>This function makes sure that there is only .gitkeep file present in the directory that component will be initialized. It is required since component folder should be existing in run_mounts by default.</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def docker_special_case(component_path: str) -&gt; bool:\n    \"\"\"Special case for docker containers.\n\n    This function makes sure that there is only .gitkeep file present in\n    the directory that component will be initialized. It is required since\n    component folder should be existing in run_mounts by default.\n    \"\"\"\n\n    files = os.listdir(component_path)\n\n    return \".gitkeep\" in files and len(files) == 1\n</code></pre>"},{"location":"developer/api/common/constants/","title":"Constants","text":"<p>Fed-BioMed constants/enums</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.CACHE_FOLDER_NAME","title":"CACHE_FOLDER_NAME  <code>module-attribute</code>","text":"<pre><code>CACHE_FOLDER_NAME = 'cache'\n</code></pre> <p>Directory/folder name where cache files are saved</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.CERTS_FOLDER_NAME","title":"CERTS_FOLDER_NAME  <code>module-attribute</code>","text":"<pre><code>CERTS_FOLDER_NAME = join(CONFIG_FOLDER_NAME, 'certs')\n</code></pre> <p>FOLDER name for Certs directory</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.CONFIG_FOLDER_NAME","title":"CONFIG_FOLDER_NAME  <code>module-attribute</code>","text":"<pre><code>CONFIG_FOLDER_NAME = 'etc'\n</code></pre> <p>Directory/folder name where configurations are saved</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DB_FOLDER_NAME","title":"DB_FOLDER_NAME  <code>module-attribute</code>","text":"<pre><code>DB_FOLDER_NAME = VAR_FOLDER_NAME\n</code></pre> <p>Directory/folder name where DB files are saved</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DB_PREFIX","title":"DB_PREFIX  <code>module-attribute</code>","text":"<pre><code>DB_PREFIX = 'db_'\n</code></pre> <p>Prefix for database files name</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DEFAULT_CERT_NAME","title":"DEFAULT_CERT_NAME  <code>module-attribute</code>","text":"<pre><code>DEFAULT_CERT_NAME = 'FBM_certificate'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DEFAULT_NODE_NAME","title":"DEFAULT_NODE_NAME  <code>module-attribute</code>","text":"<pre><code>DEFAULT_NODE_NAME = 'fbm-node'\n</code></pre> <p>Default node component folder name</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DEFAULT_RESEARCHER_NAME","title":"DEFAULT_RESEARCHER_NAME  <code>module-attribute</code>","text":"<pre><code>DEFAULT_RESEARCHER_NAME = 'fbm-researcher'\n</code></pre> <p>Default researcher component folder name</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DOCS_FOLDER_NAME","title":"DOCS_FOLDER_NAME  <code>module-attribute</code>","text":"<pre><code>DOCS_FOLDER_NAME = 'docs'\n</code></pre> <p>Directory/folder name used by default for documentation</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.EXPERIMENT_PREFIX","title":"EXPERIMENT_PREFIX  <code>module-attribute</code>","text":"<pre><code>EXPERIMENT_PREFIX = 'exper_'\n</code></pre> <p>Prefix for experiment ID</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.MAX_MESSAGE_BYTES_LENGTH","title":"MAX_MESSAGE_BYTES_LENGTH  <code>module-attribute</code>","text":"<pre><code>MAX_MESSAGE_BYTES_LENGTH = 4000000 - getsizeof(bytes('', encoding='UTF-8'))\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.MAX_RETRIEVE_ERROR_RETRIES","title":"MAX_RETRIEVE_ERROR_RETRIES  <code>module-attribute</code>","text":"<pre><code>MAX_RETRIEVE_ERROR_RETRIES = 5\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.MAX_SEND_RETRIES","title":"MAX_SEND_RETRIES  <code>module-attribute</code>","text":"<pre><code>MAX_SEND_RETRIES = 5\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.NODE_DATA_FOLDER","title":"NODE_DATA_FOLDER  <code>module-attribute</code>","text":"<pre><code>NODE_DATA_FOLDER = 'data'\n</code></pre> <p>Directory/folder name used by Nodes to save their specific dataset</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.NODE_PREFIX","title":"NODE_PREFIX  <code>module-attribute</code>","text":"<pre><code>NODE_PREFIX = 'node_'\n</code></pre> <p>Prefix for node ID</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.NODE_STATE_PREFIX","title":"NODE_STATE_PREFIX  <code>module-attribute</code>","text":"<pre><code>NODE_STATE_PREFIX = 'node_state_'\n</code></pre> <p>Prefix for Node state ID</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.NOTEBOOKS_FOLDER_NAME","title":"NOTEBOOKS_FOLDER_NAME  <code>module-attribute</code>","text":"<pre><code>NOTEBOOKS_FOLDER_NAME = 'notebooks'\n</code></pre> <p>Directory/folder name used by default for notebooks</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.REQUEST_PREFIX","title":"REQUEST_PREFIX  <code>module-attribute</code>","text":"<pre><code>REQUEST_PREFIX = 'request_'\n</code></pre> <p>Prefix for request ID</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SERVER_certificate_prefix","title":"SERVER_certificate_prefix  <code>module-attribute</code>","text":"<pre><code>SERVER_certificate_prefix = 'server_certificate'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TENSORBOARD_FOLDER_NAME","title":"TENSORBOARD_FOLDER_NAME  <code>module-attribute</code>","text":"<pre><code>TENSORBOARD_FOLDER_NAME = 'runs'\n</code></pre> <p>Directory/folder name where tensorboard logs are saved</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TIMEOUT_NODE_TO_NODE_REQUEST","title":"TIMEOUT_NODE_TO_NODE_REQUEST  <code>module-attribute</code>","text":"<pre><code>TIMEOUT_NODE_TO_NODE_REQUEST = 30\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TMP_FOLDER_NAME","title":"TMP_FOLDER_NAME  <code>module-attribute</code>","text":"<pre><code>TMP_FOLDER_NAME = 'tmp'\n</code></pre> <p>Directory/folder name where temporary files are saved</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TRACEBACK_LIMIT","title":"TRACEBACK_LIMIT  <code>module-attribute</code>","text":"<pre><code>TRACEBACK_LIMIT = 20\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TUTORIALS_FOLDER_NAME","title":"TUTORIALS_FOLDER_NAME  <code>module-attribute</code>","text":"<pre><code>TUTORIALS_FOLDER_NAME = 'tutorials'\n</code></pre> <p>Directory/folder name used by default for tutorials</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.VAR_FOLDER_NAME","title":"VAR_FOLDER_NAME  <code>module-attribute</code>","text":"<pre><code>VAR_FOLDER_NAME = 'var'\n</code></pre> <p>Directory/folder name where variable files are saved</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants-classes","title":"Classes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ComponentType","title":"ComponentType","text":"<p>               Bases: <code>_BaseEnum</code></p> <p>Enumeration class, used to characterize the type of component of the fedbiomed architecture</p> <p>Attributes:</p> Name Type Description <code>RESEARCHER</code> <code>int</code> <p>Researcher component</p> <code>NODE</code> <code>int</code> <p>Node component</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ComponentType-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ComponentType.NODE","title":"NODE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NODE = 2\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ComponentType.RESEARCHER","title":"RESEARCHER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RESEARCHER = 1\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DataLoadingBlockTypes","title":"DataLoadingBlockTypes","text":"<pre><code>DataLoadingBlockTypes(*args)\n</code></pre> <p>               Bases: <code>_BaseEnum</code></p> <p>Base class for typing purposes.</p> <p>Concrete enumeration types should be defined within the scope of their implementation or application. To define a concrete enumeration type, one must subclass this class as follows: <pre><code>class MyLoadingBlockTypes(DataLoadingBlockTypes, Enum):\n    MY_KEY: str 'myKey'\n    MY_OTHER_KEY: str 'myOtherKey'\n</code></pre></p> <p>Subclasses must respect the following conditions: - All fields must be str; - All field values must be unique.</p> <p>Warning</p> <p>This class must always be empty as it is not allowed to contain any fields!</p> Source code in <code>fedbiomed/common/constants.py</code> <pre><code>def __init__(self, *args):\n    cls = self.__class__\n    if not isinstance(self.value, str):\n        raise ValueError(\"all fields of DataLoadingBlockTypes subclasses\"\n                         \" must be of str type\")\n    if any(self.value == e.value for e in cls):\n        a = self.name\n        e = cls(self.value).name\n        raise ValueError(\n            f\"duplicate values not allowed in DataLoadingBlockTypes and \"\n            f\"its subclasses: {a} --&gt; {e}\")\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes","title":"DatasetTypes","text":"<p>               Bases: <code>_BaseEnum</code></p> <p>Types of Datasets implemented in Fed-BioMed</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = 'default'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.FLAMBY","title":"FLAMBY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FLAMBY = 'flamby'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.IMAGES","title":"IMAGES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGES = 'images'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.MEDICAL_FOLDER","title":"MEDICAL_FOLDER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEDICAL_FOLDER = 'medical-folder'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.MEDNIST","title":"MEDNIST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEDNIST = 'mednist'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = 'none'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.TABULAR","title":"TABULAR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TABULAR = 'csv'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.TEST","title":"TEST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TEST = 'test'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers","title":"ErrorNumbers","text":"<p>               Bases: <code>_BaseEnum</code></p> <p>List of all error messages types</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB100","title":"FB100  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB100 = 'FB100: undetermined messaging server error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB300","title":"FB300  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB300 = 'FB300: undetermined node error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB301","title":"FB301  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB301 = 'FB301: Protocol error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB302","title":"FB302  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB302 = 'FB302: TrainingPlan class does not load'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB303","title":"FB303  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB303 = 'FB303: TrainingPlan class does not contain expected methods'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB304","title":"FB304  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB304 = 'FB304: TrainingPlan method crashes'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB309","title":"FB309  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB309 = 'FB309: bad model params (.mpk)'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB310","title":"FB310  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB310 = 'FB310: bad data format'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB312","title":"FB312  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB312 = 'FB312: Node stopped in SIGTERM signal handler'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB313","title":"FB313  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB313 = 'FB313: no dataset matching request'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB314","title":"FB314  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB314 = 'FB314: Node round error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB315","title":"FB315  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB315 = 'FB315: Error while loading the data '\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB316","title":"FB316  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB316 = 'FB316: Data loading plan error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB317","title":"FB317  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB317 = 'FB317: FLamby package import error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB318","title":"FB318  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB318 = 'FB318: Secure aggregation setup error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB319","title":"FB319  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB319 = 'FB319: Command not found error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB320","title":"FB320  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB320 = 'FB320: bad model type'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB321","title":"FB321  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB321 = 'FB321: Secure aggregation delete error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB322","title":"FB322  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB322 = 'FB322: Dataset registration error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB323","title":"FB323  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB323 = 'FB323: Node State error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB324","title":"FB324  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB324 = 'FB324: Node to node overlay communication error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB400","title":"FB400  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB400 = 'FB400: undetermined application error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB401","title":"FB401  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB401 = 'FB401: aggregation crashes or returns an error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB402","title":"FB402  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB402 = 'FB402: strategy method crashes or sends an error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB407","title":"FB407  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB407 = 'FB407: list of nodes became empty when training (all nodes failed training or did not answer)'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB408","title":"FB408  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB408 = 'FB408: training failed on node or node did not answer during training'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB409","title":"FB409  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB409 = 'FB409: node sent Status=Error during training'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB410","title":"FB410  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB410 = 'FB410: bad type or value for experiment argument'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB411","title":"FB411  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB411 = 'FB411: cannot train an experiment that is not fully defined'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB412","title":"FB412  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB412 = 'FB412: cannot do model checking for experiment'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB413","title":"FB413  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB413 = 'FB413: cannot save or load breakpoint for experiment'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB414","title":"FB414  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB414 = 'FB414: bad type or value for training arguments'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB415","title":"FB415  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB415 = 'FB415: secure aggregation handling error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB416","title":"FB416  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB416 = 'FB416: federated dataset error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB417","title":"FB417  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB417 = 'FB417: secure aggregation error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB419","title":"FB419  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB419 = 'FB419: node state agent error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB600","title":"FB600  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB600 = 'FB600: configuration error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB601","title":"FB601  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB601 = 'FB601: message error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB603","title":"FB603  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB603 = 'FB603: task queue error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB605","title":"FB605  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB605 = 'FB605: training plan error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB606","title":"FB606  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB606 = 'FB606: model manager error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB607","title":"FB607  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB607 = 'FB607: data manager error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB608","title":"FB608  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB608 = 'FB608: torch data manager error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB609","title":"FB609  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB609 = 'FB609: scikit-learn data manager error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB610","title":"FB610  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB610 = 'FB610: Torch based tabular dataset creation error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB611","title":"FB611  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB611 = 'FB611: Error while trying to evaluate using the specified metric'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB612","title":"FB612  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB612 = 'FB612: Torch based NIFTI dataset error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB613","title":"FB613  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB613 = 'FB613: Medical Folder dataset error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB614","title":"FB614  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB614 = 'FB614: data loading block error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB615","title":"FB615  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB615 = 'FB615: data loading plan error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB616","title":"FB616  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB616 = 'FB616: differential privacy controller error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB617","title":"FB617  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB617 = 'FB617: FLamby dataset error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB618","title":"FB618  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB618 = 'FB618: FLamby data transformation error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB619","title":"FB619  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB619 = 'FB619: Certificate error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB620","title":"FB620  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB620 = 'FB620: MPC protocol error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB621","title":"FB621  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB621 = 'FB621: declearn optimizer error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB622","title":"FB622  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB622 = 'FB622: Model error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB623","title":"FB623  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB623 = 'FB623: Secure aggregation database error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB624","title":"FB624  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB624 = 'FB624: Secure aggregation crypter error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB625","title":"FB625  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB625 = 'FB625: Component version error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB626","title":"FB626  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB626 = 'FB626: Fed-BioMed optimizer error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB627","title":"FB627  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB627 = 'FB627: Utility function error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB628","title":"FB628  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB628 = 'FB628: Communication error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB629","title":"FB629  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB629 = 'FB629: Diffie-Hellman KA error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB630","title":"FB630  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB630 = 'FB630: Additive Secret Sharing error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB631","title":"FB631  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB631 = 'FB631: Node to node channels database error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB999","title":"FB999  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB999 = 'FB999: unknown error code sent by the node'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms","title":"HashingAlgorithms","text":"<p>               Bases: <code>_BaseEnum</code></p> <p>Enumeration class, used to characterize the hashing algorithms</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.BLAKE2B","title":"BLAKE2B  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BLAKE2B = 'BLAKE2B'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.BLAKE2S","title":"BLAKE2S  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BLAKE2S = 'BLAKE2S'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.SHA256","title":"SHA256  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA256 = 'SHA256'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.SHA384","title":"SHA384  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA384 = 'SHA384'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.SHA3_256","title":"SHA3_256  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA3_256 = 'SHA3_256'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.SHA3_384","title":"SHA3_384  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA3_384 = 'SHA3_384'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.SHA3_512","title":"SHA3_512  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA3_512 = 'SHA3_512'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.SHA512","title":"SHA512  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA512 = 'SHA512'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.MessageType","title":"MessageType","text":"<p>               Bases: <code>_BaseEnum</code></p> <p>Types of messages received by researcher</p> <p>Attributes:</p> Name Type Description <code>REPLY</code> <p>reply messages (TrainReply, SearchReply, etc.)</p> <code>LOG</code> <p>'log' message (LogMessage)</p> <code>SCALAR</code> <p>'add_scalar' message (Scalar)</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.MessageType-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.MessageType.LOG","title":"LOG  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LOG = 'LOG'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.MessageType.REPLY","title":"REPLY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REPLY = 'REPLY'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.MessageType.SCALAR","title":"SCALAR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SCALAR = 'SCALAR'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.MessageType-functions","title":"Functions","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.MessageType.convert","title":"convert  <code>classmethod</code>","text":"<pre><code>convert(type_)\n</code></pre> <p>Converts given text message to to MessageType instance</p> Source code in <code>fedbiomed/common/constants.py</code> <pre><code>@classmethod\ndef convert(cls, type_):\n    \"\"\"Converts given text message to to MessageType instance\"\"\"\n    try:\n        return getattr(cls, type_.upper())\n    except AttributeError as exp:\n        raise FedbiomedError(f\"There is no MessageType as {type_}\") from exp\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ProcessTypes","title":"ProcessTypes","text":"<p>               Bases: <code>_BaseEnum</code></p> <p>Enumeration class for Preprocess types</p> <p>Attributes:</p> Name Type Description <code>DATA_LOADER</code> <p>Preprocess for DataLoader</p> <code>PARAMS</code> <p>Preprocess for model parameters</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ProcessTypes-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ProcessTypes.DATA_LOADER","title":"DATA_LOADER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DATA_LOADER = 0\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ProcessTypes.PARAMS","title":"PARAMS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PARAMS = 1\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SAParameters","title":"SAParameters","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SAParameters-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SAParameters.CLIPPING_RANGE","title":"CLIPPING_RANGE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CLIPPING_RANGE = 3\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SAParameters.KEY_SIZE","title":"KEY_SIZE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KEY_SIZE = 2048\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SAParameters.TARGET_RANGE","title":"TARGET_RANGE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TARGET_RANGE = 2 ** 13\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SAParameters.WEIGHT_RANGE","title":"WEIGHT_RANGE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WEIGHT_RANGE = 2 ** 17\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecaggElementTypes","title":"SecaggElementTypes","text":"<p>               Bases: <code>_BaseEnum</code></p> <p>Enumeration class for secure aggregation element types</p> <p>Attributes:</p> Name Type Description <code>SERVER_KEY</code> <code>int</code> <p>server key split between the parties</p> <code>DIFFIE_HELLMAN</code> <code>int</code> <p>one pair of DH key for each node party, public key shared with other node parties</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecaggElementTypes-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecaggElementTypes.DIFFIE_HELLMAN","title":"DIFFIE_HELLMAN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DIFFIE_HELLMAN = 1\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecaggElementTypes.SERVER_KEY","title":"SERVER_KEY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SERVER_KEY = 0\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecaggElementTypes-functions","title":"Functions","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecaggElementTypes.get_element_from_value","title":"get_element_from_value  <code>staticmethod</code>","text":"<pre><code>get_element_from_value(element_value)\n</code></pre> Source code in <code>fedbiomed/common/constants.py</code> <pre><code>@staticmethod\ndef get_element_from_value(element_value: int):\n    for element in SecaggElementTypes:\n        if element.value == element_value:\n            return element\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecureAggregationSchemes","title":"SecureAggregationSchemes","text":"<p>               Bases: <code>_BaseEnum</code></p> <p>Enumeration class for secure aggregation schemes</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecureAggregationSchemes-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecureAggregationSchemes.JOYE_LIBERT","title":"JOYE_LIBERT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>JOYE_LIBERT = 1\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecureAggregationSchemes.LOM","title":"LOM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LOM = 2\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecureAggregationSchemes.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = 0\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanApprovalStatus","title":"TrainingPlanApprovalStatus","text":"<p>               Bases: <code>_BaseEnum</code></p> <p>Enumeration class for training plan approval status of a training plan on a node when training plan approval is active.</p> <p>Attributes:</p> Name Type Description <code>APPROVED</code> <p>training plan was accepted for this node, can be executed now</p> <code>REJECTED</code> <p>training plan was disapproved for this node, cannot be executed</p> <code>PENDING</code> <p>training plan is waiting for review and approval, cannot be executed yet</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanApprovalStatus-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanApprovalStatus.APPROVED","title":"APPROVED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>APPROVED = 'Approved'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanApprovalStatus.PENDING","title":"PENDING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PENDING = 'Pending'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanApprovalStatus.REJECTED","title":"REJECTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REJECTED = 'Rejected'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanApprovalStatus-functions","title":"Functions","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanApprovalStatus.str2enum","title":"str2enum","text":"<pre><code>str2enum(name)\n</code></pre> Source code in <code>fedbiomed/common/constants.py</code> <pre><code>def str2enum(name: str):\n    for e in TrainingPlanApprovalStatus:\n        if e.value == name:\n            return e\n    return None\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanStatus","title":"TrainingPlanStatus","text":"<p>               Bases: <code>_BaseEnum</code></p> <p>Constant values for training plan type that will be saved into db</p> <p>Attributes:</p> Name Type Description <code>REQUESTED</code> <p>means training plan submitted in-application by the researcher</p> <code>REGISTERED</code> <p>means training plan added by a hospital/node</p> <code>DEFAULT</code> <p>means training plan is default training plan provided by Fed-BioMed</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanStatus-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanStatus.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = 'default'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanStatus.REGISTERED","title":"REGISTERED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REGISTERED = 'registered'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanStatus.REQUESTED","title":"REQUESTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REQUESTED = 'requested'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlans","title":"TrainingPlans","text":"<p>               Bases: <code>_BaseEnum</code></p> <p>Enumeration class for Training plans</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlans-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlans.SkLearnTrainingPlan","title":"SkLearnTrainingPlan  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SkLearnTrainingPlan = 'SkLearnTrainingPlan'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlans.TorchTrainingPlan","title":"TorchTrainingPlan  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TorchTrainingPlan = 'TorchTrainingPlan'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRequestStatus","title":"UserRequestStatus","text":"<p>               Bases: <code>str</code>, <code>_BaseEnum</code></p> <p>Enumeration class, used to characterize the status for user registration requests</p> <p>Attributes:</p> Name Type Description <code>NEW</code> <p>New user registration</p> <code>REJECTED</code> <p>Rejected status</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRequestStatus-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRequestStatus.NEW","title":"NEW  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NEW = 'NEW'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRequestStatus.REJECTED","title":"REJECTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REJECTED = 'REJECTED'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRoleType","title":"UserRoleType","text":"<p>               Bases: <code>int</code>, <code>_BaseEnum</code></p> <p>Enumeration class, used to characterize the type of component of the fedbiomed architecture</p> <p>Attributes:</p> Name Type Description <code>ADMIN</code> <p>User with Admin role</p> <code>USER</code> <p>Simple user</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRoleType-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRoleType.ADMIN","title":"ADMIN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ADMIN = 1\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRoleType.USER","title":"USER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>USER = 2\n</code></pre>"},{"location":"developer/api/common/data/","title":"Data","text":"<p>Classes that simplify imports from fedbiomed.common.data</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data-classes","title":"Classes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingBlock","title":"DataLoadingBlock","text":"<pre><code>DataLoadingBlock()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>The building blocks of a DataLoadingPlan.</p> <p>A DataLoadingBlock describes an intermediary layer between the researcher and the node's filesystem. It allows the node to specify a customization in the way data is \"perceived\" by the data loaders during training.</p> <p>A DataLoadingBlock is identified by its type_id attribute. Thus, this attribute should be unique among all DataLoadingBlockTypes in the same DataLoadingPlan. Moreover, we may test equality between a DataLoadingBlock and a string by checking its type_id, as a means of easily testing whether a DataLoadingBlock is contained in a collection.</p> <p>Correct usage of this class requires creating ad-hoc subclasses. The DataLoadingBlock class is not intended to be instantiated directly.</p> <p>Subclasses of DataLoadingBlock must respect the following conditions:</p> <ol> <li>implement a default constructor</li> <li>the implemented constructor must call <code>super().__init__()</code></li> <li>extend the serialize(self) and the deserialize(self, load_from: dict) functions</li> <li>both serialize and deserialize must call super's serialize and deserialize respectively</li> <li>the deserialize function must always return self</li> <li>the serialize function must update the dict returned by super's serialize</li> <li>implement an apply function that takes arbitrary arguments and applies         the logic of the loading_block</li> <li>update the _validation_scheme to define rules for all new fields returned by the serialize function</li> </ol> <p>Attributes:</p> Name Type Description <code>__serialization_id</code> <p>(str) identifies one serialized instance of the DataLoadingBlock</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def __init__(self):\n    self.__serialization_id = 'serialized_dlb_' + str(uuid.uuid4())\n    self._serialization_validator = SerializationValidation()\n    self._serialization_validator.update_validation_scheme(SerializationValidation.dlb_default_scheme())\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingBlock-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingBlock.apply","title":"apply  <code>abstractmethod</code>","text":"<pre><code>apply(*args, **kwargs)\n</code></pre> <p>Abstract method representing an application of the DataLoadingBlock</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>@abstractmethod\ndef apply(self, *args, **kwargs):\n    \"\"\"Abstract method representing an application of the DataLoadingBlock\n    \"\"\"\n    pass\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingBlock.deserialize","title":"deserialize","text":"<pre><code>deserialize(load_from)\n</code></pre> <p>Reconstruct the DataLoadingBlock from a serialized version.</p> <p>Parameters:</p> Name Type Description Default <code>load_from</code> <code>dict</code> <p>a dictionary as obtained by the serialize function.</p> required <p>Returns:     the self instance</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def deserialize(self, load_from: dict) -&gt; TDataLoadingBlock:\n    \"\"\"Reconstruct the DataLoadingBlock from a serialized version.\n\n    Args:\n        load_from (dict): a dictionary as obtained by the serialize function.\n    Returns:\n        the self instance\n    \"\"\"\n    self._serialization_validator.validate(load_from, FedbiomedLoadingBlockValueError)\n    self.__serialization_id = load_from['dlb_id']\n    return self\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingBlock.get_serialization_id","title":"get_serialization_id","text":"<pre><code>get_serialization_id()\n</code></pre> <p>Expose serialization id as read-only</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def get_serialization_id(self):\n    \"\"\"Expose serialization id as read-only\"\"\"\n    return self.__serialization_id\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingBlock.instantiate_class","title":"instantiate_class  <code>staticmethod</code>","text":"<pre><code>instantiate_class(loading_block)\n</code></pre> <p>Instantiate one DataLoadingBlock object of the type defined in the arguments.</p> <p>Uses the <code>loading_block_module</code> and <code>loading_block_class</code> fields of the loading_block argument to identify the type of DataLoadingBlock to be instantiated, then calls its default constructor. Note that this function does not call deserialize.</p> <p>Parameters:</p> Name Type Description Default <code>loading_block</code> <code>dict</code> <p>DataLoadingBlock metadata in the format returned by the serialize function.</p> required <p>Returns:     A default-constructed instance of a         DataLoadingBlock         of the type defined in the metadata. Raises:    FedbiomedLoadingBlockError: if the instantiation process raised any exception.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>@staticmethod\ndef instantiate_class(loading_block: dict) -&gt; TDataLoadingBlock:\n    \"\"\"Instantiate one [DataLoadingBlock][fedbiomed.common.data._data_loading_plan.DataLoadingBlock]\n    object of the type defined in the arguments.\n\n    Uses the `loading_block_module` and `loading_block_class` fields of the loading_block argument to\n    identify the type of [DataLoadingBlock][fedbiomed.common.data._data_loading_plan.DataLoadingBlock]\n    to be instantiated, then calls its default constructor.\n    Note that this function **does not call deserialize**.\n\n    Args:\n        loading_block (dict): [DataLoadingBlock][fedbiomed.common.data._data_loading_plan.DataLoadingBlock]\n            metadata in the format returned by the serialize function.\n    Returns:\n        A default-constructed instance of a\n            [DataLoadingBlock][fedbiomed.common.data._data_loading_plan.DataLoadingBlock]\n            of the type defined in the metadata.\n    Raises:\n       FedbiomedLoadingBlockError: if the instantiation process raised any exception.\n    \"\"\"\n    try:\n        dlb_module = import_module(loading_block['loading_block_module'])\n        dlb = eval(f\"dlb_module.{loading_block['loading_block_class']}()\")\n    except Exception as e:\n        msg = f\"{ErrorNumbers.FB614.value}: could not instantiate DataLoadingBlock from the following metadata: \" +\\\n              f\"{loading_block} because of {type(e).__name__}: {e}\"\n        logger.debug(msg)\n        raise FedbiomedLoadingBlockError(msg)\n    return dlb\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingBlock.instantiate_key","title":"instantiate_key  <code>staticmethod</code>","text":"<pre><code>instantiate_key(key_module, key_classname, loading_block_key_str)\n</code></pre> <p>Imports and loads DataLoadingBlockTypes regarding the passed arguments</p> <p>Parameters:</p> Name Type Description Default <code>key_module</code> <code>str</code> <p>description</p> required <code>key_classname</code> <code>str</code> <p>description</p> required <code>loading_block_key_str</code> <code>str</code> <p>description</p> required <p>Raises:</p> Type Description <code>FedbiomedDataLoadingPlanError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>DataLoadingBlockTypes</code> <code>DataLoadingBlockTypes</code> <p>description</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>@staticmethod\ndef instantiate_key(key_module: str, key_classname: str, loading_block_key_str: str) -&gt; DataLoadingBlockTypes:\n    \"\"\"Imports and loads [DataLoadingBlockTypes][fedbiomed.common.constants.DataLoadingBlockTypes]\n    regarding the passed arguments\n\n    Args:\n        key_module (str): _description_\n        key_classname (str): _description_\n        loading_block_key_str (str): _description_\n\n    Raises:\n        FedbiomedDataLoadingPlanError: _description_\n\n    Returns:\n        DataLoadingBlockTypes: _description_\n    \"\"\"\n    try:\n        keys = import_module(key_module)\n        loading_block_key = eval(f\"keys.{key_classname}('{loading_block_key_str}')\")\n    except Exception as e:\n        msg = f\"{ErrorNumbers.FB615.value} Error deserializing loading block key \" + \\\n              f\"{loading_block_key_str} with path {key_module}.{key_classname} \" + \\\n              f\"because of {type(e).__name__}: {e}\"\n        logger.debug(msg)\n        raise FedbiomedDataLoadingPlanError(msg)\n    return loading_block_key\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingBlock.serialize","title":"serialize","text":"<pre><code>serialize()\n</code></pre> <p>Serializes the class in a format similar to json.</p> <p>Returns:</p> Type Description <code>dict</code> <p>a dictionary of key-value pairs sufficient for reconstructing</p> <code>dict</code> <p>the DataLoadingBlock.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def serialize(self) -&gt; dict:\n    \"\"\"Serializes the class in a format similar to json.\n\n    Returns:\n        a dictionary of key-value pairs sufficient for reconstructing\n        the DataLoadingBlock.\n    \"\"\"\n    return dict(\n        loading_block_class=self.__class__.__qualname__,\n        loading_block_module=self.__module__,\n        dlb_id=self.__serialization_id\n    )\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlan","title":"DataLoadingPlan","text":"<pre><code>DataLoadingPlan(*args, **kwargs)\n</code></pre> <p>               Bases: <code>Dict[DataLoadingBlockTypes, DataLoadingBlock]</code></p> <p>Customizations to the way the data is loaded and presented for training.</p> <p>A DataLoadingPlan is a dictionary of {name: DataLoadingBlock} pairs. Each DataLoadingBlock represents a customization to the way data is loaded and presented to the researcher. These customizations are defined by the node, but they operate on a Dataset class, which is defined by the library and instantiated by the researcher.</p> <p>To exploit this functionality, a Dataset must be modified to accept the customizations provided by the DataLoadingPlan. To simplify this process, we provide the DataLoadingPlanMixin class below.</p> <p>The DataLoadingPlan class should be instantiated directly, no subclassing is needed. The DataLoadingPlan is a dict, and exposes the same interface as a dict.</p> <p>Attributes:</p> Name Type Description <code>dlp_id</code> <p>str representing a unique plan id (auto-generated)</p> <code>desc</code> <p>str representing an optional user-friendly short description</p> <code>target_dataset_type</code> <p>a DatasetTypes enum representing the type of dataset targeted by this DataLoadingPlan</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    super(DataLoadingPlan, self).__init__(*args, **kwargs)\n    self.dlp_id = 'dlp_' + str(uuid.uuid4())\n    self.desc = \"\"\n    self.target_dataset_type = DatasetTypes.NONE\n    self._serialization_validation = SerializationValidation()\n    self._serialization_validation.update_validation_scheme(SerializationValidation.dlp_default_scheme())\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlan-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlan.desc","title":"desc  <code>instance-attribute</code>","text":"<pre><code>desc = ''\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlan.dlp_id","title":"dlp_id  <code>instance-attribute</code>","text":"<pre><code>dlp_id = 'dlp_' + str(uuid4())\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlan.target_dataset_type","title":"target_dataset_type  <code>instance-attribute</code>","text":"<pre><code>target_dataset_type = NONE\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlan-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlan.deserialize","title":"deserialize","text":"<pre><code>deserialize(serialized_dlp, serialized_loading_blocks)\n</code></pre> <p>Reconstruct the DataLoadingPlan][fedbiomed.common.data._data_loading_plan.DataLoadingPlan] from a serialized version.</p> <p>Calling this function will clear the contained [DataLoadingBlockTypes].</p> <p>This function may not be used to \"update\" nor to \"append to\" a DataLoadingPlan.</p> <p>Parameters:</p> Name Type Description Default <code>serialized_dlp</code> <code>dict</code> <p>a dictionary of data loading plan metadata, as obtained from the first output of the serialize function</p> required <code>serialized_loading_blocks</code> <code>List[dict]</code> <p>a list of dictionaries of loading_block metadata, as obtained from the second output of the serialize function</p> required <p>Returns:     the self instance</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def deserialize(self, serialized_dlp: dict, serialized_loading_blocks: List[dict]) -&gt; TDataLoadingPlan:\n    \"\"\"Reconstruct the DataLoadingPlan][fedbiomed.common.data._data_loading_plan.DataLoadingPlan] from a serialized version.\n\n    !!! warning \"Calling this function will *clear* the contained [DataLoadingBlockTypes].\"\n        This function may not be used to \"update\" nor to \"append to\"\n        a [DataLoadingPlan][fedbiomed.common.data._data_loading_plan.DataLoadingPlan].\n\n    Args:\n        serialized_dlp: a dictionary of data loading plan metadata, as obtained from the first output of the\n            serialize function\n        serialized_loading_blocks: a list of dictionaries of loading_block metadata, as obtained from the\n            second output of the serialize function\n    Returns:\n        the self instance\n    \"\"\"\n    self._serialization_validation.validate(serialized_dlp, FedbiomedDataLoadingPlanValueError)\n\n    self.clear()\n    self.dlp_id = serialized_dlp['dlp_id']\n    self.desc = serialized_dlp['dlp_name']\n    self.target_dataset_type = DatasetTypes(serialized_dlp['target_dataset_type'])\n    for loading_block_key_str, dlb_id in serialized_dlp['loading_blocks'].items():\n        key_module, key_classname = serialized_dlp['key_paths'][loading_block_key_str]\n        loading_block_key = DataLoadingBlock.instantiate_key(key_module, key_classname, loading_block_key_str)\n        loading_block = next(filter(lambda x: x['dlb_id'] == dlb_id,\n                                    serialized_loading_blocks))\n        dlb = DataLoadingBlock.instantiate_class(loading_block)\n        self[loading_block_key] = dlb.deserialize(loading_block)\n    return self\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlan.infer_dataset_type","title":"infer_dataset_type  <code>staticmethod</code>","text":"<pre><code>infer_dataset_type(dataset)\n</code></pre> <p>Infer the type of a given dataset.</p> <p>This function provides the mapping between a dataset's class and the DatasetTypes enum. If the dataset exposes the correct interface (i.e. the get_dataset_type method) then it directly calls that, otherwise it tries to apply some heuristics to guess the type of dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Any</code> <p>the dataset whose type we want to infer.</p> required <p>Returns:     a DatasetTypes enum element which identifies the type of the dataset. Raises:     FedbiomedDataLoadingPlanValueError: if the dataset does not have a <code>get_dataset_type</code> method and moreover         the type could not be guessed.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>@staticmethod\ndef infer_dataset_type(dataset: Any) -&gt; DatasetTypes:\n    \"\"\"Infer the type of a given dataset.\n\n    This function provides the mapping between a dataset's class and the DatasetTypes enum. If the dataset exposes\n    the correct interface (i.e. the get_dataset_type method) then it directly calls that, otherwise it tries to\n    apply some heuristics to guess the type of dataset.\n\n    Args:\n        dataset: the dataset whose type we want to infer.\n    Returns:\n        a DatasetTypes enum element which identifies the type of the dataset.\n    Raises:\n        FedbiomedDataLoadingPlanValueError: if the dataset does not have a `get_dataset_type` method and moreover\n            the type could not be guessed.\n    \"\"\"\n    if hasattr(dataset, 'get_dataset_type'):\n        return dataset.get_dataset_type()\n    elif dataset.__class__.__name__ == 'ImageFolder':\n        # ImageFolder could be both an images type or mednist. Try to identify mednist with some heuristic.\n        if hasattr(dataset, 'classes') and \\\n                all([x in dataset.classes for x in ['AbdomenCT', 'BreastMRI', 'CXR', 'ChestCT', 'Hand', 'HeadCT']]):\n            return DatasetTypes.MEDNIST\n        else:\n            return DatasetTypes.IMAGES\n    elif dataset.__class__.__name__ == 'MNIST':\n        return DatasetTypes.DEFAULT\n    msg = f\"{ErrorNumbers.FB615.value} Trying to infer dataset type of {dataset} is not supported \" + \\\n        f\"for datasets of type {dataset.__class__.__qualname__}\"\n    logger.debug(msg)\n    raise FedbiomedDataLoadingPlanValueError(msg)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlan.serialize","title":"serialize","text":"<pre><code>serialize()\n</code></pre> <p>Serializes the class in a format similar to json.</p> <p>Returns:</p> Type Description <code>Tuple[dict, List]</code> <p>a tuple sufficient for reconstructing the DataLoading plan. It includes: - a dictionary of key-value pairs with the DataLoadingPlan parameters. - a list of dict containing the data for reconstruction all the DataLoadingBlock     of the DataLoadingPlan</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def serialize(self) -&gt; Tuple[dict, List]:\n    \"\"\"Serializes the class in a format similar to json.\n\n    Returns:\n        a tuple sufficient for reconstructing the DataLoading plan. It includes:\n            - a dictionary of key-value pairs with the\n            [DataLoadingPlan][fedbiomed.common.data._data_loading_plan.DataLoadingPlan] parameters.\n            - a list of dict containing the data for reconstruction all the DataLoadingBlock\n                of the [DataLoadingPlan][fedbiomed.common.data._data_loading_plan.DataLoadingPlan] \n    \"\"\"\n    return dict(\n        dlp_id=self.dlp_id,\n        dlp_name=self.desc,\n        target_dataset_type=self.target_dataset_type.value,\n        loading_blocks={key.value: dlb.get_serialization_id() for key, dlb in self.items()},\n        key_paths={key.value: (f\"{key.__module__}\", f\"{key.__class__.__qualname__}\") for key in self.keys()}\n    ), [dlb.serialize() for dlb in self.values()]\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlanMixin","title":"DataLoadingPlanMixin","text":"<pre><code>DataLoadingPlanMixin()\n</code></pre> <p>Utility class to enable DLP functionality in a dataset.</p> <p>Any Dataset class that inherits from [DataLoadingPlanMixin] will have the basic tools necessary to support a DataLoadingPlan. Typically, the logic of each specific DataLoadingBlock in the DataLoadingPlan will be implemented in the form of hooks that are called within the Dataset's implementation using the helper function apply_dlb defined below.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def __init__(self):\n    self._dlp = None\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlanMixin-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlanMixin.apply_dlb","title":"apply_dlb","text":"<pre><code>apply_dlb(default_ret_value, dlb_key, *args, **kwargs)\n</code></pre> <p>Apply one DataLoadingBlock identified by its key.</p> <p>Note that we want to easily support the case where the DataLoadingPlan is not activated, or the requested loading block is not contained in the DataLoadingPlan. This is achieved by providing a default return value to be returned when the above conditions are met. Hence, most of the calls to apply_dlb will look like this: <pre><code>value = self.apply_dlb(value, 'my-loading-block', my_apply_args)\n</code></pre> This will ensure that value is not changed if the DataLoadingPlan is not active.</p> <p>Parameters:</p> Name Type Description Default <code>default_ret_value</code> <code>Any</code> <p>the value to be returned in case that the dlp functionality is not required</p> required <code>dlb_key</code> <code>DataLoadingBlockTypes</code> <p>the key of the DataLoadingBlock to be applied</p> required <code>*args</code> <code>Optional[Any]</code> <p>forwarded to the DataLoadingBlock's apply function</p> <code>()</code> <code>**kwargs</code> <code>Optional[Any]</code> <p>forwarded to the DataLoadingBlock's apply function</p> <code>{}</code> <p>Returns:     the output of the DataLoadingBlock's apply function, or         the default_ret_value when dlp is None or it does not contain         the requested loading block</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def apply_dlb(self, default_ret_value: Any, dlb_key: DataLoadingBlockTypes,\n              *args: Optional[Any], **kwargs: Optional[Any]) -&gt; Any:\n    \"\"\"Apply one DataLoadingBlock identified by its key.\n\n    Note that we want to easily support the case where the DataLoadingPlan\n    is not activated, or the requested loading block is not contained in the\n    DataLoadingPlan. This is achieved by providing a default return value\n    to be returned when the above conditions are met. Hence, most of the\n    calls to apply_dlb will look like this:\n    ```\n    value = self.apply_dlb(value, 'my-loading-block', my_apply_args)\n    ```\n    This will ensure that value is not changed if the DataLoadingPlan is\n    not active.\n\n    Args:\n        default_ret_value: the value to be returned in case that the dlp\n            functionality is not required\n        dlb_key: the key of the DataLoadingBlock to be applied\n        *args: forwarded to the DataLoadingBlock's apply function\n        **kwargs: forwarded to the DataLoadingBlock's apply function\n    Returns:\n        the output of the DataLoadingBlock's apply function, or\n            the default_ret_value when dlp is None or it does not contain\n            the requested loading block\n    \"\"\"\n    if not isinstance(dlb_key, DataLoadingBlockTypes):\n        raise FedbiomedDataLoadingPlanValueError(f\"Key {dlb_key} is not of enum type DataLoadingBlockTypes\"\n                                                 f\" in DataLoadingPlanMixin.apply_dlb\")\n    if self._dlp is not None and dlb_key in self._dlp:\n        return self._dlp[dlb_key].apply(*args, **kwargs)\n    else:\n        return default_ret_value\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlanMixin.clear_dlp","title":"clear_dlp","text":"<pre><code>clear_dlp()\n</code></pre> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def clear_dlp(self):\n    self._dlp = None\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlanMixin.set_dlp","title":"set_dlp","text":"<pre><code>set_dlp(dlp)\n</code></pre> <p>Sets the dlp if the target dataset type is appropriate</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def set_dlp(self, dlp: DataLoadingPlan):\n    \"\"\"Sets the dlp if the target dataset type is appropriate\"\"\"\n    if not isinstance(dlp, DataLoadingPlan):\n        msg = f\"{ErrorNumbers.FB615.value} Trying to set a DataLoadingPlan but the argument is of type \" + \\\n              f\"{type(dlp).__name__}\"\n        logger.debug(msg)\n        raise FedbiomedDataLoadingPlanValueError(msg)\n\n    dataset_type = DataLoadingPlan.infer_dataset_type(self)  # `self` here will refer to the Dataset instance\n    if dlp.target_dataset_type != DatasetTypes.NONE and dataset_type != dlp.target_dataset_type:\n        raise FedbiomedDataLoadingPlanValueError(f\"Trying to set {dlp} on dataset of type {dataset_type.value} but \"\n                                                 f\"the target type is {dlp.target_dataset_type}\")\n    elif dlp.target_dataset_type == DatasetTypes.NONE:\n        dlp.target_dataset_type = dataset_type\n    self._dlp = dlp\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataManager","title":"DataManager","text":"<pre><code>DataManager(dataset, target=None, **kwargs)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Factory class that build different data loader/datasets based on the type of <code>dataset</code>. The argument <code>dataset</code> should be provided as <code>torch.utils.data.Dataset</code> object for to be used in PyTorch training.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[ndarray, DataFrame, Series, Dataset]</code> <p>Dataset object. It can be an instance, PyTorch Dataset or Tuple.</p> required <code>target</code> <code>Union[ndarray, DataFrame, Series]</code> <p>Target variable or variables.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional parameters that are going to be used for data loader</p> <code>{}</code> Source code in <code>fedbiomed/common/data/_data_manager.py</code> <pre><code>def __init__(self,\n             dataset: Union[np.ndarray, pd.DataFrame, pd.Series, Dataset],\n             target: Union[np.ndarray, pd.DataFrame, pd.Series] = None,\n             **kwargs: dict) -&gt; None:\n\n    \"\"\"Constructor of DataManager,\n\n    Args:\n        dataset: Dataset object. It can be an instance, PyTorch Dataset or Tuple.\n        target: Target variable or variables.\n        **kwargs: Additional parameters that are going to be used for data loader\n    \"\"\"\n\n    # TODO: Improve datamanager for auto loading by given dataset_path and other information\n    # such as inputs variable indexes and target variables indexes\n\n    self._dataset = dataset\n    self._target = target\n    self._loader_arguments: Dict = kwargs\n    self._data_manager_instance = None\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataManager-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.DataManager.extend_loader_args","title":"extend_loader_args","text":"<pre><code>extend_loader_args(extension)\n</code></pre> <p>Extends the class' loader arguments</p> <p>Extends the class's <code>_loader_arguments</code> attribute with additional key-values from the <code>extension</code> argument. If a key already exists in the <code>_loader_arguments</code>, then it is not replaced.</p> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>Optional[Dict]</code> <p>the mapping used to extend the loader arguments</p> required Source code in <code>fedbiomed/common/data/_data_manager.py</code> <pre><code>def extend_loader_args(self, extension: Optional[Dict]):\n    \"\"\"Extends the class' loader arguments\n\n    Extends the class's `_loader_arguments` attribute with additional key-values from\n    the `extension` argument. If a key already exists in the `_loader_arguments`, then\n    it is not replaced.\n\n    Args:\n        extension: the mapping used to extend the loader arguments\n    \"\"\"\n    if extension:\n        self._loader_arguments.update(\n            {key: value for key, value in extension.items() if key not in self._loader_arguments}\n        )\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataManager.load","title":"load","text":"<pre><code>load(tp_type)\n</code></pre> <p>Loads proper DataManager based on given TrainingPlan and <code>dataset</code>, <code>target</code> attributes.</p> <p>Parameters:</p> Name Type Description Default <code>tp_type</code> <code>TrainingPlans</code> <p>Enumeration instance of TrainingPlans that stands for type of training plan.</p> required <p>Raises:</p> Type Description <code>FedbiomedDataManagerError</code> <p>If requested DataManager does not match with given arguments.</p> Source code in <code>fedbiomed/common/data/_data_manager.py</code> <pre><code>def load(self, tp_type: TrainingPlans):\n    \"\"\"Loads proper DataManager based on given TrainingPlan and\n    `dataset`, `target` attributes.\n\n    Args:\n        tp_type: Enumeration instance of TrainingPlans that stands for type of training plan.\n\n    Raises:\n        FedbiomedDataManagerError: If requested DataManager does not match with given arguments.\n\n    \"\"\"\n\n    # Training plan is type of TorcTrainingPlan\n    if tp_type == TrainingPlans.TorchTrainingPlan:\n        if self._target is None and isinstance(self._dataset, Dataset):\n            # Create Dataset for pytorch\n            self._data_manager_instance = TorchDataManager(dataset=self._dataset, **self._loader_arguments)\n        elif isinstance(self._dataset, (pd.DataFrame, pd.Series, np.ndarray)) and \\\n                isinstance(self._target, (pd.DataFrame, pd.Series, np.ndarray)):\n            # If `dataset` and `target` attributes are array-like object\n            # create TabularDataset object to instantiate a TorchDataManager\n            torch_dataset = TabularDataset(inputs=self._dataset, target=self._target)\n            self._data_manager_instance = TorchDataManager(dataset=torch_dataset, **self._loader_arguments)\n        else:\n            raise FedbiomedDataManagerError(f\"{ErrorNumbers.FB607.value}: Invalid arguments for torch based \"\n                                            f\"training plan, either provide the argument  `dataset` as PyTorch \"\n                                            f\"Dataset instance, or provide `dataset` and `target` arguments as \"\n                                            f\"an instance one of pd.DataFrame, pd.Series or np.ndarray \")\n\n    elif tp_type == TrainingPlans.SkLearnTrainingPlan:\n        # Try to convert `torch.utils.Data.Dataset` to SkLearnBased dataset/datamanager\n        if self._target is None and isinstance(self._dataset, Dataset):\n            torch_data_manager = TorchDataManager(dataset=self._dataset)\n            try:\n                self._data_manager_instance = torch_data_manager.to_sklearn()\n            except Exception as e:\n                raise FedbiomedDataManagerError(f\"{ErrorNumbers.FB607.value}: PyTorch based `Dataset` object \"\n                                                \"has been instantiated with DataManager. An error occurred while\"\n                                                \"trying to convert torch.utils.data.Dataset to numpy based \"\n                                                f\"dataset: {str(e)}\")\n\n        # For scikit-learn based training plans, the arguments `dataset` and `target` should be an instance\n        # one of `pd.DataFrame`, `pd.Series`, `np.ndarray`\n        elif isinstance(self._dataset, (pd.DataFrame, pd.Series, np.ndarray)) and \\\n                isinstance(self._target, (pd.DataFrame, pd.Series, np.ndarray)):\n            # Create Dataset for SkLearn training plans\n            self._data_manager_instance = SkLearnDataManager(inputs=self._dataset, target=self._target,\n                                                             **self._loader_arguments)\n        else:\n            raise FedbiomedDataManagerError(f\"{ErrorNumbers.FB607.value}: The argument `dataset` and `target` \"\n                                            f\"should be instance of pd.DataFrame, pd.Series or np.ndarray \")\n    else:\n        raise FedbiomedDataManagerError(f\"{ErrorNumbers.FB607.value}: Undefined training plan\")\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MapperBlock","title":"MapperBlock","text":"<pre><code>MapperBlock()\n</code></pre> <p>               Bases: <code>DataLoadingBlock</code></p> <p>A DataLoadingBlock for mapping values.</p> <p>This DataLoadingBlock can be used whenever an \"indirect mapping\" is needed. For example, it can be used to implement a correspondence between a set of \"logical\" abstract names and a set of folder names on the filesystem.</p> <p>The apply function of this DataLoadingBlock takes a \"key\" as input (a str) and returns the mapped value corresponding to map[key]. Note that while the constructor of this class sets a value for type_id, developers are recommended to set a more meaningful value that better speaks to their application.</p> <p>Multiple instances of this loading_block may be used in the same DataLoadingPlan, provided that they are given different type_id via the constructor.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def __init__(self):\n    super(MapperBlock, self).__init__()\n    self.map = {}\n    self._serialization_validator.update_validation_scheme(MapperBlock._extra_validation_scheme())\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MapperBlock-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.MapperBlock.map","title":"map  <code>instance-attribute</code>","text":"<pre><code>map = {}\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MapperBlock-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.MapperBlock.apply","title":"apply","text":"<pre><code>apply(key)\n</code></pre> <p>Returns the value mapped to the key, if it exists.</p> <p>Raises:</p> Type Description <code>FedbiomedLoadingBlockError</code> <p>if map is not a dict or the key does not exist.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def apply(self, key):\n    \"\"\"Returns the value mapped to the key, if it exists.\n\n    Raises:\n        FedbiomedLoadingBlockError: if map is not a dict or the key does not exist.\n    \"\"\"\n    if not isinstance(self.map, dict) or key not in self.map:\n        msg = f\"{ErrorNumbers.FB614.value} Mapper block error: no key '{key}' in mapping dictionary\"\n        logger.debug(msg)\n        raise FedbiomedLoadingBlockError(msg)\n    return self.map[key]\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MapperBlock.deserialize","title":"deserialize","text":"<pre><code>deserialize(load_from)\n</code></pre> <p>Reconstruct the DataLoadingBlock from a serialized version.</p> <p>Parameters:</p> Name Type Description Default <code>load_from</code> <code>dict</code> <p>a dictionary as obtained by the serialize function.</p> required <p>Returns:     the self instance</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def deserialize(self, load_from: dict) -&gt; DataLoadingBlock:\n    \"\"\"Reconstruct the [DataLoadingBlock][fedbiomed.common.data._data_loading_plan.DataLoadingBlock]\n    from a serialized version.\n\n    Args:\n        load_from (dict): a dictionary as obtained by the serialize function.\n    Returns:\n        the self instance\n    \"\"\"\n    super(MapperBlock, self).deserialize(load_from)\n    self.map = load_from['map']\n    return self\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MapperBlock.serialize","title":"serialize","text":"<pre><code>serialize()\n</code></pre> <p>Serializes the class in a format similar to json.</p> <p>Returns:</p> Type Description <code>dict</code> <p>a dictionary of key-value pairs sufficient for reconstructing</p> <code>dict</code> <p>the DataLoadingBlock.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def serialize(self) -&gt; dict:\n    \"\"\"Serializes the class in a format similar to json.\n\n    Returns:\n        a dictionary of key-value pairs sufficient for reconstructing\n        the [DataLoadingBlock][fedbiomed.common.data._data_loading_plan.DataLoadingBlock].\n    \"\"\"\n    ret = super(MapperBlock, self).serialize()\n    ret.update({'map': self.map})\n    return ret\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase","title":"MedicalFolderBase","text":"<pre><code>MedicalFolderBase(root=None)\n</code></pre> <p>               Bases: <code>DataLoadingPlanMixin</code></p> <p>Controller class for Medical Folder dataset.</p> <p>Contains methods to validate the MedicalFolder folder hierarchy and extract folder-base metadata information such as modalities, number of subject etc.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Union[str, Path, None]</code> <p>path to Medical Folder root folder.</p> <code>None</code> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def __init__(self, root: Union[str, Path, None] = None):\n    \"\"\"Constructs MedicalFolderBase\n\n    Args:\n        root: path to Medical Folder root folder.\n    \"\"\"\n    super(MedicalFolderBase, self).__init__()\n\n    if root is not None:\n        root = self.validate_MedicalFolder_root_folder(root)\n\n    self._root = root\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase.default_modality_names","title":"default_modality_names  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_modality_names = ['T1', 'T2', 'label']\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase.root","title":"root  <code>property</code> <code>writable</code>","text":"<pre><code>root\n</code></pre> <p>Root property of MedicalFolderController</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase.available_subjects","title":"available_subjects","text":"<pre><code>available_subjects(subjects_from_index, subjects_from_folder=None)\n</code></pre> <p>Checks missing subject folders and missing entries in demographics</p> <p>Parameters:</p> Name Type Description Default <code>subjects_from_index</code> <code>Union[list, Series]</code> <p>Given subject folder names in demographics</p> required <code>subjects_from_folder</code> <code>list</code> <p>List of subject folder names to get intersection of given subject_from_index</p> <code>None</code> <p>Returns:</p> Name Type Description <code>available_subjects</code> <code>list[str]</code> <p>subjects that have an imaging data folder and are also present in the demographics file</p> <code>missing_subject_folders</code> <code>list[str]</code> <p>subjects that are in the demographics file but do not have an imaging data folder</p> <code>missing_entries</code> <code>list[str]</code> <p>subjects that have an imaging data folder but are not present in the demographics file</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def available_subjects(self,\n                       subjects_from_index: Union[list, pd.Series],\n                       subjects_from_folder: list = None) -&gt; tuple[list[str], list[str], list[str]]:\n    \"\"\"Checks missing subject folders and missing entries in demographics\n\n    Args:\n        subjects_from_index: Given subject folder names in demographics\n        subjects_from_folder: List of subject folder names to get intersection of given subject_from_index\n\n    Returns:\n        available_subjects: subjects that have an imaging data folder and are also present in the demographics file\n        missing_subject_folders: subjects that are in the demographics file but do not have an imaging data folder\n        missing_entries: subjects that have an imaging data folder but are not present in the demographics file\n    \"\"\"\n\n    # Select all subject folders if it is not given\n    if subjects_from_folder is None:\n        subjects_from_folder = self.subjects_with_imaging_data_folders()\n\n    # Missing subject that will cause warnings\n    missing_subject_folders = list(set(subjects_from_index) - set(subjects_from_folder))\n\n    # Missing entries that will cause errors\n    missing_entries = list(set(subjects_from_folder) - set(subjects_from_index))\n\n    # Intersection\n    available_subjects = list(set(subjects_from_index).intersection(set(subjects_from_folder)))\n\n    return available_subjects, missing_subject_folders, missing_entries\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase.complete_subjects","title":"complete_subjects","text":"<pre><code>complete_subjects(subjects, modalities)\n</code></pre> <p>Retrieves subjects that have given all the modalities.</p> <p>Parameters:</p> Name Type Description Default <code>subjects</code> <code>List[str]</code> <p>List of subject folder names</p> required <code>modalities</code> <code>List[str]</code> <p>List of required modalities</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of subject folder names that have required modalities</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def complete_subjects(self, subjects: List[str], modalities: List[str]) -&gt; List[str]:\n    \"\"\"Retrieves subjects that have given all the modalities.\n\n    Args:\n        subjects: List of subject folder names\n        modalities: List of required modalities\n\n    Returns:\n        List of subject folder names that have required modalities\n    \"\"\"\n    return [subject for subject in subjects if all(self.is_modalities_existing(subject, modalities))]\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase.demographics_column_names","title":"demographics_column_names  <code>staticmethod</code>","text":"<pre><code>demographics_column_names(path)\n</code></pre> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>@staticmethod\ndef demographics_column_names(path: Union[str, Path]):\n    return MedicalFolderBase.read_demographics(path).columns.values\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase.get_dataset_type","title":"get_dataset_type  <code>staticmethod</code>","text":"<pre><code>get_dataset_type()\n</code></pre> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>@staticmethod\ndef get_dataset_type() -&gt; DatasetTypes:\n    return DatasetTypes.MEDICAL_FOLDER\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase.is_modalities_existing","title":"is_modalities_existing","text":"<pre><code>is_modalities_existing(subject, modalities)\n</code></pre> <p>Checks whether given modalities exists in the subject directory</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>str</code> <p>Subject ID or subject folder name</p> required <code>modalities</code> <code>List[str]</code> <p>List of modalities to check</p> required <p>Returns:</p> Type Description <code>List[bool]</code> <p>List of <code>bool</code> that represents whether modality is existing respectively for each of modality.</p> <p>Raises:</p> Type Description <code>FedbiomedDatasetError</code> <p>bad argument type</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def is_modalities_existing(self, subject: str, modalities: List[str]) -&gt; List[bool]:\n    \"\"\"Checks whether given modalities exists in the subject directory\n\n    Args:\n        subject: Subject ID or subject folder name\n        modalities: List of modalities to check\n\n    Returns:\n        List of `bool` that represents whether modality is existing respectively for each of modality.\n\n    Raises:\n        FedbiomedDatasetError: bad argument type\n    \"\"\"\n    if not isinstance(subject, str):\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Expected string for subject folder/ID, \"\n                                    f\"but got {type(subject)}\")\n    if not isinstance(modalities, list):\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Expected a list for modalities, \"\n                                    f\"but got {type(modalities)}\")\n    if not all([type(m) is str for m in modalities]):\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Expected a list of string for modalities, \"\n                                    f\"but some modalities are \"\n                                    f\"{' '.join([ str(type(m) for m in modalities if type(m) != str)])}\")\n    are_modalities_existing = list()\n    for modality in modalities:\n        modality_folder = self._subject_modality_folder(subject, modality)\n        are_modalities_existing.append(bool(modality_folder) and\n                                       self._root.joinpath(subject, modality_folder).is_dir())\n    return are_modalities_existing\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase.modalities","title":"modalities","text":"<pre><code>modalities()\n</code></pre> <p>Gets all modalities based either on all possible candidates or those provided by the DataLoadingPlan.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of unique available modalities</p> <code>list</code> <p>List of all encountered modality folders in each subject folder, appearing once per folder</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def modalities(self) -&gt; Tuple[list, list]:\n    \"\"\"Gets all modalities based either on all possible candidates or those provided by the DataLoadingPlan.\n\n    Returns:\n         List of unique available modalities\n         List of all encountered modality folders in each subject folder, appearing once per folder\n    \"\"\"\n    modality_candidates, modality_folders_list = self.modalities_candidates_from_subfolders()\n    if self._dlp is not None and MedicalFolderLoadingBlockTypes.MODALITIES_TO_FOLDERS in self._dlp:\n        modalities = list(self._dlp[MedicalFolderLoadingBlockTypes.MODALITIES_TO_FOLDERS].map.keys())\n        return modalities, modality_folders_list\n    else:\n        return modality_candidates, modality_folders_list\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase.modalities_candidates_from_subfolders","title":"modalities_candidates_from_subfolders","text":"<pre><code>modalities_candidates_from_subfolders()\n</code></pre> <p>Gets all possible modality folders under root directory</p> <p>Returns:</p> Type Description <code>list</code> <p>List of unique available modality folders appearing at least once</p> <code>list</code> <p>List of all encountered modality folders in each subject folder, appearing once per folder</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def modalities_candidates_from_subfolders(self) -&gt; Tuple[list, list]:\n    \"\"\" Gets all possible modality folders under root directory\n\n    Returns:\n         List of unique available modality folders appearing at least once\n         List of all encountered modality folders in each subject folder, appearing once per folder\n    \"\"\"\n\n    # Accept only folders that don't start with \".\" and \"_\"\n    modalities = [f.name for f in self._root.glob(\"*/*\") if f.is_dir() and not f.name.startswith((\".\", \"_\"))]\n    return sorted(list(set(modalities))), modalities\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase.read_demographics","title":"read_demographics  <code>staticmethod</code>","text":"<pre><code>read_demographics(path, index_col=None)\n</code></pre> <p>Read demographics tabular file for Medical Folder dataset</p> <p>Raises:</p> Type Description <code>FedbiomedDatasetError</code> <p>bad file format</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>@staticmethod\ndef read_demographics(path: Union[str, Path], index_col: Optional[int] = None):\n    \"\"\" Read demographics tabular file for Medical Folder dataset\n\n    Raises:\n        FedbiomedDatasetError: bad file format\n    \"\"\"\n    path = Path(path)\n    if not path.is_file() or path.suffix.lower() not in [\".csv\", \".tsv\"]:\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Demographics should be CSV or TSV files\")\n\n    return pd.read_csv(path, index_col=index_col, engine='python')\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase.subjects_with_imaging_data_folders","title":"subjects_with_imaging_data_folders","text":"<pre><code>subjects_with_imaging_data_folders()\n</code></pre> <p>Retrieves subject folder names under Medical Folder root directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>subject folder names under Medical Folder root directory.</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def subjects_with_imaging_data_folders(self) -&gt; List[str]:\n    \"\"\"Retrieves subject folder names under Medical Folder root directory.\n\n    Returns:\n        subject folder names under Medical Folder root directory.\n    \"\"\"\n    return [f.name for f in self._root.iterdir() if f.is_dir() and not f.name.startswith(\".\")]\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase.validate_MedicalFolder_root_folder","title":"validate_MedicalFolder_root_folder  <code>staticmethod</code>","text":"<pre><code>validate_MedicalFolder_root_folder(path)\n</code></pre> <p>Validates Medical Folder root directory by checking folder structure</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>path to root directory</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to root folder of Medical Folder dataset</p> <p>Raises:</p> Type Description <code>FedbiomedDatasetError</code> <ul> <li>If path is not an instance of <code>str</code> or <code>pathlib.Path</code>                    - If path is not a directory</li> </ul> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>@staticmethod\ndef validate_MedicalFolder_root_folder(path: Union[str, Path]) -&gt; Path:\n    \"\"\" Validates Medical Folder root directory by checking folder structure\n\n    Args:\n        path: path to root directory\n\n    Returns:\n        Path to root folder of Medical Folder dataset\n\n    Raises:\n        FedbiomedDatasetError: - If path is not an instance of `str` or `pathlib.Path`\n                               - If path is not a directory\n    \"\"\"\n    if not isinstance(path, (Path, str)):\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: The argument root should an instance of \"\n                                    f\"`Path` or `str`, but got {type(path)}\")\n\n    if not isinstance(path, Path):\n        path = Path(path)\n\n    path = Path(path).expanduser().resolve()\n\n    if not path.exists():\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Folder or file {path} not found on system\")\n    if not path.is_dir():\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Root for Medical Folder dataset \"\n                                    f\"should be a directory.\")\n\n    directories = [f for f in path.iterdir() if f.is_dir()]\n    if len(directories) == 0:\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Root folder of Medical Folder should \"\n                                    f\"contain subject folders, but no sub folder has been found. \")\n\n    modalities = [f for f in path.glob(\"*/*\") if f.is_dir()]\n    if len(modalities) == 0:\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value} Subject folders for Medical Folder should \"\n                                    f\"contain modalities as folders. Folder structure should be \"\n                                    f\"root/&lt;subjects&gt;/&lt;modalities&gt;\")\n\n    return path\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderController","title":"MedicalFolderController","text":"<pre><code>MedicalFolderController(root=None)\n</code></pre> <p>               Bases: <code>MedicalFolderBase</code></p> <p>Utility class to construct and verify Medical Folder datasets without knowledge of the experiment.</p> <p>The purpose of this class is to enable key functionalities related to the MedicalFolderDataset at the time of dataset deployment, i.e. when the data is being added to the node's database.</p> <p>Specifically, the MedicalFolderController class can be used to: - construct a MedicalFolderDataset with all available data modalities, without knowing which ones will be used as     targets or features during an experiment - validate that the proper folder structure has been respected by the data managers preparing the data - identify which subjects have which modalities</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Folder path to dataset. Defaults to None.</p> <code>None</code> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def __init__(self, root: str = None):\n    \"\"\"Constructs MedicalFolderController\n\n    Args:\n        root: Folder path to dataset. Defaults to None.\n    \"\"\"\n    super(MedicalFolderController, self).__init__(root=root)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderController-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderController.load_MedicalFolder","title":"load_MedicalFolder","text":"<pre><code>load_MedicalFolder(tabular_file=None, index_col=None)\n</code></pre> <p>Load Medical Folder dataset with given tabular_file and index_col</p> <p>Parameters:</p> Name Type Description Default <code>tabular_file</code> <code>Union[str, Path]</code> <p>File path to demographics data set</p> <code>None</code> <code>index_col</code> <code>Union[str, int]</code> <p>Column index that represents subject folder names</p> <code>None</code> <p>Returns:</p> Type Description <code>MedicalFolderDataset</code> <p>MedicalFolderDataset object</p> <p>Raises:</p> Type Description <code>FedbiomedDatasetError</code> <p>If Medical Folder dataset is not successfully loaded</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def load_MedicalFolder(self,\n                       tabular_file: Union[str, Path] = None,\n                       index_col: Union[str, int] = None) -&gt; MedicalFolderDataset:\n    \"\"\" Load Medical Folder dataset with given tabular_file and index_col\n\n    Args:\n        tabular_file: File path to demographics data set\n        index_col: Column index that represents subject folder names\n\n    Returns:\n        MedicalFolderDataset object\n\n    Raises:\n        FedbiomedDatasetError: If Medical Folder dataset is not successfully loaded\n    \"\"\"\n    if self._root is None:\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Can not load Medical Folder dataset without \"\n                                    f\"declaring root directory. Please set root or build MedicalFolderController \"\n                                    f\"with by providing `root` argument use\")\n\n    modalities, _ = self.modalities()\n\n    try:\n        dataset = MedicalFolderDataset(root=self._root,\n                                       tabular_file=tabular_file,\n                                       index_col=index_col,\n                                       data_modalities=modalities,\n                                       target_modalities=modalities)\n    except FedbiomedError as e:\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Can not create Medical Folder dataset. {e}\")\n\n    if self._dlp is not None:\n        dataset.set_dlp(self._dlp)\n    return dataset\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderController.subject_modality_status","title":"subject_modality_status","text":"<pre><code>subject_modality_status(index=None)\n</code></pre> <p>Scans subjects and checks which modalities are existing for each subject</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[List, Series]</code> <p>Array-like index that comes from reference csv file of Medical Folder dataset. It represents subject folder names. Defaults to None.</p> <code>None</code> <p>Returns:     Modality status for each subject that indicates which modalities are available</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def subject_modality_status(self, index: Union[List, pd.Series] = None) -&gt; Dict:\n    \"\"\"Scans subjects and checks which modalities are existing for each subject\n\n    Args:\n        index: Array-like index that comes from reference csv file of Medical Folder dataset. It represents subject\n            folder names. Defaults to None.\n    Returns:\n        Modality status for each subject that indicates which modalities are available\n    \"\"\"\n\n    modalities, _ = self.modalities()\n    subjects = self.subjects_with_imaging_data_folders()\n    modality_status = {\"columns\": [*modalities], \"data\": [], \"index\": []}\n\n    if index is not None:\n        _, missing_subjects, missing_entries = self.available_subjects(subjects_from_index=index)\n        modality_status[\"columns\"].extend([\"in_folder\", \"in_index\"])\n\n    for subject in subjects:\n        modality_report = self.is_modalities_existing(subject, modalities)\n        status_list = [status for status in modality_report]\n        if index is not None:\n            status_list.append(False if subject in missing_subjects else True)\n            status_list.append(False if subject in missing_entries else True)\n\n        modality_status[\"data\"].append(status_list)\n        modality_status[\"index\"].append(subject)\n\n    return modality_status\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset","title":"MedicalFolderDataset","text":"<pre><code>MedicalFolderDataset(root, data_modalities='T1', transform=None, target_modalities='label', target_transform=None, demographics_transform=None, tabular_file=None, index_col=None)\n</code></pre> <p>               Bases: <code>Dataset</code>, <code>MedicalFolderBase</code></p> <p>Torch dataset following the Medical Folder Structure.</p> <p>The Medical Folder structure is loosely inspired by the BIDS standard [1]. It should respect the following pattern: <pre><code>\u2514\u2500 MedicalFolder_root/\n    \u2514\u2500 demographics.csv\n    \u2514\u2500 sub-01/\n        \u251c\u2500 T1/\n        \u2502  \u2514\u2500 sub-01_xxx.nii.gz\n        \u2514\u2500 T2/\n            \u251c\u2500 sub-01_xxx.nii.gz\n</code></pre> where the first-level subfolders or the root correspond to the subjects, and each subject's folder contains subfolders for each imaging modality. Images should be in Nifti format, with either the .nii or .nii.gz extensions. Finally, within the root folder there should also be a demographics file containing at least one index column with the names of the subject folders. This column will be used to explore the data and load the images. The demographics file may contain additional information about each subject and will be loaded alongside the images by our framework.</p> <p>[1] https://bids.neuroimaging.io/</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Union[str, PathLike, Path]</code> <p>Root folder containing all the subject directories.</p> required <code>data_modalities</code> <code>(str, Iterable)</code> <p>Modality or modalities to be used as data sources.</p> <code>'T1'</code> <code>transform</code> <code>Union[Callable, Dict[str, Callable]]</code> <p>A function or dict of function transform(s) that preprocess each data source.</p> <code>None</code> <code>target_modalities</code> <code>Optional[Union[str, Iterable[str]]]</code> <p>(str, Iterable): Modality or modalities to be used as target sources.</p> <code>'label'</code> <code>target_transform</code> <code>Union[Callable, Dict[str, Callable]]</code> <p>A function or dict of function transform(s) that preprocess each target source.</p> <code>None</code> <code>demographics_transform</code> <code>Optional[Callable]</code> <p>TODO</p> <code>None</code> <code>tabular_file</code> <code>Union[str, PathLike, Path, None]</code> <p>Path to a CSV or Excel file containing the demographic information from the patients.</p> <code>None</code> <code>index_col</code> <code>Union[int, str, None]</code> <p>Column name in the tabular file containing the subject ids which mush match the folder names.</p> <code>None</code> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def __init__(self,\n             root: Union[str, PathLike, Path],\n             data_modalities: Optional[Union[str, Iterable[str]]] = 'T1',\n             transform: Union[Callable, Dict[str, Callable]] = None,\n             target_modalities: Optional[Union[str, Iterable[str]]] = 'label',\n             target_transform: Union[Callable, Dict[str, Callable]] = None,\n             demographics_transform: Optional[Callable] = None,\n             tabular_file: Union[str, PathLike, Path, None] = None,\n             index_col: Union[int, str, None] = None,\n             ):\n    \"\"\"Constructor for class `MedicalFolderDataset`.\n\n    Args:\n        root: Root folder containing all the subject directories.\n        data_modalities (str, Iterable): Modality or modalities to be used as data sources.\n        transform: A function or dict of function transform(s) that preprocess each data source.\n        target_modalities: (str, Iterable): Modality or modalities to be used as target sources.\n        target_transform: A function or dict of function transform(s) that preprocess each target source.\n        demographics_transform: TODO\n        tabular_file: Path to a CSV or Excel file containing the demographic information from the patients.\n        index_col: Column name in the tabular file containing the subject ids which mush match the folder names.\n    \"\"\"\n    super(MedicalFolderDataset, self).__init__(root=root)\n\n    self._tabular_file = tabular_file\n    self._index_col = index_col\n\n    self._data_modalities = [data_modalities] if isinstance(data_modalities, str) else data_modalities\n    self._target_modalities = [target_modalities] if isinstance(target_modalities, str) else target_modalities\n\n    self._transform = self._check_and_reformat_transforms(transform, data_modalities)\n    self._target_transform = self._check_and_reformat_transforms(target_transform, target_modalities)\n    self._demographics_transform = demographics_transform if demographics_transform is not None else lambda x: {}\n\n    # Image loader\n    self._reader = Compose([\n        LoadImage(ITKReader(), image_only=True),\n        ToTensor()\n    ])\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset.ALLOWED_EXTENSIONS","title":"ALLOWED_EXTENSIONS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ALLOWED_EXTENSIONS = ['.nii', '.nii.gz']\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset.demographics","title":"demographics  <code>cached</code> <code>property</code>","text":"<pre><code>demographics\n</code></pre> <p>Loads tabular data file (supports excel, csv, tsv and colon separated value files).</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset.index_col","title":"index_col  <code>property</code> <code>writable</code>","text":"<pre><code>index_col\n</code></pre> <p>Getter/setter of the column containing folder's name (in the tabular file)</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset.subjects_has_all_modalities","title":"subjects_has_all_modalities  <code>property</code>","text":"<pre><code>subjects_has_all_modalities\n</code></pre> <p>Gets only the subjects that have all required modalities</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset.subjects_registered_in_demographics","title":"subjects_registered_in_demographics  <code>cached</code> <code>property</code>","text":"<pre><code>subjects_registered_in_demographics\n</code></pre> <p>Gets the subject only those who are present in the demographics file.</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset.tabular_file","title":"tabular_file  <code>property</code> <code>writable</code>","text":"<pre><code>tabular_file\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset.get_nontransformed_item","title":"get_nontransformed_item","text":"<pre><code>get_nontransformed_item(item)\n</code></pre> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def get_nontransformed_item(self, item):\n    # For the first item retrieve complete subject folders\n    subjects = self.subject_folders()\n\n    if not subjects:\n        # case where subjects is an empty list (subject folders have not been found)\n        raise FedbiomedDatasetError(\n            f\"{ErrorNumbers.FB613.value}: Cannot find complete subject folders with all the modalities\")\n    # Get subject folder\n    subject_folder = subjects[item]\n\n    # Load data modalities\n    data = self.load_images(subject_folder, modalities=self._data_modalities)\n\n    # Load target modalities\n    targets = self.load_images(subject_folder, modalities=self._target_modalities)\n\n    # Demographics\n    demographics = self._get_from_demographics(subject_id=subject_folder.name)\n    return (data, demographics), targets\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset.load_images","title":"load_images","text":"<pre><code>load_images(subject_folder, modalities)\n</code></pre> <p>Loads modality images in given subject folder</p> <p>Parameters:</p> Name Type Description Default <code>subject_folder</code> <code>Path</code> <p>Subject folder where modalities are stored</p> required <code>modalities</code> <code>list</code> <p>List of available modalities</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Subject image data as victories where keys represent each modality.</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def load_images(self, subject_folder: Path, modalities: list) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Loads modality images in given subject folder\n\n    Args:\n        subject_folder: Subject folder where modalities are stored\n        modalities: List of available modalities\n\n    Returns:\n        Subject image data as victories where keys represent each modality.\n    \"\"\"\n    # FIXME: improvment suggestion of this function at #1279\n\n    subject_data = {}\n\n    for modality in modalities:\n        modality_folder = self._subject_modality_folder(subject_folder, modality)\n        image_folder = subject_folder.joinpath(modality_folder)\n        nii_files = [p.resolve() for p in image_folder.glob(\"**/*\")]\n\n        # Load the first, we assume there is going to be a single image per modality for now.\n\n        nii_files = tuple(\n            img for img in nii_files if any(str(img).endswith(fmt) for fmt in self.ALLOWED_EXTENSIONS)\n                          )\n        if len(nii_files) &lt; 1:\n            raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: folder {os.path.join(image_folder, modality)}\"\n                                        \" is empty, but should contain an niftii image. Aborting\")\n\n        elif len(nii_files) &gt; 1:\n            raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: more than one niftii file has been detected\"\n                                        \" {', '.join(tuple(str(f) for f in nii_files))}. \"\n                                        \"\\nThere should be only one niftii image per modality. Aborting\")\n\n        img_path = nii_files[0]\n        img = self._reader(img_path)\n        subject_data[modality] = img\n\n    return subject_data\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset.set_dataset_parameters","title":"set_dataset_parameters","text":"<pre><code>set_dataset_parameters(parameters)\n</code></pre> <p>Sets dataset parameters.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>dict</code> <p>Parameters to initialize</p> required <p>Raises:</p> Type Description <code>FedbiomedDatasetError</code> <p>If given parameters are not of <code>dict</code> type</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def set_dataset_parameters(self, parameters: dict):\n    \"\"\"Sets dataset parameters.\n\n    Args:\n        parameters: Parameters to initialize\n\n    Raises:\n        FedbiomedDatasetError: If given parameters are not of `dict` type\n    \"\"\"\n    if not isinstance(parameters, dict):\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Expected type for `parameters` is `dict, \"\n                                    f\"but got {type(parameters)}`\")\n\n    for key, value in parameters.items():\n        if hasattr(self, key):\n            setattr(self, key, value)\n        else:\n            raise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Trying to set non existing attribute '{key}'\")\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset.shape","title":"shape","text":"<pre><code>shape()\n</code></pre> <p>Retrieves shape information for modalities and demographics csv</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def shape(self) -&gt; dict:\n    \"\"\"Retrieves shape information for modalities and demographics csv\"\"\"\n\n    # Get all modalities\n    data_modalities = list(set(self._data_modalities))\n    target_modalities = list(set(self._target_modalities))\n    modalities = list(set(self._data_modalities + self._target_modalities))\n    (image, _), targets = self.get_nontransformed_item(0)\n\n    result = {modality: list(image[modality].shape) for modality in data_modalities}\n\n    result.update({modality: list(targets[modality].shape) for modality in target_modalities})\n    num_modalities = len(modalities)\n    demographics_shape = self.demographics.shape if self.demographics is not None else None\n    result.update({\"demographics\": demographics_shape, \"num_modalities\": num_modalities})\n\n    return result\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset.subject_folders","title":"subject_folders","text":"<pre><code>subject_folders()\n</code></pre> <p>Retrieves subject folder names of only those who have their complete modalities</p> <p>Returns:</p> Type Description <code>List[Path]</code> <p>List of subject directories that has all requested modalities</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def subject_folders(self) -&gt; List[Path]:\n    \"\"\"Retrieves subject folder names of only those who have their complete modalities\n\n    Returns:\n        List of subject directories that has all requested modalities\n    \"\"\"\n\n    # If demographics are present\n    if self._tabular_file and self._index_col is not None:\n        complete_subject_folders = self.subjects_registered_in_demographics\n    else:\n        complete_subject_folders = self.subjects_has_all_modalities\n\n    return [self._root.joinpath(folder) for folder in complete_subject_folders]\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderLoadingBlockTypes","title":"MedicalFolderLoadingBlockTypes","text":"<pre><code>MedicalFolderLoadingBlockTypes(*args)\n</code></pre> <p>               Bases: <code>DataLoadingBlockTypes</code>, <code>Enum</code></p> Source code in <code>fedbiomed/common/constants.py</code> <pre><code>def __init__(self, *args):\n    cls = self.__class__\n    if not isinstance(self.value, str):\n        raise ValueError(\"all fields of DataLoadingBlockTypes subclasses\"\n                         \" must be of str type\")\n    if any(self.value == e.value for e in cls):\n        a = self.name\n        e = cls(self.value).name\n        raise ValueError(\n            f\"duplicate values not allowed in DataLoadingBlockTypes and \"\n            f\"its subclasses: {a} --&gt; {e}\")\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderLoadingBlockTypes-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderLoadingBlockTypes.MODALITIES_TO_FOLDERS","title":"MODALITIES_TO_FOLDERS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MODALITIES_TO_FOLDERS = 'modalities_to_folders'\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NIFTIFolderDataset","title":"NIFTIFolderDataset","text":"<pre><code>NIFTIFolderDataset(root, transform=None, target_transform=None)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>A Generic class for loading NIFTI Images using the folder structure as the target classes' labels.</p> <p>Supported formats: - NIFTI and compressed NIFTI files: <code>.nii</code>, <code>.nii.gz</code></p> <p>This is a Dataset useful in classification tasks. Its usage is quite simple, quite similar to <code>torchvision.datasets.ImageFolder</code>. Images must be contained in first level sub-folders (level 2+ sub-folders are ignored) that describe the target class they belong to (target class label is the name of the folder).</p> <pre><code>nifti_dataset_root_folder\n\u251c\u2500\u2500 control_group\n\u2502   \u251c\u2500\u2500 subject_1.nii\n\u2502   \u2514\u2500\u2500 subject_2.nii\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 disease_group\n    \u251c\u2500\u2500 subject_3.nii\n    \u2514\u2500\u2500 subject_4.nii\n    \u2514\u2500\u2500 ...\n</code></pre> <p>In this example, there are 4 samples (one from each *.nii file), 2 target class, with labels <code>control_group</code> and <code>disease_group</code>. <code>subject_1.nii</code> has class label <code>control_group</code>, <code>subject_3.nii</code> has class label <code>disease_group</code>,etc.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Union[str, PathLike, Path]</code> <p>folder where the data is located.</p> required <code>transform</code> <code>Union[Callable, None]</code> <p>transforms to be applied on data.</p> <code>None</code> <code>target_transform</code> <code>Union[Callable, None]</code> <p>transforms to be applied on target indexes.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedDatasetError</code> <p>bad argument type</p> <code>FedbiomedDatasetError</code> <p>bad root path</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def __init__(self, root: Union[str, PathLike, Path],\n             transform: Union[Callable, None] = None,\n             target_transform: Union[Callable, None] = None\n             ):\n    \"\"\"Constructor of the class\n\n    Args:\n        root: folder where the data is located.\n        transform: transforms to be applied on data.\n        target_transform: transforms to be applied on target indexes.\n\n    Raises:\n        FedbiomedDatasetError: bad argument type\n        FedbiomedDatasetError: bad root path\n    \"\"\"\n    # check parameters type\n    for tr, trname in ((transform, 'transform'), (target_transform, 'target_transform')):\n        if not callable(tr) and tr is not None:\n            raise FedbiomedDatasetError(f\"{ErrorNumbers.FB612.value}: Parameter {trname} has incorrect \"\n                                        f\"type {type(tr)}, cannot create dataset.\")\n\n    if not isinstance(root, str) and not isinstance(root, PathLike) and not isinstance(root, Path):\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB612.value}: Parameter `root` has incorrect type \"\n                                    f\"{type(root)}, cannot create dataset.\")\n\n    # initialize object variables\n    self._files = []\n    self._class_labels = []\n    self._targets = []\n\n    try:\n        self._root_dir = Path(root).expanduser()\n    except RuntimeError as e:\n        raise FedbiomedDatasetError(\n            f\"{ErrorNumbers.FB612.value}: Cannot expand path {root}, error message is: {e}\")\n\n    self._transform = transform\n    self._target_transform = target_transform\n    self._reader = Compose([\n        LoadImage(ITKReader(), image_only=True),\n        ToTensor()\n    ])\n\n    self._explore_root_folder()\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NIFTIFolderDataset-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.NIFTIFolderDataset.files","title":"files","text":"<pre><code>files()\n</code></pre> <p>Retrieves the paths to the sample images.</p> <p>Gives sames order as when retrieving the sample images (eg <code>self.files[0]</code> is the path to <code>self.__getitem__[0]</code>)</p> <p>Returns:</p> Type Description <code>List[Path]</code> <p>List of the absolute paths to the sample images</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def files(self) -&gt; List[Path]:\n    \"\"\"Retrieves the paths to the sample images.\n\n    Gives sames order as when retrieving the sample images (eg `self.files[0]`\n    is the path to `self.__getitem__[0]`)\n\n    Returns:\n        List of the absolute paths to the sample images\n    \"\"\"\n    return self._files\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NIFTIFolderDataset.labels","title":"labels","text":"<pre><code>labels()\n</code></pre> <p>Retrieves the labels of the target classes.</p> <p>Target label index is the index of the corresponding label in this list.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of the labels of the target classes.</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def labels(self) -&gt; List[str]:\n    \"\"\"Retrieves the labels of the target classes.\n\n    Target label index is the index of the corresponding label in this list.\n\n    Returns:\n        List of the labels of the target classes.\n    \"\"\"\n    return self._class_labels\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NPDataLoader","title":"NPDataLoader","text":"<pre><code>NPDataLoader(dataset, target, batch_size=1, shuffle=False, random_seed=None, drop_last=False)\n</code></pre> <p>DataLoader for a Numpy dataset.</p> <p>This data loader encapsulates a dataset composed of numpy arrays and presents an Iterable interface. One design principle was to try to make the interface as similar as possible to a torch.DataLoader.</p> <p>Attributes:</p> Name Type Description <code>_dataset</code> <p>(np.ndarray) a 2d array of features</p> <code>_target</code> <p>(np.ndarray) an optional array of target values</p> <code>_batch_size</code> <p>(int) the number of elements in one batch</p> <code>_shuffle</code> <p>(bool) if True, shuffle the data at the beginning of every epoch</p> <code>_drop_last</code> <p>(bool) if True, drop the last batch if it does not contain batch_size elements</p> <code>_rng</code> <p>(np.random.Generator) the random number generator for shuffling</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ndarray</code> <p>2D Numpy array</p> required <code>target</code> <code>ndarray</code> <p>Numpy array of target values</p> required <code>batch_size</code> <code>int</code> <p>batch size for each iteration</p> <code>1</code> <code>shuffle</code> <code>bool</code> <p>shuffle before iteration</p> <code>False</code> <code>random_seed</code> <code>Optional[int | Generator]</code> <p>an optional integer to set the numpy random seed for shuffling. If it equals None, then no attempt will be made to set the random seed.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>whether to drop the last batch in case it does not fill the whole batch size</p> <code>False</code> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def __init__(\n    self,\n    dataset: np.ndarray,\n    target: np.ndarray,\n    batch_size: int = 1,\n    shuffle: bool = False,\n    random_seed: Optional[int | np.random.Generator] = None,\n    drop_last: bool = False,\n):\n    \"\"\"Construct numpy data loader\n\n    Args:\n        dataset: 2D Numpy array\n        target: Numpy array of target values\n        batch_size: batch size for each iteration\n        shuffle: shuffle before iteration\n        random_seed: an optional integer to set the numpy random seed for shuffling. If it equals\n            None, then no attempt will be made to set the random seed.\n        drop_last: whether to drop the last batch in case it does not fill the whole batch size\n    \"\"\"\n\n    if not isinstance(dataset, np.ndarray) or not isinstance(target, np.ndarray):\n        msg = (\n            f\"{ErrorNumbers.FB609.value}. Wrong input type for `dataset` or `target` in NPDataLoader. \"\n            f\"Expected type np.ndarray for both, instead got {type(dataset)} and\"\n            f\"{type(target)} respectively.\"\n        )\n        logger.error(msg)\n        raise FedbiomedTypeError(msg)\n\n    # If the researcher gave a 1-dimensional dataset, we expand it to 2 dimensions\n    if dataset.ndim == 1:\n        dataset = dataset[:, np.newaxis]\n\n    # If the researcher gave a 1-dimensional target, we expand it to 2 dimensions\n    if target.ndim == 1:\n        target = target[:, np.newaxis]\n\n    if dataset.ndim != 2 or target.ndim != 2:\n        raise FedbiomedValueError(\n            f\"{ErrorNumbers.FB609.value}. Wrong shape for `dataset` or `target` in \"\n            f\"NPDataLoader. Expected 2-dimensional arrays, instead got {dataset.ndim}- \"\n            f\"dimensional and {target.ndim}-dimensional arrays respectively.\"\n        )\n\n    if len(dataset) != len(target):\n        raise FedbiomedValueError(\n            f\"{ErrorNumbers.FB609.value}. Inconsistent length for `dataset` and `target` \"\n            f\"in NPDataLoader. Expected same length, instead got len(dataset)={len(dataset)}, \"\n            f\"len(target)={len(target)}\"\n        )\n\n    if not isinstance(batch_size, int) or batch_size &lt;= 0:\n        raise FedbiomedValueError(\n            f\"{ErrorNumbers.FB609.value}. Wrong value for `batch_size` parameter of \"\n            f\"NPDataLoader. Expected a non-zero positive integer, instead got value {batch_size}.\"\n        )\n\n    if random_seed is not None and not isinstance(\n        random_seed, (int, np.random.Generator)\n    ):\n        raise FedbiomedTypeError(\n            f\"{ErrorNumbers.FB609.value}. Wrong type for `random_seed` parameter of \"\n            f\"NPDataLoader. Expected int or None, instead got {type(random_seed)}.\"\n        )\n\n    self._dataset = dataset\n    self._target = target\n    self._batch_size = batch_size\n    self._shuffle = shuffle\n    self._drop_last = drop_last\n    self._rng = (\n        np.random.default_rng(random_seed)\n        if isinstance(random_seed, (int, type(None)))\n        else random_seed\n    )\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NPDataLoader-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.NPDataLoader.dataset","title":"dataset  <code>property</code>","text":"<pre><code>dataset\n</code></pre> <p>Returns the encapsulated dataset</p> <p>This needs to be a property to harmonize the API with torch.DataLoader, enabling us to write generic code for both DataLoaders.</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NPDataLoader.target","title":"target  <code>property</code>","text":"<pre><code>target\n</code></pre> <p>Returns the array of target values</p> <p>This has been made a property to have a homogeneous interface with the dataset property above.</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NPDataLoader-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.NPDataLoader.batch_size","title":"batch_size","text":"<pre><code>batch_size()\n</code></pre> <p>Returns the batch size</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def batch_size(self) -&gt; int:\n    \"\"\"Returns the batch size\"\"\"\n    return self._batch_size\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NPDataLoader.drop_last","title":"drop_last","text":"<pre><code>drop_last()\n</code></pre> <p>Returns the boolean drop_last attribute</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def drop_last(self) -&gt; bool:\n    \"\"\"Returns the boolean drop_last attribute\"\"\"\n    return self._drop_last\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NPDataLoader.n_remainder_samples","title":"n_remainder_samples","text":"<pre><code>n_remainder_samples()\n</code></pre> <p>Returns the remainder of the division between dataset length and batch size.</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def n_remainder_samples(self) -&gt; int:\n    \"\"\"Returns the remainder of the division between dataset length and batch size.\"\"\"\n    return len(self._dataset) % self._batch_size\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NPDataLoader.rng","title":"rng","text":"<pre><code>rng()\n</code></pre> <p>Returns the random number generator</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def rng(self) -&gt; np.random.Generator:\n    \"\"\"Returns the random number generator\"\"\"\n    return self._rng\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NPDataLoader.shuffle","title":"shuffle","text":"<pre><code>shuffle()\n</code></pre> <p>Returns the boolean shuffle attribute</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def shuffle(self) -&gt; bool:\n    \"\"\"Returns the boolean shuffle attribute\"\"\"\n    return self._shuffle\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SerializationValidation","title":"SerializationValidation","text":"<pre><code>SerializationValidation()\n</code></pre> <p>Provide Validation capabilities for serializing/deserializing a [DataLoadingBlock] or [DataLoadingPlan].</p> <p>When a developer inherits from [DataLoadingBlock] to define a custom loading block, they are required to call the <code>_serialization_validator.update_validation_scheme</code> function with a dictionary argument containing the rules to validate all the additional fields that will be used in the serialization of their loading block.</p> <p>These rules must follow the syntax explained in the SchemeValidator class.</p> <p>For example <pre><code>    class MyLoadingBlock(DataLoadingBlock):\n        def __init__(self):\n            self.my_custom_data = {}\n            self._serialization_validator.update_validation_scheme({\n                'custom_data': {\n                    'rules': [dict, ...any other rules],\n                    'required': True\n                }\n            })\n        def serialize(self):\n            serialized = super().serialize()\n            serialized.update({'custom_data': self.my_custom_data})\n            return serialized\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>_validation_scheme</code> <p>(dict) an extensible set of rules to validate the DataLoadingBlock metadata.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def __init__(self):\n    self._validation_scheme = {}\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SerializationValidation-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.SerializationValidation.dlb_default_scheme","title":"dlb_default_scheme  <code>classmethod</code>","text":"<pre><code>dlb_default_scheme()\n</code></pre> <p>The dictionary of default validation rules for a serialized [DataLoadingBlock].</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>@classmethod\ndef dlb_default_scheme(cls) -&gt; Dict:\n    \"\"\"The dictionary of default validation rules for a serialized [DataLoadingBlock].\"\"\"\n    return {\n        'loading_block_class': {\n            'rules': [str, cls._identifier_validation_hook],\n            'required': True,\n        },\n        'loading_block_module': {\n            'rules': [str, cls._identifier_validation_hook],\n            'required': True,\n        },\n        'dlb_id': {\n            'rules': [str, cls._serial_id_validation_hook],\n            'required': True,\n        },\n    }\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SerializationValidation.dlp_default_scheme","title":"dlp_default_scheme  <code>classmethod</code>","text":"<pre><code>dlp_default_scheme()\n</code></pre> <p>The dictionary of default validation rules for a serialized [DataLoadingPlan].</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>@classmethod\ndef dlp_default_scheme(cls) -&gt; Dict:\n    \"\"\"The dictionary of default validation rules for a serialized [DataLoadingPlan].\"\"\"\n    return {\n        'dlp_id': {\n            'rules': [str],\n            'required': True,\n        },\n        'dlp_name': {\n            'rules': [str],\n            'required': True,\n        },\n        'target_dataset_type': {\n            'rules': [str, cls._target_dataset_type_validator],\n            'required': True,\n        },\n        'loading_blocks': {\n            'rules': [dict, cls._loading_blocks_types_validator],\n            'required': True\n        },\n        'key_paths': {\n            'rules': [dict, cls._key_paths_validator],\n            'required': True\n        }\n    }\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SerializationValidation.update_validation_scheme","title":"update_validation_scheme","text":"<pre><code>update_validation_scheme(new_scheme)\n</code></pre> <p>Updates the validation scheme.</p> <p>Parameters:</p> Name Type Description Default <code>new_scheme</code> <code>dict</code> <p>(dict) new dict of rules</p> required Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def update_validation_scheme(self, new_scheme: dict) -&gt; None:\n    \"\"\"Updates the validation scheme.\n\n    Args:\n        new_scheme: (dict) new dict of rules\n    \"\"\"\n    self._validation_scheme.update(new_scheme)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SerializationValidation.validate","title":"validate","text":"<pre><code>validate(dlb_metadata, exception_type, only_required=True)\n</code></pre> <p>Validate a dict of dlb_metadata according to the _validation_scheme.</p> <p>Parameters:</p> Name Type Description Default <code>dlb_metadata</code> <code>dict) </code> <p>the [DataLoadingBlock] metadata, as returned by serialize or as loaded from the node database.</p> required <code>exception_type</code> <code>Type[FedbiomedError]</code> <p>the type of the exception to be raised when validation fails.</p> required <code>only_required</code> <code>bool) </code> <p>see SchemeValidator.populate_with_defaults</p> <code>True</code> <p>Raises:     exception_type: if the validation fails.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def validate(self,\n             dlb_metadata: Dict,\n             exception_type: Type[FedbiomedError],\n             only_required: bool = True) -&gt; None:\n    \"\"\"Validate a dict of dlb_metadata according to the _validation_scheme.\n\n    Args:\n        dlb_metadata (dict) : the [DataLoadingBlock] metadata, as returned by serialize or as loaded from the\n            node database.\n        exception_type (Type[FedbiomedError]): the type of the exception to be raised when validation fails.\n        only_required (bool) : see SchemeValidator.populate_with_defaults\n    Raises:\n        exception_type: if the validation fails.\n    \"\"\"\n    try:\n        sc = SchemeValidator(self._validation_scheme)\n    except RuleError as e:\n        msg = ErrorNumbers.FB614.value + f\": {e}\"\n        logger.critical(msg)\n        raise exception_type(msg)\n\n    try:\n        dlb_metadata = sc.populate_with_defaults(dlb_metadata,\n                                                 only_required=only_required)\n    except ValidatorError as e:\n        msg = ErrorNumbers.FB614.value + f\": {e}\"\n        logger.critical(msg)\n        raise exception_type(msg)\n\n    try:\n        sc.validate(dlb_metadata)\n    except ValidateError as e:\n        msg = ErrorNumbers.FB614.value + f\": {e}\"\n        logger.critical(msg)\n        raise exception_type(msg)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SkLearnDataManager","title":"SkLearnDataManager","text":"<pre><code>SkLearnDataManager(inputs, target, **kwargs)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Wrapper for <code>pd.DataFrame</code>, <code>pd.Series</code> and <code>np.ndarray</code> datasets.</p> <p>Manages datasets for scikit-learn based model training. Responsible for managing inputs, and target variables that have been provided in <code>training_data</code> of scikit-learn based training plans.</p> <p>The loader arguments will be passed to the [fedbiomed.common.data.NPDataLoader] classes instantiated when split is called. They may include batch_size, shuffle, drop_last, and others. Please see the [fedbiomed.common.data.NPDataLoader] class for more details.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[ndarray, DataFrame, Series]</code> <p>Independent variables (inputs, features) for model training</p> required <code>target</code> <code>Union[ndarray, DataFrame, Series]</code> <p>Dependent variable/s (target) for model training and validation</p> required <code>**kwargs</code> <code>dict</code> <p>Loader arguments</p> <code>{}</code> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def __init__(\n    self,\n    inputs: Union[np.ndarray, pd.DataFrame, pd.Series],\n    target: Union[np.ndarray, pd.DataFrame, pd.Series],\n    **kwargs: dict,\n):\n    \"\"\"Construct a SkLearnDataManager from an array of inputs and an array of targets.\n\n    The loader arguments will be passed to the [fedbiomed.common.data.NPDataLoader] classes instantiated\n    when split is called. They may include batch_size, shuffle, drop_last, and others. Please see the\n    [fedbiomed.common.data.NPDataLoader] class for more details.\n\n    Args:\n        inputs: Independent variables (inputs, features) for model training\n        target: Dependent variable/s (target) for model training and validation\n        **kwargs: Loader arguments\n    \"\"\"\n\n    if not isinstance(\n        inputs, (np.ndarray, pd.DataFrame, pd.Series)\n    ) or not isinstance(target, (np.ndarray, pd.DataFrame, pd.Series)):\n        msg = (\n            f\"{ErrorNumbers.FB609.value}. Parameters `inputs` and `target` for \"\n            f\"initialization of {self.__class__.__name__} should be one of np.ndarray, pd.DataFrame, pd.Series\"\n        )\n        logger.error(msg)\n        raise FedbiomedTypeError(msg)\n\n    # Convert pd.DataFrame or pd.Series to np.ndarray for `inputs`\n    if isinstance(inputs, (pd.DataFrame, pd.Series)):\n        self._inputs = inputs.to_numpy()\n    else:\n        self._inputs = inputs\n\n    # Convert pd.DataFrame or pd.Series to np.ndarray for `target`\n    if isinstance(target, (pd.DataFrame, pd.Series)):\n        self._target = target.to_numpy()\n    else:\n        self._target = target\n\n    # rand_seed = kwargs.get('random_seed')\n    # self.rng(rand_seed)\n\n    # Subset None means that train/validation split has not been performed\n    self._subset_test: Union[Tuple[np.ndarray, np.ndarray], None] = None\n    self._subset_train: Union[Tuple[np.ndarray, np.ndarray], None] = None\n\n    self.training_index: List[int] = []\n    self.testing_index: List[int] = []\n    self.test_ratio: Optional[float] = None\n    self._is_shuffled_testing_dataset: bool = False\n    if \"shuffle_testing_dataset\" in kwargs:\n        self._is_shuffled_testing_dataset: bool = kwargs.pop(\n            \"shuffle_testing_dataset\"\n        )\n\n    # Additional loader arguments\n    self._loader_arguments = kwargs\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SkLearnDataManager-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.SkLearnDataManager.test_ratio","title":"test_ratio  <code>instance-attribute</code>","text":"<pre><code>test_ratio = None\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SkLearnDataManager.testing_index","title":"testing_index  <code>instance-attribute</code>","text":"<pre><code>testing_index = []\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SkLearnDataManager.training_index","title":"training_index  <code>instance-attribute</code>","text":"<pre><code>training_index = []\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SkLearnDataManager-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.SkLearnDataManager.dataset","title":"dataset","text":"<pre><code>dataset()\n</code></pre> <p>Gets the entire registered dataset.</p> <p>This method returns whole dataset as it is without any split.</p> <p>Returns:</p> Name Type Description <code>inputs</code> <code>ndarray</code> <p>Input variables for model training</p> <code>targets</code> <code>ndarray</code> <p>Target variable for model training</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def dataset(self) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Gets the entire registered dataset.\n\n    This method returns whole dataset as it is without any split.\n\n    Returns:\n         inputs: Input variables for model training\n         targets: Target variable for model training\n    \"\"\"\n    return self._inputs, self._target\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SkLearnDataManager.load_state","title":"load_state","text":"<pre><code>load_state(state)\n</code></pre> <p>Loads state of the data loader</p> <p>It currently keep only testing index, training index and test ratio as state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Dict</code> <p>Object containing data loader state.</p> required Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def load_state(self, state: Dict) -&gt; None:\n    \"\"\"Loads state of the data loader\n\n\n    It currently keep only testing index, training index and test ratio\n    as state.\n\n    Args:\n        state: Object containing data loader state.\n    \"\"\"\n\n    self.testing_index = state.get(\"testing_index\", [])\n    self.training_index = state.get(\"training_index\", [])\n    self.test_ratio = state.get(\"test_ratio\", None)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SkLearnDataManager.save_state","title":"save_state","text":"<pre><code>save_state()\n</code></pre> <p>Gets state of the data loader.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>A Dict containing data loader state.</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def save_state(self) -&gt; Dict:\n    \"\"\"Gets state of the data loader.\n\n    Returns:\n        A Dict containing data loader state.\n    \"\"\"\n    _loader_args = {}\n    _loader_args[\"training_index\"], _loader_args[\"testing_index\"] = (\n        self.training_index,\n        self.testing_index,\n    )\n    _loader_args[\"test_ratio\"] = self.test_ratio\n\n    return _loader_args\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SkLearnDataManager.split","title":"split","text":"<pre><code>split(test_ratio, test_batch_size, is_shuffled_testing_dataset=False)\n</code></pre> <p>Splits <code>np.ndarray</code> dataset into train and validation.</p> <p>Parameters:</p> Name Type Description Default <code>test_ratio</code> <code>float</code> <p>Ratio for validation set partition. Rest of the samples will be used for training</p> required <p>Raises:</p> Type Description <code>FedbiomedSkLearnDataManagerError</code> <p>If the <code>test_ratio</code> is not between 0 and 1</p> <p>Returns:</p> Name Type Description <code>train_loader</code> <code>NPDataLoader</code> <p>NPDataLoader of input variables for model training</p> <code>test_loader</code> <code>NPDataLoader</code> <p>NPDataLoader of target variable for model training</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def split(\n    self,\n    test_ratio: float,\n    test_batch_size: int,\n    is_shuffled_testing_dataset: bool = False,\n) -&gt; Tuple[NPDataLoader, NPDataLoader]:\n    \"\"\"Splits `np.ndarray` dataset into train and validation.\n\n    Args:\n         test_ratio: Ratio for validation set partition. Rest of the samples will be used for training\n\n    Raises:\n        FedbiomedSkLearnDataManagerError: If the `test_ratio` is not between 0 and 1\n\n    Returns:\n         train_loader: NPDataLoader of input variables for model training\n         test_loader: NPDataLoader of target variable for model training\n    \"\"\"\n    if not isinstance(test_ratio, float):\n        raise FedbiomedTypeError(\n            f\"{ErrorNumbers.FB609.value}: The argument `ratio` should be type \"\n            f\"`float` not {type(test_ratio)}\"\n        )\n\n    if test_ratio &lt; 0.0 or test_ratio &gt; 1.0:\n        raise FedbiomedTypeError(\n            f\"{ErrorNumbers.FB609.value}: The argument `ratio` should be equal or between \"\n            f\"0 and 1, not {test_ratio}\"\n        )\n\n    empty_subset = (np.array([]), np.array([]))\n\n    if self.test_ratio != test_ratio and self.test_ratio is not None:\n        if not is_shuffled_testing_dataset:\n            logger.info(\n                \"`test_ratio` value has changed: this will change the testing dataset\"\n            )\n        is_shuffled_testing_dataset = True\n\n    if test_ratio &lt;= 0.0:\n        self._subset_train = (self._inputs, self._target)\n        self._subset_test = empty_subset\n        self.training_index, self.testing_index = list(range(len(self._inputs))), []\n    elif test_ratio &gt;= 1.0:\n        self._subset_train = empty_subset\n        self._subset_test = (self._inputs, self._target)\n        self.training_index, self.testing_index = [], list(range(len(self._inputs)))\n\n    else:\n        _is_loading_failed: bool = False\n        if self.testing_index and not is_shuffled_testing_dataset:\n            # reloading testing dataset from previous rounds\n            try:\n                self._load_indexes(self.training_index, self.testing_index)\n            except IndexError:\n                _is_loading_failed = True\n        if (\n            not self.testing_index\n            or is_shuffled_testing_dataset\n            or _is_loading_failed\n        ):\n            (x_train, x_test, y_train, y_test, idx_train, idx_test) = (\n                train_test_split(\n                    self._inputs,\n                    self._target,\n                    np.arange(len(self._inputs)),\n                    test_size=test_ratio,\n                )\n            )\n            self._subset_test = (x_test, y_test)\n            self._subset_train = (x_train, y_train)\n            self.training_index = idx_train.tolist()\n            self.testing_index = idx_test.tolist()\n\n    if not test_batch_size:\n        test_batch_size = len(self._subset_test)\n\n    self.test_ratio = test_ratio  # float(np.clip(0, 1, test_ratio))\n\n    # self._loader_arguments['random_seed'] = self._rng\n    return self._subset_loader(\n        self._subset_train, **self._loader_arguments\n    ), self._subset_loader(self._subset_test, batch_size=test_batch_size)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SkLearnDataManager.subset_test","title":"subset_test","text":"<pre><code>subset_test()\n</code></pre> <p>Gets Subset of dataset for validation partition.</p> <p>Returns:</p> Name Type Description <code>test_inputs</code> <code>Union[Tuple[ndarray, ndarray], None]</code> <p>Input variables of validation subset for model validation</p> <code>test_target</code> <code>Union[Tuple[ndarray, ndarray], None]</code> <p>Target variable of validation subset for model validation</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def subset_test(self) -&gt; Union[Tuple[np.ndarray, np.ndarray], None]:\n    \"\"\"Gets Subset of dataset for validation partition.\n\n    Returns:\n        test_inputs: Input variables of validation subset for model validation\n        test_target: Target variable of validation subset for model validation\n    \"\"\"\n    return self._subset_test\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SkLearnDataManager.subset_train","title":"subset_train","text":"<pre><code>subset_train()\n</code></pre> <p>Gets Subset for train partition.</p> <p>Returns:</p> Name Type Description <code>test_inputs</code> <code>Union[Tuple[ndarray, ndarray], None]</code> <p>Input variables of training subset for model training</p> <code>test_target</code> <code>Union[Tuple[ndarray, ndarray], None]</code> <p>Target variable of training subset for model training</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def subset_train(self) -&gt; Union[Tuple[np.ndarray, np.ndarray], None]:\n    \"\"\"Gets Subset for train partition.\n\n    Returns:\n        test_inputs: Input variables of training subset for model training\n        test_target: Target variable of training subset for model training\n    \"\"\"\n\n    return self._subset_train\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TabularDataset","title":"TabularDataset","text":"<pre><code>TabularDataset(inputs, target)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Torch based Dataset object to create torch Dataset from given numpy or dataframe type of input and target variables</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[ndarray, DataFrame, Series]</code> <p>Input variables that will be passed to network</p> required <code>target</code> <code>Union[ndarray, DataFrame, Series]</code> <p>Target variable for output layer</p> required <p>Raises:</p> Type Description <code>FedbiomedTorchDatasetError</code> <p>If input variables and target variable does not have equal length/size</p> Source code in <code>fedbiomed/common/data/_tabular_dataset.py</code> <pre><code>def __init__(self,\n             inputs: Union[np.ndarray, pd.DataFrame, pd.Series],\n             target: Union[np.ndarray, pd.DataFrame, pd.Series]):\n    \"\"\"Constructs PyTorch dataset object\n\n    Args:\n        inputs: Input variables that will be passed to network\n        target: Target variable for output layer\n\n    Raises:\n        FedbiomedTorchDatasetError: If input variables and target variable does not have\n            equal length/size\n    \"\"\"\n\n    # Inputs and target variable should be converted to the torch tensors\n    # PyTorch provides `from_numpy` function to convert numpy arrays to\n    # torch tensor. Therefore, if the arguments `inputs` and `target` are\n    # instance one of `pd.DataFrame` or `pd.Series`, they should be converted to\n    # numpy arrays\n    if isinstance(inputs, (pd.DataFrame, pd.Series)):\n        self.inputs = inputs.to_numpy()\n    elif isinstance(inputs, np.ndarray):\n        self.inputs = inputs\n    else:\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB610.value}: The argument `inputs` should be \"\n                                                f\"an instance one of np.ndarray, pd.DataFrame or pd.Series\")\n    # Configuring self.target attribute\n    if isinstance(target, (pd.DataFrame, pd.Series)):\n        self.target = target.to_numpy()\n    elif isinstance(inputs, np.ndarray):\n        self.target = target\n    else:\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB610.value}: The argument `target` should be \"\n                                                f\"an instance one of np.ndarray, pd.DataFrame or pd.Series\")\n\n    # The lengths should be equal\n    if len(self.inputs) != len(self.target):\n        raise FedbiomedDatasetError(f\"{ErrorNumbers.FB610.value}: Length of input variables and target \"\n                                                f\"variable does not match. Please make sure that they have \"\n                                                f\"equal size while creating the method `training_data` of \"\n                                                f\"TrainingPlan\")\n\n    # Convert `inputs` adn `target` to Torch floats\n    self.inputs = from_numpy(self.inputs).float()\n    self.target = from_numpy(self.target).float()\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TabularDataset-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.TabularDataset.inputs","title":"inputs  <code>instance-attribute</code>","text":"<pre><code>inputs = float()\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TabularDataset.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target = float()\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TabularDataset-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.TabularDataset.get_dataset_type","title":"get_dataset_type  <code>staticmethod</code>","text":"<pre><code>get_dataset_type()\n</code></pre> Source code in <code>fedbiomed/common/data/_tabular_dataset.py</code> <pre><code>@staticmethod\ndef get_dataset_type() -&gt; DatasetTypes:\n    return DatasetTypes.TABULAR\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager","title":"TorchDataManager","text":"<pre><code>TorchDataManager(dataset, **kwargs)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Wrapper for PyTorch Dataset to manage loading operations for validation and train.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset object for torch.utils.data.DataLoader</p> required <code>**kwargs</code> <code>dict</code> <p>Arguments for PyTorch <code>DataLoader</code></p> <code>{}</code> <p>Raises:</p> Type Description <code>FedbiomedTorchDataManagerError</code> <p>If the argument <code>dataset</code> is not an instance of <code>torch.utils.data.Dataset</code></p> Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>def __init__(self, dataset: Dataset, **kwargs: dict):\n    \"\"\"Construct  of class\n\n    Args:\n        dataset: Dataset object for torch.utils.data.DataLoader\n        **kwargs: Arguments for PyTorch `DataLoader`\n\n    Raises:\n        FedbiomedTorchDataManagerError: If the argument `dataset` is not an instance of `torch.utils.data.Dataset`\n    \"\"\"\n\n    # TorchDataManager should get `dataset` argument as an instance of torch.utils.data.Dataset\n    if not isinstance(dataset, Dataset):\n        raise FedbiomedTorchDataManagerError(\n            f\"{ErrorNumbers.FB608.value}: The attribute `dataset` should an instance \"\n            f\"of `torch.utils.data.Dataset`, please use `Dataset` as parent class for\"\n            f\"your custom torch dataset object\"\n        )\n\n    self._dataset = dataset\n    self._loader_arguments = kwargs\n\n    self._rng = self.rng(self._loader_arguments.get(\"random_state\"))\n    self._subset_test: Union[Subset, None] = None\n    self._subset_train: Union[Subset, None] = None\n    self._is_shuffled_testing_dataset: bool = (\n        False  # self._loader_arguments.get('shuffle_testing_dataset', False)\n    )\n    self.training_index: List[int] = []\n    self.testing_index: List[int] = []\n    self.test_ratio: Optional[float] = None\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager.dataset","title":"dataset  <code>property</code>","text":"<pre><code>dataset\n</code></pre> <p>Gets dataset.</p> <p>Returns:</p> Type Description <code>Dataset</code> <p>PyTorch dataset instance</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager.test_ratio","title":"test_ratio  <code>instance-attribute</code>","text":"<pre><code>test_ratio = None\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager.testing_index","title":"testing_index  <code>instance-attribute</code>","text":"<pre><code>testing_index = []\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager.training_index","title":"training_index  <code>instance-attribute</code>","text":"<pre><code>training_index = []\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager.load_all_samples","title":"load_all_samples","text":"<pre><code>load_all_samples()\n</code></pre> <p>Loading all samples as PyTorch DataLoader without splitting.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>Dataloader for entire datasets. <code>DataLoader</code> arguments will be retrieved from the <code>**kwargs</code> which is defined while initializing the class</p> Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>def load_all_samples(self) -&gt; DataLoader:\n    \"\"\"Loading all samples as PyTorch DataLoader without splitting.\n\n    Returns:\n        Dataloader for entire datasets. `DataLoader` arguments will be retrieved from the `**kwargs` which\n            is defined while initializing the class\n    \"\"\"\n    return self._create_torch_data_loader(self._dataset, **self._loader_arguments)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager.load_state","title":"load_state","text":"<pre><code>load_state(state)\n</code></pre> <p>Loads state of the data loader</p> <p>It currently keep only testing index, training index and test ratio as state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Dict</code> <p>Object containing data loader state.</p> required Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>def load_state(self, state: Dict):\n    \"\"\"Loads state of the data loader\n\n\n    It currently keep only testing index, training index and test ratio\n    as state.\n\n    Args:\n        state: Object containing data loader state.\n    \"\"\"\n    self.testing_index = state.get(\"testing_index\", [])\n    self.training_index = state.get(\"training_index\", [])\n    self.test_ratio = state.get(\"test_ratio\", None)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager.rng","title":"rng  <code>staticmethod</code>","text":"<pre><code>rng(rng=None, device=None)\n</code></pre> <p>Random number generator</p> <p>Returns:</p> Type Description <code>Union[None, Generator]</code> <p>None if rng is None else a torch generator.</p> Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>@staticmethod\ndef rng(\n    rng: Optional[int] = None,\n    device: Optional[str | torch.device] = None\n) -&gt; Union[None, torch.Generator]:\n    \"\"\"Random number generator\n\n    Returns:\n        None if rng is None else a torch generator.\n    \"\"\"\n\n    return None if rng is None else torch.Generator(device).manual_seed(rng)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager.save_state","title":"save_state","text":"<pre><code>save_state()\n</code></pre> <p>Gets state of the data loader.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>A Dict containing data loader state.</p> Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>def save_state(self) -&gt; Dict:\n    \"\"\"Gets state of the data loader.\n\n    Returns:\n        A Dict containing data loader state.\n    \"\"\"\n\n    data_manager_state = {}\n    data_manager_state[\"training_index\"] = self.training_index\n    data_manager_state[\"testing_index\"] = self.testing_index\n    data_manager_state[\"test_ratio\"] = self.test_ratio\n    return data_manager_state\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager.split","title":"split","text":"<pre><code>split(test_ratio, test_batch_size, is_shuffled_testing_dataset=False)\n</code></pre> <p>Splitting PyTorch Dataset into train and validation.</p> <p>Parameters:</p> Name Type Description Default <code>test_ratio</code> <code>float</code> <p>Split ratio for validation set ratio. Rest of the samples will be used for training</p> required <p>Raises:     FedbiomedTorchDataManagerError: If the ratio is not in good format</p> <p>Returns:</p> Name Type Description <code>train_loader</code> <code>Union[DataLoader, None]</code> <p>DataLoader for training subset. <code>None</code> if the <code>test_ratio</code> is <code>1</code></p> <code>test_loader</code> <code>Union[DataLoader, None]</code> <p>DataLoader for validation subset. <code>None</code> if the <code>test_ratio</code> is <code>0</code></p> Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>def split(\n    self,\n    test_ratio: float,\n    test_batch_size: Union[int, None],\n    is_shuffled_testing_dataset: bool = False,\n) -&gt; Tuple[Union[DataLoader, None], Union[DataLoader, None]]:\n    \"\"\"Splitting PyTorch Dataset into train and validation.\n\n    Args:\n         test_ratio: Split ratio for validation set ratio. Rest of the samples will be used for training\n    Raises:\n        FedbiomedTorchDataManagerError: If the ratio is not in good format\n\n    Returns:\n         train_loader: DataLoader for training subset. `None` if the `test_ratio` is `1`\n         test_loader: DataLoader for validation subset. `None` if the `test_ratio` is `0`\n    \"\"\"\n\n    # Check the argument `ratio` is of type `float`\n    if not isinstance(test_ratio, (float, int)):\n        raise FedbiomedTorchDataManagerError(\n            f\"{ErrorNumbers.FB608.value}: The argument `ratio` should be \"\n            f\"type `float` or `int` not {type(test_ratio)}\"\n        )\n\n    # Check ratio is valid for splitting\n    if test_ratio &lt; 0 or test_ratio &gt; 1:\n        raise FedbiomedTorchDataManagerError(\n            f\"{ErrorNumbers.FB608.value}: The argument `ratio` should be \"\n            f\"equal or between 0 and 1, not {test_ratio}\"\n        )\n\n    # If `Dataset` has proper data attribute\n    # try to get shape from self.data\n    if not hasattr(self._dataset, \"__len__\"):\n        raise FedbiomedTorchDataManagerError(\n            f\"{ErrorNumbers.FB608.value}: Can not get number of samples from \"\n            f\"{str(self._dataset)} without `__len__`.  Please make sure \"\n            f\"that `__len__` method has been added to custom dataset. \"\n            f\"This method should return total number of samples.\"\n        )\n\n    try:\n        samples = len(self._dataset)\n    except AttributeError as e:\n        raise FedbiomedTorchDataManagerError(\n            f\"{ErrorNumbers.FB608.value}: Can not get number of samples from \"\n            f\"{str(self._dataset)} due to undefined attribute, {str(e)}\"\n        )\n    except TypeError as e:\n        raise FedbiomedTorchDataManagerError(\n            f\"{ErrorNumbers.FB608.value}: Can not get number of samples from \"\n            f\"{str(self._dataset)}, {str(e)}\"\n        )\n\n    if self.test_ratio != test_ratio and self.test_ratio is not None:\n        if not is_shuffled_testing_dataset:\n            logger.info(\n                \"`test_ratio` value has changed: this will change the testing dataset\"\n            )\n        is_shuffled_testing_dataset = True\n    _is_loading_failed: bool = False\n    # Calculate number of samples for train and validation subsets\n    test_samples = math.floor(samples * test_ratio)\n    if self.testing_index and not is_shuffled_testing_dataset:\n        try:\n            self._load_indexes(self.training_index, self.testing_index)\n        except IndexError:\n            _is_loading_failed = True\n    if (\n        not self.testing_index or is_shuffled_testing_dataset\n    ) or _is_loading_failed:\n        train_samples = samples - test_samples\n\n        self._subset_train, self._subset_test = random_split(\n            self._dataset, [train_samples, test_samples], generator=self.rng()\n        )\n\n        self.testing_index = self._subset_test.indices\n        self.training_index = self._subset_train.indices\n\n    if not test_batch_size:\n\n        test_batch_size = len(self._subset_test)\n\n    self.test_ratio = test_ratio\n\n    loaders = (\n        self._subset_loader(self._subset_train, **self._loader_arguments),\n        self._subset_loader(self._subset_test, batch_size=test_batch_size),\n    )\n\n    return loaders\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager.subset_test","title":"subset_test","text":"<pre><code>subset_test()\n</code></pre> <p>Gets validation subset of the dataset.</p> <p>Returns:</p> Type Description <code>Subset</code> <p>Validation subset</p> Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>def subset_test(self) -&gt; Subset:\n    \"\"\"Gets validation subset of the dataset.\n\n    Returns:\n        Validation subset\n    \"\"\"\n\n    return self._subset_test\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager.subset_train","title":"subset_train","text":"<pre><code>subset_train()\n</code></pre> <p>Gets train subset of the dataset.</p> <p>Returns:</p> Type Description <code>Subset</code> <p>Train subset</p> Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>def subset_train(self) -&gt; Subset:\n    \"\"\"Gets train subset of the dataset.\n\n    Returns:\n        Train subset\n    \"\"\"\n    return self._subset_train\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager.to_sklearn","title":"to_sklearn","text":"<pre><code>to_sklearn()\n</code></pre> <p>Converts PyTorch <code>Dataset</code> to sklearn data manager of Fed-BioMed.</p> <p>Returns:</p> Type Description <code>SkLearnDataManager</code> <p>Data manager to use in SkLearn base training plans</p> Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>def to_sklearn(self) -&gt; SkLearnDataManager:\n    \"\"\"Converts PyTorch `Dataset` to sklearn data manager of Fed-BioMed.\n\n    Returns:\n        Data manager to use in SkLearn base training plans\n    \"\"\"\n\n    loader = self._create_torch_data_loader(\n        self._dataset, batch_size=len(self._dataset)\n    )\n    # Iterate over samples and get input variable and target variable\n    inputs = next(iter(loader))[0].numpy()\n    target = next(iter(loader))[1].numpy()\n    sklearn_data_manager = SkLearnDataManager(\n        inputs=inputs, target=target, **self._loader_arguments\n    )\n    sklearn_data_manager.testing_index = self.testing_index\n    sklearn_data_manager.training_index = self.training_index\n    sklearn_data_manager.test_ratio = self.test_ratio\n    return sklearn_data_manager\n</code></pre>"},{"location":"developer/api/common/db/","title":"DB","text":"<p>Interfaces with a tinyDB database for converting search results to dict.</p>"},{"location":"developer/api/common/db/#fedbiomed.common.db-classes","title":"Classes","text":""},{"location":"developer/api/common/db/#fedbiomed.common.db.DBTable","title":"DBTable","text":"<p>               Bases: <code>Table</code></p> <p>Extends TinyDB table to cast Document type to dict</p>"},{"location":"developer/api/common/db/#fedbiomed.common.db.DBTable-functions","title":"Functions","text":""},{"location":"developer/api/common/db/#fedbiomed.common.db.DBTable.all","title":"all","text":"<pre><code>all(*args, **kwargs)\n</code></pre> Source code in <code>fedbiomed/common/db.py</code> <pre><code>@cast_\ndef all(self, *args, **kwargs):\n    return super().all(*args, **kwargs)\n</code></pre>"},{"location":"developer/api/common/db/#fedbiomed.common.db.DBTable.get","title":"get","text":"<pre><code>get(*args, **kwargs)\n</code></pre> Source code in <code>fedbiomed/common/db.py</code> <pre><code>@cast_\ndef get(self, *args, **kwargs):\n    return super().get(*args, **kwargs)\n</code></pre>"},{"location":"developer/api/common/db/#fedbiomed.common.db.DBTable.search","title":"search","text":"<pre><code>search(*args, **kwargs)\n</code></pre> Source code in <code>fedbiomed/common/db.py</code> <pre><code>@cast_\ndef search(self, *args, **kwargs):\n    return super().search(*args, **kwargs)\n</code></pre>"},{"location":"developer/api/common/db/#fedbiomed.common.db-functions","title":"Functions","text":""},{"location":"developer/api/common/db/#fedbiomed.common.db.cast_","title":"cast_","text":"<pre><code>cast_(func)\n</code></pre> <p>Decorator function for typing casting</p> Source code in <code>fedbiomed/common/db.py</code> <pre><code>def cast_(func):\n    \"\"\"Decorator function for typing casting\"\"\"\n    # Wraps TinyDb get, all, search and upsert methods\n    def wrapped(*args, **kwargs):\n        add_docs = kwargs.get(\"add_docs\")\n        if add_docs is not None:\n            kwargs.pop(\"add_docs\")\n\n        document = func(*args, **kwargs)\n        if isinstance(document, list):\n            casted = [dict(r) for r in document]\n        elif isinstance(document, Document):\n            casted = dict(document)\n        else:\n            # Plain python type \n            casted = document\n\n        if add_docs:\n            return casted, document\n        else: \n            return casted\n\n    return wrapped\n</code></pre>"},{"location":"developer/api/common/exceptions/","title":"Exceptions","text":"<p>All the fedbiomed errors/Exceptions</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions-classes","title":"Classes","text":""},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedAggregatorError","title":"FedbiomedAggregatorError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception specific to the Aggregator classes/subclasses.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedCertificateError","title":"FedbiomedCertificateError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Certificate error</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedCommunicationError","title":"FedbiomedCommunicationError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Fedbiomed errors related to gRPC communication</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedConfigurationError","title":"FedbiomedConfigurationError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception specific to the Config classes.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDPControllerError","title":"FedbiomedDPControllerError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exceptions specific for the class DPController</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDataLoadingPlanError","title":"FedbiomedDataLoadingPlanError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exceptions specific for the class fedbiomed.common.data.DataLoadingPlan.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDataLoadingPlanValueError","title":"FedbiomedDataLoadingPlanValueError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exceptions similar to Value Error for a DataLoadingPlan.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDataManagerError","title":"FedbiomedDataManagerError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception for DataManager errors.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDatasetError","title":"FedbiomedDatasetError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Generic exception for a Dataset class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDatasetManagerError","title":"FedbiomedDatasetManagerError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exceptions specific for the class DatasetManager.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDatasetValueError","title":"FedbiomedDatasetValueError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>ValueErrors raised by any Dataset class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedError","title":"FedbiomedError","text":"<p>               Bases: <code>Exception</code></p> <p>Top class of all our exceptions.</p> <p>this allows to catch every Fedbiomed*Errors in a single except block</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedExperimentError","title":"FedbiomedExperimentError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception specific to the Experiment class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedFederatedDataSetError","title":"FedbiomedFederatedDataSetError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception specific to the FederatedDataSetError class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedLoadingBlockError","title":"FedbiomedLoadingBlockError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception specific to the DataLoadingBlock classes/subclasses.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedLoadingBlockValueError","title":"FedbiomedLoadingBlockValueError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception similar to ValueError for a DataLoadingBlock.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedLoggerError","title":"FedbiomedLoggerError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception specific to the Logger class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedMessageError","title":"FedbiomedMessageError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception specific to the Message class, usually a badly formed message.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedMessagingError","title":"FedbiomedMessagingError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception specific to the Messaging (communication) class.</p> <p>Usually a problem with the communication framework</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedMetricError","title":"FedbiomedMetricError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception raised when evualution fails because of inconsistence in using the metric.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedModelError","title":"FedbiomedModelError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exceptions triggered from Model class</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedNodeStateAgentError","title":"FedbiomedNodeStateAgentError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Error in Node State Agent</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedNodeStateManagerError","title":"FedbiomedNodeStateManagerError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Error in Node State Manager</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedNodeToNodeError","title":"FedbiomedNodeToNodeError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Error in Node to Node communications</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedOptimizerError","title":"FedbiomedOptimizerError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception raised when an error is encountered within <code>Optimizer</code> code.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedRoundError","title":"FedbiomedRoundError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exceptions specific for the node round class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedSecaggCrypterError","title":"FedbiomedSecaggCrypterError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Secure aggregation encryption error</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedSecaggError","title":"FedbiomedSecaggError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exceptions specific for the researcher secure aggregation class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedSecureAggregationError","title":"FedbiomedSecureAggregationError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Secure aggregation error</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedSilentTerminationError","title":"FedbiomedSilentTerminationError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception for silently terminating the researcher from a notebook.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedSkLearnDataManagerError","title":"FedbiomedSkLearnDataManagerError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exceptions specific for the class SkLearnDataset.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedStrategyError","title":"FedbiomedStrategyError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception specific to the Strategy class and subclasses.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedSynchroError","title":"FedbiomedSynchroError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Error in synchro objects</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedTaskQueueError","title":"FedbiomedTaskQueueError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception specific to the internal queuing system.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedTorchDataManagerError","title":"FedbiomedTorchDataManagerError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exceptions specific for the class TorchDataset.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedTrainingError","title":"FedbiomedTrainingError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception raised then training fails.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedTrainingPlanError","title":"FedbiomedTrainingPlanError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception specific to errors while getting source of the model class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedTrainingPlanSecurityManagerError","title":"FedbiomedTrainingPlanSecurityManagerError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception specific to the TrainingPlanSecurityManager.</p> <p>(from fedbiomed.common.model_manager)</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedTypeError","title":"FedbiomedTypeError","text":"<p>               Bases: <code>FedbiomedError</code>, <code>TypeError</code></p> <p>TypeError for Fed-BioMed</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedUserInputError","title":"FedbiomedUserInputError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Exception raised then user input is invalid.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedValueError","title":"FedbiomedValueError","text":"<p>               Bases: <code>FedbiomedError</code>, <code>ValueError</code></p> <p>ValueError for Fed-BioMed</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedVersionError","title":"FedbiomedVersionError","text":"<p>               Bases: <code>FedbiomedError</code></p> <p>Error in the versions of one of Fed-BioMed's components</p>"},{"location":"developer/api/common/ipython/","title":"IPython","text":""},{"location":"developer/api/common/ipython/#fedbiomed.common.ipython-functions","title":"Functions","text":""},{"location":"developer/api/common/ipython/#fedbiomed.common.ipython.is_ipython","title":"is_ipython","text":"<pre><code>is_ipython()\n</code></pre> <p>Function that checks whether the codes (function itself) is executed in ipython kernel or not</p> <p>Returns:</p> Type Description <code>bool</code> <p>True, if python interpreter is IPython</p> Source code in <code>fedbiomed/common/ipython.py</code> <pre><code>def is_ipython() -&gt; bool:\n    \"\"\"\n    Function that checks whether the codes (function itself) is executed in ipython kernel or not\n\n    Returns:\n        True, if python interpreter is IPython\n    \"\"\"\n\n    ipython_shells = ['ZMQInteractiveShell', 'TerminalInteractiveShell', 'Shell']\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell in ipython_shells:\n            return True\n        else:\n            return False\n    except NameError:\n        return False\n</code></pre>"},{"location":"developer/api/common/json/","title":"Json","text":"<p>This module defines message serializer and deserializer for sending / receiving / parsing messages through Messaging.</p> <p>Compared to the usual json module, it deals with some fedbiomed data types which are not serialized by default (eg: enumerations)</p>"},{"location":"developer/api/common/json/#fedbiomed.common.json-classes","title":"Classes","text":""},{"location":"developer/api/common/json/#fedbiomed.common.json-functions","title":"Functions","text":""},{"location":"developer/api/common/json/#fedbiomed.common.json.deserialize_msg","title":"deserialize_msg","text":"<pre><code>deserialize_msg(msg)\n</code></pre> <p>Deserializes a JSON string or bytes message as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Union[str, bytes]</code> <p>message in JSON format but encoded as string or bytes</p> required <p>Returns:     Parsed message as python dictionary.</p> Source code in <code>fedbiomed/common/json.py</code> <pre><code>def deserialize_msg(msg: Union[str, bytes]) -&gt; dict:\n    \"\"\"Deserializes a JSON string or bytes message as a dictionary.\n\n    Args:\n        msg: message in JSON format but encoded as string or bytes\n    Returns:\n        Parsed message as python dictionary.\n    \"\"\"\n    decode = json.loads(msg)\n\n    # deserialize our own types/classes\n    decode = _deserialize_test_metric(decode)\n\n    # errnum is present in ErrorMessage and is an Enum\n    # which need to be deserialized\n    if 'errnum' in decode:\n        errnum = decode['errnum']\n        found = False\n        for e in ErrorNumbers:\n            if e.value == errnum:\n                found = True\n                decode['errnum'] = e\n                break\n        if not found:\n            # error code sent by the node is unknown\n            decode['errnum'] = ErrorNumbers.FB999\n\n    return decode\n</code></pre>"},{"location":"developer/api/common/json/#fedbiomed.common.json.serialize_msg","title":"serialize_msg","text":"<pre><code>serialize_msg(msg)\n</code></pre> <p>Serialize an object as a JSON message (applies for dict-like objects)</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict</code> <p>dict-like object containing the message to send.</p> required <p>Returns:</p> Type Description <code>str</code> <p>JSON parsed message ready to transmit.</p> Source code in <code>fedbiomed/common/json.py</code> <pre><code>def serialize_msg(msg: dict) -&gt; str:\n    \"\"\"Serialize an object as a JSON message (applies for dict-like objects)\n\n    Args:\n        msg: dict-like object containing the message to send.\n\n    Returns:\n        JSON parsed message ready to transmit.\n    \"\"\"\n\n    # serialize our own types/classes\n    msg = _serialize_training_args(msg)\n    msg = _serialize_test_metric(msg)\n\n    # Errnum is present in ErrorMessage and is an Enum\n    # which need to be serialized\n\n    if 'errnum' in msg:\n        msg['errnum'] = msg['errnum'].value\n    return json.dumps(msg)\n</code></pre>"},{"location":"developer/api/common/logger/","title":"Logger","text":"<p>Global logger for fedbiomed</p> <p>Written above origin Logger class provided by python.</p> <p>Following features were added from to the original module:</p> <ul> <li>provides a logger instance of FedLogger, which is also a singleton, so it can be used \"as is\"</li> <li>provides a dedicated file handler</li> <li>provides a JSON/gRPC handler (this permit to send error messages from a node to a researcher)</li> <li>works on python scripts / ipython / notebook</li> <li>manages a dictionary of handlers. Default keys are 'CONSOLE', 'GRPC', 'FILE',   but any key is allowed (only one handler by key)</li> <li>allow changing log level globally, or on a specific handler (using its key)</li> <li>log levels can be provided as string instead of logging.* levels (no need to   import logging in caller's code) just as in the initial python logger</li> </ul> <p>A typical usage is:</p> <pre><code>from fedbiomed.common.logger import logger\n\nlogger.info(\"information message\")\n</code></pre> <p>All methods of the original python logger are provided. To name a few:</p> <ul> <li>logger.debug()</li> <li>logger.info()</li> <li>logger.warning()</li> <li>logger.error()</li> <li>logger.critical()</li> </ul> <p>Contrary to other Fed-BioMed classes, the API of FedLogger is compliant with the coding conventions used for logger (lowerCameCase)</p> <p>Dependency issue</p> <p>Please pay attention to not create dependency loop then importing other fedbiomed package</p>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger-attributes","title":"Attributes","text":""},{"location":"developer/api/common/logger/#fedbiomed.common.logger.DEFAULT_FORMAT","title":"DEFAULT_FORMAT  <code>module-attribute</code>","text":"<pre><code>DEFAULT_FORMAT = '%(asctime)s %(name)s %(levelname)s - %(message)s'\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.DEFAULT_LOG_FILE","title":"DEFAULT_LOG_FILE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_LOG_FILE = 'mylog.log'\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.DEFAULT_LOG_LEVEL","title":"DEFAULT_LOG_LEVEL  <code>module-attribute</code>","text":"<pre><code>DEFAULT_LOG_LEVEL = WARNING\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = FedLogger()\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger-classes","title":"Classes","text":""},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger","title":"FedLogger","text":"<pre><code>FedLogger(level=DEFAULT_LOG_LEVEL)\n</code></pre> <p>Base class for the logger.</p> <p>It uses python logging module by composition (only log() method is overwritten)</p> <p>All methods from the logging module can be accessed through the _logger member of the class if necessary (instead of overloading all the methods) (ex:  logger._logger.getEffectiveLevel() )</p> <p>Should not be imported</p> <p>An initial console logger is installed (so the logger has at minimum one handler)</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>initial loglevel. This loglevel will be the default for all handlers, if called without the default level</p> <code>DEFAULT_LOG_LEVEL</code> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def __init__(self, level: str = DEFAULT_LOG_LEVEL):\n    \"\"\"Constructor of base class\n\n    An initial console logger is installed (so the logger has at minimum one handler)\n\n    Args:\n        level: initial loglevel. This loglevel will be the default for all handlers, if called\n            without the default level\n\n\n    \"\"\"\n\n    # internal tables\n    # transform string to logging.level\n    self._nameToLevel = {\n        \"DEBUG\": logging.DEBUG,\n        \"INFO\": logging.INFO,\n        \"WARNING\": logging.WARNING,\n        \"ERROR\": logging.ERROR,\n        \"CRITICAL\": logging.CRITICAL,\n    }\n\n    # transform logging.level to string\n    self._levelToName = {\n        logging.DEBUG: \"DEBUG\",\n        logging.INFO: \"INFO\",\n        logging.WARNING: \"WARNING\",\n        logging.ERROR: \"ERROR\",\n        logging.CRITICAL: \"CRITICAL\"\n    }\n\n    # name this logger\n    self._logger = logging.getLogger(\"fedbiomed\")\n\n    # Do not propagate (avoids log duplication when third party libraries uses logging module)\n    self._logger.propagate = False\n\n    self._default_level = DEFAULT_LOG_LEVEL  # MANDATORY ! KEEP THIS PLEASE !!!\n    self._default_level = self._internal_level_translator(level)\n\n    self._logger.setLevel(self._default_level)\n\n    # init the handlers list and add a console handler on startup\n    self._handlers = {}\n    self.add_console_handler()\n\n    pass\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger-functions","title":"Functions","text":""},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.add_console_handler","title":"add_console_handler","text":"<pre><code>add_console_handler(format=DEFAULT_FORMAT, level=DEFAULT_LOG_LEVEL)\n</code></pre> <p>Adds a console handler</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>str</code> <p>the format string of the logger</p> <code>DEFAULT_FORMAT</code> <code>level</code> <code>Any</code> <p>initial level of the logger for this handler (optional) if not given, the default level is set</p> <code>DEFAULT_LOG_LEVEL</code> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def add_console_handler(self,\n                        format: str = DEFAULT_FORMAT,\n                        level: Any = DEFAULT_LOG_LEVEL):\n\n    \"\"\"Adds a console handler\n\n    Args:\n        format: the format string of the logger\n        level: initial level of the logger for this handler (optional) if not given, the default level is set\n    \"\"\"\n    if is_ipython():\n        handler = _IpythonConsoleHandler()\n    else:\n        handler = logging.StreamHandler()\n\n    handler.setLevel(self._internal_level_translator(level))\n\n    formatter = logging.Formatter(format)\n    handler.setFormatter(formatter)\n    self._internal_add_handler(\"CONSOLE\", handler)\n\n    pass\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.add_file_handler","title":"add_file_handler","text":"<pre><code>add_file_handler(filename=DEFAULT_LOG_FILE, format=DEFAULT_FORMAT, level=DEFAULT_LOG_LEVEL)\n</code></pre> <p>Adds a file handler</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>File to log to</p> <code>DEFAULT_LOG_FILE</code> <code>format</code> <code>str</code> <p>Log format</p> <code>DEFAULT_FORMAT</code> <code>level</code> <code>any</code> <p>Initial level of the logger</p> <code>DEFAULT_LOG_LEVEL</code> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def add_file_handler(\n        self,\n        filename: str = DEFAULT_LOG_FILE,\n        format: str = DEFAULT_FORMAT,\n        level: any = DEFAULT_LOG_LEVEL):\n    \"\"\"Adds a file handler\n\n    Args:\n        filename: File to log to\n        format: Log format\n        level: Initial level of the logger\n    \"\"\"\n\n    handler = logging.FileHandler(filename=filename, mode='a')\n    handler.setLevel(self._internal_level_translator(level))\n\n    formatter = logging.Formatter(format)\n    handler.setFormatter(formatter)\n\n    self._internal_add_handler(\"FILE\", handler)\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.add_grpc_handler","title":"add_grpc_handler","text":"<pre><code>add_grpc_handler(on_log=None, node_id=None, level=logging.INFO)\n</code></pre> <p>Adds a gRPC handler, to publish error message on a topic</p> <p>Parameters:</p> Name Type Description Default <code>on_log</code> <code>Callable</code> <p>Provided by higher level GRPC implementation</p> <code>None</code> <code>node_id</code> <code>str</code> <p>id of the caller (necessary for msg formatting to the researcher)</p> <code>None</code> <code>level</code> <code>Any</code> <p>level of this handler (non-mandatory) level must be lower than ERROR to ensure that the research get all ERROR/CRITICAL messages</p> <code>INFO</code> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def add_grpc_handler(self,\n                     on_log: Callable = None,\n                     node_id: str = None,\n                     level: Any = logging.INFO\n                     ):\n\n    \"\"\"Adds a gRPC handler, to publish error message on a topic\n\n    Args:\n        on_log: Provided by higher level GRPC implementation\n        node_id: id of the caller (necessary for msg formatting to the researcher)\n        level: level of this handler (non-mandatory) level must be lower than ERROR to ensure that the\n            research get all ERROR/CRITICAL messages\n    \"\"\"\n\n    handler = _GrpcHandler(\n        on_log=on_log,\n        node_id=node_id,\n    )\n\n    # may be not necessary ?\n    handler.setLevel(self._internal_level_translator(level))\n    formatter = _GrpcFormatter(node_id)\n\n    handler.setFormatter(formatter)\n    self._internal_add_handler(\"GRPC\", handler)\n\n    # as a side effect this will set the minimal level to ERROR\n    self.setLevel(level, \"GRPC\")\n\n    pass\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.critical","title":"critical","text":"<pre><code>critical(msg, *args, broadcast=False, researcher_id=None, **kwargs)\n</code></pre> <p>Same as info message</p> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def critical(self, msg, *args, broadcast=False, researcher_id=None, **kwargs):\n    \"\"\"Same as info message\"\"\"\n    self._logger.critical(msg, *args, **kwargs,\n                          extra={\"researcher_id\": researcher_id, 'broadcast': broadcast})\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.debug","title":"debug","text":"<pre><code>debug(msg, *args, broadcast=False, researcher_id=None, **kwargs)\n</code></pre> <p>Same as info message</p> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def debug(self, msg, *args, broadcast=False, researcher_id=None, **kwargs):\n    \"\"\"Same as info message\"\"\"\n    self._logger.debug(msg, *args, **kwargs,\n                       extra={\"researcher_id\": researcher_id, 'broadcast': broadcast})\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.error","title":"error","text":"<pre><code>error(msg, *args, broadcast=False, researcher_id=None, **kwargs)\n</code></pre> <p>Same as info message</p> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def error(self, msg, *args, broadcast=False, researcher_id=None, **kwargs):\n    \"\"\"Same as info message\"\"\"\n    self._logger.error(msg, *args, **kwargs,\n                       extra={\"researcher_id\": researcher_id, 'broadcast': broadcast})\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.info","title":"info","text":"<pre><code>info(msg, *args, broadcast=False, researcher_id=None, **kwargs)\n</code></pre> <p>Extends arguments of info message.</p> <p>Valid only GrpcHandler is existing</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <p>Message to log</p> required <code>broadcast</code> <p>Broadcast message to all available researchers</p> <code>False</code> <code>researcher_id</code> <p>ID of the researcher that the message will be sent. If broadcast True researcher id will be ignored</p> <code>None</code> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def info(self, msg, *args, broadcast=False, researcher_id=None, **kwargs):\n    \"\"\"Extends arguments of info message.\n\n    Valid only GrpcHandler is existing\n\n    Args:\n        msg: Message to log\n        broadcast: Broadcast message to all available researchers\n        researcher_id: ID of the researcher that the message will be sent.\n            If broadcast True researcher id will be ignored\n    \"\"\"\n    self._logger.info(msg, *args, **kwargs,\n                      extra={\"researcher_id\": researcher_id, 'broadcast': broadcast})\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.log","title":"log","text":"<pre><code>log(level, msg)\n</code></pre> <p>Overrides the logging.log() method to allow the use of string instead of a logging.* level</p> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def log(self, level: Any, msg: str):\n    \"\"\"Overrides the logging.log() method to allow the use of string instead of a logging.* level \"\"\"\n\n    level = logger._internal_level_translator(level)\n    self._logger.log(\n        level,\n        msg\n    )\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.setLevel","title":"setLevel","text":"<pre><code>setLevel(level, htype=None)\n</code></pre> <p>Overrides the setLevel method, to deal with level given as a string and to change le level of one or all known handlers</p> <p>This also change the default level for all future handlers.</p> <p>Remark</p> <p>Level should not be lower than CRITICAL (meaning CRITICAL errors are always displayed)</p> <p>Example: <pre><code>setLevel( logging.DEBUG, 'FILE')\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>level</code> <p>level to modify, can be a string or a logging.* level (mandatory)</p> required <code>htype</code> <p>if provided (non-mandatory), change the level of the given handler. if not  provided (or None), change the level of all known handlers</p> <code>None</code> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def setLevel(self, level: Any, htype: Any = None):\n    \"\"\"Overrides the setLevel method, to deal with level given as a string and to change le level of\n    one or all known handlers\n\n    This also change the default level for all future handlers.\n\n    !!! info \"Remark\"\n\n        Level should not be lower than CRITICAL (meaning CRITICAL errors are always displayed)\n\n        Example:\n        ```python\n        setLevel( logging.DEBUG, 'FILE')\n        ```\n\n    Args:\n        level : level to modify, can be a string or a logging.* level (mandatory)\n        htype : if provided (non-mandatory), change the level of the given handler. if not  provided (or None),\n            change the level of all known handlers\n    \"\"\"\n\n    level = self._internal_level_translator(level)\n\n    if htype is None:\n        # store this level (for future handler adding)\n        self._logger.setLevel(level)\n\n        for h in self._handlers:\n            self._handlers[h].setLevel(level)\n        return\n\n    if htype in self._handlers:\n        self._handlers[htype].setLevel(level)\n        return\n\n    # htype provided but no handler for this type exists\n    self._logger.warning(htype + \" handler not initialized yet\")\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.warning","title":"warning","text":"<pre><code>warning(msg, *args, broadcast=False, researcher_id=None, **kwargs)\n</code></pre> <p>Same as info message</p> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def warning(self, msg, *args, broadcast=False, researcher_id=None, **kwargs):\n    \"\"\"Same as info message\"\"\"\n    self._logger.warning(msg, *args, **kwargs,\n                         extra={\"researcher_id\": researcher_id, 'broadcast': broadcast})\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger-functions","title":"Functions","text":""},{"location":"developer/api/common/message/","title":"Message","text":"<p>Definition of messages exchanged by the researcher and the nodes</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message-classes","title":"Classes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.AdditiveSSSetupReply","title":"AdditiveSSSetupReply  <code>dataclass</code>","text":"<pre><code>AdditiveSSSetupReply(*, protocol_version=str(__messaging_protocol_version__), request_id=None, researcher_id, secagg_id, success, node_id, msg, share)\n</code></pre> <p>               Bases: <code>SecaggReply</code></p> <p>Message that instantiated on the node side to reply secagg setup request from researcher</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AdditiveSSSetupReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.AdditiveSSSetupReply.share","title":"share  <code>instance-attribute</code>","text":"<pre><code>share\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AdditiveSSSetupRequest","title":"AdditiveSSSetupRequest  <code>dataclass</code>","text":"<pre><code>AdditiveSSSetupRequest(*, protocol_version=str(__messaging_protocol_version__), request_id=None, researcher_id, secagg_id, element, experiment_id, parties)\n</code></pre> <p>               Bases: <code>SecaggRequest</code></p> <p>Message to request secure aggregation setup from researcher to nodes</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AdditiveSSharingReply","title":"AdditiveSSharingReply  <code>dataclass</code>","text":"<pre><code>AdditiveSSharingReply(secagg_id, share, *, protocol_version=str(__messaging_protocol_version__), node_id, dest_node_id, request_id=None)\n</code></pre> <p>               Bases: <code>InnerRequestReply</code>, <code>RequiresProtocolVersion</code></p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AdditiveSSharingReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.AdditiveSSharingReply.secagg_id","title":"secagg_id  <code>instance-attribute</code>","text":"<pre><code>secagg_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AdditiveSSharingReply.share","title":"share  <code>instance-attribute</code>","text":"<pre><code>share\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AdditiveSSharingRequest","title":"AdditiveSSharingRequest  <code>dataclass</code>","text":"<pre><code>AdditiveSSharingRequest(secagg_id, *, protocol_version=str(__messaging_protocol_version__), node_id, dest_node_id, request_id=None)\n</code></pre> <p>               Bases: <code>InnerRequestReply</code>, <code>RequiresProtocolVersion</code></p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AdditiveSSharingRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.AdditiveSSharingRequest.secagg_id","title":"secagg_id  <code>instance-attribute</code>","text":"<pre><code>secagg_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply","title":"ApprovalReply  <code>dataclass</code>","text":"<pre><code>ApprovalReply(researcher_id, training_plan_id, message, node_id, status, success, *, protocol_version=str(__messaging_protocol_version__), request_id=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes the TrainingPlan approval reply (acknoledge) from node to researcher.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that will receive the reply</p> <code>training_plan_id</code> <code>str | None</code> <p>Unique training plan identifier, can be none in case of success false.</p> <code>message</code> <code>str</code> <p>currently unused (empty string)</p> <code>node_id</code> <code>str</code> <p>Node id that replys the request</p> <code>status</code> <code>int</code> <p>status code for the request (obsolete, always 0)</p> <code>success</code> <code>bool</code> <p>Request was successfully sumbitted to node (not yet approved)</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply.success","title":"success  <code>instance-attribute</code>","text":"<pre><code>success\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply.training_plan_id","title":"training_plan_id  <code>instance-attribute</code>","text":"<pre><code>training_plan_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalRequest","title":"ApprovalRequest  <code>dataclass</code>","text":"<pre><code>ApprovalRequest(researcher_id, description, training_plan, *, protocol_version=str(__messaging_protocol_version__), request_id=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes the TrainingPlan approval request from researcher to node.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>id of the researcher that sends the request</p> <code>description</code> <code>str</code> <p>description of the training plan</p> <code>training_plan</code> <code>str</code> <p>The training plan that is going to be checked for approval</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalRequest.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalRequest.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalRequest.training_plan","title":"training_plan  <code>instance-attribute</code>","text":"<pre><code>training_plan\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ChannelSetupReply","title":"ChannelSetupReply  <code>dataclass</code>","text":"<pre><code>ChannelSetupReply(public_key, *, protocol_version=str(__messaging_protocol_version__), node_id, dest_node_id, request_id=None)\n</code></pre> <p>               Bases: <code>InnerRequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Message for reply peer node key for securing a n2n channel.</p> <p>Attributes:</p> Name Type Description <code>public_key</code> <code>bytes</code> <p>public key of replying node</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ChannelSetupReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.ChannelSetupReply.public_key","title":"public_key  <code>instance-attribute</code>","text":"<pre><code>public_key\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ChannelSetupRequest","title":"ChannelSetupRequest  <code>dataclass</code>","text":"<pre><code>ChannelSetupRequest(*, protocol_version=str(__messaging_protocol_version__), node_id, dest_node_id, request_id=None)\n</code></pre> <p>               Bases: <code>InnerRequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Message for requesting peer node key for securing a n2n channel.</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ErrorMessage","title":"ErrorMessage  <code>dataclass</code>","text":"<pre><code>ErrorMessage(*, protocol_version=str(__messaging_protocol_version__), request_id=None, researcher_id, node_id, extra_msg, errnum=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes an error message sent by the node.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that receives the error message</p> <code>node_id</code> <code>str</code> <p>ID of the node that sends error message</p> <code>errnum</code> <code>Optional[str]</code> <p>Error ID/Number</p> <code>extra_msg</code> <code>str</code> <p>Additional message regarding the error</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ErrorMessage-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.ErrorMessage.errnum","title":"errnum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>errnum = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ErrorMessage.extra_msg","title":"extra_msg  <code>instance-attribute</code>","text":"<pre><code>extra_msg\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ErrorMessage.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ErrorMessage.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.FeedbackMessage","title":"FeedbackMessage  <code>dataclass</code>","text":"<pre><code>FeedbackMessage(researcher_id=None, log=None, scalar=None, *, protocol_version=str(__messaging_protocol_version__))\n</code></pre> <p>               Bases: <code>ProtoSerializableMessage</code>, <code>RequiresProtocolVersion</code></p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.FeedbackMessage-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.FeedbackMessage.log","title":"log  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.FeedbackMessage.researcher_id","title":"researcher_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>researcher_id = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.FeedbackMessage.scalar","title":"scalar  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scalar = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.InnerMessage","title":"InnerMessage  <code>dataclass</code>","text":"<pre><code>InnerMessage(*, node_id, dest_node_id)\n</code></pre> <p>               Bases: <code>Message</code></p> <p>Parent class of messages sent from node to node.</p> <p>Node to node messages are sent as inner message (payload) of an overlay message</p> <p>Attributes:</p> Name Type Description <code>node_id</code> <code>str</code> <p>Id of the source node sending the mess</p> <code>dest_node_id</code> <code>str</code> <p>Id of the destination node of the overlay message</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.InnerMessage-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.InnerMessage.dest_node_id","title":"dest_node_id  <code>instance-attribute</code>","text":"<pre><code>dest_node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.InnerMessage.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.InnerRequestReply","title":"InnerRequestReply  <code>dataclass</code>","text":"<pre><code>InnerRequestReply(*, node_id, dest_node_id, request_id=None)\n</code></pre> <p>               Bases: <code>InnerMessage</code></p> <p>Common attribute for Request and Reply Inner Message.</p> <p>Attributes:</p> Name Type Description <code>request_id</code> <code>Optional[str]</code> <p>unique ID for this request-reply</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.InnerRequestReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.InnerRequestReply.request_id","title":"request_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>request_id = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.KeyReply","title":"KeyReply  <code>dataclass</code>","text":"<pre><code>KeyReply(public_key, secagg_id, *, protocol_version=str(__messaging_protocol_version__), node_id, dest_node_id, request_id=None)\n</code></pre> <p>               Bases: <code>InnerRequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Message for continuing an exchange for creating crypto key material.</p> <p>Currently only Diffie-Hellman key exchange is supported</p> <p>Attributes:</p> Name Type Description <code>public_key</code> <code>bytes</code> <p>public key of replying node</p> <code>secagg_id</code> <code>str</code> <p>unique ID of this secagg context element</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.KeyReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.KeyReply.public_key","title":"public_key  <code>instance-attribute</code>","text":"<pre><code>public_key\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.KeyReply.secagg_id","title":"secagg_id  <code>instance-attribute</code>","text":"<pre><code>secagg_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.KeyRequest","title":"KeyRequest  <code>dataclass</code>","text":"<pre><code>KeyRequest(secagg_id, *, protocol_version=str(__messaging_protocol_version__), node_id, dest_node_id, request_id=None)\n</code></pre> <p>               Bases: <code>InnerRequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Message for starting a new exchange for creating crypto key material.</p> <p>Currently only Diffie-Hellman key exchange is supported</p> <p>Attributes:</p> Name Type Description <code>secagg_id</code> <code>str</code> <p>unique ID of this secagg context element</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.KeyRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.KeyRequest.secagg_id","title":"secagg_id  <code>instance-attribute</code>","text":"<pre><code>secagg_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply","title":"ListReply  <code>dataclass</code>","text":"<pre><code>ListReply(researcher_id, success, databases, node_id, count, *, protocol_version=str(__messaging_protocol_version__), request_id=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>This class describes a list reply message sent by the node that includes list of datasets. It is a reply for ListRequest message from the researcher.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that sends the request</p> <code>succes</code> <code>str</code> <p>True if the node process the request as expected, false if any exception occurs</p> <code>databases</code> <code>list</code> <p>List of datasets</p> <code>node_id</code> <code>str</code> <p>Node id that replys the request</p> <code>count</code> <code>int</code> <p>Number of datasets</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply.count","title":"count  <code>instance-attribute</code>","text":"<pre><code>count\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply.databases","title":"databases  <code>instance-attribute</code>","text":"<pre><code>databases\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply.success","title":"success  <code>instance-attribute</code>","text":"<pre><code>success\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListRequest","title":"ListRequest  <code>dataclass</code>","text":"<pre><code>ListRequest(researcher_id, *, protocol_version=str(__messaging_protocol_version__), request_id=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a list request message sent by the researcher to nodes in order to list datasets belonging to each node.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that sends the request</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.ListRequest.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Log","title":"Log  <code>dataclass</code>","text":"<pre><code>Log(node_id, level, msg)\n</code></pre> <p>               Bases: <code>ProtoSerializableMessage</code></p> <p>Describes the message type for log coming from node to researcher</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Log-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.Log.level","title":"level  <code>instance-attribute</code>","text":"<pre><code>level\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Log.msg","title":"msg  <code>instance-attribute</code>","text":"<pre><code>msg\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Log.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Message","title":"Message","text":"<p>Base class for all fedbiomed messages providing all methods to access the messages</p> <p>The subclass of this class will be pure data containers (no provided functions)</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Message-functions","title":"Functions","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.Message.from_dict","title":"from_dict  <code>staticmethod</code>","text":"<pre><code>from_dict(obj)\n</code></pre> <p>De-serializes the message</p> Source code in <code>fedbiomed/common/message.py</code> <pre><code>@staticmethod\ndef from_dict(obj: Dict):\n    \"\"\"De-serializes the message\"\"\"\n\n    message = {**obj}\n\n    if (\n        \"__type_message__\" not in message\n        or not isinstance(message[\"__type_message__\"], dict)\n        or not all(\n            x_ in message[\"__type_message__\"].keys() for x_ in [\"class\", \"module\"]\n        )\n    ):\n        raise FedbiomedValueError(\n            \"Message does not include valid '__type_message__' entry.\"\n        )\n\n    type_ = message[\"__type_message__\"]\n    cls_ = import_object(\"fedbiomed.common.message\", type_[\"class\"])\n\n    if not inspect.isclass(cls_) or not issubclass(cls_, Message):\n        raise FedbiomedMessageError(\n            \"Given object class is not subclass of 'Message'\"\n        )\n\n    raise_for_version_compatibility(\n        message[\"protocol_version\"], __messaging_protocol_version__\n    )\n\n    message.pop(\"__type_message__\")\n\n    return cls_(**message)\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Message.from_proto","title":"from_proto  <code>classmethod</code>","text":"<pre><code>from_proto(proto)\n</code></pre> <p>Converts given protobuf to python Dict</p> Source code in <code>fedbiomed/common/message.py</code> <pre><code>@classmethod\ndef from_proto(cls, proto: ProtobufMessage) -&gt; Dict[str, Any]:\n    \"\"\"Converts given protobuf to python Dict\"\"\"\n\n    dict_ = {}\n    one_ofs = proto.DESCRIPTOR.oneofs_by_name\n    for field in proto.DESCRIPTOR.fields:\n\n        one_of_field = False\n        for one_of, _ in one_ofs.items():\n            if field.name == proto.WhichOneof(one_of):\n                one_of_field = True\n\n        # If the field is oneof and options are in message type\n        if one_of_field and field.type == FieldDescriptor.TYPE_MESSAGE:\n\n            field_ = cls.__dataclass_fields__[field.name]\n            args = get_args(field_.type)\n\n            # Make sure oneof message is typed as Optional\n            if not args:\n                raise FedbiomedMessageError(\n                    f\"Please make sure the field '{field_.name}' in dataclass '{cls.__name__}' \"\n                    \"is typed as Optional[&lt;dataclass&gt;]. The field that are typed as `oneof` \"\n                    \"in proto file should be typed as Optional in python dataclass\"\n                )\n\n            if not hasattr(args[0], \"__PROTO_TYPE__\"):\n                raise FedbiomedMessageError(\n                    f\"Dataclass {args[0]} should have attribute '__PROTO_TYPE__'\"\n                )\n\n            dict_.update(\n                {field.name: args[0].from_proto(getattr(proto, field.name))}\n            )\n\n        # Detects the types that are declared as `optional`\n        # NOTE: In proto3 all fields are labeled as `LABEL_OPTIONAL` by default.\n        # However, if the field is labeled as `optional` explicitly, it will have\n        # presence, otherwise, `has_presence` returns False\n        elif field.has_presence and field.label == FieldDescriptor.LABEL_OPTIONAL:\n\n            # If proto has the field it means that the value is not None\n            if proto.HasField(field.name):\n                dict_.update({field.name: getattr(proto, field.name)})\n\n        elif field.label == FieldDescriptor.LABEL_REPEATED:\n\n            if field.type == FieldDescriptor.TYPE_MESSAGE:\n                dict_.update({field.name: dict(getattr(proto, field.name))})\n            else:\n                dict_.update({field.name: list(getattr(proto, field.name))})\n\n        else:\n            dict_.update({field.name: getattr(proto, field.name)})\n\n    return cls(**dict_)\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Message.get_dict","title":"get_dict","text":"<pre><code>get_dict()\n</code></pre> <p>Returns pairs (Message class attributes name, attributes values) into a dictionary</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Message as dictionary</p> Source code in <code>fedbiomed/common/message.py</code> <pre><code>def get_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns pairs (Message class attributes name, attributes values) into a dictionary\n\n    Returns:\n        Message as dictionary\n    \"\"\"\n    return {**self.__dict__}\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Message.get_param","title":"get_param","text":"<pre><code>get_param(param)\n</code></pre> <p>Get the value of a given param</p> <p>Parameters:</p> Name Type Description Default <code>param</code> <code>str</code> <p>name of the param</p> required Source code in <code>fedbiomed/common/message.py</code> <pre><code>def get_param(self, param: str):\n    \"\"\"Get the value of a given param\n\n    Args:\n        param: name of the param\n    \"\"\"\n    return getattr(self, param, None)\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Message.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Serializes the message</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Serializes data class into a dict</p> Source code in <code>fedbiomed/common/message.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serializes the message\n\n    Returns:\n        Serializes data class into a dict\n    \"\"\"\n    class_ = type(self).__name__\n    module_ = type(self).__module__\n\n    return {\n        **self.get_dict(),\n        \"__type_message__\": {\"module\": module_, \"class\": class_},\n    }\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Message.to_proto","title":"to_proto","text":"<pre><code>to_proto()\n</code></pre> <p>Converts recursively python dataclass to gRPC proto</p> Source code in <code>fedbiomed/common/message.py</code> <pre><code>def to_proto(self):\n    \"\"\"Converts recursively python dataclass to gRPC proto\"\"\"\n\n    proto_dict = {}\n    for key, _ in self.__dataclass_fields__.items():\n        param = self.get_param(key)\n        if hasattr(param, \"__PROTO_TYPE__\"):\n            proto_dict.update({key: self.get_param(key).to_proto()})\n        else:\n            proto_dict.update({key: self.get_param(key)})\n\n    return self.__PROTO_TYPE__(**proto_dict)\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.OverlayMessage","title":"OverlayMessage  <code>dataclass</code>","text":"<pre><code>OverlayMessage(*, protocol_version=str(__messaging_protocol_version__), researcher_id, node_id, dest_node_id, overlay, setup, salt, nonce)\n</code></pre> <p>               Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Message for handling overlay trafic.</p> <p>Same message used from source node to researcher, and from researcher to destination node.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher relaying the overlay message</p> <code>node_id</code> <code>str</code> <p>Id of the source node of the overlay message</p> <code>dest_node_id</code> <code>str</code> <p>Id of the destination node of the overlay message</p> <code>overlay</code> <code>bytes</code> <p>payload of the message to be forwarded unchanged to the destination node</p> <code>setup</code> <code>bool</code> <p>True if this is a channel setup message, False if this is an application layer message</p> <code>salt</code> <code>bytes</code> <p>value used for salting the key derivation for this message</p> <code>nonce</code> <code>bytes</code> <p>value used for noncing the encryption for this message</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.OverlayMessage-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.OverlayMessage.dest_node_id","title":"dest_node_id  <code>instance-attribute</code>","text":"<pre><code>dest_node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.OverlayMessage.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.OverlayMessage.nonce","title":"nonce  <code>instance-attribute</code>","text":"<pre><code>nonce\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.OverlayMessage.overlay","title":"overlay  <code>instance-attribute</code>","text":"<pre><code>overlay\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.OverlayMessage.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.OverlayMessage.salt","title":"salt  <code>instance-attribute</code>","text":"<pre><code>salt\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.OverlayMessage.setup","title":"setup  <code>instance-attribute</code>","text":"<pre><code>setup\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingReply","title":"PingReply  <code>dataclass</code>","text":"<pre><code>PingReply(researcher_id, node_id, *, protocol_version=str(__messaging_protocol_version__), request_id=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>This class describes a ping message sent by the node.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that will receive the reply</p> <code>node_id</code> <code>str</code> <p>Node id that replys the request</p> <code>succes</code> <code>str</code> <p>True if the node process the request as expected, false if any exception occurs</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.PingReply.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingReply.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingRequest","title":"PingRequest  <code>dataclass</code>","text":"<pre><code>PingRequest(researcher_id, *, protocol_version=str(__messaging_protocol_version__), request_id=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a ping message sent by the researcher</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that send ping request</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.PingRequest.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ProtoSerializableMessage","title":"ProtoSerializableMessage  <code>dataclass</code>","text":"<pre><code>ProtoSerializableMessage()\n</code></pre> <p>               Bases: <code>Message</code></p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.RequestReply","title":"RequestReply  <code>dataclass</code>","text":"<pre><code>RequestReply(*, request_id=None)\n</code></pre> <p>               Bases: <code>Message</code></p> <p>Common attribute for Request and Reply Message.</p> <p>Attributes:</p> Name Type Description <code>request_id</code> <code>Optional[str]</code> <p>unique ID for this request-reply</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.RequestReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.RequestReply.request_id","title":"request_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>request_id = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.RequiresProtocolVersion","title":"RequiresProtocolVersion  <code>dataclass</code>","text":"<pre><code>RequiresProtocolVersion(*, protocol_version=str(__messaging_protocol_version__))\n</code></pre> <p>Mixin class for messages that must be endowed with a version field.</p> <p>Attributes:</p> Name Type Description <code>protocol_version</code> <code>str</code> <p>version of the messaging protocol used</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.RequiresProtocolVersion-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.RequiresProtocolVersion.protocol_version","title":"protocol_version  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>protocol_version = str(__messaging_protocol_version__)\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar","title":"Scalar  <code>dataclass</code>","text":"<pre><code>Scalar(node_id, experiment_id, train, test, test_on_local_updates, test_on_global_updates, metric, total_samples, batch_samples, num_batches, iteration, epoch=None, num_samples_trained=None)\n</code></pre> <p>               Bases: <code>ProtoSerializableMessage</code></p> <p>Describes a add_scalar message sent by the node.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <p>ID of the researcher that receives the reply</p> <code>experiment_id</code> <code>str</code> <p>ID of the experiment that is sent by researcher</p> <code>train</code> <code>bool</code> <p>Declares whether scalar value is for training</p> <code>test</code> <code>bool</code> <p>Declares whether scalar value is for validation</p> <code>test_on_local_updates</code> <code>bool</code> <p>Declares whether validation is performed over locally updated parameters</p> <code>test_on_global_updates</code> <code>bool</code> <p>Declares whether validation is performed over aggregated parameters</p> <code>metric</code> <code>dict</code> <p>Evaluation metroc</p> <code>epoch</code> <code>Optional[int]</code> <p>Scalar is received at</p> <code>total_samples</code> <code>int</code> <p>Number of all samples in dataset</p> <code>batch_samples</code> <code>int</code> <p>Number of samples in batch</p> <code>num_batches</code> <code>int</code> <p>Number of batches in single epoch</p> <code>iteration</code> <code>int</code> <p>Scalar is received at</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar.batch_samples","title":"batch_samples  <code>instance-attribute</code>","text":"<pre><code>batch_samples\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar.epoch","title":"epoch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>epoch = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar.experiment_id","title":"experiment_id  <code>instance-attribute</code>","text":"<pre><code>experiment_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar.iteration","title":"iteration  <code>instance-attribute</code>","text":"<pre><code>iteration\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar.metric","title":"metric  <code>instance-attribute</code>","text":"<pre><code>metric\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar.num_batches","title":"num_batches  <code>instance-attribute</code>","text":"<pre><code>num_batches\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar.num_samples_trained","title":"num_samples_trained  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_samples_trained = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar.test","title":"test  <code>instance-attribute</code>","text":"<pre><code>test\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar.test_on_global_updates","title":"test_on_global_updates  <code>instance-attribute</code>","text":"<pre><code>test_on_global_updates\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar.test_on_local_updates","title":"test_on_local_updates  <code>instance-attribute</code>","text":"<pre><code>test_on_local_updates\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar.total_samples","title":"total_samples  <code>instance-attribute</code>","text":"<pre><code>total_samples\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Scalar.train","title":"train  <code>instance-attribute</code>","text":"<pre><code>train\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchReply","title":"SearchReply  <code>dataclass</code>","text":"<pre><code>SearchReply(researcher_id, databases, node_id, count, *, protocol_version=str(__messaging_protocol_version__), request_id=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a search message sent by the node</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that sends the request</p> <code>succes</code> <code>str</code> <p>True if the node process the request as expected, false if any exception occurs</p> <code>databases</code> <code>list</code> <p>List of datasets</p> <code>node_id</code> <code>str</code> <p>Node id that replys the request</p> <code>count</code> <code>int</code> <p>Number of datasets</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchReply.count","title":"count  <code>instance-attribute</code>","text":"<pre><code>count\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchReply.databases","title":"databases  <code>instance-attribute</code>","text":"<pre><code>databases\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchReply.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchReply.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchRequest","title":"SearchRequest  <code>dataclass</code>","text":"<pre><code>SearchRequest(researcher_id, tags, *, protocol_version=str(__messaging_protocol_version__), request_id=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a search message sent by the researcher.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that sends the request</p> <code>tags</code> <code>list</code> <p>Tags for search request</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchRequest.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchRequest.tags","title":"tags  <code>instance-attribute</code>","text":"<pre><code>tags\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply","title":"SecaggDeleteReply  <code>dataclass</code>","text":"<pre><code>SecaggDeleteReply(*, protocol_version=str(__messaging_protocol_version__), request_id=None, researcher_id, secagg_id, success, node_id, msg=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a secagg context element delete reply message sent by the node</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that requests deletion</p> <code>secagg_id</code> <code>str</code> <p>ID of secagg context element that is sent by researcher</p> <code>success</code> <code>bool</code> <p>True if the node process the request as expected, false if any exception occurs</p> <code>node_id</code> <code>str</code> <p>Node id that replies to the request</p> <code>msg</code> <code>Optional[str]</code> <p>Custom message</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply.msg","title":"msg  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>msg = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply.secagg_id","title":"secagg_id  <code>instance-attribute</code>","text":"<pre><code>secagg_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply.success","title":"success  <code>instance-attribute</code>","text":"<pre><code>success\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteRequest","title":"SecaggDeleteRequest  <code>dataclass</code>","text":"<pre><code>SecaggDeleteRequest(researcher_id, secagg_id, element, experiment_id, *, protocol_version=str(__messaging_protocol_version__), request_id=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a secagg context element delete request message sent by the researcher</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that requests deletion</p> <code>secagg_id</code> <code>str</code> <p>ID of secagg context element that is sent by researcher</p> <code>element</code> <code>int</code> <p>Type of secagg context element</p> <code>experiment_id</code> <code>Optional[str]</code> <p>Id of the experiment to which this secagg context element is attached</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteRequest.element","title":"element  <code>instance-attribute</code>","text":"<pre><code>element\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteRequest.experiment_id","title":"experiment_id  <code>instance-attribute</code>","text":"<pre><code>experiment_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteRequest.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteRequest.secagg_id","title":"secagg_id  <code>instance-attribute</code>","text":"<pre><code>secagg_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply","title":"SecaggReply  <code>dataclass</code>","text":"<pre><code>SecaggReply(*, protocol_version=str(__messaging_protocol_version__), request_id=None, researcher_id, secagg_id, success, node_id, msg)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a secagg context element setup reply message sent by the node</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that requests setup</p> <code>secagg_id</code> <code>str</code> <p>ID of secagg context element that is sent by researcher</p> <code>success</code> <code>bool</code> <p>True if the node process the request as expected, false if any exception occurs</p> <code>node_id</code> <code>str</code> <p>Node id that replies to the request</p> <code>msg</code> <code>str</code> <p>Custom message</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply.msg","title":"msg  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>msg\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply.secagg_id","title":"secagg_id  <code>instance-attribute</code>","text":"<pre><code>secagg_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply.success","title":"success  <code>instance-attribute</code>","text":"<pre><code>success\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest","title":"SecaggRequest  <code>dataclass</code>","text":"<pre><code>SecaggRequest(*, protocol_version=str(__messaging_protocol_version__), request_id=None, researcher_id, secagg_id, element, experiment_id, parties)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a secagg context element setup request message sent by the researcher</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that requests setup</p> <code>secagg_id</code> <code>str</code> <p>ID of secagg context element that is sent by researcher</p> <code>element</code> <code>int</code> <p>Type of secagg context element</p> <code>experiment_id</code> <code>str</code> <p>Id of the experiment to which this secagg context element is attached</p> <code>parties</code> <code>list</code> <p>List of parties participating to the secagg context element setup</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest.element","title":"element  <code>instance-attribute</code>","text":"<pre><code>element\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest.experiment_id","title":"experiment_id  <code>instance-attribute</code>","text":"<pre><code>experiment_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest.parties","title":"parties  <code>instance-attribute</code>","text":"<pre><code>parties\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest.secagg_id","title":"secagg_id  <code>instance-attribute</code>","text":"<pre><code>secagg_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TaskRequest","title":"TaskRequest  <code>dataclass</code>","text":"<pre><code>TaskRequest(node, *, protocol_version=str(__messaging_protocol_version__))\n</code></pre> <p>               Bases: <code>ProtoSerializableMessage</code>, <code>RequiresProtocolVersion</code></p> <p>Task request message from node to researcher</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TaskRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.TaskRequest.node","title":"node  <code>instance-attribute</code>","text":"<pre><code>node\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TaskResponse","title":"TaskResponse  <code>dataclass</code>","text":"<pre><code>TaskResponse(size, iteration, bytes_)\n</code></pre> <p>               Bases: <code>ProtoSerializableMessage</code></p> <p>Response for task request</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TaskResponse-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.TaskResponse.bytes_","title":"bytes_  <code>instance-attribute</code>","text":"<pre><code>bytes_\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TaskResponse.iteration","title":"iteration  <code>instance-attribute</code>","text":"<pre><code>iteration\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TaskResponse.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TaskResult","title":"TaskResult  <code>dataclass</code>","text":"<pre><code>TaskResult(size, iteration, bytes_)\n</code></pre> <p>               Bases: <code>ProtoSerializableMessage</code></p> <p>Response for task request</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TaskResult-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.TaskResult.bytes_","title":"bytes_  <code>instance-attribute</code>","text":"<pre><code>bytes_\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TaskResult.iteration","title":"iteration  <code>instance-attribute</code>","text":"<pre><code>iteration\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TaskResult.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply","title":"TrainReply  <code>dataclass</code>","text":"<pre><code>TrainReply(researcher_id, experiment_id, success, node_id, dataset_id, timing, msg, state_id=None, sample_size=None, encrypted=False, params=None, optimizer_args=None, optim_aux_var=None, encryption_factor=None, *, protocol_version=str(__messaging_protocol_version__), request_id=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a train message sent by the node.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that receives the reply</p> <code>experiment_id</code> <code>str</code> <p>Id of the experiment that is sent by researcher</p> <code>success</code> <code>bool</code> <p>True if the node process the request as expected, false if any exception occurs</p> <code>node_id</code> <code>str</code> <p>Node id that replies the request</p> <code>dataset_id</code> <code>str</code> <p>id of the dataset that is used for training</p> <code>params_url</code> <code>str</code> <p>URL of parameters uploaded by node</p> <code>timing</code> <code>dict</code> <p>Timing statistics</p> <code>msg</code> <code>str</code> <p>Custom message</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.dataset_id","title":"dataset_id  <code>instance-attribute</code>","text":"<pre><code>dataset_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.encrypted","title":"encrypted  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>encrypted = False\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.encryption_factor","title":"encryption_factor  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>encryption_factor = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.experiment_id","title":"experiment_id  <code>instance-attribute</code>","text":"<pre><code>experiment_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.msg","title":"msg  <code>instance-attribute</code>","text":"<pre><code>msg\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.optim_aux_var","title":"optim_aux_var  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>optim_aux_var = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.optimizer_args","title":"optimizer_args  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>optimizer_args = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.params","title":"params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>params = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.sample_size","title":"sample_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sample_size = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.state_id","title":"state_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>state_id = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.success","title":"success  <code>instance-attribute</code>","text":"<pre><code>success\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.timing","title":"timing  <code>instance-attribute</code>","text":"<pre><code>timing\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest","title":"TrainRequest  <code>dataclass</code>","text":"<pre><code>TrainRequest(researcher_id, experiment_id, state_id, training_args, dataset_id, training, model_args, params, training_plan, training_plan_class, round, aggregator_args, secagg_arguments=None, optim_aux_var=None, *, protocol_version=str(__messaging_protocol_version__), request_id=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a train message sent by the researcher</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that requests training</p> <code>experiment_id</code> <code>str</code> <p>Id of the experiment that is sent by researcher</p> <code>state_id</code> <code>Optional[str]</code> <p>ID of state associated to this request on node</p> <code>training_args</code> <code>dict</code> <p>Arguments for training routine</p> <code>dataset_id</code> <code>str</code> <p>id of the dataset that is used for training</p> <code>training</code> <code>bool</code> <p>Declares whether training will be performed</p> <code>model_args</code> <code>dict</code> <p>Arguments to initialize training plan class</p> <code>training_plan</code> <code>str</code> <p>Source code of training plan</p> <code>training_plan_class</code> <code>str</code> <p>Class name of the training plan</p> <code>round</code> <code>int</code> <p>number of rounds already executed for this experiment</p> <code>aggregator_args</code> <code>Dict</code> <p>??</p> <code>aux_var</code> <code>Dict</code> <p>Optimizer auxiliary variables</p> <code>optim_aux_var</code> <code>Optional[Dict]</code> <p>Optional dict of Optimizer auxiliary variables</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.aggregator_args","title":"aggregator_args  <code>instance-attribute</code>","text":"<pre><code>aggregator_args\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.dataset_id","title":"dataset_id  <code>instance-attribute</code>","text":"<pre><code>dataset_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.experiment_id","title":"experiment_id  <code>instance-attribute</code>","text":"<pre><code>experiment_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.model_args","title":"model_args  <code>instance-attribute</code>","text":"<pre><code>model_args\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.optim_aux_var","title":"optim_aux_var  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>optim_aux_var = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.round","title":"round  <code>instance-attribute</code>","text":"<pre><code>round\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.secagg_arguments","title":"secagg_arguments  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>secagg_arguments = None\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.state_id","title":"state_id  <code>instance-attribute</code>","text":"<pre><code>state_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.training","title":"training  <code>instance-attribute</code>","text":"<pre><code>training\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.training_args","title":"training_args  <code>instance-attribute</code>","text":"<pre><code>training_args\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.training_plan","title":"training_plan  <code>instance-attribute</code>","text":"<pre><code>training_plan\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.training_plan_class","title":"training_plan_class  <code>instance-attribute</code>","text":"<pre><code>training_plan_class\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply","title":"TrainingPlanStatusReply  <code>dataclass</code>","text":"<pre><code>TrainingPlanStatusReply(researcher_id, node_id, experiment_id, success, approval_obligation, status, msg, training_plan, training_plan_id, *, protocol_version=str(__messaging_protocol_version__), request_id=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a training plan approve status check message sent by the node</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that sends the request</p> <code>node_id</code> <code>str</code> <p>Node id that replies the request</p> <code>experiment_id</code> <code>str</code> <p>experiment id related to the experiment</p> <code>success</code> <code>bool</code> <p>True if the node process the request as expected, false if any exception occurs</p> <code>approval_obligation</code> <p>Approval mode for node. True, if training plan approval is enabled/required in the node for training.</p> <code>status</code> <code>str</code> <p>a <code>TrainingPlanApprovalStatus</code> value describing the approval status</p> <code>msg</code> <code>str</code> <p>Message from node based on state of the reply</p> <code>training_plan</code> <code>str</code> <p>The training plan that has been checked for approval</p> <code>training_plan_id</code> <code>Optional[str]</code> <p>Unique training plan identifier, can be None in case of success false.</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.approval_obligation","title":"approval_obligation  <code>instance-attribute</code>","text":"<pre><code>approval_obligation\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.experiment_id","title":"experiment_id  <code>instance-attribute</code>","text":"<pre><code>experiment_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.msg","title":"msg  <code>instance-attribute</code>","text":"<pre><code>msg\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.success","title":"success  <code>instance-attribute</code>","text":"<pre><code>success\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.training_plan","title":"training_plan  <code>instance-attribute</code>","text":"<pre><code>training_plan\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.training_plan_id","title":"training_plan_id  <code>instance-attribute</code>","text":"<pre><code>training_plan_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusRequest","title":"TrainingPlanStatusRequest  <code>dataclass</code>","text":"<pre><code>TrainingPlanStatusRequest(researcher_id, experiment_id, training_plan, *, protocol_version=str(__messaging_protocol_version__), request_id=None)\n</code></pre> <p>               Bases: <code>RequestReply</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a training plan approve status check message sent by the researcher.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that sends the request</p> <code>experiment_id</code> <code>str</code> <p>experiment id related to the experiment.</p> <code>training_plan_url</code> <code>str</code> <p>The training plan that is going to be checked for approval</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusRequest.experiment_id","title":"experiment_id  <code>instance-attribute</code>","text":"<pre><code>experiment_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusRequest.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusRequest.training_plan","title":"training_plan  <code>instance-attribute</code>","text":"<pre><code>training_plan\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message-functions","title":"Functions","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.catch_dataclass_exception","title":"catch_dataclass_exception","text":"<pre><code>catch_dataclass_exception(cls)\n</code></pre> <p>Encapsulates the init() method of dataclass in order to transform the exceptions sent by the dataclass (TypeError) into our own exception (FedbiomedMessageError)</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Callable</code> <p>Dataclass to validate</p> required Source code in <code>fedbiomed/common/message.py</code> <pre><code>def catch_dataclass_exception(cls: Callable):\n    \"\"\"Encapsulates the __init__() method of dataclass in order to transform the exceptions sent\n    by the dataclass (TypeError) into our own exception (FedbiomedMessageError)\n\n    Args:\n        cls: Dataclass to validate\n    \"\"\"\n\n    def __cde_init__(self: Any, *args: list, **kwargs: dict):\n        \"\"\"This is the __init__() replacement.\n\n        Its purpose is to catch the TypeError created by the __init__\n        method of the @dataclass decorator and replace this exception by  FedbiomedMessageError\n\n        Raises:\n          FedbiomedMessageError if number/type of arguments is wrong\n        \"\"\"\n\n        try:\n            self.__class__.__dict__[\"__initial_init__\"](self, *args, **kwargs)\n\n        except TypeError as e:\n            # this is the error raised by dataclass if number of parameter is wrong\n            _msg = ErrorNumbers.FB601.value + \": bad number of parameters: \" + str(e)\n            logger.error(_msg)\n            raise FedbiomedMessageError(_msg) from e\n\n    @functools.wraps(cls)\n    def wrap(cls: Callable):\n        \"\"\"Wrapper to the class given as parameter\n\n        Class wrapping should keep some attributes (__doc__, etc) of the initial class\n        or the API documentation tools will be mistaken\n\n        \"\"\"\n        cls.__initial_init__ = cls.__init__\n        setattr(cls, \"__init__\", __cde_init__)\n\n        return cls\n\n    return wrap(cls)\n</code></pre>"},{"location":"developer/api/common/metrics/","title":"Metrics","text":"<p>Provide test metrics, both MetricTypes to use in TrainingArgs but also calculation routines.</p>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics-attributes","title":"Attributes","text":""},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics-classes","title":"Classes","text":""},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes","title":"MetricTypes","text":"<pre><code>MetricTypes(idx, metric_category)\n</code></pre> <p>               Bases: <code>_BaseEnum</code></p> <p>List of Performance metrics used to evaluate the model.</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>def __init__(self, idx: int, metric_category: _MetricCategory) -&gt; None:\n    self._idx = idx\n    self._metric_category = metric_category\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes-attributes","title":"Attributes","text":""},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.ACCURACY","title":"ACCURACY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ACCURACY = (0, CLASSIFICATION_LABELS)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.EXPLAINED_VARIANCE","title":"EXPLAINED_VARIANCE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EXPLAINED_VARIANCE = (6, REGRESSION)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.F1_SCORE","title":"F1_SCORE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>F1_SCORE = (1, CLASSIFICATION_LABELS)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.MEAN_ABSOLUTE_ERROR","title":"MEAN_ABSOLUTE_ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEAN_ABSOLUTE_ERROR = (5, REGRESSION)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.MEAN_SQUARE_ERROR","title":"MEAN_SQUARE_ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEAN_SQUARE_ERROR = (4, REGRESSION)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.PRECISION","title":"PRECISION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PRECISION = (2, CLASSIFICATION_LABELS)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.RECALL","title":"RECALL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RECALL = (3, CLASSIFICATION_LABELS)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes-functions","title":"Functions","text":""},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.get_all_metrics","title":"get_all_metrics  <code>staticmethod</code>","text":"<pre><code>get_all_metrics()\n</code></pre> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef get_all_metrics() -&gt; List[str]:\n    return [metric.name for metric in MetricTypes]\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.get_metric_type_by_name","title":"get_metric_type_by_name  <code>staticmethod</code>","text":"<pre><code>get_metric_type_by_name(metric_name)\n</code></pre> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef get_metric_type_by_name(metric_name: str):\n    for metric in MetricTypes:\n        if metric.name == metric_name:\n            return metric\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.metric_category","title":"metric_category","text":"<pre><code>metric_category()\n</code></pre> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>def metric_category(self) -&gt; _MetricCategory:\n    return self._metric_category\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics","title":"Metrics","text":"<pre><code>Metrics()\n</code></pre> <p>               Bases: <code>object</code></p> <p>Class of performance metrics used in validation evaluation.</p> Attrs <p>metrics: Provided metrics in form of <code>{ MetricTypes : skleran.metrics }</code></p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>def __init__(self):\n    \"\"\"Constructs metric class with provided metric types: metric function\n\n    Attrs:\n        metrics: Provided metrics in form of `{ MetricTypes : skleran.metrics }`\n    \"\"\"\n\n    self.metrics = {\n        MetricTypes.ACCURACY.name: self.accuracy,\n        MetricTypes.PRECISION.name: self.precision,\n        MetricTypes.RECALL.name: self.recall,\n        MetricTypes.F1_SCORE.name: self.f1_score,\n        MetricTypes.MEAN_SQUARE_ERROR.name: self.mse,\n        MetricTypes.MEAN_ABSOLUTE_ERROR.name: self.mae,\n        MetricTypes.EXPLAINED_VARIANCE.name: self.explained_variance,\n    }\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics-attributes","title":"Attributes","text":""},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.metrics","title":"metrics  <code>instance-attribute</code>","text":"<pre><code>metrics = {name: accuracy, name: precision, name: recall, name: f1_score, name: mse, name: mae, name: explained_variance}\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics-functions","title":"Functions","text":""},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.accuracy","title":"accuracy  <code>staticmethod</code>","text":"<pre><code>accuracy(y_true, y_pred, **kwargs)\n</code></pre> <p>Evaluate the accuracy score</p> <p>Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[ndarray, list]</code> <p>Predicted values</p> required <code>**kwargs</code> <code>dict</code> <p>Extra arguments from <code>sklearn.metrics.accuracy_score</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>Accuracy score</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>raised if above sklearn method raises</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef accuracy(y_true: Union[np.ndarray, list],\n             y_pred: Union[np.ndarray, list],\n             **kwargs: dict) -&gt; float:\n    \"\"\" Evaluate the accuracy score\n\n    Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        **kwargs: Extra arguments from [`sklearn.metrics.accuracy_score`][sklearn.metrics.accuracy_score]\n\n    Returns:\n        Accuracy score\n\n    Raises:\n        FedbiomedMetricError: raised if above sklearn method raises\n    \"\"\"\n\n    try:\n        y_true, y_pred, _, _ = Metrics._configure_multiclass_parameters(y_true, y_pred, kwargs, 'ACCURACY')\n        return metrics.accuracy_score(y_true, y_pred, **kwargs)\n    except Exception as e:\n        msg = ErrorNumbers.FB611.value + \" Exception raised from SKLEARN metrics: \" + str(e)\n        raise FedbiomedMetricError(msg)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.evaluate","title":"evaluate","text":"<pre><code>evaluate(y_true, y_pred, metric, **kwargs)\n</code></pre> <p>Perform evaluation based on given metric.</p> <p>This method configures given y_pred and y_true to make them compatible with default evaluation methods.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[ndarray, list]</code> <p>Predicted values</p> required <code>metric</code> <code>MetricTypes</code> <p>An instance of MetricTypes to chose metric that will be used for evaluation</p> required <code>**kwargs</code> <code>dict</code> <p>The arguments specifics to each type of metrics.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[int, float]</code> <p>Result of the evaluation function</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>in case of invalid metric, y_true and y_pred types</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>def evaluate(self,\n             y_true: Union[np.ndarray, list],\n             y_pred: Union[np.ndarray, list],\n             metric: MetricTypes,\n             **kwargs: dict) -&gt; Union[int, float]:\n    \"\"\"Perform evaluation based on given metric.\n\n    This method configures given y_pred and y_true to make them compatible with default evaluation methods.\n\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        metric: An instance of MetricTypes to chose metric that will be used for evaluation\n        **kwargs: The arguments specifics to each type of metrics.\n\n    Returns:\n        Result of the evaluation function\n\n    Raises:\n        FedbiomedMetricError: in case of invalid metric, y_true and y_pred types\n    \"\"\"\n    if not isinstance(metric, MetricTypes):\n        raise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: Metric should instance of `MetricTypes`\")\n\n    if y_true is not None and not isinstance(y_true, (np.ndarray, list)):\n        raise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: The argument `y_true` should an instance \"\n                                   f\"of `np.ndarray`, but got {type(y_true)} \")\n\n    if y_pred is not None and not isinstance(y_pred, (np.ndarray, list)):\n        raise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: The argument `y_pred` should an instance \"\n                                   f\"of `np.ndarray`, but got {type(y_pred)} \")\n\n    y_true, y_pred = self._configure_y_true_pred_(y_true=y_true, y_pred=y_pred, metric=metric)\n    result = self.metrics[metric.name](y_true, y_pred, **kwargs)\n\n    return result\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.explained_variance","title":"explained_variance  <code>staticmethod</code>","text":"<pre><code>explained_variance(y_true, y_pred, **kwargs)\n</code></pre> <p>Evaluate the Explained variance regression score.</p> <p>Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html]</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[ndarray, list]</code> <p>Predicted values</p> required <code>**kwargs</code> <code>dict</code> <p>Extra arguments from <code>sklearn.metrics.explained_variance_score</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>EV score (float or ndarray of floats)</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>raised if above sklearn method for computing precision raises</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef explained_variance(y_true: Union[np.ndarray, list],\n                       y_pred: Union[np.ndarray, list],\n                       **kwargs: dict) -&gt; float:\n    \"\"\"Evaluate the Explained variance regression score.\n\n    Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html]\n\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        **kwargs: Extra arguments from [`sklearn.metrics.explained_variance_score`]\n            [sklearn.metrics.explained_variance_score]\n\n    Returns:\n        EV score (float or ndarray of floats)\n\n    Raises:\n        FedbiomedMetricError: raised if above sklearn method for computing precision raises\n    \"\"\"\n\n    # Set multiouput as raw_values is it is not defined by researcher\n    if len(y_true.shape) &gt; 1:\n        multi_output = kwargs.get('multioutput', 'raw_values')\n    else:\n        multi_output = None\n\n    kwargs.pop('multioutput', None)\n\n    try:\n        return metrics.explained_variance_score(y_true, y_pred, multioutput=multi_output, **kwargs)\n    except Exception as e:\n        raise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: Error during calculation of `EXPLAINED_VARIANCE`\"\n                                   f\" {str(e)}\")\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.f1_score","title":"f1_score  <code>staticmethod</code>","text":"<pre><code>f1_score(y_true, y_pred, **kwargs)\n</code></pre> <p>Evaluate the F1 score.</p> <p>Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[ndarray, list]</code> <p>Predicted values</p> required <code>**kwargs</code> <code>dict</code> <p>Extra arguments from <code>sklearn.metrics.f1_score</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>f1_score (float or array of float, shape = [n_unique_labels])</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>raised if above sklearn method for computing precision raises</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef f1_score(y_true: Union[np.ndarray, list],\n             y_pred: Union[np.ndarray, list],\n             **kwargs: dict) -&gt; float:\n    \"\"\"Evaluate the F1 score.\n\n    Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        **kwargs: Extra arguments from [`sklearn.metrics.f1_score`][sklearn.metrics.f1_score]\n\n    Returns:\n        f1_score (float or array of float, shape = [n_unique_labels])\n\n    Raises:\n        FedbiomedMetricError: raised if above sklearn method for computing precision raises\n    \"\"\"\n\n    # Get average and pob_label argument based on multiclass status\n    y_true, y_pred, average, pos_label = Metrics._configure_multiclass_parameters(y_true,\n                                                                                  y_pred,\n                                                                                  kwargs,\n                                                                                  'F1_SCORE')\n\n    kwargs.pop(\"average\", None)\n    kwargs.pop(\"pos_label\", None)\n\n    try:\n        return metrics.f1_score(y_true, y_pred, average=average, pos_label=pos_label, **kwargs)\n    except Exception as e:\n        raise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: Error during calculation of `F1_SCORE` {str(e)}\")\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.mae","title":"mae  <code>staticmethod</code>","text":"<pre><code>mae(y_true, y_pred, **kwargs)\n</code></pre> <p>Evaluate the mean absolute error.</p> <p>Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[ndarray, list]</code> <p>Predicted values</p> required <code>**kwargs</code> <code>dict</code> <p>Extra arguments from <code>sklearn.metrics.mean_absolute_error</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>MAE score (float or ndarray of floats)</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>raised if above sklearn method for computing precision raises</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef mae(y_true: Union[np.ndarray, list],\n        y_pred: Union[np.ndarray, list],\n        **kwargs: dict) -&gt; float:\n    \"\"\"Evaluate the mean absolute error.\n\n    Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html\n\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        **kwargs: Extra arguments from [`sklearn.metrics.mean_absolute_error`][sklearn.metrics.mean_absolute_error]\n\n    Returns:\n        MAE score (float or ndarray of floats)\n\n    Raises:\n        FedbiomedMetricError: raised if above sklearn method for computing precision raises\n    \"\"\"\n    # Set multiouput as raw_values is it is not defined by researcher\n    if len(y_true.shape) &gt; 1:\n        multi_output = kwargs.get('multioutput', 'raw_values')\n    else:\n        multi_output = None\n\n    kwargs.pop('multioutput', None)\n\n    try:\n        return metrics.mean_absolute_error(y_true, y_pred, multioutput=multi_output, **kwargs)\n    except Exception as e:\n        raise FedbiomedMetricError(\n            f\"{ErrorNumbers.FB611.value}: Error during calculation of `MEAN_ABSOLUTE_ERROR`\"\n            f\" {str(e)}\") from e\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.mse","title":"mse  <code>staticmethod</code>","text":"<pre><code>mse(y_true, y_pred, **kwargs)\n</code></pre> <p>Evaluate the mean squared error.</p> <p>Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[ndarray, list]</code> <p>Predicted values</p> required <code>**kwargs</code> <code>dict</code> <p>Extra arguments from <code>sklearn.metrics.mean_squared_error</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>MSE score (float or ndarray of floats)</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>raised if above sklearn method for computing precision raises</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef mse(y_true: Union[np.ndarray, list],\n        y_pred: Union[np.ndarray, list],\n        **kwargs: dict) -&gt; float:\n    \"\"\"Evaluate the mean squared error.\n\n    Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        **kwargs: Extra arguments from [`sklearn.metrics.mean_squared_error`][sklearn.metrics.mean_squared_error]\n\n    Returns:\n        MSE score (float or ndarray of floats)\n\n    Raises:\n        FedbiomedMetricError: raised if above sklearn method for computing precision raises\n    \"\"\"\n\n    # Set multiouput as raw_values is it is not defined by researcher\n    if len(y_true.shape) &gt; 1:\n        multi_output = kwargs.get('multioutput', 'raw_values')\n    else:\n        multi_output = None\n\n    kwargs.pop('multioutput', None)\n\n    try:\n        return metrics.mean_squared_error(y_true, y_pred, multioutput=multi_output, **kwargs)\n    except Exception as e:\n        raise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: Error during calculation of `MEAN_SQUARED_ERROR`\"\n                                   f\" {str(e)}\")\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.precision","title":"precision  <code>staticmethod</code>","text":"<pre><code>precision(y_true, y_pred, **kwargs)\n</code></pre> <p>Evaluate the precision score [source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html]</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[ndarray, list]</code> <p>Predicted values</p> required <code>**kwargs</code> <code>dict</code> <p>Extra arguments from <code>sklearn.metrics.precision_score</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>precision (float, or array of float of shape (n_unique_labels,))</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>raised if above sklearn method for computing precision raises</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef precision(y_true: Union[np.ndarray, list],\n              y_pred: Union[np.ndarray, list],\n              **kwargs: dict) -&gt; float:\n    \"\"\"Evaluate the precision score\n    [source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html]\n\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        **kwargs: Extra arguments from [`sklearn.metrics.precision_score`][sklearn.metrics.precision_score]\n\n    Returns:\n        precision (float, or array of float of shape (n_unique_labels,))\n\n    Raises:\n        FedbiomedMetricError: raised if above sklearn method for computing precision raises\n    \"\"\"\n    # Get average and pob_label argument based on multiclass status\n    y_true, y_pred, average, pos_label = Metrics._configure_multiclass_parameters(y_true,\n                                                                                  y_pred,\n                                                                                  kwargs,\n                                                                                  'PRECISION')\n\n    kwargs.pop(\"average\", None)\n    kwargs.pop(\"pos_label\", None)\n\n    try:\n        return metrics.precision_score(y_true, y_pred, average=average, pos_label=pos_label, **kwargs)\n    except Exception as e:\n        raise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: Error during calculation of `PRECISION` \"\n                                   f\"calculation: {str(e)}\")\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.recall","title":"recall  <code>staticmethod</code>","text":"<pre><code>recall(y_true, y_pred, **kwargs)\n</code></pre> <p>Evaluate the recall. [source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html]</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[ndarray, list]</code> <p>Predicted values</p> required <code>**kwargs</code> <code>dict</code> <p>Extra arguments from <code>sklearn.metrics.recall_score</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>recall (float (if average is not None) or array of float of shape (n_unique_labels,))</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>raised if above sklearn method for computing precision raises</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef recall(y_true: Union[np.ndarray, list],\n           y_pred: Union[np.ndarray, list],\n           **kwargs: dict) -&gt; float:\n    \"\"\"Evaluate the recall.\n    [source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html]\n\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        **kwargs: Extra arguments from [`sklearn.metrics.recall_score`][sklearn.metrics.recall_score]\n\n    Returns:\n        recall (float (if average is not None) or array of float of shape (n_unique_labels,))\n\n    Raises:\n        FedbiomedMetricError: raised if above sklearn method for computing precision raises\n    \"\"\"\n\n    # Get average and pob_label argument based on multiclass status\n    y_true, y_pred, average, pos_label = Metrics._configure_multiclass_parameters(y_true, y_pred, kwargs, 'RECALL')\n\n    kwargs.pop(\"average\", None)\n    kwargs.pop(\"pos_label\", None)\n\n    try:\n        return metrics.recall_score(y_true, y_pred, average=average, pos_label=pos_label, **kwargs)\n    except Exception as e:\n        raise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: Error during calculation of `RECALL` \"\n                                   f\"calculation: {str(e)}\")\n</code></pre>"},{"location":"developer/api/common/models/","title":"Model","text":"<p>The <code>fedbiomed.common.models</code> module includes model abstraction classes that can be used with plain framework specific models.</p> <p>Please visit Declearn repository for the \"TorchVector\" and \"NumpyVector\" classes used in this module.</p>"},{"location":"developer/api/common/models/#fedbiomed.common.models-classes","title":"Classes","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel","title":"BaseSkLearnModel","text":"<pre><code>BaseSkLearnModel(model)\n</code></pre> <p>               Bases: <code>Model</code></p> <p>Wrapper of Scikit learn models.</p> <p>This class implements all abstract methods from the <code>Model</code> API, but adds some scikit-learn-specific ones that need implementing by its children.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>BaseEstimator</code> <p>Wrapped model</p> <code>param_list</code> <code>List[str]</code> <p>List that contains layer attributes. Should be set when calling <code>set_init_params</code> method</p> <p>Class attributes:</p> Name Type Description <code>is_classification</code> <code>bool</code> <p>Boolean flag indicating whether the wrapped model is designed for classification or for regression supervised-learning tasks.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseEstimator</code> <p>Model object as an instance of BaseEstimator</p> required <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>if model is not as scikit learn BaseEstimator object</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def __init__(\n    self,\n    model: BaseEstimator,\n) -&gt; None:\n    \"\"\"Instantiate the wrapper over a scikit-learn BaseEstimator.\n\n    Args:\n        model: Model object as an instance of [BaseEstimator][sklearn.base.BaseEstimator]\n\n    Raises:\n        FedbiomedModelError: if model is not as scikit learn [BaseEstimator][sklearn.base.BaseEstimator] object\n    \"\"\"\n    super().__init__(model)\n    self._gradients: Dict[str, np.ndarray] = {}\n    self.param_list: List[str] = []\n    self._optim_params: Dict[str, Any] = {}\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel-attributes","title":"Attributes","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.is_classification","title":"is_classification  <code>class-attribute</code>","text":"<pre><code>is_classification\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.param_list","title":"param_list  <code>instance-attribute</code>","text":"<pre><code>param_list = []\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel-functions","title":"Functions","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.apply_updates","title":"apply_updates","text":"<pre><code>apply_updates(updates)\n</code></pre> <p>Apply incoming updates to the wrapped model's parameters.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>Dict[str, ndarray]</code> <p>Model parameters' updates to add (apply) to existing parameters' values.</p> required Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def apply_updates(\n    self,\n    updates: Dict[str, np.ndarray],\n) -&gt; None:\n    \"\"\"Apply incoming updates to the wrapped model's parameters.\n\n    Args:\n        updates: Model parameters' updates to add (apply) to existing\n            parameters' values.\n    \"\"\"\n    self._assert_dict_inputs(updates)\n    for key, val in updates.items():\n        weights = getattr(self.model, key)\n        setattr(self.model, key, weights + val)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.disable_internal_optimizer","title":"disable_internal_optimizer","text":"<pre><code>disable_internal_optimizer()\n</code></pre> <p>Disable the scikit-learn internal optimizer.</p> <p>Calling this method alters the wrapped model so that raw gradients are computed and attached to it (rather than relying on scikit-learn to apply a learning rate that may be scheduled to vary along time).</p> <p>''' warning \"Call it only if using an external optimizer\"</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def disable_internal_optimizer(self) -&gt; None:\n    \"\"\"Disable the scikit-learn internal optimizer.\n\n    Calling this method alters the wrapped model so that raw gradients are\n    computed and attached to it (rather than relying on scikit-learn to\n    apply a learning rate that may be scheduled to vary along time).\n\n    ''' warning \"Call it only if using an external optimizer\"\n    \"\"\"\n    # Record initial params, then override optimizer ones.\n    self._optim_params = self.get_params()\n    self.set_params(**self._null_optim_params)\n    # Warn about overridden values.\n    changed_params: List[str] = []\n    for key, val in self._null_optim_params.items():\n        param = self._optim_params.get(key)\n        if param is not None and param != val:\n            changed_params.append(key)\n    if changed_params:\n        changed = \",\\n\\t\".join(changed_params)\n        logger.warning(\n            \"The following non-default model parameters were overridden \"\n            f\"due to the disabling of the scikit-learn internal optimizer:\\n\\t{changed}\",\n            broadcast=True\n        )\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.enable_internal_optimizer","title":"enable_internal_optimizer","text":"<pre><code>enable_internal_optimizer()\n</code></pre> <p>Enable the scikit-learn internal optimizer.</p> <p>Calling this method restores any model parameter previously overridden due to calling the counterpart <code>disable_internal_optimizer</code> method.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def enable_internal_optimizer(self) -&gt; None:\n    \"\"\"Enable the scikit-learn internal optimizer.\n\n    Calling this method restores any model parameter previously overridden\n    due to calling the counterpart `disable_internal_optimizer` method.\n    \"\"\"\n    if self._optim_params:\n        self.set_params(**self._optim_params)\n        logger.debug(\"Internal Optimizer restored\")\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.export","title":"export","text":"<pre><code>export(filename)\n</code></pre> <p>Export the wrapped model to a dump file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to the file where the model will be saved.</p> required <p>!!! info \"Notes\":     This method is designed to save the model to a local dump     file for easy re-use by the same user, possibly outside of     Fed-BioMed. It is not designed to produce trustworthy data     dumps and is not used to exchange models and their weights     as part of the federated learning process.</p> <p>!!! warning \"Warning\":     This method uses <code>joblib.dump</code>, which relies on pickle and     is therefore hard to trust by third-party loading methods.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def export(self, filename: str) -&gt; None:\n    \"\"\"Export the wrapped model to a dump file.\n\n    Args:\n        filename: path to the file where the model will be saved.\n\n    !!! info \"Notes\":\n        This method is designed to save the model to a local dump\n        file for easy re-use by the same user, possibly outside of\n        Fed-BioMed. It is not designed to produce trustworthy data\n        dumps and is not used to exchange models and their weights\n        as part of the federated learning process.\n\n    !!! warning \"Warning\":\n        This method uses `joblib.dump`, which relies on pickle and\n        is therefore hard to trust by third-party loading methods.\n    \"\"\"\n    with open(filename, \"wb\") as file:\n        joblib.dump(self.model, file)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.flatten","title":"flatten","text":"<pre><code>flatten(only_trainable=False, exclude_buffers=True)\n</code></pre> <p>Gets weights as flatten vector</p> <p>Parameters:</p> Name Type Description Default <code>only_trainable</code> <code>bool</code> <p>Unused for scikit-learn models. (Whether to ignore non-trainable model parameters.)</p> <code>False</code> <code>exclude_buffers</code> <code>bool</code> <p>Unused for scikit-learn models. (Whether to ignore buffers.)</p> <code>True</code> <p>Returns:</p> Name Type Description <code>to_list</code> <code>List[float]</code> <p>Convert np.ndarray to a list if it is True.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def flatten(self,\n            only_trainable: bool = False,\n            exclude_buffers: bool = True) -&gt; List[float]:\n    \"\"\"Gets weights as flatten vector\n\n    Args:\n        only_trainable: Unused for scikit-learn models. (Whether to ignore\n            non-trainable model parameters.)\n        exclude_buffers: Unused for scikit-learn models. (Whether to ignore\n            buffers.)\n\n    Returns:\n        to_list: Convert np.ndarray to a list if it is True.\n    \"\"\"\n\n    weights = self.get_weights()\n    flatten = []\n    for _, w in weights.items():\n        w_: List[float] = list(w.flatten().astype(float))\n        flatten.extend(w_)\n\n    return flatten\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.get_gradients","title":"get_gradients","text":"<pre><code>get_gradients()\n</code></pre> <p>Return computed gradients attached to the model.</p> <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>If no gradients have been computed yet (i.e. the model has not been trained).</p> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Gradients, as a dict mapping parameters' names to their gradient's numpy array.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def get_gradients(\n    self,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Return computed gradients attached to the model.\n\n    Raises:\n        FedbiomedModelError: If no gradients have been computed yet\n            (i.e. the model has not been trained).\n\n    Returns:\n        Gradients, as a dict mapping parameters' names to their\n            gradient's numpy array.\n    \"\"\"\n    if not self._gradients:\n        raise FedbiomedModelError(\n            f\"{ErrorNumbers.FB622.value}. Cannot get gradients if the \"\n            \"model has not been trained beforehand.\"\n        )\n    gradients = self._gradients\n    return gradients\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.get_learning_rate","title":"get_learning_rate  <code>abstractmethod</code>","text":"<pre><code>get_learning_rate()\n</code></pre> <p>Retrieves learning rate of the model. Method implementation will depend on the attribute used to set up these arbitrary arguments</p> <p>Returns:</p> Type Description <code>List[float]</code> <p>Initial learning rate value(s); a single value if only on learning rate has been used, and a list of several learning rates, one for each layer of the model.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>@abstractmethod\ndef get_learning_rate(self) -&gt; List[float]:\n    \"\"\"Retrieves learning rate of the model. Method implementation will\n    depend on the attribute used to set up these arbitrary arguments\n\n    Returns:\n        Initial learning rate value(s); a single value if only on learning rate has been used, and\n            a list of several learning rates, one for each layer of the model.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.get_params","title":"get_params","text":"<pre><code>get_params(value=None)\n</code></pre> <p>Return the wrapped scikit-learn model's hyperparameters.</p> <p>Please refer to <code>baseEstimator documentation</code> <code>get_params</code> method for further details.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>if specified, returns a specific hyperparameter, otherwise, returns a dictionary with all the hyperparameters. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary mapping model hyperparameter names to their values</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def get_params(self, value: Any = None) -&gt; Dict[str, Any]:\n    \"\"\"Return the wrapped scikit-learn model's hyperparameters.\n\n    Please refer to [`baseEstimator documentation`]\n    [https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html] `get_params` method\n    for further details.\n\n    Args:\n        value: if specified, returns a specific hyperparameter, otherwise, returns a dictionary\n            with all the hyperparameters. Defaults to None.\n\n    Returns:\n        Dictionary mapping model hyperparameter names to their values\n    \"\"\"\n    if value is not None:\n        return self.model.get_params().get(value)\n    return self.model.get_params()\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.get_weights","title":"get_weights","text":"<pre><code>get_weights(only_trainable=False, exclude_buffers=True)\n</code></pre> <p>Return a copy of the model's trainable weights.</p> <p>Parameters:</p> Name Type Description Default <code>only_trainable</code> <code>bool</code> <p>Unused for scikit-learn models. (Whether to ignore non-trainable model parameters.)</p> <code>False</code> <code>exclude_buffers</code> <code>bool</code> <p>Unused for scikit-learn models. (Whether to ignore buffers.)</p> <code>True</code> <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>If the model parameters are not initialized.</p> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Model weights, as a dictionary mapping parameters' names to their numpy array, or as a declearn NumpyVector wrapping such a dict.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def get_weights(\n    self,\n    only_trainable: bool = False,\n    exclude_buffers: bool = True\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Return a copy of the model's trainable weights.\n\n    Args:\n        only_trainable: Unused for scikit-learn models. (Whether to ignore\n            non-trainable model parameters.)\n        exclude_buffers: Unused for scikit-learn models. (Whether to ignore\n            buffers.)\n\n    Raises:\n        FedbiomedModelError: If the model parameters are not initialized.\n\n    Returns:\n        Model weights, as a dictionary mapping parameters' names to their\n            numpy array, or as a declearn NumpyVector wrapping such a dict.\n    \"\"\"\n    if not self.param_list:\n        raise FedbiomedModelError(\n            f\"{ErrorNumbers.FB622.value}. Attribute `param_list` is empty. You should \"\n            f\"have initialized the model beforehand (try calling `set_init_params`)\"\n        )\n    # Gather copies of the model weights.\n    weights = {}  # type: Dict[str, np.ndarray]\n    try:\n        for key in self.param_list:\n            val = getattr(self.model, key)\n            if not isinstance(val, np.ndarray):\n                raise FedbiomedModelError(\n                    f\"{ErrorNumbers.FB622.value}: SklearnModel parameter is not a numpy array.\"\n                )\n            weights[key] = val.copy()\n    except AttributeError as err:\n        raise FedbiomedModelError(\n            f\"{ErrorNumbers.FB622.value}. Unable to access weights of BaseEstimator \"\n            f\"model {self.model} (details {err})\"\n        ) from err\n    return weights\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.init_training","title":"init_training","text":"<pre><code>init_training()\n</code></pre> <p>Initialises the training by setting up attributes.</p> <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>raised if <code>param_list</code> has not been defined</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def init_training(self):\n    \"\"\"Initialises the training by setting up attributes.\n\n    Raises:\n        FedbiomedModelError: raised if `param_list` has not been defined\n    \"\"\"\n    if not self.param_list:\n        raise FedbiomedModelError(\n            f\"{ErrorNumbers.FB622.value}. Attribute `param_list` is empty. You should \"\n            f\"have initialized the model beforehand (try calling `set_init_params`)\"\n        )\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.predict","title":"predict","text":"<pre><code>predict(inputs)\n</code></pre> <p>Computes prediction given input data.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>ndarray</code> <p>input data</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Model predictions</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def predict(\n    self,\n    inputs: np.ndarray,\n) -&gt; np.ndarray:\n    \"\"\"Computes prediction given input data.\n\n    Args:\n        inputs: input data\n\n    Returns:\n        Model predictions\n    \"\"\"\n    return self.model.predict(inputs)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.reload","title":"reload","text":"<pre><code>reload(filename)\n</code></pre> <p>Import and replace the wrapped model from a dump file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to the file where the model has been exported.</p> required <p>!!! info \"Notes\":     This method is designed to load the model from a local dump     file, that might not be in a trustworthy format. It should     therefore only be used to re-load data exported locally and     not received from someone else, including other FL peers.</p> <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>if the reloaded instance is of unproper type.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def reload(self, filename: str) -&gt; None:\n    \"\"\"Import and replace the wrapped model from a dump file.\n\n    Args:\n        filename: path to the file where the model has been exported.\n\n    !!! info \"Notes\":\n        This method is designed to load the model from a local dump\n        file, that might not be in a trustworthy format. It should\n        therefore only be used to re-load data exported locally and\n        not received from someone else, including other FL peers.\n\n    Raises:\n        FedbiomedModelError: if the reloaded instance is of unproper type.\n    \"\"\"\n    model = self._reload(filename)\n    if not isinstance(model, self._model_type):\n        err_msg = (\n            f\"{ErrorNumbers.FB622.value}: unproper type for imported model\"\n            f\": expected '{self._model_type}', but 'got {type(model)}'.\"\n        )\n        logger.critical(err_msg)\n        raise FedbiomedModelError(err_msg)\n    self.model = model\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.set_gradients","title":"set_gradients","text":"<pre><code>set_gradients(gradients)\n</code></pre> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def set_gradients(self, gradients: Dict[str, np.ndarray]) -&gt; None:\n    # TODO: either document or remove this (useless) method\n    self._gradients = gradients\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.set_init_params","title":"set_init_params  <code>abstractmethod</code>","text":"<pre><code>set_init_params(model_args)\n</code></pre> <p>Zeroes scikit learn model parameters.</p> <p>Should be used before any training, as it sets the scikit learn model parameters and makes them accessible through the use of attributes. Model parameter attribute names will depend on the scikit learn model wrapped.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict</code> <p>dictionary that contains specifications for setting initial model</p> required Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>@abstractmethod\ndef set_init_params(self, model_args: Dict) -&gt; None:\n    \"\"\"Zeroes scikit learn model parameters.\n\n    Should be used before any training, as it sets the scikit learn model parameters\n    and makes them accessible through the use of attributes. Model parameter attribute names\n    will depend on the scikit learn model wrapped.\n\n    Args:\n        model_args: dictionary that contains specifications for setting initial model\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.set_params","title":"set_params","text":"<pre><code>set_params(**params)\n</code></pre> <p>Assign some hyperparameters to the wrapped scikit-learn model.</p> <p>Please refer to BaseEstimator [https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html] <code>set_params</code> method for further details.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Any</code> <p>new hyperparameters to assign to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: dictionary containing new hyperparameter values.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def set_params(self, **params: Any) -&gt; Dict[str, Any]:\n    \"\"\"Assign some hyperparameters to the wrapped scikit-learn model.\n\n    Please refer to [BaseEstimator][sklearn.base.BaseEstimator]\n    [https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html] `set_params` method\n    for further details.\n\n    Args:\n        params: new hyperparameters to assign to the model.\n\n    Returns:\n        Dict[str, Any]: dictionary containing new hyperparameter values.\n    \"\"\"\n    self.model.set_params(**params)\n    return params\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.set_weights","title":"set_weights","text":"<pre><code>set_weights(weights)\n</code></pre> <p>Assign new values to the model's trainable weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Dict[str, ndarray]</code> <p>Model weights, as a dict mapping parameters' names to their numpy array.</p> required Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def set_weights(\n    self,\n    weights: Dict[str, np.ndarray],\n) -&gt; None:\n    \"\"\"Assign new values to the model's trainable weights.\n\n    Args:\n        weights: Model weights, as a dict mapping parameters' names\n            to their numpy array.\n    \"\"\"\n    self._assert_dict_inputs(weights)\n    for key, val in weights.items():\n        setattr(self.model, key, val.copy())\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.train","title":"train","text":"<pre><code>train(inputs, targets, stdout=None, **kwargs)\n</code></pre> <p>Run a training step, and record associated gradients.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>ndarray</code> <p>inputs data.</p> required <code>targets</code> <code>ndarray</code> <p>targets, to be fit with inputs data.</p> required <code>stdout</code> <code>Optional[List[List[str]]]</code> <p>list of console outputs that have been collected during training, that contains losses values. Used to plot model losses. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>if training has not been initialized.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def train(\n    self,\n    inputs: np.ndarray,\n    targets: np.ndarray,\n    stdout: Optional[List[List[str]]] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Run a training step, and record associated gradients.\n\n    Args:\n        inputs: inputs data.\n        targets: targets, to be fit with inputs data.\n        stdout: list of console outputs that have been collected\n            during training, that contains losses values.\n            Used to plot model losses. Defaults to None.\n\n    Raises:\n        FedbiomedModelError: if training has not been initialized.\n    \"\"\"\n    batch_size = inputs.shape[0]\n    w_init = self.get_weights()\n    w_updt = {key: np.zeros_like(val) for key, val in w_init.items()}\n    # Iterate over the batch; accumulate sample-wise gradients (and loss).\n    for idx in range(batch_size):\n        # Compute updated weights based on the sample. Capture loss prints.\n        with capture_stdout() as console:\n            self.model.partial_fit(inputs[idx : idx + 1], targets[idx])\n        if stdout is not None:\n            stdout.append(console)\n        # Accumulate updated weights (weights + sum of gradients).\n        # Reset the model's weights and iteration counter.\n        for key in self.param_list:\n            w_updt[key] += getattr(self.model, key)\n            setattr(self.model, key, w_init[key].copy())\n        self.model.n_iter_ -= 1\n    # Compute the batch-averaged, learning-rate-scaled gradients.\n    # Note: w_init: {w_t}, w_updt: {w_t - eta_t * sum_{s=1}^B(grad_s)}\n    #       hence eta_t * avg(grad_s) = w_init - (w_updt / B)\n\n    self._gradients = {\n        key: w_init[key] - (w_updt[key] / batch_size)\n        for key in self.param_list\n    }\n\n    # ------------------------------ WARNINGS ----------------------------------\n    #\n    # Warning 1: if `disable_internal_optimizer` has not been called before, gradients won't be scaled\n    # (you will get un-scaled gradients, that need to be scaled back by dividing gradients by the learning rate)\n    # here is a way to do so (with `lrate` as the learning rate):\n    # ```python\n    # for key, val in self._gradients.items():\n    #        val /= lrate\n    # ````\n    # Warning 2:  `_gradients` has different meanings, when using `disable_internal_optimizer`\n    # if it is not called (ie when using native sklearn optimizer), it is not plain gradients,\n    # but rather the quantity `lr * grads`\n\n    # Finally, increment the model's iteration counter.\n    self.model.n_iter_ += 1\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel.unflatten","title":"unflatten","text":"<pre><code>unflatten(weights_vector, only_trainable=False, exclude_buffers=True)\n</code></pre> <p>Unflatten vectorized model weights</p> <p>Parameters:</p> Name Type Description Default <code>weights_vector</code> <code>List[float]</code> <p>Vectorized model weights to convert dict</p> required <code>only_trainable</code> <code>bool</code> <p>Unused for scikit-learn models. (Whether to ignore non-trainable model parameters.)</p> <code>False</code> <code>exclude_buffers</code> <code>bool</code> <p>Unused for scikit-learn models. (Whether to ignore buffers.)</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Model dictionary</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def unflatten(\n        self,\n        weights_vector: List[float],\n        only_trainable: bool = False,\n        exclude_buffers: bool = True\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Unflatten vectorized model weights\n\n    Args:\n        weights_vector: Vectorized model weights to convert dict\n        only_trainable: Unused for scikit-learn models. (Whether to ignore\n            non-trainable model parameters.)\n        exclude_buffers: Unused for scikit-learn models. (Whether to ignore\n            buffers.)\n\n    Returns:\n        Model dictionary\n    \"\"\"\n\n    super().unflatten(weights_vector, only_trainable, exclude_buffers)\n\n    weights_vector = np.array(weights_vector)\n    weights = self.get_weights()\n    pointer = 0\n\n    params = {}\n    for key, w in weights.items():\n        num_param = w.size\n        params[key] = weights_vector[pointer: pointer + num_param].reshape(w.shape)\n\n        pointer += num_param\n\n    return params\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.MLPSklearnModel","title":"MLPSklearnModel","text":"<pre><code>MLPSklearnModel(model)\n</code></pre> <p>               Bases: <code>BaseSkLearnModel</code></p> <p>BaseSklearnModel abstract subclass for multi-layer perceptron models.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def __init__(self, model: BaseEstimator) -&gt; None:\n    self._null_optim_params: Dict[str, Any] = {\n        \"learning_rate_init\": 1.0,\n        \"learning_rate\": \"constant\",\n    }\n    super().__init__(model)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.MLPSklearnModel-attributes","title":"Attributes","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models.MLPSklearnModel.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.MLPSklearnModel-functions","title":"Functions","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models.MLPSklearnModel.get_learning_rate","title":"get_learning_rate","text":"<pre><code>get_learning_rate()\n</code></pre> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def get_learning_rate(self) -&gt; List[float]:\n    return [self.model.learning_rate_init]\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model","title":"Model","text":"<pre><code>Model(model)\n</code></pre> <p>               Bases: <code>Generic[_MT, DT]</code></p> <p>Model abstraction, that wraps and handles both native models</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Any</code> <p>native model, written in a framework supported by Fed-BioMed.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>_MT</code> <p>native model wrapped, of child-class-specific type.</p> required Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>def __init__(self, model: _MT):\n    \"\"\"Constructor of Model abstract class\n\n    Args:\n        model: native model wrapped, of child-class-specific type.\n    \"\"\"\n    self._validate_model_type(model)\n    self.model: Any = model\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model-attributes","title":"Attributes","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models.Model.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model-functions","title":"Functions","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models.Model.apply_updates","title":"apply_updates  <code>abstractmethod</code>","text":"<pre><code>apply_updates(updates)\n</code></pre> <p>Applies updates to the model.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>Dict[str, DT]</code> <p>model updates.</p> required Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef apply_updates(self, updates: Dict[str, DT]):\n    \"\"\"Applies updates to the model.\n\n    Args:\n        updates: model updates.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model.export","title":"export  <code>abstractmethod</code>","text":"<pre><code>export(filename)\n</code></pre> <p>Export the wrapped model to a dump file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to the file where the model will be saved.</p> required <p>!!! info \"Notes\":     This method is designed to save the model to a local dump     file for easy re-use by the same user, possibly outside of     Fed-BioMed. It is not designed to produce trustworthy data     dumps and is not used to exchange models and their weights     as part of the federated learning process.</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef export(self, filename: str) -&gt; None:\n    \"\"\"Export the wrapped model to a dump file.\n\n    Args:\n        filename: path to the file where the model will be saved.\n\n    !!! info \"Notes\":\n        This method is designed to save the model to a local dump\n        file for easy re-use by the same user, possibly outside of\n        Fed-BioMed. It is not designed to produce trustworthy data\n        dumps and is not used to exchange models and their weights\n        as part of the federated learning process.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model.flatten","title":"flatten  <code>abstractmethod</code>","text":"<pre><code>flatten(only_trainable=False, exclude_buffers=True)\n</code></pre> <p>Flattens model weights</p> <p>Parameters:</p> Name Type Description Default <code>only_trainable</code> <code>bool</code> <p>Whether to ignore non-trainable model parameters from outputs (e.g. frozen neural network layers' parameters), or include all model parameters (the default).</p> <code>False</code> <code>exclude_buffers</code> <code>bool</code> <p>Whether to ignore buffers (the default), or  include them.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>List of model weights as float.</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef flatten(self,\n            only_trainable: bool = False,\n            exclude_buffers: bool = True) -&gt; List[float]:\n    \"\"\"Flattens model weights\n\n    Args:\n        only_trainable: Whether to ignore non-trainable model parameters\n            from outputs (e.g. frozen neural network layers' parameters),\n            or include all model parameters (the default).\n        exclude_buffers: Whether to ignore buffers (the default), or \n            include them.\n\n    Returns:\n        List of model weights as float.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model.get_gradients","title":"get_gradients  <code>abstractmethod</code>","text":"<pre><code>get_gradients()\n</code></pre> <p>Return computed gradients attached to the model.</p> <p>Returns:</p> Type Description <code>Dict[str, DT]</code> <p>Gradients, as a dict mapping parameters' names to their gradient's value.</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef get_gradients(self) -&gt; Dict[str, DT]:\n    \"\"\"Return computed gradients attached to the model.\n\n    Returns:\n        Gradients, as a dict mapping parameters' names to their\n            gradient's value.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model.get_weights","title":"get_weights  <code>abstractmethod</code>","text":"<pre><code>get_weights(only_trainable=False, exclude_buffers=True)\n</code></pre> <p>Return a copy of the model's trainable weights.</p> <p>Parameters:</p> Name Type Description Default <code>only_trainable</code> <code>bool</code> <p>Whether to ignore non-trainable model parameters from outputs (e.g. frozen neural network layers' parameters), or include all model parameters (the default).</p> <code>False</code> <code>exclude_buffers</code> <code>bool</code> <p>Whether to ignore buffers (the default), or  include them.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, DT]</code> <p>Model weights, as a dict mapping parameters' names to their value.</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef get_weights(self, only_trainable: bool = False, exclude_buffers: bool = True) -&gt; Dict[str, DT]:\n    \"\"\"Return a copy of the model's trainable weights.\n\n    Args:\n        only_trainable: Whether to ignore non-trainable model parameters\n            from outputs (e.g. frozen neural network layers' parameters),\n            or include all model parameters (the default).\n        exclude_buffers: Whether to ignore buffers (the default), or \n            include them.\n\n    Returns:\n        Model weights, as a dict mapping parameters' names to their value.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model.init_training","title":"init_training  <code>abstractmethod</code>","text":"<pre><code>init_training()\n</code></pre> <p>Initialize parameters before model training.</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef init_training(self):\n    \"\"\"Initialize parameters before model training.\"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(inputs)\n</code></pre> <p>Return model predictions given input values.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Any</code> <p>input values.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>predictions.</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef predict(self, inputs: Any) -&gt; Any:\n    \"\"\"Return model predictions given input values.\n\n    Args:\n        inputs: input values.\n\n    Returns:\n        Any: predictions.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model.reload","title":"reload  <code>abstractmethod</code>","text":"<pre><code>reload(filename)\n</code></pre> <p>Import and replace the wrapped model from a dump file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to the file where the model has been exported.</p> required <p>!!! info \"Notes\":     This method is designed to load the model from a local dump     file, that might not be in a trustworthy format. It should     therefore only be used to re-load data exported locally and     not received from someone else, including other FL peers.</p> <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>if the reloaded instance is of unproper type.</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef reload(self, filename: str) -&gt; None:\n    \"\"\"Import and replace the wrapped model from a dump file.\n\n    Args:\n        filename: path to the file where the model has been exported.\n\n    !!! info \"Notes\":\n        This method is designed to load the model from a local dump\n        file, that might not be in a trustworthy format. It should\n        therefore only be used to re-load data exported locally and\n        not received from someone else, including other FL peers.\n\n    Raises:\n        FedbiomedModelError: if the reloaded instance is of unproper type.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model.set_model","title":"set_model","text":"<pre><code>set_model(model)\n</code></pre> <p>Replace the wrapped model with a new one.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>_MT</code> <p>New model instance that needs assignment as the <code>model</code> attribute.</p> required Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>def set_model(self, model: _MT) -&gt; None:\n    \"\"\"Replace the wrapped model with a new one.\n\n    Args:\n        model: New model instance that needs assignment as the `model`\n            attribute.\n    \"\"\"\n    self._validate_model_type(model)\n    self.model = model\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model.set_weights","title":"set_weights  <code>abstractmethod</code>","text":"<pre><code>set_weights(weights)\n</code></pre> <p>Assign new values to the model's trainable weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Dict[str, DT]</code> <p>Model weights, as a dict mapping parameters' names to their value.</p> required Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef set_weights(self, weights: Dict[str, DT]) -&gt; None:\n    \"\"\"Assign new values to the model's trainable weights.\n\n    Args:\n        weights: Model weights, as a dict mapping parameters' names\n            to their value.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model.train","title":"train  <code>abstractmethod</code>","text":"<pre><code>train(inputs, targets, **kwargs)\n</code></pre> <p>Perform a training step given inputs and targets data.</p> <p>Warning</p> <p>Please run <code>init_training</code> method before running <code>train</code> method, so to initialize parameters needed for model training\"</p> <p>Warning</p> <p>This function usually does not update weights. You need to call <code>apply_updates</code> to ensure updates are applied to the model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Any</code> <p>input (training) data.</p> required <code>targets</code> <code>Any</code> <p>target values.</p> required Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef train(self, inputs: Any, targets: Any, **kwargs) -&gt; None:\n    \"\"\"Perform a training step given inputs and targets data.\n\n    !!! warning \"Warning\"\n        Please run `init_training` method before running `train` method,\n        so to initialize parameters needed for model training\"\n\n    !!! warning \"Warning\"\n        This function usually does not update weights. You need to call\n        `apply_updates` to ensure updates are applied to the model.\n\n    Args:\n        inputs: input (training) data.\n        targets: target values.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model.unflatten","title":"unflatten  <code>abstractmethod</code>","text":"<pre><code>unflatten(weights_vector, only_trainable=False, exclude_buffers=True)\n</code></pre> <p>Revert flatten model weights back model-dict form.</p> <p>Parameters:</p> Name Type Description Default <code>weights_vector</code> <code>List[float]</code> <p>Vectorized model weights to convert dict</p> required <code>only_trainable</code> <code>bool</code> <p>Whether to ignore non-trainable model parameters from outputs (e.g. frozen neural network layers' parameters), or include all model parameters (the default).</p> <code>False</code> <code>exclude_buffers</code> <code>bool</code> <p>Whether to ignore buffers (the default), or  include them.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>Model dictionary</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef unflatten(\n        self,\n        weights_vector: List[float],\n        only_trainable: bool = False,\n        exclude_buffers: bool = True\n) -&gt; None:\n    \"\"\"Revert flatten model weights back model-dict form.\n\n    Args:\n        weights_vector: Vectorized model weights to convert dict\n        only_trainable: Whether to ignore non-trainable model parameters\n            from outputs (e.g. frozen neural network layers' parameters),\n            or include all model parameters (the default).\n        exclude_buffers: Whether to ignore buffers (the default), or \n            include them.\n\n    Returns:\n        Model dictionary\n    \"\"\"\n\n    if not isinstance(weights_vector, list) or not all([isinstance(w, float) for w in weights_vector]):\n        raise FedbiomedModelError(\n            f\"{ErrorNumbers.FB622} `weights_vector should be 1D list of float containing flatten model parameters`\"\n        )\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDClassifierSKLearnModel","title":"SGDClassifierSKLearnModel","text":"<pre><code>SGDClassifierSKLearnModel(model)\n</code></pre> <p>               Bases: <code>SGDSkLearnModel</code></p> <p>BaseSkLearnModel subclass for SGDClassifier models.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def __init__(self, model: BaseEstimator) -&gt; None:\n    super().__init__(model)\n    self._null_optim_params: Dict[str, Any] = {\n        'eta0': 1.0,\n        'learning_rate': \"constant\",\n    }\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDClassifierSKLearnModel-attributes","title":"Attributes","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDClassifierSKLearnModel.is_classification","title":"is_classification  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_classification = True\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDClassifierSKLearnModel.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDClassifierSKLearnModel-functions","title":"Functions","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDClassifierSKLearnModel.set_init_params","title":"set_init_params","text":"<pre><code>set_init_params(model_args)\n</code></pre> <p>Initialize the model's trainable parameters.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def set_init_params(self, model_args: Dict[str, Any]) -&gt; None:\n    \"\"\"Initialize the model's trainable parameters.\"\"\"\n    # Set up zero-valued start weights, for binary of multiclass classif.\n    n_classes = model_args[\"n_classes\"]\n    if n_classes == 2:\n        init_params = {\n            \"intercept_\": np.zeros((1,)),\n            \"coef_\": np.zeros((1, model_args[\"n_features\"])),\n        }\n    else:\n        init_params = {\n            \"intercept_\": np.zeros((n_classes,)),\n            \"coef_\": np.zeros((n_classes, model_args[\"n_features\"])),\n        }\n    # Assign these initialization parameters and retain their names.\n    self.param_list = list(init_params)\n    for key, val in init_params.items():\n        setattr(self.model, key, val)\n    # Also initialize the \"classes_\" slot with unique predictable labels.\n    # FIXME: this assumes target values are integers in range(n_classes).\n    setattr(self.model, \"classes_\", np.arange(n_classes))\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDRegressorSKLearnModel","title":"SGDRegressorSKLearnModel","text":"<pre><code>SGDRegressorSKLearnModel(model)\n</code></pre> <p>               Bases: <code>SGDSkLearnModel</code></p> <p>BaseSkLearnModel subclass for SGDRegressor models.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def __init__(self, model: BaseEstimator) -&gt; None:\n    super().__init__(model)\n    self._null_optim_params: Dict[str, Any] = {\n        'eta0': 1.0,\n        'learning_rate': \"constant\",\n    }\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDRegressorSKLearnModel-attributes","title":"Attributes","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDRegressorSKLearnModel.is_classification","title":"is_classification  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_classification = False\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDRegressorSKLearnModel.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDRegressorSKLearnModel-functions","title":"Functions","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDRegressorSKLearnModel.set_init_params","title":"set_init_params","text":"<pre><code>set_init_params(model_args)\n</code></pre> <p>Initialize the model's trainable parameters.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def set_init_params(self, model_args: Dict[str, Any]):\n    \"\"\"Initialize the model's trainable parameters.\"\"\"\n    init_params = {\n        \"intercept_\": np.array([0.0]),\n        \"coef_\": np.array([0.0] * model_args[\"n_features\"]),\n    }\n    self.param_list = list(init_params)\n    for key, val in init_params.items():\n        setattr(self.model, key, val)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SkLearnModel","title":"SkLearnModel","text":"<pre><code>SkLearnModel(model)\n</code></pre> <p>Sklearn model builder.</p> <p>It wraps one of Fed-BioMed <code>BaseSkLearnModel</code> object children, by passing a (BaseEstimator)(https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html) object to the constructor, as shown below.</p> <p>Usage <pre><code>    from sklearn.linear_model import SGDClassifier\n    model = SkLearnModel(SGDClassifier)\n    model.set_weights(some_weights)\n    type(model.model)\n    # Output: &lt;class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'&gt;\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>_instance</code> <code>BaseSkLearnModel</code> <p>instance of BaseSkLearnModel</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[BaseEstimator]</code> <p>non-initialized BaseEstimator object</p> required <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>raised if model does not belong to the implemented models.</p> <code>FedbiomedModelError</code> <p>raised if <code>__name__</code> attribute does not belong to object. This may happen when passing an instantiated object instead of the class object (e.g. instance of SGDClassifier() instead of SGDClassifier object)</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def __init__(self, model: Type[BaseEstimator]):\n    \"\"\"Constructor of the model builder.\n\n    Args:\n        model: non-initialized [BaseEstimator][sklearn.base.BaseEstimator] object\n\n    Raises:\n        FedbiomedModelError: raised if model does not belong to the implemented models.\n        FedbiomedModelError: raised if `__name__` attribute does not belong to object. This may happen\n            when passing an instantiated object instead of the class object (e.g. instance of\n            SGDClassifier() instead of SGDClassifier object)\n    \"\"\"\n    if not isinstance(model, type):\n        raise FedbiomedModelError(\n            f\"{ErrorNumbers.FB622.value}: 'SkLearnModel' received a '{type(model)}' instance as 'model' \"\n            \"input while it was expecting a scikit-learn BaseEstimator subclass constructor.\"\n        )\n    if not issubclass(model, BaseEstimator):\n        raise FedbiomedModelError(\n            f\"{ErrorNumbers.FB622.value}: 'SkLearnModel' received a 'model' class that is not \"\n            f\"a scikit-learn BaseEstimator subclass: '{model}'.\"\n        )\n    if model.__name__ not in SKLEARN_MODELS:\n        raise FedbiomedModelError(\n            f\"{ErrorNumbers.FB622.value}: 'SkLearnModel' received '{model}' as 'model' class, \"\n            f\"support for which has not yet been implemented in Fed-BioMed.\"\n        )\n    self._instance: BaseSkLearnModel = SKLEARN_MODELS[model.__name__](model())\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel","title":"TorchModel","text":"<pre><code>TorchModel(model)\n</code></pre> <p>               Bases: <code>Model</code></p> <p>PyTorch model wrapper that ease the handling of a pytorch model</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Module</code> <p>torch.nn.Module. Pytorch model wrapped.</p> <code>init_params</code> <code>Dict[str, Tensor]</code> <p>OrderedDict. Model initial parameters. Set when calling <code>init_training</code>.</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def __init__(self, model: torch.nn.Module) -&gt; None:\n    \"\"\"Instantiates the wrapper over a torch Module instance.\"\"\"\n    super().__init__(model)\n    self.init_params: Dict[str, torch.Tensor] = {}\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel-attributes","title":"Attributes","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.init_params","title":"init_params  <code>instance-attribute</code>","text":"<pre><code>init_params = {}\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel-functions","title":"Functions","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.add_corrections_to_gradients","title":"add_corrections_to_gradients","text":"<pre><code>add_corrections_to_gradients(corrections)\n</code></pre> <p>Add values to the gradients currently attached to the model.</p> <p>Parameters:</p> Name Type Description Default <code>corrections</code> <code>Dict[str, Tensor]</code> <p>corrections to be added to the model's gradients.</p> required Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def add_corrections_to_gradients(\n    self,\n    corrections: Dict[str, torch.Tensor],\n) -&gt; None:\n    \"\"\"Add values to the gradients currently attached to the model.\n\n    Args:\n        corrections: corrections to be added to the model's gradients.\n    \"\"\"\n    self._assert_dict_inputs(corrections)\n    for name, update in corrections.items():\n        param = self.model.get_parameter(name)\n        if param.grad is not None:\n            param.grad.add_(update.to(param.grad.device))\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.apply_updates","title":"apply_updates","text":"<pre><code>apply_updates(updates)\n</code></pre> <p>Apply incoming updates to the wrapped model's parameters.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>Dict[str, Tensor]</code> <p>model updates to be added to the model.</p> required Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def apply_updates(\n    self,\n    updates: Dict[str, torch.Tensor],\n) -&gt; None:\n    \"\"\"Apply incoming updates to the wrapped model's parameters.\n\n    Args:\n        updates: model updates to be added to the model.\n    \"\"\"\n    self._assert_dict_inputs(updates)\n    with torch.no_grad():\n        for name, update in updates.items():\n            param = self.model.get_parameter(name)\n            param.add_(update.to(param.device))\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.export","title":"export","text":"<pre><code>export(filename)\n</code></pre> <p>Export the wrapped model to a dump file.</p> <p>For PyTorch only export the model weights.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to the file where the model will be saved.</p> required <p>!!! info \"Notes\":     This method is designed to save the model to a local dump     file for easy re-use by the same user, possibly outside of     Fed-BioMed. It is not designed to produce trustworthy data     dumps and is not used to exchange models and their weights     as part of the federated learning process.</p> <p>!!! warning \"Warning\":     This method uses <code>torch.save</code>, which relies on pickle and     is therefore hard to trust by third-party loading methods.</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def export(self, filename: str) -&gt; None:\n    \"\"\"Export the wrapped model to a dump file.\n\n    For PyTorch only export the model weights.\n\n    Args:\n        filename: path to the file where the model will be saved.\n\n    !!! info \"Notes\":\n        This method is designed to save the model to a local dump\n        file for easy re-use by the same user, possibly outside of\n        Fed-BioMed. It is not designed to produce trustworthy data\n        dumps and is not used to exchange models and their weights\n        as part of the federated learning process.\n\n    !!! warning \"Warning\":\n        This method uses `torch.save`, which relies on pickle and\n        is therefore hard to trust by third-party loading methods.\n    \"\"\"\n    torch.save(self.model.state_dict(), filename)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.flatten","title":"flatten","text":"<pre><code>flatten(only_trainable=False, exclude_buffers=True)\n</code></pre> <p>Gets weights as flatten vector</p> <p>Parameters:</p> Name Type Description Default <code>only_trainable</code> <code>bool</code> <p>Whether to ignore non-trainable model parameters from outputs (e.g. frozen neural network layers' parameters), or include all model parameters (the default).</p> <code>False</code> <code>exclude_buffers</code> <code>bool</code> <p>Whether to ignore buffers (the default), or  include them.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>to_list</code> <code>List[float]</code> <p>Convert np.ndarray to a list if it is True.</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def flatten(self,\n            only_trainable: bool = False,\n            exclude_buffers: bool = True) -&gt; List[float]:\n    \"\"\"Gets weights as flatten vector\n\n    Args:\n        only_trainable: Whether to ignore non-trainable model parameters\n            from outputs (e.g. frozen neural network layers' parameters),\n            or include all model parameters (the default).\n        exclude_buffers: Whether to ignore buffers (the default), or \n            include them.\n\n    Returns:\n        to_list: Convert np.ndarray to a list if it is True.\n    \"\"\"\n    params: List[float] = torch.nn.utils.parameters_to_vector(\n        self.get_weights(only_trainable=only_trainable, exclude_buffers=exclude_buffers).values()\n    ).tolist()\n\n    return params\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.get_gradients","title":"get_gradients","text":"<pre><code>get_gradients()\n</code></pre> <p>Return the gradients attached to the model.</p> <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Gradients, as a dict mapping parameters' names to their gradient's torch tensor.</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def get_gradients(\n    self,\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Return the gradients attached to the model.\n\n    Returns:\n        Gradients, as a dict mapping parameters' names to their gradient's\n            torch tensor.\n    \"\"\"\n    gradients = {\n        name: param.grad.detach().clone()\n        for name, param in self.model.named_parameters()\n        if (param.requires_grad and param.grad is not None)\n    }\n    if len(gradients) &lt; len(list(self.model.named_parameters())):\n        # FIXME: this will be triggered when having some frozen weights\n        #        even if training was properly conducted\n        logger.warning(\n            \"Warning: can not retrieve all gradients from the model. \"\n            \"Are you sure you have trained the model beforehand?\"\n        )\n    return gradients\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.get_weights","title":"get_weights","text":"<pre><code>get_weights(only_trainable=False, exclude_buffers=True)\n</code></pre> <p>Return the model's parameters.</p> <p>Parameters:</p> Name Type Description Default <code>only_trainable</code> <code>bool</code> <p>Whether to ignore non-trainable model parameters from outputs (e.g. frozen neural network layers' parameters), or include all model parameters (the default).</p> <code>False</code> <code>exclude_buffers</code> <code>bool</code> <p>Whether to ignore buffers (the default), or  include them.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Model weights, as a dictionary mapping parameters' names to their torch tensor.</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def get_weights(\n    self,\n    only_trainable: bool = False,\n    exclude_buffers: bool = True\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Return the model's parameters.\n\n    Args:\n        only_trainable: Whether to ignore non-trainable model parameters\n            from outputs (e.g. frozen neural network layers' parameters),\n            or include all model parameters (the default).\n        exclude_buffers: Whether to ignore buffers (the default), or \n            include them.\n\n    Returns:\n        Model weights, as a dictionary mapping parameters' names to their\n            torch tensor.\n    \"\"\"\n    param_iterator = self.model.named_parameters() if exclude_buffers else self.model.state_dict().items()\n    parameters = {\n        name: param.detach().clone()\n        for name, param in param_iterator\n        if param.requires_grad or not only_trainable\n    }\n    return parameters\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.init_training","title":"init_training","text":"<pre><code>init_training()\n</code></pre> <p>Initializes and sets attributes before the training.</p> <p>Initializes <code>init_params</code> as a copy of the initial parameters of the model</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def init_training(self) -&gt; None:\n    \"\"\"Initializes and sets attributes before the training.\n\n    Initializes `init_params` as a copy of the initial parameters of the model\n    \"\"\"\n    # initial aggregated model parameters\n    self.init_params = {\n        key: param.data.detach().clone()\n        for key, param in self.model.named_parameters()\n    }\n    self.model.train()  # pytorch switch for training\n    self.model.zero_grad()\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.predict","title":"predict","text":"<pre><code>predict(inputs)\n</code></pre> <p>Computes prediction given input data.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>input data</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Model predictions returned as a numpy array</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def predict(\n    self,\n    inputs: torch.Tensor,\n) -&gt; np.ndarray:\n    \"\"\"Computes prediction given input data.\n\n    Args:\n        inputs: input data\n\n    Returns:\n        Model predictions returned as a numpy array\n    \"\"\"\n    self.model.eval()  # pytorch switch for model inference-mode\n    with torch.no_grad():\n        pred = self.model(inputs)\n    return pred.cpu().numpy()\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.reload","title":"reload","text":"<pre><code>reload(filename)\n</code></pre> <p>Import and replace the wrapped model from a dump file.</p> <p>For PyTorch, only import the model weights.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to the file where the model has been exported.</p> required <p>!!! info \"Notes\":     This method is designed to load the model from a local dump     file, that might not be in a trustworthy format. It should     therefore only be used to re-load data exported locally and     not received from someone else, including other FL peers.</p> <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>if the reloaded instance is of unproper type.</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def reload(self, filename: str) -&gt; None:\n    \"\"\"Import and replace the wrapped model from a dump file.\n\n    For PyTorch, only import the model weights.\n\n    Args:\n        filename: path to the file where the model has been exported.\n\n    !!! info \"Notes\":\n        This method is designed to load the model from a local dump\n        file, that might not be in a trustworthy format. It should\n        therefore only be used to re-load data exported locally and\n        not received from someone else, including other FL peers.\n\n    Raises:\n        FedbiomedModelError: if the reloaded instance is of unproper type.\n    \"\"\"\n    weights = torch.load(filename)\n    # check format of weights and apply them to the model\n    self.set_weights(weights)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.send_to_device","title":"send_to_device","text":"<pre><code>send_to_device(device)\n</code></pre> <p>Sends model to device</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>device set for using GPU or CPU.</p> required Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def send_to_device(\n    self,\n    device: torch.device,\n) -&gt; None:\n    \"\"\"Sends model to device\n\n    Args:\n        device: device set for using GPU or CPU.\n    \"\"\"\n    self.model.to(device)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.set_weights","title":"set_weights","text":"<pre><code>set_weights(weights)\n</code></pre> <p>Sets model weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Dict[str, Tensor]</code> <p>Model weights, as a dict mapping parameters' names to their torch tensor.</p> required Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def set_weights(\n    self,\n    weights: Dict[str, torch.Tensor],\n) -&gt; None:\n    \"\"\"Sets model weights.\n\n    Args:\n        weights: Model weights, as a dict mapping parameters' names\n            to their torch tensor.\n    \"\"\"\n    self._assert_dict_inputs(weights)\n    incompatible = self.model.load_state_dict(weights, strict=False)\n    # Warn about (probably-)missing trainable weights.\n    # Note: state_dict may include values that do not belong to the model's\n    # parameters, and/or input weights may exclude non-trainable weights,\n    # without requiring a warning.\n    if incompatible.missing_keys:\n        params = {key for key, prm in self.model.named_parameters() if prm.requires_grad}\n        missing = params.intersection(incompatible.missing_keys)\n        if missing:\n            logger.warning(\n                \"'TorchModel.set_weights' received inputs that did not cover all\"\n                \"trainable model parameters; missing weights: %s\",\n                missing\n            )\n    # Warn about invalid (hence, unused) inputs.\n    if incompatible.unexpected_keys:\n        logger.warning(\n            \"'TorchModel.set_weights' received inputs with unexpected names: %s\",\n            incompatible.unexpected_keys\n        )\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.train","title":"train","text":"<pre><code>train(inputs, targets, **kwargs)\n</code></pre> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def train(\n    self,\n    inputs: torch.Tensor,\n    targets: torch.Tensor,\n    **kwargs,\n) -&gt; None:\n    # TODO: should we pass loss function here? and do the backward prop?\n    if not self.init_params:\n        raise FedbiomedModelError(\n            f\"{ErrorNumbers.FB622.value}. Training has not been initialized, please initialize it beforehand\"\n        )\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel.unflatten","title":"unflatten","text":"<pre><code>unflatten(weights_vector, only_trainable=False, exclude_buffers=True)\n</code></pre> <p>Unflatten vectorized model weights using <code>vector_to_parameters</code></p> <p>This method does not manipulate current model weights modify model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>weights_vector</code> <code>List[float]</code> <p>Vectorized model weights to convert dict</p> required <code>only_trainable</code> <code>bool</code> <p>Whether to ignore non-trainable model parameters from outputs (e.g. frozen neural network layers' parameters), or include all model parameters (the default).</p> <code>False</code> <code>exclude_buffers</code> <code>bool</code> <p>Whether to ignore buffers (the default), or  include them.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Model dictionary</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def unflatten(\n        self,\n        weights_vector: List[float],\n        only_trainable: bool = False,\n        exclude_buffers: bool = True\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Unflatten vectorized model weights using [`vector_to_parameters`][torch.nn.utils.vector_to_parameters]\n\n    This method does not manipulate current model weights modify model parameters.\n\n    Args:\n        weights_vector: Vectorized model weights to convert dict\n        only_trainable: Whether to ignore non-trainable model parameters\n            from outputs (e.g. frozen neural network layers' parameters),\n            or include all model parameters (the default).\n        exclude_buffers: Whether to ignore buffers (the default), or \n            include them.\n\n    Returns:\n        Model dictionary\n    \"\"\"\n\n    super().unflatten(weights_vector, only_trainable, exclude_buffers)\n\n    # Copy model to make sure global model parameters won't be overwritten\n    model = copy.deepcopy(self)\n    vector = torch.as_tensor(weights_vector).type(torch.DoubleTensor)\n    weights = model.get_weights(only_trainable=only_trainable, exclude_buffers=exclude_buffers)\n\n    # Following operation updates model parameters of copied model object\n    try:\n        torch.nn.utils.vector_to_parameters(vector, weights.values())\n    except TypeError as e:\n        FedbiomedModelError(\n            f\"{ErrorNumbers.FB622.value} Can not unflatten model parameters. {e}\"\n        )\n    return weights\n</code></pre>"},{"location":"developer/api/common/optimizers/","title":"Optimizers","text":"<p>Optimizer is an interface that enables the use of declearn 's optimizers for Federated Learning inside Fed-BioMed</p>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers-classes","title":"Classes","text":""},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.BaseOptimizer","title":"BaseOptimizer","text":"<pre><code>BaseOptimizer(model, optimizer)\n</code></pre> <p>               Bases: <code>Generic[OT]</code></p> <p>Abstract base class for Optimizer and Model wrappers.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>model to train, interfaced via a framework-specific Model.</p> required <code>optimizer</code> <code>OT</code> <p>optimizer that will be used for optimizing the model.</p> required <p>Raises:</p> Type Description <code>FedbiomedOptimizerError</code> <p>Raised if model is not an instance of <code>_model_cls</code> (which may be a subset of the generic Model type).</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def __init__(self, model: Model, optimizer: OT):\n    \"\"\"Constuctor of the optimizer wrapper that sets a reference to model and optimizer.\n\n    Args:\n        model: model to train, interfaced via a framework-specific Model.\n        optimizer: optimizer that will be used for optimizing the model.\n\n    Raises:\n        FedbiomedOptimizerError:\n            Raised if model is not an instance of `_model_cls` (which may\n            be a subset of the generic Model type).\n    \"\"\"\n    if not isinstance(model, self._model_cls):\n        raise FedbiomedOptimizerError(\n            f\"{ErrorNumbers.FB626.value}, in `model` argument, expected an instance \"\n            f\"of {self._model_cls} but got an object of type {type(model)}.\"\n        )\n    self._model: Model = model\n    self.optimizer: OT = optimizer\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.BaseOptimizer-attributes","title":"Attributes","text":""},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.BaseOptimizer.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = optimizer\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.BaseOptimizer-functions","title":"Functions","text":""},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.BaseOptimizer.count_nb_auxvar","title":"count_nb_auxvar","text":"<pre><code>count_nb_auxvar()\n</code></pre> <p>Counts number of auxiliary variables needed for the given optimizer</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def count_nb_auxvar(self) -&gt; int:\n    \"\"\"Counts number of auxiliary variables needed for the given optimizer\"\"\"\n    return 0\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.BaseOptimizer.init_training","title":"init_training","text":"<pre><code>init_training()\n</code></pre> <p>Sets up training and misceallenous parameters so the model is ready for training</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def init_training(self):\n    \"\"\"Sets up training and misceallenous parameters so the model is ready for training\n    \"\"\"\n    self._model.init_training()\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.BaseOptimizer.load_state","title":"load_state","text":"<pre><code>load_state(optim_state, load_from_state=False)\n</code></pre> <p>Reconfigures optimizer from a given state.</p> <p>This is the default method for optimizers that don't support state. Does nothing.</p> <p>Parameters:</p> Name Type Description Default <code>optim_state</code> <code>Dict</code> <p>not used</p> required <code>load_from_state</code> <code>optional</code> <p>not used</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[BaseOptimizer, None]</code> <p>None</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def load_state(self, optim_state: Dict, load_from_state: bool = False) -&gt; Union['BaseOptimizer', None]:\n    \"\"\"Reconfigures optimizer from a given state.\n\n    This is the default method for optimizers that don't support state. Does nothing.\n\n    Args:\n        optim_state: not used\n        load_from_state (optional): not used\n\n    Returns:\n        None\n    \"\"\"\n    logger.warning(\"load_state method of optimizer not implemented, cannot load optimizer status\")\n    return None\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.BaseOptimizer.save_state","title":"save_state","text":"<pre><code>save_state()\n</code></pre> <p>Gets optimizer state.</p> <p>This is the default method for optimizers that don't support state. Does nothing.</p> <p>Returns:</p> Type Description <code>Union[Dict, None]</code> <p>None</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def save_state(self) -&gt; Union[Dict, None]:\n    \"\"\"Gets optimizer state.\n\n    This is the default method for optimizers that don't support state. Does nothing.\n\n    Returns:\n        None\n    \"\"\"\n    logger.warning(\"save_state method of optimizer not implemented, cannot save optimizer status\")\n    return None\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.BaseOptimizer.send_to_device","title":"send_to_device","text":"<pre><code>send_to_device(device, idx=None)\n</code></pre> <p>GPU support</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def send_to_device(self, device: str, idx: Optional[int] = None):\n    \"\"\"GPU support\"\"\"\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.BaseOptimizer.step","title":"step  <code>abstractmethod</code>","text":"<pre><code>step()\n</code></pre> <p>Performs an optimisation step and updates model weights.</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>@abstractmethod\ndef step(self):\n    \"\"\"Performs an optimisation step and updates model weights.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.DeclearnOptimizer","title":"DeclearnOptimizer","text":"<pre><code>DeclearnOptimizer(model, optimizer)\n</code></pre> <p>               Bases: <code>BaseOptimizer</code></p> <p>Base Optimizer subclass to use a declearn-backed Optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Model that wraps the actual model</p> required <code>optimizer</code> <code>Union[Optimizer, Optimizer]</code> <p>declearn optimizer, or fedbiomed optimizer (that wraps declearn optimizer)</p> required Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def __init__(self, model: Model, optimizer: Union[FedOptimizer, declearn.optimizer.Optimizer]):\n    \"\"\"Constructor of Optimizer wrapper for declearn's optimizers\n\n    Args:\n        model: Model that wraps the actual model\n        optimizer: declearn optimizer,\n            or fedbiomed optimizer (that wraps declearn optimizer)\n    \"\"\"\n    logger.debug(\"Using declearn optimizer\")\n    if isinstance(optimizer, declearn.optimizer.Optimizer):\n        # convert declearn optimizer into a fedbiomed optimizer wrapper\n        optimizer = FedOptimizer.from_declearn_optimizer(optimizer)\n    elif not isinstance(optimizer, FedOptimizer):\n        raise FedbiomedOptimizerError(\n            f\"{ErrorNumbers.FB626.value}: expected a declearn optimizer,\"\n            f\" but got an object with type {type(optimizer)}.\"\n        )\n    super().__init__(model, optimizer)\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.DeclearnOptimizer-attributes","title":"Attributes","text":""},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.DeclearnOptimizer.optimizer","title":"optimizer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>optimizer = None\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.DeclearnOptimizer-functions","title":"Functions","text":""},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.DeclearnOptimizer.count_nb_auxvar","title":"count_nb_auxvar","text":"<pre><code>count_nb_auxvar()\n</code></pre> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def count_nb_auxvar(self) -&gt; int:\n    return len(self.optimizer.get_aux_names())\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.DeclearnOptimizer.get_aux","title":"get_aux","text":"<pre><code>get_aux()\n</code></pre> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def get_aux(self) -&gt; Optional[Dict[str, AuxVar]]:\n    aux = self.optimizer.get_aux()\n    return aux\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.DeclearnOptimizer.init_training","title":"init_training","text":"<pre><code>init_training()\n</code></pre> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def init_training(self):\n    super().init_training()\n    self.optimizer.init_round()\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.DeclearnOptimizer.load_state","title":"load_state","text":"<pre><code>load_state(optim_state, load_from_state=False)\n</code></pre> <p>Reconfigures optimizer from a given state (contained in <code>optim_state</code> argument). Usage: <pre><code>&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; from fedbiomed.common.optimizers import Optimizer\n&gt;&gt;&gt; from fedbiomed.common.models import TorchModel\n&gt;&gt;&gt; model = TorchModel(nn.Linear(4, 2))\n&gt;&gt;&gt; optimizer = Optimizer(lr=.1)\n&gt;&gt;&gt; optim = DeclearnOptimizer(model, optimizer)\n\n&gt;&gt;&gt; optim.load_state(state)  # provided state contains the state one wants to load the optimizer with\n</code></pre> If <code>load_from_state</code> argument is True, it completes the current optimizer state with <code>optim_state</code> argument</p> <p><pre><code>&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; from fedbiomed.common.optimizers import Optimizer\n&gt;&gt;&gt; from fedbiomed.common.optimizers.declearn import MomentumModule, AdamModule\n&gt;&gt;&gt; from fedbiomed.common.models import TorchModel\n&gt;&gt;&gt; model = TorchModel(nn.Linear(4, 2))\n&gt;&gt;&gt; optimizer = Optimizer(lr=.1, modules=[MomentumModule(), AdamModule()])\n&gt;&gt;&gt; optim_1 = DeclearnOptimizer(model, optimizer)\n\n&gt;&gt;&gt; optimizer = Optimizer(lr=.1, modules=[AdamModule(), MomentumModule()])\n&gt;&gt;&gt; optim_2 = DeclearnOptimizer(model, optimizer)\n&gt;&gt;&gt; optim_2.load_state(optim_1.save_state())\n&gt;&gt;&gt; optim_2.save_state()['states']\n{'modules': [('momentum', {'velocity': 0.0}),\n        ('adam',\n        {'steps': 0,\n            'vmax': None,\n            'momentum': {'state': 0.0},\n            'velocity': {'state': 0.0}})]}\n</code></pre> Modules of DeclearnOptimizer will be reloaded provided that Module is the same and occupying the same index. Eg if the state contains following modules: <code>modules=[AdamModule(), AdagradModule(), MomemtumModule()]</code>  And the Optimizer contained in the TrainingPlan has the following modules: <code>modules=[AdamModule(), MomemtumModule()]</code> Then only <code>AdamModule</code> module will be reloaded, <code>MomentumModule</code> will be set with default argument (they don't share the same index in the modules list).</p> <p>Parameters:</p> Name Type Description Default <code>optim_state</code> <code>Dict[str, Any]</code> <p>state of the Optimizer to be loaded. It will change the current state of the optimizer with the one loaded</p> required <code>load_from_state</code> <code>optional</code> <p>strategy for loading states: whether to load from saved states (True) or from breakpoint (False). If set to True, loading is done partially in the sense that if some of the OptimModules is different in the optim_state and the original state of the optimizer, it loads only the OptiModule(s) from the latest state that both state has in common. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>FedbiomedOptimizerError</code> <p>raised if state is not of dict type.</p> <p>Returns:</p> Type Description <code>DeclearnOptimizer</code> <p>Optimizer wrapper reloaded from <code>optim_state</code> argument.</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def load_state(self, optim_state: Dict[str, Any], load_from_state: bool = False) -&gt; 'DeclearnOptimizer':\n    \"\"\"Reconfigures optimizer from a given state (contained in `optim_state` argument).\n    Usage:\n    ```python\n    &gt;&gt;&gt; import torch.nn as nn\n    &gt;&gt;&gt; from fedbiomed.common.optimizers import Optimizer\n    &gt;&gt;&gt; from fedbiomed.common.models import TorchModel\n    &gt;&gt;&gt; model = TorchModel(nn.Linear(4, 2))\n    &gt;&gt;&gt; optimizer = Optimizer(lr=.1)\n    &gt;&gt;&gt; optim = DeclearnOptimizer(model, optimizer)\n\n    &gt;&gt;&gt; optim.load_state(state)  # provided state contains the state one wants to load the optimizer with\n    ```\n    If `load_from_state` argument is True, it completes the current optimizer state with `optim_state` argument\n\n    ```python\n    &gt;&gt;&gt; import torch.nn as nn\n    &gt;&gt;&gt; from fedbiomed.common.optimizers import Optimizer\n    &gt;&gt;&gt; from fedbiomed.common.optimizers.declearn import MomentumModule, AdamModule\n    &gt;&gt;&gt; from fedbiomed.common.models import TorchModel\n    &gt;&gt;&gt; model = TorchModel(nn.Linear(4, 2))\n    &gt;&gt;&gt; optimizer = Optimizer(lr=.1, modules=[MomentumModule(), AdamModule()])\n    &gt;&gt;&gt; optim_1 = DeclearnOptimizer(model, optimizer)\n\n    &gt;&gt;&gt; optimizer = Optimizer(lr=.1, modules=[AdamModule(), MomentumModule()])\n    &gt;&gt;&gt; optim_2 = DeclearnOptimizer(model, optimizer)\n    &gt;&gt;&gt; optim_2.load_state(optim_1.save_state())\n    &gt;&gt;&gt; optim_2.save_state()['states']\n    {'modules': [('momentum', {'velocity': 0.0}),\n            ('adam',\n            {'steps': 0,\n                'vmax': None,\n                'momentum': {'state': 0.0},\n                'velocity': {'state': 0.0}})]}\n    ```\n    Modules of DeclearnOptimizer will be reloaded provided that Module is the same and occupying the same index.\n    Eg if the state contains following modules:\n    ```modules=[AdamModule(), AdagradModule(), MomemtumModule()]```\n     And the Optimizer contained in the TrainingPlan has the following modules:\n    ```modules=[AdamModule(), MomemtumModule()]```\n    Then only `AdamModule` module will be reloaded, `MomentumModule` will be set with default argument (they don't\n    share the same index in the modules list).\n\n    Args:\n        optim_state: state of the Optimizer to be loaded. It will change the current state of the optimizer\n            with the one loaded\n        load_from_state (optional): strategy for loading states: whether to load from saved states (True) or\n            from breakpoint (False).\n            If set to True, loading is done partially in the sense that if some of the OptimModules is different in\n            the optim_state and the original state of the optimizer, it loads only the OptiModule(s) from the\n            latest state that both state has in common. Defaults to False.\n\n    Raises:\n        FedbiomedOptimizerError: raised if state is not of dict type.\n\n    Returns:\n        Optimizer wrapper reloaded from `optim_state` argument.\n    \"\"\"\n    # state: breakpoint content for optimizer\n    if not isinstance(optim_state, Dict):\n        raise FedbiomedOptimizerError(f\"{ErrorNumbers.FB626.value}, incorrect type of argument `optim_state`: \"\n                                      f\"expecting a dict, but got {type(optim_state)}\")\n\n    if load_from_state:\n        # first get Optimizer detailed in the TrainingPlan.\n\n        init_optim_state = self.optimizer.get_state()  # we have to get states since it is the only way we can\n        # gather modules (other methods of `Optimizer are private`)\n\n        optim_state_copy = copy.deepcopy(optim_state)\n        optim_state.update(init_optim_state)  # optim_state will be updated with current optimizer state\n        # check if opimizer state has changed from last optimizer to the current one\n        # if it has changed, find common modules and update common states\n        for component in ( 'modules', 'regularizers',):\n            components_to_keep: List[Tuple[str, int]] = []  # we store here common Module between current Optimizer\n            # and the ones in the `optim_state` tuple (common Module name, index in List)\n\n            if not init_optim_state['states'].get(component) or not optim_state_copy['states'].get(component):\n                continue\n            self._collect_common_optimodules(\n                init_optim_state,\n                optim_state_copy,\n                component,\n                components_to_keep\n            )\n\n            for mod in components_to_keep:\n                for mod_state in optim_state_copy['states'][component]:\n                    if mod[0] == mod_state[0]:\n                        # if we do find same module in the current optimizer than the previous one,\n                        # we load the previous optimizer module state into the current one\n                        optim_state['states'][component][mod[1]] = mod_state\n\n        logger.info(\"Loading optimizer state from saved state\")\n\n    reloaded_optim = FedOptimizer.load_state(optim_state)\n    self.optimizer = reloaded_optim\n\n    return self\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.DeclearnOptimizer.optimizer_processing","title":"optimizer_processing","text":"<pre><code>optimizer_processing()\n</code></pre> <p>Provides a context manager able to do some actions before and after setting up an Optimizer, mainly disabling scikit-learn internal optimizer.</p> <p>Also, checks if <code>model_args</code> dictionary contains training parameters that won't be used or have any effect on the training, because of disabling the scikit-learn optimizer ( such as initial learning rate, learnig rate scheduler, ...). If disabling the internal optimizer leads to such changes, displays a warning.</p> <p>Returns:</p> Name Type Description <code>SklearnOptimizerProcessing</code> <code>SklearnOptimizerProcessing</code> <p>context manager providing extra logic</p> <p>Usage: <pre><code>    &gt;&gt;&gt; dlo = DeclearnSklearnOptimizer(model, optimizer)\n    &gt;&gt;&gt; with dlo.optimizer_processing():\n            model.train(inputs,targets)\n</code></pre></p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def optimizer_processing(self) -&gt; SklearnOptimizerProcessing:\n    \"\"\"Provides a context manager able to do some actions before and after setting up an Optimizer, mainly\n    disabling scikit-learn internal optimizer.\n\n    Also, checks if `model_args` dictionary contains training parameters that\n    won't be used or have any effect on the training, because of disabling the scikit-learn optimizer (\n    such as initial learning rate, learnig rate scheduler, ...). If disabling the internal optimizer leads\n    to such changes, displays a warning.\n\n    Returns:\n        SklearnOptimizerProcessing: context manager providing extra logic\n\n    Usage:\n    ```python\n        &gt;&gt;&gt; dlo = DeclearnSklearnOptimizer(model, optimizer)\n        &gt;&gt;&gt; with dlo.optimizer_processing():\n                model.train(inputs,targets)\n    ```\n    \"\"\"\n    if isinstance(self._model, SkLearnModel):\n        return SklearnOptimizerProcessing(self._model, disable_internal_optimizer=True)\n    else:\n        raise FedbiomedOptimizerError(f\"{ErrorNumbers.FB626.value}: Method optimizer_processing should be used \"\n                                      f\"only with SkLearnModel, but model is {self._model}\")\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.DeclearnOptimizer.save_state","title":"save_state","text":"<pre><code>save_state()\n</code></pre> <p>Gets optimizer state.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>optimizer state</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def save_state(self) -&gt; Dict:\n    \"\"\"Gets optimizer state.\n\n    Returns:\n        optimizer state\n    \"\"\"\n    optim_state = self.optimizer.get_state()\n    return optim_state\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.DeclearnOptimizer.send_to_device","title":"send_to_device","text":"<pre><code>send_to_device(device, idx=None)\n</code></pre> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def send_to_device(self, device: str, idx: int | None = None):\n    self.optimizer.send_to_device(device, idx)\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.DeclearnOptimizer.set_aux","title":"set_aux","text":"<pre><code>set_aux(aux)\n</code></pre> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def set_aux(self, aux: Dict[str, AuxVar]):\n    # FIXME: for imported tensors in PyTorch sent as auxiliary variables,\n    # we should push it on the appropriate device (ie cpu/gpu)\n    # TODO-PAUL: call the proper declearn routines\n    self.optimizer.set_aux(aux)\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.DeclearnOptimizer.step","title":"step","text":"<pre><code>step()\n</code></pre> <p>Performs one optimization step</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def step(self):\n    \"\"\"Performs one optimization step\"\"\"\n    # NOTA: for sklearn, gradients retrieved are unscaled because we are using learning rate equal to 1.\n    # Therefore, it is necessary to disable the sklearn internal optimizer beforehand\n    # otherwise, computation will be incorrect\n    grad = declearn.model.api.Vector.build(self._model.get_gradients())\n    weights = declearn.model.api.Vector.build(self._model.get_weights(\n        only_trainable=False,\n        exclude_buffers=True\n    ))\n    updates = self.optimizer.step(grad, weights)\n    self._model.apply_updates(updates.coefs)\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.DeclearnOptimizer.zero_grad","title":"zero_grad","text":"<pre><code>zero_grad()\n</code></pre> <p>Zeroes gradients of the Pytorch model. Basically calls the <code>zero_grad</code> method of the model.</p> <p>Raises:</p> Type Description <code>FedbiomedOptimizerError</code> <p>triggered if model has no method called <code>zero_grad</code></p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def zero_grad(self):\n    \"\"\"Zeroes gradients of the Pytorch model. Basically calls the `zero_grad`\n    method of the model.\n\n    Raises:\n        FedbiomedOptimizerError: triggered if model has no method called `zero_grad`\n    \"\"\"\n    # warning: specific for pytorch\n    if not isinstance(self._model, TorchModel):\n        raise FedbiomedOptimizerError(f\"{ErrorNumbers.FB626.value}. This method can only be used for TorchModel, \"\n                                      f\"but got {self._model}\")\n    self._model.model.zero_grad()\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.EncryptedAuxVar","title":"EncryptedAuxVar","text":"<pre><code>EncryptedAuxVar(encrypted, enc_specs, cleartext, clear_cls)\n</code></pre> <p>Container for encrypted optimizer auxiliary variables.</p> <p>This ad hoc data structure is designed to enable performing secure aggregation over auxiliary variables of declearn-backed optimizers.</p> <p>It is designed to be used in four steps:</p> <ul> <li>Encrypt the outputs of a declearn optimizer's <code>collect_aux_var</code> call,   using <code>flatten_auxvar_for_secagg</code> and a <code>SecaggCrypter</code>, then wrap the   results into an <code>EncryptedAuxVar</code></li> <li>Convert the <code>EncryptedAuxVar</code> to and from a serializable dict, enabling   to transmit it across network communications (from nodes to researcher).</li> <li>Aggregate node-wise encrypted values by summing nodes' <code>EncryptedAuxVar</code>   instances (or calling directly their <code>aggregate</code> method).</li> <li>Decrypt the resulting instance's <code>encrypted</code> values with a <code>SecaggCrypter</code>   and use <code>unflatten_auxvar_after_secagg</code> on the decrypted values and the   rest of the instance's attributes to recover auxiliary variables that can   be passed to the researcher's optimizer's <code>process_aux_var</code> method.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>encrypted</code> <code>List[List[int]]</code> <p>List of node-wise flattened, encrypted values.</p> required <code>enc_specs</code> <code>List[ValueSpec]</code> <p>List of module-wise specifications describing the flattened values' initial names, types and shapes.</p> required <code>cleartext</code> <code>List[Optional[Dict[str, Any]]]</code> <p>List of module-wise optional cleartext values, that need sharing and aggregation but not encryption.</p> required <code>clear_cls</code> <code>List[Tuple[str, Type[AuxVar]]]</code> <p>List of module-wise tuples storing module name and source <code>AuxVar</code> subtype.</p> required Source code in <code>fedbiomed/common/optimizers/_secagg.py</code> <pre><code>def __init__(\n    self,\n    encrypted: List[List[int]],\n    enc_specs: List[ValueSpec],\n    cleartext: List[Optional[Dict[str, Any]]],\n    clear_cls: List[Tuple[str, Type[AuxVar]]],\n) -&gt; None:\n    \"\"\"Instantiate a container from already encrypted information.\n\n    Args:\n        encrypted: List of node-wise flattened, encrypted values.\n        enc_specs: List of module-wise specifications describing the\n            flattened values' initial names, types and shapes.\n        cleartext: List of module-wise optional cleartext values, that\n            need sharing and aggregation but not encryption.\n        clear_cls: List of module-wise tuples storing module name and\n            source `AuxVar` subtype.\n    \"\"\"\n    self.encrypted = encrypted\n    self.enc_specs = enc_specs\n    self.cleartext = cleartext\n    self.clear_cls = clear_cls\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.EncryptedAuxVar-attributes","title":"Attributes","text":""},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.EncryptedAuxVar.clear_cls","title":"clear_cls  <code>instance-attribute</code>","text":"<pre><code>clear_cls = clear_cls\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.EncryptedAuxVar.cleartext","title":"cleartext  <code>instance-attribute</code>","text":"<pre><code>cleartext = cleartext\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.EncryptedAuxVar.enc_specs","title":"enc_specs  <code>instance-attribute</code>","text":"<pre><code>enc_specs = enc_specs\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.EncryptedAuxVar.encrypted","title":"encrypted  <code>instance-attribute</code>","text":"<pre><code>encrypted = encrypted\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.EncryptedAuxVar-functions","title":"Functions","text":""},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.EncryptedAuxVar.concatenate","title":"concatenate","text":"<pre><code>concatenate(other)\n</code></pre> <p>Concatenates a pair of EncryptedAuxVar into a single instance.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Self</code> <p><code>EncryptedAuxVar</code> instance to be aggregated with this one.</p> required <p>Returns:</p> Type Description <code>Self</code> <p><code>EncryptedAuxVar</code> instance resulting from the aggregation (with</p> <code>Self</code> <p>concatenated encrypted values, and aggregated cleartext ones).</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>if <code>other</code> is not an <code>EncryptedAuxVar</code> instance.</p> <code>ValueError</code> <p>if <code>other</code> has distinct specs from this one.</p> Source code in <code>fedbiomed/common/optimizers/_secagg.py</code> <pre><code>def concatenate(\n    self,\n    other: Self,\n) -&gt; Self:\n    \"\"\"Concatenates a pair of EncryptedAuxVar into a single instance.\n\n    Args:\n        other: `EncryptedAuxVar` instance to be aggregated with this one.\n\n    Returns:\n        `EncryptedAuxVar` instance resulting from the aggregation (with\n        concatenated encrypted values, and aggregated cleartext ones).\n\n    Raises:\n        TypeError: if `other` is not an `EncryptedAuxVar` instance.\n        ValueError: if `other` has distinct specs from this one.\n    \"\"\"\n    # Raise if instances do not match.\n    if not isinstance(other, self.__class__):\n        raise TypeError(\n            f\"'{self.__class__.__name__}.aggregate' expects an input \"\n            f\"with the same type, but received '{type(other)}'.\"\n        )\n    if self.enc_specs != other.enc_specs:\n        raise ValueError(\n            f\"Cannot sum '{self.__class__.__name__}' instances with\"\n            \" distinct specs for encrypted values.\"\n        )\n    if self.clear_cls != other.clear_cls:\n        raise ValueError(\n            f\"Cannot sum '{self.__class__.__name__}' instances with\"\n            \" distinct specs for base AuxVar classes.\"\n        )\n    # Concatenate lists of encrypted values for future sum-decryption.\n    encrypted = self.encrypted + other.encrypted\n    # Perform aggregation of cleartext values, using type-specific rules.\n    cleartext = [\n        self._aggregate_cleartext(\n            aux_cls, self.cleartext[i], other.cleartext[i]\n        )\n        for i, (_, aux_cls) in enumerate(self.clear_cls)\n    ]\n    # Wrap up results in an EncryptedAuxVar instance and return it.\n    return self.__class__(\n        encrypted=encrypted,\n        enc_specs=self.enc_specs,\n        cleartext=cleartext,\n        clear_cls=self.clear_cls,\n    )\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.EncryptedAuxVar.concatenate_from_dict","title":"concatenate_from_dict  <code>classmethod</code>","text":"<pre><code>concatenate_from_dict(data)\n</code></pre> Source code in <code>fedbiomed/common/optimizers/_secagg.py</code> <pre><code>@classmethod\ndef concatenate_from_dict(cls, data: Dict[str, Self]) -&gt; Self:\n    auxvar = list(data.values())\n    # this converts List[List[List[int]]] -&gt; List[List[int]]\n    obj = sum(auxvar[1:], start=auxvar[0])\n    return cls(obj.encrypted, obj.enc_specs, obj.cleartext, obj.clear_cls)\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.EncryptedAuxVar.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data)\n</code></pre> <p>Instantiate from a dict representation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dict representation, as emitted by this class's <code>to_dict</code>.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any required key is missing or improper.</p> Source code in <code>fedbiomed/common/optimizers/_secagg.py</code> <pre><code>@classmethod\ndef from_dict(\n    cls,\n    data: Dict[str, Any],\n) -&gt; Self:\n    \"\"\"Instantiate from a dict representation.\n\n    Args:\n        data: Dict representation, as emitted by this class's `to_dict`.\n\n    Raises:\n        TypeError: If any required key is missing or improper.\n    \"\"\"\n    try:\n        # Recover wrapped AuxVar classes from the type registry.\n        clear_cls = [\n            (name, access_registered(*info))\n            for name, info in data[\"clear_cls\"]\n        ]\n        # Ensure tuples are preserved (as serialization converts to list).\n        enc_specs = [\n            [tuple(value_specs) for value_specs in module_specs]\n            for module_specs in data[\"enc_specs\"]\n        ]\n        # Try instantiating from the input data.\n        return cls(\n            encrypted=data[\"encrypted\"],\n            enc_specs=enc_specs,  # type: ignore\n            cleartext=data[\"cleartext\"],\n            clear_cls=clear_cls,\n        )\n    except Exception as exc:\n        raise TypeError(\n            f\"Cannot instantiate '{cls.__name__}' from input dict: \"\n            f\"raised '{repr(exc)}'.\"\n        ) from exc\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.EncryptedAuxVar.get_mapping_encrypted_aux_var","title":"get_mapping_encrypted_aux_var","text":"<pre><code>get_mapping_encrypted_aux_var()\n</code></pre> Source code in <code>fedbiomed/common/optimizers/_secagg.py</code> <pre><code>def get_mapping_encrypted_aux_var(self) -&gt; Dict[str, List[int]]:\n    nodes_id = list(self.cleartext[0]['clients'])\n    return {n: p for n,p in zip(nodes_id, self.encrypted)}\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.EncryptedAuxVar.get_num_expected_params","title":"get_num_expected_params","text":"<pre><code>get_num_expected_params()\n</code></pre> <p>Return the number of flat values that should be decrypted.</p> Source code in <code>fedbiomed/common/optimizers/_secagg.py</code> <pre><code>def get_num_expected_params(\n    self,\n) -&gt; int:\n    \"\"\"Return the number of flat values that should be decrypted.\"\"\"\n    return sum(\n        size\n        for module_specs in self.enc_specs\n        for _, size, _ in module_specs\n    )\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.EncryptedAuxVar.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Return a dict representation of this instance.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict representation of this instance.</p> Source code in <code>fedbiomed/common/optimizers/_secagg.py</code> <pre><code>def to_dict(\n    self,\n) -&gt; Dict[str, Any]:\n    \"\"\"Return a dict representation of this instance.\n\n    Returns:\n        Dict representation of this instance.\n    \"\"\"\n    aux_cls_info = [\n        (name, access_registration_info(aux_cls))\n        for name, aux_cls in self.clear_cls\n    ]\n    return {\n        \"encrypted\": self.encrypted,\n        \"enc_specs\": self.enc_specs,\n        \"cleartext\": self.cleartext,\n        \"clear_cls\": aux_cls_info,\n    }\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.NativeSkLearnOptimizer","title":"NativeSkLearnOptimizer","text":"<pre><code>NativeSkLearnOptimizer(model, optimizer=None)\n</code></pre> <p>               Bases: <code>BaseOptimizer</code></p> <p>Optimizer wrapper for scikit-learn native models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SkLearnModel</code> <p>SkLearnModel model that builds a scikit-learn model.</p> required <code>optimizer</code> <code>Optional[None]</code> <p>unused. Defaults to None.</p> <code>None</code> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def __init__(self, model: SkLearnModel, optimizer: Optional[None] = None):\n    \"\"\"Constructor of the Optimizer wrapper for scikit-learn native models.\n\n    Args:\n        model: SkLearnModel model that builds a scikit-learn model.\n        optimizer: unused. Defaults to None.\n    \"\"\"\n\n    if optimizer is not None:\n        logger.info(f\"Passed Optimizer {optimizer} won't be used (using only native scikit learn optimization)\")\n    super().__init__(model, None)\n    logger.debug(\"Using native Sklearn Optimizer\")\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.NativeSkLearnOptimizer-functions","title":"Functions","text":""},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.NativeSkLearnOptimizer.optimizer_processing","title":"optimizer_processing","text":"<pre><code>optimizer_processing()\n</code></pre> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def optimizer_processing(self) -&gt; SklearnOptimizerProcessing:\n    return SklearnOptimizerProcessing(self._model, disable_internal_optimizer=False)\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.NativeSkLearnOptimizer.step","title":"step","text":"<pre><code>step()\n</code></pre> <p>Performs an optimization step and updates model weights.</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def step(self):\n    \"\"\"Performs an optimization step and updates model weights.\"\"\"\n    gradients = self._model.get_gradients()\n    updates = {k: -v for k, v in gradients.items()}\n    self._model.apply_updates(updates)\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.NativeTorchOptimizer","title":"NativeTorchOptimizer","text":"<pre><code>NativeTorchOptimizer(model, optimizer)\n</code></pre> <p>               Bases: <code>BaseOptimizer</code></p> <p>Optimizer wrapper for pytorch native optimizers and models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>TorchModel</code> <p>fedbiomed model wrapper that warps the pytorch model</p> required <code>optimizer</code> <code>Optimizer</code> <p>pytorch native optimizers (inhereting from <code>torch.optim.Optimizer</code>)</p> required <p>Raises:</p> Type Description <code>FedbiomedOptimizerError</code> <p>raised if optimizer is not a pytorch native optimizer ie a <code>torch.optim.Optimizer</code> object.</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def __init__(self, model: TorchModel, optimizer: torch.optim.Optimizer):\n    \"\"\"Constructor of the optimizer wrapper\n\n    Args:\n        model: fedbiomed model wrapper that warps the pytorch model\n        optimizer: pytorch native optimizers (inhereting from `torch.optim.Optimizer`)\n\n    Raises:\n        FedbiomedOptimizerError: raised if optimizer is not a pytorch native optimizer ie a `torch.optim.Optimizer`\n            object.\n    \"\"\"\n    if not isinstance(optimizer, torch.optim.Optimizer):\n        raise FedbiomedOptimizerError(f\"{ErrorNumbers.FB626.value} Expected a native pytorch `torch.optim` \"\n                                      f\"optimizer, but got {type(optimizer)}\")\n    super().__init__(model, optimizer)\n    logger.debug(\"using native torch optimizer\")\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.NativeTorchOptimizer-functions","title":"Functions","text":""},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.NativeTorchOptimizer.get_learning_rate","title":"get_learning_rate","text":"<pre><code>get_learning_rate()\n</code></pre> <p>Gets learning rates from param groups in Pytorch optimizer.</p> <p>For each optimizer param group, it iterates over all parameters in that parameter group and searches for the \" corresponding parameter of the model by iterating over all model parameters. If it finds a correspondence, it saves the learning rate value. This function assumes that the parameters in the optimizer and the model have the same reference.</p> <p>Warning</p> <p>This function gathers the base learning rate applied to the model weights, including alterations due to any LR scheduler. However, it does not catch any adaptive component, e.g. due to RMSProp, Adam or such.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>List[float]: list of single learning rate or multiple learning rates (as many as the number of the layers contained in the model)</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def get_learning_rate(self) -&gt; Dict[str, float]:\n    \"\"\"Gets learning rates from param groups in Pytorch optimizer.\n\n    For each optimizer param group, it iterates over all parameters in that parameter group and searches for the \"\n    corresponding parameter of the model by iterating over all model parameters. If it finds a correspondence,\n    it saves the learning rate value. This function assumes that the parameters in the optimizer and the model\n    have the same reference.\n\n\n    !!! warning\n        This function gathers the base learning rate applied to the model weights,\n        including alterations due to any LR scheduler. However, it does not catch\n        any adaptive component, e.g. due to RMSProp, Adam or such.\n\n    Returns:\n        List[float]: list of single learning rate or multiple learning rates\n            (as many as the number of the layers contained in the model)\n    \"\"\"\n    logger.warning(\n        \"`get_learning_rate` is deprecated and will be removed in future Fed-BioMed releases\",\n        broadcast=True)\n\n    mapping_lr_layer_name: Dict[str, float] = {}\n\n    for param_group in self.optimizer.param_groups:\n        for layer_params in param_group['params']:\n            for layer_name, tensor in self._model.model.named_parameters():\n                if layer_params is tensor:\n                    mapping_lr_layer_name[layer_name] = param_group['lr']\n    return mapping_lr_layer_name\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.NativeTorchOptimizer.step","title":"step","text":"<pre><code>step()\n</code></pre> <p>Performs an optimization step and updates model weights</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def step(self):\n    \"\"\"Performs an optimization step and updates model weights\n    \"\"\"\n    self.optimizer.step()\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.NativeTorchOptimizer.zero_grad","title":"zero_grad","text":"<pre><code>zero_grad()\n</code></pre> <p>Zeroes gradients of the Pytorch model. Basically calls the <code>zero_grad</code> method of the optimizer.</p> Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def zero_grad(self):\n    \"\"\"Zeroes gradients of the Pytorch model. Basically calls the `zero_grad`\n    method of the optimizer.\n    \"\"\"\n    self.optimizer.zero_grad()\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.Optimizer","title":"Optimizer","text":"<pre><code>Optimizer(lr, decay=0.0, modules=None, regularizers=None)\n</code></pre> <p>Optimizer class with a declearn-backed modular SGD-core algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>Base learning rate (i.e. step size) applied to gradients-based updates upon applying them to a model's weights.</p> required <code>decay</code> <code>float</code> <p>Optional weight decay parameter, used to parameterize a decoupled weight decay regularization term (see [1]) added to the updates right before the learning rate is applied and model weights are effectively updated.</p> <code>0.0</code> <code>modules</code> <code>Optional[Sequence[Union[OptiModule, str, Tuple[str, Dict[str, Any]]]]]</code> <p>Optional list of plug-in modules implementing gradients' alteration into model weights' udpates. Modules will be applied to gradients following this list's ordering. See <code>declearn.optimizer.modules.OptiModule</code> for details. See Notes section below for details on the \"specs\" format.</p> <code>None</code> <code>regularizers</code> <code>Optional[Sequence[Union[Regularizer, str, Tuple[str, Dict[str, Any]]]]]</code> <p>Optional list of plug-in loss regularizers. Regularizers will be applied to gradients following this list's order, prior to any other alteration (see <code>modules</code> above). See <code>declearn.optimizer.regularizers.Regularizer</code> for details. See Notes section below for details on the \"specs\" format.</p> <code>None</code> <p>Note</p> <p><code>Regularizer</code> and <code>OptiModule</code> to be used by this optimizer, specified using the <code>regularizers</code> and <code>modules</code> parameters, may be passed as ready-for-use instances, or be instantiated from specs, consisting either of a single string (the <code>name</code> attribute of the class to build) or a tuple grouping this name and a config dict (to specify some hyper-parameters).</p> <p>References</p> <p>[1] Loshchilov &amp; Hutter, 2019.     Decoupled Weight Decay Regularization.     https://arxiv.org/abs/1711.05101</p> Source code in <code>fedbiomed/common/optimizers/optimizer.py</code> <pre><code>def __init__(\n    self,\n    lr: float,\n    decay: float = 0.0,\n    modules: Optional[\n        Sequence[Union[OptiModule, str, Tuple[str, Dict[str, Any]]]]\n    ] = None,\n    regularizers: Optional[\n        Sequence[Union[Regularizer, str, Tuple[str, Dict[str, Any]]]]\n    ] = None,\n) -&gt; None:\n    \"\"\"Instantiate the declearn-issued gradient-descent optimizer.\n\n    Args:\n        lr: Base learning rate (i.e. step size) applied to gradients-based\n            updates upon applying them to a model's weights.\n        decay: Optional weight decay parameter, used to parameterize a\n            decoupled weight decay regularization term (see [1]) added to\n            the updates right before the learning rate is applied and model\n            weights are effectively updated.\n        modules: Optional list of plug-in modules implementing gradients'\n            alteration into model weights' udpates. Modules will be applied\n            to gradients following this list's ordering.\n            See `declearn.optimizer.modules.OptiModule` for details.\n            See Notes section below for details on the \"specs\" format.\n        regularizers: Optional list of plug-in loss regularizers.\n            Regularizers will be applied to gradients following this list's\n            order, prior to any other alteration (see `modules` above).\n            See `declearn.optimizer.regularizers.Regularizer` for details.\n            See Notes section below for details on the \"specs\" format.\n\n    !!! info \"Note\"\n        `Regularizer` and `OptiModule` to be used by this optimizer,\n        specified using the `regularizers` and `modules` parameters,\n        may be passed as ready-for-use instances, or be instantiated\n        from specs, consisting either of a single string (the `name`\n        attribute of the class to build) or a tuple grouping this\n        name and a config dict (to specify some hyper-parameters).\n\n    !!! info \"References\"\n        [1] Loshchilov &amp; Hutter, 2019.\n            Decoupled Weight Decay Regularization.\n            https://arxiv.org/abs/1711.05101\n    \"\"\"\n    try:\n        self._optimizer = DeclearnOptimizer(\n            lrate=lr,\n            w_decay=decay,\n            modules=modules,\n            regularizers=regularizers,\n        )\n    except (KeyError, TypeError) as exc:\n        raise FedbiomedOptimizerError(\n            f\"{ErrorNumbers.FB621.value}: declearn Optimizer instantiation\"\n            f\" raised the following exception: {repr(exc)}\"\n        ) from exc\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.Optimizer-functions","title":"Functions","text":""},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.Optimizer.from_declearn_optimizer","title":"from_declearn_optimizer  <code>classmethod</code>","text":"<pre><code>from_declearn_optimizer(declearn_optimizer)\n</code></pre> <p>Wrap a declearn Optimizer into a fed-biomed one.</p> <p>Parameters:</p> Name Type Description Default <code>declearn_optimizer</code> <code>Optimizer</code> <p>declearn.optimizer.Optimizer instance that needs to be wrapped.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>Fed-BioMed <code>Optimizer</code> instance wrapping a copy of the input declearn optimizer.</p> Source code in <code>fedbiomed/common/optimizers/optimizer.py</code> <pre><code>@classmethod\ndef from_declearn_optimizer(\n    cls,\n    declearn_optimizer: DeclearnOptimizer,\n) -&gt; Self:\n    \"\"\"Wrap a declearn Optimizer into a fed-biomed one.\n\n    Args:\n        declearn_optimizer: [declearn.optimizer.Optimizer][] instance that\n            needs to be wrapped.\n\n    Returns:\n        Fed-BioMed `Optimizer` instance wrapping a copy of the input\n            declearn optimizer.\n    \"\"\"\n    config = declearn_optimizer.get_config()\n    optim = cls(\n        lr=config[\"lrate\"],\n        decay=config[\"w_decay\"],\n        modules=config[\"modules\"],\n        regularizers=config[\"regularizers\"],\n    )\n    optim._optimizer.set_state(declearn_optimizer.get_state())\n    return optim\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.Optimizer.get_aux","title":"get_aux","text":"<pre><code>get_aux()\n</code></pre> <p>Return auxiliary variables that need to be shared across network.</p> <p>Returns:</p> Type Description <code>Dict[str, AuxVar]</code> <p>Aux-var dict that associates <code>module.collect_aux_var()</code> values to <code>module.name</code> keys for each and every module plugged in this Optimizer that has some auxiliary variables to share.</p> <p>Note</p> <p>\"Auxiliary variables\" are information that needs to be shared between the nodes and the researcher between training rounds, to synchronize some optimizer plug-ins that work by pair. Their production via this method can have internal side effects; <code>get_aux</code> should therefore be called sparingly.</p> Source code in <code>fedbiomed/common/optimizers/optimizer.py</code> <pre><code>def get_aux(self) -&gt; Dict[str, AuxVar]:\n    \"\"\"Return auxiliary variables that need to be shared across network.\n\n    Returns:\n        Aux-var dict that associates `module.collect_aux_var()` values to\n            `module.name` keys for each and every module plugged in this\n            Optimizer that has some auxiliary variables to share.\n\n    !!! info \"Note\"\n        \"Auxiliary variables\" are information that needs to be shared\n        between the nodes and the researcher between training rounds, to\n        synchronize some optimizer plug-ins that work by pair. Their\n        production via this method can have internal side effects;\n        `get_aux` should therefore be called sparingly.\n    \"\"\"\n    try:\n        return self._optimizer.collect_aux_var()\n    except Exception as exc:\n        raise FedbiomedOptimizerError(\n            f\"{ErrorNumbers.FB621.value}: error in 'get_aux': {exc}\"\n        ) from exc\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.Optimizer.get_aux_names","title":"get_aux_names","text":"<pre><code>get_aux_names()\n</code></pre> <p>Gathers list of names of modules requiring auxiliary variables</p> Source code in <code>fedbiomed/common/optimizers/optimizer.py</code> <pre><code>def get_aux_names(self) -&gt; List[str]:\n    \"\"\"Gathers list of names of modules requiring auxiliary variables\"\"\"\n    aux_names = []\n\n    for module in self._optimizer.modules:\n        if module.aux_name is not None:\n            aux_names.append(module.aux_name)\n    return aux_names\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.Optimizer.get_state","title":"get_state","text":"<pre><code>get_state()\n</code></pre> <p>Return the configuration and current states of this Optimizer.</p> <p>This method is to be used for creating breakpoints.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>State-and-config dict that may be saved as part of a breakpoint file, and used to re-create this Optimizer using the <code>Optimizer.load_state</code> classmethod constructor.</p> Source code in <code>fedbiomed/common/optimizers/optimizer.py</code> <pre><code>def get_state(self) -&gt; Dict[str, Any]:\n    \"\"\"Return the configuration and current states of this Optimizer.\n\n    This method is to be used for creating breakpoints.\n\n    Returns:\n        State-and-config dict that may be saved as part of a breakpoint\n            file, and used to re-create this Optimizer using the\n            `Optimizer.load_state` classmethod constructor.\n    \"\"\"\n    try:\n        config = self._optimizer.get_config()\n        states = self._optimizer.get_state()\n        return {\"config\": config, \"states\": states}\n    except Exception as exc:\n        raise FedbiomedOptimizerError(\n            f\"{ErrorNumbers.FB621.value}: error in 'get_state': {exc}\"\n        ) from exc\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.Optimizer.init_round","title":"init_round","text":"<pre><code>init_round()\n</code></pre> <p>Trigger start-of-training-round behavior of wrapped regularizers.</p> Source code in <code>fedbiomed/common/optimizers/optimizer.py</code> <pre><code>def init_round(self) -&gt; None:\n    \"\"\"Trigger start-of-training-round behavior of wrapped regularizers.\"\"\"\n    try:\n        self._optimizer.start_round()\n    except Exception as exc:\n        raise FedbiomedOptimizerError(\n            f\"{ErrorNumbers.FB621.value}: error in 'init_round': {exc}\"\n        ) from exc\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.Optimizer.load_state","title":"load_state  <code>classmethod</code>","text":"<pre><code>load_state(state)\n</code></pre> <p>Instantiate an Optimizer from its breakpoint state dict.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Dict[str, Any]</code> <p>state-and-config dict created using the <code>get_state</code> method.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>Optimizer instance re-created from the <code>state</code> dict.</p> <p>Raises:</p> Type Description <code>FedbiomedOptimizerError</code> <p>If the input <code>state</code> dict has improper keys or fails to set up a declearn Optimizer and set back its state.</p> Source code in <code>fedbiomed/common/optimizers/optimizer.py</code> <pre><code>@classmethod\ndef load_state(cls, state: Dict[str, Any]) -&gt; Self:\n    \"\"\"Instantiate an Optimizer from its breakpoint state dict.\n\n    Args:\n        state: state-and-config dict created using the `get_state` method.\n\n    Returns:\n        Optimizer instance re-created from the `state` dict.\n\n    Raises:\n        FedbiomedOptimizerError: If the input `state` dict has improper keys\n            or fails to set up a declearn Optimizer and set back its state.\n    \"\"\"\n    try:\n        optim = DeclearnOptimizer.from_config(state[\"config\"])\n        optim.set_state(state[\"states\"])\n    except KeyError as exc:\n        raise FedbiomedOptimizerError(\n            f\"{ErrorNumbers.FB621.value}: Missing field in the breakpoints state: {exc}\"\n        ) from exc\n    except Exception as exc:\n        raise FedbiomedOptimizerError(\n            f\"{ErrorNumbers.FB621.value}: `Optimizer.load_state`: {exc}\"\n        ) from exc\n    return cls(\n        lr=optim.lrate,\n        decay=optim.w_decay,\n        modules=optim.modules,\n        regularizers=optim.regularizers,\n    )\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.Optimizer.send_to_device","title":"send_to_device","text":"<pre><code>send_to_device(device, idx=None)\n</code></pre> <p>GPU support</p> Source code in <code>fedbiomed/common/optimizers/optimizer.py</code> <pre><code>def send_to_device(self, device: Union[str, bool], idx: Optional[int] = None):\n    \"\"\"GPU support\"\"\"\n    # for now GPU support on Researcher side is disabled\n    set_device_policy(device, idx)\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.Optimizer.set_aux","title":"set_aux","text":"<pre><code>set_aux(aux)\n</code></pre> <p>Update plug-in modules based on received shared auxiliary variables.</p> <p>Parameters:</p> Name Type Description Default <code>aux</code> <code>Dict[str, AuxVar]</code> <p>Auxiliary variables received from the counterpart optimizer (on the other side of the node-researcher frontier). On the researcher side, values must have been pre-aggregated based on the ones sent by nodes.</p> required <p>Raises:</p> Type Description <code>FedbiomedOptimizerError</code> <p>If a key from <code>aux_var</code> does not match the name of any module plugged in this optimizer (i.e. if received variables cannot be mapped to a destinatory module).</p> <p>Note</p> <p>\"Auxiliary variables\" are information that is shared between the nodes and researcher between training rounds, to synchronize some optimizer plug-ins that work by pair. The inputs to this method are not simply stored by the Optimizer, but are processed into internal side effects; this method should therefore be called sparingly.</p> Source code in <code>fedbiomed/common/optimizers/optimizer.py</code> <pre><code>def set_aux(self, aux: Dict[str, AuxVar]) -&gt; None:\n    \"\"\"Update plug-in modules based on received shared auxiliary variables.\n\n    Args:\n        aux: Auxiliary variables received from the counterpart optimizer\n            (on the other side of the node-researcher frontier). On the\n            researcher side, values must have been pre-aggregated based\n            on the ones sent by nodes.\n\n    Raises:\n        FedbiomedOptimizerError: If a key from `aux_var` does not match the\n            name of any module plugged in this optimizer (i.e. if received\n            variables cannot be mapped to a destinatory module).\n\n    !!! info \"Note\"\n        \"Auxiliary variables\" are information that is shared between the\n        nodes and researcher between training rounds, to synchronize some\n        optimizer plug-ins that work by pair. The inputs to this method are\n        not simply stored by the Optimizer, but are processed into internal\n        side effects; this method should therefore be called sparingly.\n    \"\"\"\n    try:\n        self._optimizer.process_aux_var(aux)\n    except Exception as exc:\n        raise FedbiomedOptimizerError(\n            f\"{ErrorNumbers.FB621.value}: `Optimizer.set_aux`: {exc}\"\n        ) from exc\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.Optimizer.step","title":"step","text":"<pre><code>step(grads, weights)\n</code></pre> <p>Run an optimization step to compute and return model weight updates.</p> <p>Use the pre-assigned <code>weights</code> and <code>grads</code> (set using the <code>set_weights</code> and <code>set_grads</code> methods) to compute weight updates, using the pipeline defined by this instance.</p> <p>Parameters:</p> Name Type Description Default <code>grads</code> <code>Vector</code> <p>Raw gradients based on which to compute weights updates, wrapped into a declearn Vector structure.</p> required <code>weights</code> <code>Vector</code> <p>Current values of the weights with respect to which the gradients were computed, wrapped into a declearn Vector with the same concrete type as <code>grads</code>.</p> required <p>Returns:</p> Type Description <code>Vector</code> <p>Updates to be applied to the model weights, computed by: - running wrapped gradients and weights through the regularizer   plug-ins (that add loss-regularization terms' derivatives); - running resulting gradients through the optimodule plug-ins   (that perform any defined gradient-alteration operation); - adding a decoupled weight-decay term, if one is to be used; - scaling the updates by the base learning rate. The results are wrapped into a declearn Vector structure, the concrete type of which is same as input <code>grads</code> and <code>weights</code>.</p> Source code in <code>fedbiomed/common/optimizers/optimizer.py</code> <pre><code>def step(self, grads: Vector, weights: Vector) -&gt; Vector:\n    \"\"\"Run an optimization step to compute and return model weight updates.\n\n    Use the pre-assigned `weights` and `grads` (set using the `set_weights`\n    and `set_grads` methods) to compute weight updates, using the pipeline\n    defined by this instance.\n\n    Args:\n        grads: Raw gradients based on which to compute weights updates,\n            wrapped into a declearn Vector structure.\n        weights: Current values of the weights with respect to which the\n            gradients were computed, wrapped into a declearn Vector with\n            the same concrete type as `grads`.\n\n    Returns:\n        Updates to be applied to the model weights, computed by:\n            - running wrapped gradients and weights through the regularizer\n              plug-ins (that add loss-regularization terms' derivatives);\n            - running resulting gradients through the optimodule plug-ins\n              (that perform any defined gradient-alteration operation);\n            - adding a decoupled weight-decay term, if one is to be used;\n            - scaling the updates by the base learning rate.\n            The results are wrapped into a declearn Vector structure, the\n            concrete type of which is same as input `grads` and `weights`.\n    \"\"\"\n    # This code mostly replicates that of\n    # `declearn.optimizer.Optimizer.compute_updates_from_gradients`.\n    try:\n        # Add loss-regularization terms' derivatives to the raw gradients.\n        for reg in self._optimizer.regularizers:\n            grads = reg.run(grads, weights)\n        # Iteratively refine updates by running them through the optimodules.\n        for mod in self._optimizer.modules:\n            grads = mod.run(grads)\n        # Apply the base learning rate.\n        updates = - self._optimizer.lrate * grads\n        # Optionally add the decoupled weight decay term.\n        if self._optimizer.w_decay:\n            updates -= self._optimizer.w_decay * weights\n        # Return the model updates.\n        return updates\n    except Exception as exc:\n        raise FedbiomedOptimizerError(\n            f\"{ErrorNumbers.FB621.value}: error in 'step': {exc}\"\n        ) from exc\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.SklearnOptimizerProcessing","title":"SklearnOptimizerProcessing","text":"<pre><code>SklearnOptimizerProcessing(model, disable_internal_optimizer)\n</code></pre> <p>Context manager used for scikit-learn model, that checks if model parameter(s) has(ve) been changed when disabling scikit-learn internal optimizer - ie when calling <code>disable_internal_optimizer</code> method</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SkLearnModel</code> <p>a SkLearnModel that wraps a scikit-learn model</p> required <code>disable_internal_optimizer</code> <code>bool</code> <p>whether to disable scikit-learn model internal optimizer (True) in order to apply declearn one or to keep it (False)</p> required Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code> <pre><code>def __init__(\n    self,\n    model: SkLearnModel,\n    disable_internal_optimizer: bool\n) -&gt; None:\n    \"\"\"Constructor of the object. Sets internal variables\n\n    Args:\n        model: a SkLearnModel that wraps a scikit-learn model\n        disable_internal_optimizer: whether to disable scikit-learn model internal optimizer (True) in order\n            to apply declearn one or to keep it (False)\n    \"\"\"\n    self._model = model\n    self._disable_internal_optimizer = disable_internal_optimizer\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers-functions","title":"Functions","text":""},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.flatten_auxvar_for_secagg","title":"flatten_auxvar_for_secagg","text":"<pre><code>flatten_auxvar_for_secagg(aux_var)\n</code></pre> <p>Flatten a node's optimizer auxiliary variables for secure aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>aux_var</code> <code>Dict[str, AuxVar]</code> <p>Optimizer auxiliary variables that are meant to be encrypted, formatted as a <code>{module_name: module_aux_var}</code> dict.</p> required <p>Returns:</p> Name Type Description <code>cryptable</code> <code>List[float]</code> <p>List of flattened encryptable values.</p> <code>enc_specs</code> <code>List[ValueSpec]</code> <p>List of module-wise specifications describing the flattened values' initial names, types and shapes.</p> <code>cleartext</code> <code>List[Optional[Dict[str, Any]]]</code> <p>List of module-wise optional cleartext values, that need sharing and aggregation but not encryption.</p> <code>clear_cls</code> <code>List[Tuple[str, Type[AuxVar]]]</code> <p>List of module-wise tuples storing module name and source <code>AuxVar</code> subtype.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>if a module is not compatible with SecAgg.</p> Source code in <code>fedbiomed/common/optimizers/_secagg.py</code> <pre><code>def flatten_auxvar_for_secagg(\n    aux_var: Dict[str, AuxVar],\n) -&gt; Tuple[\n    List[float],\n    List[ValueSpec],\n    List[Optional[Dict[str, Any]]],\n    List[Tuple[str, Type[AuxVar]]],\n]:\n    \"\"\"Flatten a node's optimizer auxiliary variables for secure aggregation.\n\n    Args:\n        aux_var: Optimizer auxiliary variables that are meant to be encrypted,\n            formatted as a `{module_name: module_aux_var}` dict.\n\n    Returns:\n        cryptable: List of flattened encryptable values.\n        enc_specs: List of module-wise specifications describing the\n            flattened values' initial names, types and shapes.\n        cleartext: List of module-wise optional cleartext values, that\n            need sharing and aggregation but not encryption.\n        clear_cls: List of module-wise tuples storing module name and\n            source `AuxVar` subtype.\n\n    Raises:\n        NotImplementedError: if a module is not compatible with SecAgg.\n    \"\"\"\n    # Iteratively flatten and gather specs from module-wise AuxVar objects.\n    flattened = []  # type: List[float]\n    enc_specs = []  # type: List[ValueSpec]\n    cleartext = []  # type: List[Optional[Dict[str, Any]]]\n    clear_cls = []  # type: List[Tuple[str, Type[AuxVar]]]\n    for module_name, module_auxv in aux_var.items():\n        flat, spec, clrt = _flatten_aux_var(module_auxv)\n        flattened.extend(flat)\n        enc_specs.append(spec)\n        cleartext.append(clrt)\n        clear_cls.append((module_name, type(module_auxv)))\n    # Wrap up the results into an EncryptedAuxVar instance.\n    return flattened, enc_specs, cleartext, clear_cls\n</code></pre>"},{"location":"developer/api/common/optimizers/#fedbiomed.common.optimizers.unflatten_auxvar_after_secagg","title":"unflatten_auxvar_after_secagg","text":"<pre><code>unflatten_auxvar_after_secagg(decrypted, enc_specs, cleartext, clear_cls)\n</code></pre> <p>Unflatten secure-aggregate optimizer auxiliary variables.</p> <p>Parameters:</p> Name Type Description Default <code>decrypted</code> <code>List[float]</code> <p>List of flattened decrypted (encryptable) values.</p> required <code>enc_specs</code> <code>List[ValueSpec]</code> <p>List of module-wise specifications describing the flattened values' initial names, types and shapes.</p> required <code>cleartext</code> <code>List[Optional[Dict[str, Any]]]</code> <p>List of module-wise optional cleartext values, that need sharing and aggregation but not encryption.</p> required <code>clear_cls</code> <code>List[Tuple[str, Type[AuxVar]]]</code> <p>List of module-wise tuples storing module name and source <code>AuxVar</code> subtype.</p> required <p>Returns:</p> Type Description <code>Dict[str, AuxVar]</code> <p>Unflattened optimizer auxiliary variables, as a dict</p> <code>Dict[str, AuxVar]</code> <p>with format <code>{module_name: module_aux_var}</code>.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if auxiliary variables unflattening fails.</p> Source code in <code>fedbiomed/common/optimizers/_secagg.py</code> <pre><code>def unflatten_auxvar_after_secagg(\n    decrypted: List[float],\n    enc_specs: List[ValueSpec],\n    cleartext: List[Optional[Dict[str, Any]]],\n    clear_cls: List[Tuple[str, Type[AuxVar]]],\n) -&gt; Dict[str, AuxVar]:\n    \"\"\"Unflatten secure-aggregate optimizer auxiliary variables.\n\n    Args:\n        decrypted: List of flattened decrypted (encryptable) values.\n        enc_specs: List of module-wise specifications describing the\n            flattened values' initial names, types and shapes.\n        cleartext: List of module-wise optional cleartext values, that\n            need sharing and aggregation but not encryption.\n        clear_cls: List of module-wise tuples storing module name and\n            source `AuxVar` subtype.\n\n    Returns:\n        Unflattened optimizer auxiliary variables, as a dict\n        with format `{module_name: module_aux_var}`.\n\n    Raises:\n        RuntimeError: if auxiliary variables unflattening fails.\n    \"\"\"\n    # Iteratively rebuild AuxVar instances, then return.\n    aux_var = {}  # type: Dict[str, AuxVar]\n    indx = 0\n    for i, (name, aux_cls) in enumerate(clear_cls):\n        size = sum(size for _, size, _ in enc_specs[i])\n        aux_var[name] = _unflatten_aux_var(\n            aux_cls=aux_cls,\n            flattened=decrypted[indx:indx+size],\n            enc_specs=enc_specs[i],\n            cleartext=cleartext[i],\n        )\n    return aux_var\n</code></pre>"},{"location":"developer/api/common/privacy/","title":"Privacy","text":""},{"location":"developer/api/common/privacy/#fedbiomed.common.privacy-classes","title":"Classes","text":""},{"location":"developer/api/common/privacy/#fedbiomed.common.privacy.DPController","title":"DPController","text":"<pre><code>DPController(dp_args=None)\n</code></pre> <p>Controls DP action during training.</p> <p>Parameters:</p> Name Type Description Default <code>dp_args</code> <code>Union[Dict, None]</code> <p>Arguments for differential privacy</p> <code>None</code> Source code in <code>fedbiomed/common/privacy/_dp_controller.py</code> <pre><code>def __init__(self, dp_args: Union[Dict, None] = None) -&gt; None:\n    \"\"\"Constructs DPController with given model.\n\n    Args:\n        dp_args: Arguments for differential privacy\n    \"\"\"\n    self._privacy_engine = PrivacyEngine()\n    self._dp_args = dp_args or {}\n    self._is_active = dp_args is not None\n    # Configure/validate dp arguments\n    if self._is_active:\n        self._configure_dp_args()\n</code></pre>"},{"location":"developer/api/common/privacy/#fedbiomed.common.privacy.DPController-functions","title":"Functions","text":""},{"location":"developer/api/common/privacy/#fedbiomed.common.privacy.DPController.after_training","title":"after_training","text":"<pre><code>after_training(params)\n</code></pre> <p>DP actions after the training.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict</code> <p>Contains model parameters after training with DP</p> required <p>Returns:     <code>params</code> fixed model parameters after applying differential privacy</p> Source code in <code>fedbiomed/common/privacy/_dp_controller.py</code> <pre><code>def after_training(self, params: Dict) -&gt; Dict:\n    \"\"\"DP actions after the training.\n\n    Args:\n        params: Contains model parameters after training with DP\n    Returns:\n        `params` fixed model parameters after applying differential privacy\n    \"\"\"\n    if self._is_active:\n        params = self._postprocess_dp(params)\n    return params\n</code></pre>"},{"location":"developer/api/common/privacy/#fedbiomed.common.privacy.DPController.before_training","title":"before_training","text":"<pre><code>before_training(optimizer, loader)\n</code></pre> <p>DP action before starting training.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>NativeTorchOptimizer</code> <p>NativeTorchOptimizer for training</p> required <code>loader</code> <code>DataLoader</code> <p>Data loader for training</p> required <p>Returns:</p> Type Description <code>Tuple[NativeTorchOptimizer, DPDataLoader]</code> <p>Differential privacy applied Optimizer and data loader</p> Source code in <code>fedbiomed/common/privacy/_dp_controller.py</code> <pre><code>def before_training(self,\n                    optimizer: NativeTorchOptimizer,\n                    loader: DataLoader) -&gt; Tuple[NativeTorchOptimizer, DPDataLoader]:\n    \"\"\"DP action before starting training.\n\n    Args:\n        optimizer: NativeTorchOptimizer for training\n        loader: Data loader for training\n\n    Returns:\n        Differential privacy applied Optimizer and data loader\n    \"\"\"\n\n\n    if self._is_active:\n        if not isinstance(optimizer.optimizer, torch.optim.Optimizer):\n            raise FedbiomedDPControllerError(\n                f\"{ErrorNumbers.FB616.value}: \"\n                f\"Optimizer must be an instance of torch.optim.Optimizer, but got {optimizer}\"\n                \"\\nDeclearn optimizers are not yet compatible with Differential Privacy\"\n        )\n        if not isinstance(loader, DataLoader):\n            raise FedbiomedDPControllerError(\n                f\"{ErrorNumbers.FB616.value}: \"\n                \"Data loader must be an instance of torch.utils.data.DataLoader\"\n            )\n        try:\n            optimizer._model.model, optimizer.optimizer, loader = self._privacy_engine.make_private(\n                module=optimizer._model.model,\n                optimizer=optimizer.optimizer,\n                data_loader=loader,\n                noise_multiplier=float(self._dp_args['sigma']),\n                max_grad_norm=float(self._dp_args['clip'])\n            )\n        except Exception as e:\n            raise FedbiomedDPControllerError(\n                f\"{ErrorNumbers.FB616.value}: \"\n                f\"Error while running privacy engine: {e}\"\n            )\n    return optimizer, loader\n</code></pre>"},{"location":"developer/api/common/privacy/#fedbiomed.common.privacy.DPController.validate_and_fix_model","title":"validate_and_fix_model","text":"<pre><code>validate_and_fix_model(model)\n</code></pre> <p>Validate and Fix model to be DP-compliant.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>An instance of <code>Module</code></p> required <p>Returns:</p> Type Description <code>Module</code> <p>Fixed or validated model</p> Source code in <code>fedbiomed/common/privacy/_dp_controller.py</code> <pre><code>def validate_and_fix_model(self, model: Module) -&gt; Module:\n    \"\"\"Validate and Fix model to be DP-compliant.\n\n    Args:\n        model: An instance of [`Module`][torch.nn.Module]\n\n    Returns:\n        Fixed or validated model\n    \"\"\"\n    if self._is_active and not ModuleValidator.is_valid(model):\n        try:\n            model = ModuleValidator.fix(model)\n        except Exception as e:\n            raise FedbiomedDPControllerError(\n                f\"{ErrorNumbers.FB616.value}: \"\n                f\"Error while making model DP-compliant: {e}\"\n            )\n    return model\n</code></pre>"},{"location":"developer/api/common/secagg/","title":"Secagg","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg-classes","title":"Classes","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.AdditiveSecret","title":"AdditiveSecret","text":"<pre><code>AdditiveSecret(secret)\n</code></pre> <p>Manages additive secret.</p> <p>Parameters:</p> Name Type Description Default <code>secret</code> <code>Union[int, List[int]]</code> <p>The secret to be shared, either an integer or a list of integers.</p> required <p>Raises:</p> Type Description <code>FedbiomedValueError</code> <p>If the secret is not an int or a list of integers.</p> Source code in <code>fedbiomed/common/secagg/_additive_ss.py</code> <pre><code>def __init__(self, secret: Union[int, List[int]]) -&gt; None:\n    \"\"\"\n    Initializes the AdditiveSecret class with the provided additive secret value.\n\n    Args:\n        secret: The secret to be shared, either an integer or a list of integers.\n\n    Raises:\n        FedbiomedValueError: If the secret is not an int or a list of integers.\n    \"\"\"\n    if not (\n        isinstance(secret, int)\n        or (isinstance(secret, list) and all(isinstance(i, int) for i in secret))\n    ):\n        raise FedbiomedValueError(\"AdditiveSecret must be an int or a list of int\")\n    self._secret = secret\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.AdditiveSecret-attributes","title":"Attributes","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.AdditiveSecret.secret","title":"secret  <code>property</code>","text":"<pre><code>secret\n</code></pre> <p>Getter for secret</p> <p>Returns:</p> Type Description <code>Union[List, int]</code> <p>Secret value</p>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.AdditiveSecret-functions","title":"Functions","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.AdditiveSecret.split","title":"split","text":"<pre><code>split(num_shares, bit_length=None)\n</code></pre> <p>Splits the secret into the specified number of shares using additive secret sharing. The sum of the shares will equal the original secret.</p> <p>Parameters:</p> Name Type Description Default <code>num_shares</code> <code>int</code> <p>The number of shares to generate.</p> required <code>bit_length</code> <code>Optional[int]</code> <p>The bit length of the shares. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>AdditiveShares</code> <p>AdditiveShares  object representing the split shares.</p> <p>Raises:</p> Type Description <code>FedbiomedValueError</code> <p>If the number of shares is less than or equal to 0.</p> Source code in <code>fedbiomed/common/secagg/_additive_ss.py</code> <pre><code>def split(\n    self, num_shares: int, bit_length: Optional[int] = None\n) -&gt; \"AdditiveShares\":\n    \"\"\"\n    Splits the secret into the specified number of shares using additive secret sharing.\n    The sum of the shares will equal the original secret.\n\n    Args:\n        num_shares: The number of shares to generate.\n        bit_length: The bit length of the shares. Defaults to None.\n\n    Returns:\n        AdditiveShares  object representing the split shares.\n\n    Raises:\n        FedbiomedValueError: If the number of shares is less than or equal to 0.\n    \"\"\"\n    if num_shares &lt;= 0:\n        raise FedbiomedValueError(\"Number of shares must be greater than 0\")\n\n    if isinstance(self._secret, int):\n        shares = self._shares_int(self._secret, num_shares, bit_length)\n    else:\n        shares = []\n        for value in self._secret:\n            partial_shares = self._shares_int(value, num_shares, bit_length)\n            shares.append(partial_shares)\n\n        shares = list(map(list, zip(*shares)))\n\n    return AdditiveShares([AdditiveShare(share) for share in shares])\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.AdditiveShare","title":"AdditiveShare","text":"<pre><code>AdditiveShare(value)\n</code></pre> <p>AdditiveShare class to be used after diveding secret into multiple shares</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[int, List[int]]</code> <p>The value of the share, either an integer or a list of integers.</p> required <p>Raises:</p> Type Description <code>FedbiomedTypeError</code> <p>If the value is neither an int nor a list of integers.</p> Source code in <code>fedbiomed/common/secagg/_additive_ss.py</code> <pre><code>def __init__(self, value: Union[int, List[int]]) -&gt; None:\n    \"\"\"\n    Initializes the AdditiveShare class with a given value, representing an\n    additive secret share.\n\n    Args:\n        value (Union[int, List[int]]): The value of the share, either an integer\n            or a list of integers.\n\n    Raises:\n        FedbiomedTypeError: If the value is neither an int nor a list of integers.\n    \"\"\"\n    if not (\n        isinstance(value, int)\n        or (isinstance(value, list) and all(isinstance(i, int) for i in value))\n    ):\n        raise FedbiomedTypeError(\n            \"AdditiveShare value must be an int or a list of int\"\n        )\n    self._value = value\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.AdditiveShare-attributes","title":"Attributes","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.AdditiveShare.value","title":"value  <code>property</code>","text":"<pre><code>value\n</code></pre> <p>Gets the share's value.</p> <p>Returns:</p> Type Description <code>Union[int, List[int]]</code> <p>Union[int, List[int]]: The value of the share.</p>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.AdditiveShares","title":"AdditiveShares","text":"<pre><code>AdditiveShares(shares)\n</code></pre> <p>               Bases: <code>list</code></p> <p>A class to represent a collection of AdditiveShare objects.</p> <p>Parameters:</p> Name Type Description Default <code>shares</code> <code>List[AdditiveShare]</code> <p>A list of AdditiveShare objects.</p> required <p>Raises:</p> Type Description <code>FedbiomedTypeError</code> <p>If the shares are not of type AdditiveShare.</p> Source code in <code>fedbiomed/common/secagg/_additive_ss.py</code> <pre><code>def __init__(self, shares: List[AdditiveShare]) -&gt; None:\n    \"\"\"\n    Initializes the AdditiveShares class with a list of Share objects.\n\n    Args:\n        shares: A list of AdditiveShare objects.\n\n    Raises:\n        FedbiomedTypeError: If the shares are not of type AdditiveShare.\n    \"\"\"\n    if not all(isinstance(share, AdditiveShare) for share in shares):\n        raise FedbiomedTypeError(\"All shares must be of type Share\")\n\n    super().__init__(shares)\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.AdditiveShares-functions","title":"Functions","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.AdditiveShares.reconstruct","title":"reconstruct","text":"<pre><code>reconstruct()\n</code></pre> <p>Reconstructs the secret from the shares.</p> <p>Returns:</p> Type Description <code>Union[int, List[int]]</code> <p>Union[int, List[int]]: The reconstructed secret.</p> Source code in <code>fedbiomed/common/secagg/_additive_ss.py</code> <pre><code>def reconstruct(self) -&gt; Union[int, List[int]]:\n    \"\"\"\n    Reconstructs the secret from the shares.\n\n    Returns:\n        Union[int, List[int]]: The reconstructed secret.\n    \"\"\"\n    if all(isinstance(share.value, int) for share in self):\n        result = sum(share.value for share in self)\n    elif all(isinstance(share.value, list) for share in self):\n        result = [\n            sum(share.value[i] for share in self) for i in range(len(self[0].value))\n        ]\n    else:\n        raise FedbiomedTypeError(\"Shares must be of the same type\")\n    return result\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.AdditiveShares.to_list","title":"to_list","text":"<pre><code>to_list()\n</code></pre> <p>Gets the values of the shares.</p> <p>Returns:</p> Type Description <code>List[Union[int, List[int]]]</code> <p>List[Union[int, List[int]]]: The values of the shares.</p> Source code in <code>fedbiomed/common/secagg/_additive_ss.py</code> <pre><code>def to_list(self) -&gt; List[Union[int, List[int]]]:\n    \"\"\"\n    Gets the values of the shares.\n\n    Returns:\n        List[Union[int, List[int]]]: The values of the shares.\n    \"\"\"\n    return [share.value for share in self]\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.DHKey","title":"DHKey","text":"<pre><code>DHKey(private_key_pem=None, public_key_pem=None)\n</code></pre> <p>Key handling for ephemeral-ephemeral ECDH, a session is defined for each experiment.</p> <p>This class handles the generation and importing of ECC keys using the P-256 curve. It provides methods to export the private and public keys in PEM format as bytes.</p> <p>Attributes:</p> Name Type Description <code>private_key</code> <p>The user's private ECC key.</p> <code>public_key</code> <p>The user's public ECC key.</p> <p>Parameters:</p> Name Type Description Default <code>private_key_pem</code> <code>bytes | None</code> <p>Optional. The private key in PEM format.</p> <code>None</code> <code>public_key_pem</code> <code>bytes | None</code> <p>Optional. The public key in PEM format.</p> <code>None</code> Source code in <code>fedbiomed/common/secagg/_dh.py</code> <pre><code>def __init__(\n    self,\n    private_key_pem: bytes | None = None,\n    public_key_pem: bytes | None = None\n) -&gt; None:\n    \"\"\"\n    Initializes the DHKey instance by generating a new key pair or importing the provided keys.\n\n    Args:\n        private_key_pem: Optional. The private key in PEM format.\n        public_key_pem: Optional. The public key in PEM format.\n    \"\"\"\n    if private_key_pem:\n        self.private_key = self._import_key(\n            serialization.load_pem_private_key,\n            data=private_key_pem,\n            password=None,\n            backend=default_backend()\n        )\n    elif not public_key_pem:\n        self.private_key = ec.generate_private_key(\n            ec.SECP256R1(), default_backend()\n        )\n    else:\n        # Means that only public key is loaded\n        self.private_key = None\n\n    if public_key_pem:\n        self.public_key = self._import_key(serialization.load_pem_public_key,\n                                           data=public_key_pem,\n                                           backend=default_backend()\n                                           )\n    else:\n        self.public_key = self.private_key.public_key()\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.DHKey-attributes","title":"Attributes","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.DHKey.private_key","title":"private_key  <code>instance-attribute</code>","text":"<pre><code>private_key = _import_key(load_pem_private_key, data=private_key_pem, password=None, backend=default_backend())\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.DHKey.public_key","title":"public_key  <code>instance-attribute</code>","text":"<pre><code>public_key = _import_key(load_pem_public_key, data=public_key_pem, backend=default_backend())\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.DHKey-functions","title":"Functions","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.DHKey.export_private_key","title":"export_private_key","text":"<pre><code>export_private_key()\n</code></pre> <p>Exports the private key to PEM format.</p> <p>Returns:</p> Type Description <p>The private key in PEM format. Returns <code>None</code> if private key is not loaded.</p> Source code in <code>fedbiomed/common/secagg/_dh.py</code> <pre><code>def export_private_key(self):\n    \"\"\"\n    Exports the private key to PEM format.\n\n    Returns:\n        The private key in PEM format. Returns `None` if private key\n            is not loaded.\n    \"\"\"\n    if not self.private_key:\n        return None\n\n    return self.private_key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption(),\n    )\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.DHKey.export_public_key","title":"export_public_key","text":"<pre><code>export_public_key()\n</code></pre> <p>Exports the public key to PEM format.</p> <p>Returns:</p> Type Description <p>The public key in PEM format.</p> Source code in <code>fedbiomed/common/secagg/_dh.py</code> <pre><code>def export_public_key(self):\n    \"\"\"\n    Exports the public key to PEM format.\n\n    Returns:\n        The public key in PEM format.\n    \"\"\"\n    return self.public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo,\n    )\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.DHKeyAgreement","title":"DHKeyAgreement","text":"<pre><code>DHKeyAgreement(node_u_id, node_u_dh_key, session_salt)\n</code></pre> <p>Key Agreement with ephemeral-ephemeral ECDH, a session is defined for each experiment.</p> <p>This class handles the key agreement process using the private ECC key of the user. It imports the private key, performs the key exchange, and derives the shared secret using a KDF.</p> <p>Attributes:</p> Name Type Description <code>private_key</code> <p>The user's private ECC key.</p> <p>Parameters:</p> Name Type Description Default <code>node_u_id</code> <p>The ID of the node.</p> required <code>node_u_dh_key</code> <code>DHKey</code> <p>The keypair of the node.</p> required <code>session_salt</code> <p>A session-specific salt.</p> required Source code in <code>fedbiomed/common/secagg/_dh.py</code> <pre><code>def __init__(self, node_u_id, node_u_dh_key: DHKey, session_salt):\n    \"\"\"\n    Initializes the DHKeyAgreement instance.\n\n    Args:\n        node_u_id: The ID of the node.\n        node_u_dh_key: The keypair of the node.\n        session_salt: A session-specific salt.\n    \"\"\"\n    self._node_u_id = node_u_id\n    self._dh_key = node_u_dh_key\n    self.session_salt = session_salt\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.DHKeyAgreement-attributes","title":"Attributes","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.DHKeyAgreement.session_salt","title":"session_salt  <code>instance-attribute</code>","text":"<pre><code>session_salt = session_salt\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.DHKeyAgreement-functions","title":"Functions","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.DHKeyAgreement.agree","title":"agree","text":"<pre><code>agree(node_v_id, public_key_pem)\n</code></pre> <p>Performs the key agreement and derives the shared secret.</p> <p>Parameters:</p> Name Type Description Default <code>node_v_id</code> <p>The ID of the other node (node_v).</p> required <code>public_key_pem</code> <p>The public key of the other node in PEM format as bytes.</p> required <p>Returns:</p> Type Description <p>The derived shared secret.</p> Source code in <code>fedbiomed/common/secagg/_dh.py</code> <pre><code>def agree(self, node_v_id, public_key_pem):\n    \"\"\"\n    Performs the key agreement and derives the shared secret.\n\n    Args:\n        node_v_id: The ID of the other node (node_v).\n        public_key_pem: The public key of the other node in PEM format as bytes.\n\n    Returns:\n        The derived shared secret.\n    \"\"\"\n    dh_v_key = DHKey(public_key_pem=public_key_pem)\n    shared_secret = self._dh_key.private_key.exchange(ec.ECDH(), dh_v_key.public_key)\n    derived_key = self._kdf(shared_secret, node_v_id)\n    return derived_key\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.EncryptedNumber","title":"EncryptedNumber","text":"<pre><code>EncryptedNumber(param, ciphertext)\n</code></pre> <p>               Bases: <code>object</code></p> <p>An encrypted number by one of the user keys .</p> <p>Attributes:</p> Name Type Description <code>param</code> <p>The public parameters</p> <code>ciphertext</code> <p>The integer value of the ciphertext</p> <pre><code>ciphertext: The integer value of the ciphertext\n</code></pre> Source code in <code>fedbiomed/common/secagg/_jls.py</code> <pre><code>def __init__(self, param: PublicParam, ciphertext: int):\n    \"\"\"\n\n    Args:\n        param: The public parameters.\n        ciphertext: The integer value of the ciphertext\n\n    \"\"\"\n    self.public_param = param\n    self.ciphertext = mpz(ciphertext)\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.EncryptedNumber-attributes","title":"Attributes","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.EncryptedNumber.ciphertext","title":"ciphertext  <code>instance-attribute</code>","text":"<pre><code>ciphertext = mpz(ciphertext)\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.EncryptedNumber.public_param","title":"public_param  <code>instance-attribute</code>","text":"<pre><code>public_param = param\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.JoyeLibert","title":"JoyeLibert","text":"<pre><code>JoyeLibert()\n</code></pre> <p>The Joye-Libert scheme. It consists of three Probabilistic Polynomial Time algorithms: <code>Protect</code>, and <code>Agg</code>.</p> <p>Attributes:</p> Name Type Description <code>_vector_encoder</code> <p>The vector encoding/decoding scheme</p> <p>VEParameters.TARGET_RANGE + VEParameters.WEIGHT_RANGE should be equal or less than 2**32</p> Source code in <code>fedbiomed/common/secagg/_jls.py</code> <pre><code>def __init__(self):\n    \"\"\"Constructs the class\n\n    VEParameters.TARGET_RANGE + VEParameters.WEIGHT_RANGE should be\n    equal or less than 2**32\n    \"\"\"\n    self._vector_encoder = VES(\n        ptsize=SAParameters.KEY_SIZE // 2,\n        valuesize=ceil(log2(SAParameters.TARGET_RANGE) + log2(SAParameters.WEIGHT_RANGE))\n    )\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.JoyeLibert-functions","title":"Functions","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.JoyeLibert.aggregate","title":"aggregate","text":"<pre><code>aggregate(sk_0, tau, list_y_u_tau, num_expected_params)\n</code></pre> <p>Aggregates users protected inputs with the server's secret key</p> <p>\\(X_{\\tau} \\gets \\textbf{JL.Agg}(public_param, sk_0,\\tau, \\{y_{u,\\tau}\\}_{u \\in \\{1,..,n\\}})\\)</p> <p>This algorithm aggregates the \\(n\\) ciphers received at time period \\(\\tau\\) to obtain \\(y_{\\tau} = \\prod_1^n{y_{u,\\tau}}\\) and decrypts the result. It obtains the sum of the private inputs ( \\( X_{\\tau} = \\sum_{1}^n{x_{u,\\tau}} \\) ) as follows:</p> \\[V_{\\tau} = H(\\tau)^{sk_0} \\cdot y_{\\tau} \\qquad \\qquad X_{\\tau} = \\frac{V_{\\tau}-1}{N} \\mod N\\] <p>Parameters:</p> Name Type Description Default <code>sk_0</code> <code>ServerKey</code> <p>The server's secret key \\(sk_0\\)</p> required <code>tau</code> <code>int</code> <p>The time period \\(\\tau\\)</p> required <code>list_y_u_tau</code> <code>List[List[EncryptedNumber]]</code> <p>A list of the users' protected inputs \\(\\{y_{u,\\tau}\\}_{u \\in \\{1,..,n\\}}\\)</p> required <code>num_expected_params</code> <code>int</code> <p>Number of parameters to decode from the decrypted vectors</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The sum of the users' inputs of type <code>int</code></p> <p>Raises:</p> Type Description <code>ValueError</code> <p>bad argument value</p> Source code in <code>fedbiomed/common/secagg/_jls.py</code> <pre><code>def aggregate(\n        self,\n        sk_0: ServerKey,\n        tau: int,\n        list_y_u_tau: List[List[EncryptedNumber]],\n        num_expected_params: int\n) -&gt; List[int]:\n    \"\"\"Aggregates users protected inputs with the server's secret key\n\n\n    \\\\(X_{\\\\tau} \\\\gets \\\\textbf{JL.Agg}(public_param, sk_0,\\\\tau, \\\\{y_{u,\\\\tau}\\\\}_{u \\\\in \\\\{1,..,n\\\\}})\\\\)\n\n    This algorithm aggregates the \\\\(n\\\\) ciphers received at time period \\\\(\\\\tau\\\\) to obtain\n    \\\\(y_{\\\\tau} = \\\\prod_1^n{y_{u,\\\\tau}}\\\\) and decrypts the result.\n    It obtains the sum of the private inputs ( \\\\( X_{\\\\tau} = \\\\sum_{1}^n{x_{u,\\\\tau}} \\\\) )\n    as follows:\n\n    $$V_{\\\\tau} = H(\\\\tau)^{sk_0} \\\\cdot y_{\\\\tau} \\\\qquad \\\\qquad X_{\\\\tau} = \\\\frac{V_{\\\\tau}-1}{N} \\\\mod N$$\n\n    Args:\n        sk_0: The server's secret key \\\\(sk_0\\\\)\n        tau: The time period \\\\(\\\\tau\\\\)\n        list_y_u_tau: A list of the users' protected inputs \\\\(\\\\{y_{u,\\\\tau}\\\\}_{u \\\\in \\\\{1,..,n\\\\}}\\\\)\n        num_expected_params: Number of parameters to decode from the decrypted vectors\n\n    Returns:\n        The sum of the users' inputs of type `int`\n\n    Raises:\n        ValueError: bad argument value\n    \"\"\"\n\n    if not isinstance(sk_0, ServerKey):\n        raise ValueError(\"Key must be an instance of `ServerKey`\")\n\n    if not isinstance(list_y_u_tau, list) or not list_y_u_tau:\n        raise ValueError(\"list_y_u_tau should be a non-empty list.\")\n\n    if not isinstance(list_y_u_tau[0], list):\n        raise ValueError(\"list_y_u_tau should be a list that contains list of encrypted numbers\")\n\n    n_user = len(list_y_u_tau)\n\n    sum_of_vectors: List[EncryptedNumber] = [sum(ep) for ep in zip(*list_y_u_tau)]\n\n    decrypted_vector = sk_0.decrypt(sum_of_vectors, tau)\n\n    return self._vector_encoder.decode(decrypted_vector, add_ops=n_user, v_expected=num_expected_params)\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.JoyeLibert.protect","title":"protect","text":"<pre><code>protect(public_param, user_key, tau, x_u_tau, n_users)\n</code></pre> <p>Protect user input with the user's secret key:</p> <p>\\(y_{u,\\tau} \\gets \\textbf{JL.Protect}(public_param,sk_u,\\tau,x_{u,\\tau})\\)</p> <p>This algorithm encrypts private inputs \\(x_{u,\\tau} \\in \\mathbb{Z}_N\\) for time period \\(\\tau\\) using secret key \\(sk_u \\in \\mathbb{Z}_N^2\\) . It outputs cipher \\(y_{u,\\tau}\\) such that:</p> \\[y_{u,\\tau} = (1 + x_{u,\\tau} N) H(\\tau)^{sk_u} \\mod N^2\\] <p>Parameters:</p> Name Type Description Default <code>public_param</code> <code>PublicParam</code> <p>The public parameters \\(public_param\\)</p> required <code>user_key</code> <code>UserKey</code> <p>The user's secret key \\(sk_u\\)</p> required <code>tau</code> <code>int</code> <p>The time period \\(\\tau\\)</p> required <code>x_u_tau</code> <code>List[int]</code> <p>The user's input \\(x_{u,\\tau}\\)</p> required <code>n_users</code> <code>int</code> <p>Number of nodes/users that participates secure aggregation</p> required <p>Returns:</p> Type Description <code>List[mpz]</code> <p>The protected input of type <code>EncryptedNumber</code> or a list of <code>EncryptedNumber</code></p> <p>Raises:</p> Type Description <code>TypeError</code> <p>bad argument type</p> <code>ValueError</code> <p>bad argument value</p> Source code in <code>fedbiomed/common/secagg/_jls.py</code> <pre><code>def protect(self,\n            public_param: PublicParam,\n            user_key: UserKey,\n            tau: int,\n            x_u_tau: List[int],\n            n_users: int,\n            ) -&gt; List[mpz]:\n    \"\"\" Protect user input with the user's secret key:\n\n    \\\\(y_{u,\\\\tau} \\\\gets \\\\textbf{JL.Protect}(public_param,sk_u,\\\\tau,x_{u,\\\\tau})\\\\)\n\n    This algorithm encrypts private inputs\n    \\\\(x_{u,\\\\tau} \\\\in \\\\mathbb{Z}_N\\\\) for time period \\\\(\\\\tau\\\\)\n    using secret key \\\\(sk_u \\\\in \\\\mathbb{Z}_N^2\\\\) . It outputs cipher \\\\(y_{u,\\\\tau}\\\\) such that:\n\n    $$y_{u,\\\\tau} = (1 + x_{u,\\\\tau} N) H(\\\\tau)^{sk_u} \\\\mod N^2$$\n\n    Args:\n        public_param: The public parameters \\\\(public_param\\\\)\n        user_key: The user's secret key \\\\(sk_u\\\\)\n        tau: The time period \\\\(\\\\tau\\\\)\n        x_u_tau: The user's input \\\\(x_{u,\\\\tau}\\\\)\n        n_users: Number of nodes/users that participates secure aggregation\n\n    Returns:\n            The protected input of type `EncryptedNumber` or a list of `EncryptedNumber`\n\n    Raises:\n            TypeError: bad argument type\n            ValueError: bad argument value\n    \"\"\"\n    if not isinstance(user_key, UserKey):\n        raise TypeError(f\"Expected key for encryption type is UserKey. but got {type(user_key)}\")\n\n    if user_key.public_param != public_param:\n        raise ValueError(\"Bad public parameter. The public parameter of user key does not match the \"\n                         \"one given for encryption\")\n\n    if not isinstance(x_u_tau, list):\n        raise TypeError(f\"Bad vector for encryption. Excepted argument `x_u_tau` type list but \"\n                        f\"got {type(x_u_tau)}\")\n\n    x_u_tau = self._vector_encoder.encode(\n        V=x_u_tau,\n        add_ops=n_users\n    )\n\n    return user_key.encrypt(x_u_tau, tau)\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.LOM","title":"LOM","text":"<pre><code>LOM(nonce=None)\n</code></pre> <p>Lightweight Obfuscation Mechanism (LOM) class for protecting and aggregating data.</p> <p>Attributes:</p> Name Type Description <code>_prf</code> <code>PRF</code> <p>An instance of the PRF class.</p> <code>_vector_dtype</code> <code>str</code> <p>The data type of the vector.</p> Source code in <code>fedbiomed/common/secagg/_lom.py</code> <pre><code>def __init__(\n    self,\n    nonce: bytes | None = None\n) -&gt; None:\n\n    if not nonce:\n        nonce = secrets.token_bytes(16)\n\n    self._prf: PRF = PRF(nonce)\n    self._vector_dtype: str = 'uint32'\n    self._values_bit = np.iinfo(np.dtype(self._vector_dtype)).bits  # should be equal to 32 bit\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.LOM-functions","title":"Functions","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.LOM.aggregate","title":"aggregate","text":"<pre><code>aggregate(list_y_u_tau)\n</code></pre> <p>Aggregates multiple vectors into a single vector.</p> <p>Parameters:</p> Name Type Description Default <code>list_y_u_tau</code> <code>List[List[int]]</code> <p>A list of vectors from different nodes.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The aggregated vector.</p> Source code in <code>fedbiomed/common/secagg/_lom.py</code> <pre><code>def aggregate(self, list_y_u_tau: List[List[int]]) -&gt; List[int]:\n    \"\"\"\n    Aggregates multiple vectors into a single vector.\n\n    Args:\n        list_y_u_tau: A list of vectors from different nodes.\n\n    Returns:\n        The aggregated vector.\n    \"\"\"\n    list_y_u_tau = np.array(list_y_u_tau, dtype=self._vector_dtype)\n    decrypted_vector = np.sum(list_y_u_tau, axis=0)\n    decrypted_vector = decrypted_vector.astype(np.uint32)\n    decrypted_vector = decrypted_vector.tolist()\n\n    return decrypted_vector\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.LOM.protect","title":"protect","text":"<pre><code>protect(node_id, pairwise_secrets, tau, x_u_tau, node_ids)\n</code></pre> <p>Protects the input vector by applying a mask based on pairwise secrets.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>Id of the node that applies encryption</p> required <code>pairwise_secrets</code> <code>Dict[str, bytes]</code> <p>DH agreed secrets between node that applies encryption and others</p> required <code>tau</code> <code>int</code> <p>The current round number.</p> required <code>x_u_tau</code> <code>List[int]</code> <p>The input vector to be protected.</p> required <code>node_ids</code> <code>List[str]</code> <p>A list of node IDs participates aggregation.</p> required <p>Raises:</p> Type Description <code>FedBioMedError</code> <p>raises if the input vector <code>x_u_tau</code> contains any  values that exceed  <code>32 - log_2(numner_of_nodes)</code>, where <code>32</code> is  the number of bit for each value (<code>uint32</code>). Not respecting the above condition  can lead to computation overflow.</p> <p>Returns:</p> Type Description <code>List[int]</code> <p>The protected (masked) vector.</p> Source code in <code>fedbiomed/common/secagg/_lom.py</code> <pre><code>def protect(\n    self,\n    node_id: str,\n    pairwise_secrets: Dict[str, bytes],\n    tau: int,\n    x_u_tau: List[int],\n    node_ids: List[str]\n) -&gt; List[int]:\n    \"\"\"\n    Protects the input vector by applying a mask based on pairwise secrets.\n\n    Args:\n        node_id: Id of the node that applies encryption\n        pairwise_secrets: DH agreed secrets between node that applies encryption and others\n        tau: The current round number.\n        x_u_tau: The input vector to be protected.\n        node_ids: A list of node IDs participates aggregation.\n\n    Raises:\n        FedBioMedError: raises if the input vector `x_u_tau` contains any \n            values that exceed  `32 - log_2(numner_of_nodes)`, where `32` is \n            the number of bit for each value (`uint32`). Not respecting the above condition \n            can lead to computation overflow.\n\n    Returns:\n        The protected (masked) vector.\n    \"\"\"\n\n    num_nodes = len(node_ids)\n    _max_param_bits = max(val.bit_length() for val in x_u_tau)\n    _node_bits = math.ceil(math.log2(num_nodes))\n\n    if _max_param_bits &gt; self._values_bit - _node_bits:\n        _max_nodes = 2**(self._values_bit - _max_param_bits)\n        _missing_bits = _max_param_bits + _node_bits - self._values_bit\n        raise FedbiomedSecaggError(\n            f\"{ErrorNumbers.FB417.value}: Computation overflow using LOM secagg \"\n            f\"with current settings. Cannot use more than {_max_nodes} nodes or need to \"\n            f\"reduce sample number on this node to use {_missing_bits} bit(s) less.\"\n        )\n\n    x_u_tau = np.array(x_u_tau, dtype=self._vector_dtype)\n    mask = np.zeros(len(x_u_tau), dtype=self._vector_dtype)\n\n    for pair_id in node_ids:\n\n        if pair_id == node_id:\n            continue\n\n        secret = pairwise_secrets[pair_id]\n\n        pairwise_seed = self._prf.eval_key(\n            pairwise_secret=secret,\n            tau=tau)\n\n        # print(len(pairwise_seed))\n        pairwise_vector = self._prf.eval_vector(\n            seed=pairwise_seed,\n            tau=tau,\n            input_size=len(x_u_tau))\n\n        pairwise_vector = np.frombuffer(pairwise_vector, dtype=self._vector_dtype)\n\n        if pair_id &lt; node_id:\n            mask += pairwise_vector\n        else:\n            mask -= pairwise_vector\n\n    encrypted_params = mask + x_u_tau\n\n    return encrypted_params.tolist()\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.PRF","title":"PRF","text":"<pre><code>PRF(nonce)\n</code></pre> <p>Pseudorandom Function (PRF) class using the ChaCha20 stream cipher.</p> <p>Attributes:</p> Name Type Description <code>nonce</code> <code>bytes</code> <p>A 12-byte nonce used for encryption.</p> Source code in <code>fedbiomed/common/secagg/_lom.py</code> <pre><code>def __init__(self, nonce: bytes) -&gt; None:\n    self._nonce = nonce\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.PRF-functions","title":"Functions","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.PRF.eval_key","title":"eval_key","text":"<pre><code>eval_key(pairwise_secret, tau)\n</code></pre> <p>Evaluates a pseudorandom key for a given round.</p> <p>Parameters:</p> Name Type Description Default <code>pairwise_secret</code> <code>bytes</code> <p>A secret key shared between nodes.</p> required <code>round</code> <code>int</code> <p>The current round number.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>A 32-byte pseudorandom key.</p> Source code in <code>fedbiomed/common/secagg/_lom.py</code> <pre><code>def eval_key(self, pairwise_secret: bytes, tau: int) -&gt; bytes:\n    \"\"\"\n    Evaluates a pseudorandom key for a given round.\n\n    Args:\n        pairwise_secret (bytes): A secret key shared between nodes.\n        round (int): The current round number.\n\n    Returns:\n        bytes: A 32-byte pseudorandom key.\n    \"\"\"\n    tau = tau.to_bytes(16, 'big')\n    try:\n        encryptor = Cipher(\n            algorithms.ChaCha20(pairwise_secret, self._nonce),\n            mode=None,\n            backend=default_backend()\n        ).encryptor()\n    except ValueError as ve:\n        raise FedbiomedSecaggError(f\"{ErrorNumbers.FB417.value}: Error while ciphering: got exception {ve}\")\n    c = encryptor.update(tau) + encryptor.finalize()\n    # the output is a 16 bytes string, pad it to 32 bytes\n    c = c + b'\\x00' * 16\n\n    return c\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.PRF.eval_vector","title":"eval_vector","text":"<pre><code>eval_vector(seed, tau, input_size)\n</code></pre> <p>Evaluates a pseudorandom vector based on the seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>bytes</code> <p>A 32-byte seed for generating the vector.</p> required <code>tau</code> <code>int</code> <p>The current round number.</p> required <code>input_size</code> <code>int</code> <p>The size of the input vector.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>A pseudorandom vector of the specified size.</p> Source code in <code>fedbiomed/common/secagg/_lom.py</code> <pre><code>def eval_vector(self, seed: bytes, tau: int, input_size: int) -&gt; bytes:\n    \"\"\"\n    Evaluates a pseudorandom vector based on the seed.\n\n    Args:\n        seed (bytes): A 32-byte seed for generating the vector.\n        tau (int): The current round number.\n        input_size (int): The size of the input vector.\n\n    Returns:\n        bytes: A pseudorandom vector of the specified size.\n    \"\"\"\n    encryptor = Cipher(\n        algorithms.ChaCha20(seed, self._nonce),\n        mode=None,\n        backend=default_backend()\n    ).encryptor()\n\n    # TODO: Better handling limits for secure aggregation\n    if not (input_size + _MAX_ROUND) &lt;= 2**32:\n        raise FedbiomedSecaggError(\n            f\"{ErrorNumbers.FB417.value}: Can not perform encryiton due to large input vector. input_size \"\n            f\"({input_size}) + MAX_ROUND ({_MAX_ROUND}) allowed is greater than 2**32\"\n        )\n\n    # create a list of indices from 0 to input_size where each element is concatenated with tau\n    taus = b''.join([(i + tau).to_bytes(4, 'big') for i in range(input_size)])\n    return encryptor.update(taus) + encryptor.finalize()\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.SecaggCrypter","title":"SecaggCrypter","text":"<pre><code>SecaggCrypter()\n</code></pre> <p>Secure aggregation encryption and decryption manager.</p> <p>This class is responsible for encrypting model parameters using Joye-Libert secure aggregation scheme. It also aggregates encrypted model parameters and decrypts to retrieve final model parameters as vector. This vector can be loaded into model by converting it proper format for the framework.</p> Source code in <code>fedbiomed/common/secagg/_secagg_crypter.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Constructs ParameterEncrypter\"\"\"\n    self._jls = JoyeLibert()\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.SecaggCrypter-functions","title":"Functions","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.SecaggCrypter.aggregate","title":"aggregate","text":"<pre><code>aggregate(current_round, num_nodes, params, key, biprime, total_sample_size, clipping_range=None, num_expected_params=1)\n</code></pre> <p>Decrypt given parameters</p> <p>Parameters:</p> Name Type Description Default <code>current_round</code> <code>int</code> <p>The round that the aggregation will be done</p> required <code>params</code> <code>List[List[int]]</code> <p>Aggregated/Summed encrypted parameters</p> required <code>num_nodes</code> <code>int</code> <p>number of nodes</p> required <code>key</code> <code>int</code> <p>The key that will be used for decryption</p> required <code>biprime</code> <code>int</code> <p>Biprime number of <code>PublicParam</code></p> required <code>total_sample_size</code> <code>int</code> <p>sum of number of samples from all nodes</p> required <code>clipping_range</code> <code>Union[int, None]</code> <p>Clipping range for reverse-quantization, should be the same clipping range used for quantization</p> <code>None</code> <code>num_expected_params</code> <code>int</code> <p>number of parameters to decode from the <code>params</code></p> <code>1</code> <p>Returns:     Aggregated parameters decrypted and structured</p> <p>Raises:</p> Type Description <code>FedbiomedSecaggCrypterError</code> <p>bad parameters</p> <code>FedbiomedSecaggCrypterError</code> <p>aggregation issue</p> Source code in <code>fedbiomed/common/secagg/_secagg_crypter.py</code> <pre><code>def aggregate(\n        self,\n        current_round: int,\n        num_nodes: int,\n        params: List[List[int]],\n        key: int,\n        biprime: int,\n        total_sample_size: int,\n        clipping_range: Union[int, None] = None,\n        num_expected_params: int = 1\n) -&gt; List[float]:\n    \"\"\"Decrypt given parameters\n\n    Args:\n        current_round: The round that the aggregation will be done\n        params: Aggregated/Summed encrypted parameters\n        num_nodes: number of nodes\n        key: The key that will be used for decryption\n        biprime: Biprime number of `PublicParam`\n        total_sample_size: sum of number of samples from all nodes\n        clipping_range: Clipping range for reverse-quantization, should be the\n            same clipping range used for quantization\n        num_expected_params: number of parameters to decode from the `params`\n    Returns:\n        Aggregated parameters decrypted and structured\n\n    Raises:\n         FedbiomedSecaggCrypterError: bad parameters\n         FedbiomedSecaggCrypterError: aggregation issue\n    \"\"\"\n    start = time.process_time()\n\n    if len(params) != num_nodes:\n        raise FedbiomedSecaggCrypterError(\n            f\"{ErrorNumbers.FB624.value}: Num of parameters that are received from nodes \"\n            f\"does not match the number of nodes has been set for the encrypter. There might \"\n            f\"be some nodes did not answered to training request or num of clients of \"\n            \"`ParameterEncrypter` has not been set properly before train request.\")\n\n    if not isinstance(params, list) or not all(isinstance(p, list) for p in params):\n        raise FedbiomedSecaggCrypterError(f\"{ErrorNumbers.FB624}: The parameters to aggregate should be a \"\n                                          f\"list containing list of parameters\")\n\n    if not all(all(isinstance(p_, int) for p_ in p) for p in params):\n        raise FedbiomedSecaggCrypterError(f\"{ErrorNumbers.FB624}: Invalid parameter type. The parameters \"\n                                          f\"should be of type of integers.\")\n\n    # TODO provide dynamically created biprime. Biprime that is used\n    #  on the node-side should matched the one used for decryption\n    public_param = self._setup_public_param(biprime=biprime)\n    key = ServerKey(public_param, key)\n\n    params = self._convert_to_encrypted_number(params, public_param)\n\n    try:\n        sum_of_weights = self._jls.aggregate(\n            sk_0=key,\n            tau=current_round,  # The time period \\\\(\\\\tau\\\\)\n            list_y_u_tau=params,\n            num_expected_params=num_expected_params\n        )\n    except (ValueError, TypeError) as e:\n        raise FedbiomedSecaggCrypterError(f\"{ErrorNumbers.FB624.value}: The aggregation of encrypted parameters \"\n                                          f\"is not successful: {e}\")\n\n    # Reverse quantize and division (averaging)\n    logger.info(f\"Aggregating {len(params)} parameters from {num_nodes} nodes.\")\n    aggregated_params = self._apply_average(sum_of_weights, total_sample_size)\n\n    aggregated_params: List[float] = reverse_quantize(\n        aggregated_params,\n        clipping_range=clipping_range\n    )\n    time_elapsed = time.process_time() - start\n    logger.debug(f\"Aggregation is completed in {round(time_elapsed, ndigits=2)} seconds.\")\n\n    return aggregated_params\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.SecaggCrypter.encrypt","title":"encrypt","text":"<pre><code>encrypt(num_nodes, current_round, params, key, biprime, clipping_range=None, weight=None)\n</code></pre> <p>Encrypts model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>num_nodes</code> <code>int</code> <p>Number of nodes that is expected to encrypt parameters for aggregation</p> required <code>current_round</code> <code>int</code> <p>Current round of federated training</p> required <code>params</code> <code>List[float]</code> <p>List of flatten parameters</p> required <code>key</code> <code>int</code> <p>Key to encrypt</p> required <code>biprime</code> <code>int</code> <p>Prime number to create public parameter</p> required <code>weight</code> <code>Optional[int]</code> <p>Weight for the params</p> <code>None</code> <code>clipping_range</code> <code>Union[int, None]</code> <p>Clipping-range for quantization of float model parameters. Clipping range must grater than minimum model parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>List[int]</code> <p>List of encrypted parameters</p> <p>Raises:</p> Type Description <code>FedbiomedSecaggCrypterError</code> <p>bad parameters</p> <code>FedbiomedSecaggCrypterError</code> <p>encryption issue</p> Source code in <code>fedbiomed/common/secagg/_secagg_crypter.py</code> <pre><code>def encrypt(\n        self,\n        num_nodes: int,\n        current_round: int,\n        params: List[float],\n        key: int,\n        biprime: int,\n        clipping_range: Union[int, None] = None,\n        weight: Optional[int] = None,\n) -&gt; List[int]:\n    \"\"\"Encrypts model parameters.\n\n    Args:\n        num_nodes: Number of nodes that is expected to encrypt parameters for aggregation\n        current_round: Current round of federated training\n        params: List of flatten parameters\n        key: Key to encrypt\n        biprime: Prime number to create public parameter\n        weight: Weight for the params\n        clipping_range: Clipping-range for quantization of float model parameters. Clipping range\n            must grater than minimum model parameters\n\n    Returns:\n        List of encrypted parameters\n\n    Raises:\n        FedbiomedSecaggCrypterError: bad parameters\n        FedbiomedSecaggCrypterError: encryption issue\n    \"\"\"\n\n    start = time.process_time()\n\n    if not isinstance(params, list):\n        raise FedbiomedSecaggCrypterError(\n            f\"{ErrorNumbers.FB624.value}: Expected argument `params` type list but got {type(params)}\"\n        )\n\n    if not all([isinstance(p, float) for p in params]):\n        raise FedbiomedSecaggCrypterError(\n            f\"{ErrorNumbers.FB624.value}: The parameters to encrypt should list of floats. \"\n            f\"There are one or more than a value that is not type of float.\"\n        )\n\n    # Make use the key is instance of\n    if not isinstance(key, int):\n        raise FedbiomedSecaggCrypterError(\n            f\"{ErrorNumbers.FB624.value}: The argument `key` must be integer\"\n        )\n\n    # first we quantize the parameters, and we get params in the range [0, 2^VEParameters.TARGET_RANGE]\n    params = quantize(weights=params,\n                      clipping_range=clipping_range)\n\n    # We multiply the parameters with the weight, and we get params in\n    # the range [0, 2^(log2(VEParameters.TARGET_RANGE) + log2(VEParameters.WEIGHT_RANGE)) - 1]\n    # Check if weight if num_bits of weight is less than VEParameters.WEIGHT_RANGE\n    if weight is not None:\n        if 2**weight.bit_length() &gt; SAParameters.WEIGHT_RANGE:\n            raise FedbiomedSecaggCrypterError(\n                f\"{ErrorNumbers.FB624.value}: The weight is too large. The weight should be less than \"\n                f\"{SAParameters.WEIGHT_RANGE}, but got {weight}\"\n            )\n        params = self._apply_weighting(params, weight)\n\n\n    public_param = self._setup_public_param(biprime=biprime)\n\n    # Instantiates UserKey object\n    key = UserKey(public_param, key)\n\n    try:\n        # Encrypt parameters\n\n        encrypted_params: List[mpz] = self._jls.protect(\n            public_param=public_param,\n            user_key=key,\n            tau=current_round,\n            x_u_tau=params,\n            n_users=num_nodes\n        )\n    except (TypeError, ValueError) as exp:\n        raise FedbiomedSecaggCrypterError(\n            f\"{ErrorNumbers.FB624.value} Error during parameter encryption. {exp}\") from exp\n\n    time_elapsed = time.process_time() - start\n    logger.debug(f\"Encryption of the parameters took {time_elapsed} seconds.\")\n\n    return [int(e_p) for e_p in encrypted_params]\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.SecaggLomCrypter","title":"SecaggLomCrypter","text":"<pre><code>SecaggLomCrypter(nonce=None)\n</code></pre> <p>               Bases: <code>SecaggCrypter</code></p> <p>Low-Overhead Masking secure aggregation controller</p> <p>Parameters:</p> Name Type Description Default <code>nonce</code> <code>str | None</code> <p><code>nonce</code> to use in encryption. Needs to be the same between the parties of the LOM computation. Can be disclosed (public). Must not be re-used.</p> <code>None</code> Source code in <code>fedbiomed/common/secagg/_secagg_crypter.py</code> <pre><code>def __init__(\n    self,\n    nonce: str | None = None\n):\n    \"\"\"LOM Secure aggregation to encrypt and aggregate\n\n    Args:\n        nonce: `nonce` to use in encryption. Needs to be the same between the parties of\n            the LOM computation. Can be disclosed (public). Must not be re-used.\n    \"\"\"\n    if nonce:\n        # The security relies on the non-reuse of the nonce.\n        # We also need to ensure 128 bits\n        # Padding is enough, using `random()` is misleading (no additional security)\n        nonce = str.encode(nonce).zfill(16)[:16]\n\n    self._lom = LOM(nonce)\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.SecaggLomCrypter-functions","title":"Functions","text":""},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.SecaggLomCrypter.aggregate","title":"aggregate","text":"<pre><code>aggregate(params, total_sample_size, clipping_range=None)\n</code></pre> <p>Decrypt given parameters</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>List[List[int]]</code> <p>Aggregated/Summed encrypted parameters</p> required <code>total_sample_size</code> <code>int</code> <p>sum of number of samples from all nodes</p> required <code>clipping_range</code> <code>Union[int, None]</code> <p>Clipping range for reverse-quantization, should be the same clipping range used for quantization</p> <code>None</code> <p>Returns:     Aggregated parameters decrypted and structured</p> <p>Raises:</p> Type Description <code>FedbiomedSecaggCrypterError</code> <p>bad parameters</p> <code>FedbiomedSecaggCrypterError</code> <p>aggregation issue</p> Source code in <code>fedbiomed/common/secagg/_secagg_crypter.py</code> <pre><code>def aggregate(\n        self,\n        params: List[List[int]],\n        total_sample_size: int,\n        clipping_range: Union[int, None] = None,\n) -&gt; List[float]:\n    \"\"\"Decrypt given parameters\n\n    Args:\n        params: Aggregated/Summed encrypted parameters\n        total_sample_size: sum of number of samples from all nodes\n        clipping_range: Clipping range for reverse-quantization, should be the\n            same clipping range used for quantization\n    Returns:\n        Aggregated parameters decrypted and structured\n\n    Raises:\n         FedbiomedSecaggCrypterError: bad parameters\n         FedbiomedSecaggCrypterError: aggregation issue\n    \"\"\"\n    start = time.process_time()\n\n    if not isinstance(params, list) or not all(isinstance(p, list) for p in params):\n        raise FedbiomedSecaggCrypterError(f\"{ErrorNumbers.FB624}: The parameters to aggregate should be a \"\n                                          f\"list containing list of parameters\")\n\n    if not all(all(isinstance(p_, int) for p_ in p) for p in params):\n        raise FedbiomedSecaggCrypterError(f\"{ErrorNumbers.FB624}: Invalid parameter type. The parameters \"\n                                          f\"should be of type of integers.\")\n\n    num_nodes = len(params)\n\n    try:\n        sum_of_weights = self._lom.aggregate(\n            list_y_u_tau=params,\n        )\n    except (ValueError, TypeError) as e:\n        raise FedbiomedSecaggCrypterError(\n            f\"{ErrorNumbers.FB624.value}: The aggregation of encrypted parameters \"\n            f\"is not successful: {e}\") from e\n\n\n    # Reverse quantize and division (averaging)\n    logger.info(f\"Aggregating {len(params)} parameters from {num_nodes} nodes.\")\n    aggregated_params = self._apply_average(sum_of_weights, total_sample_size)\n\n    aggregated_params: List[float] = reverse_quantize(\n        aggregated_params,\n        clipping_range=clipping_range\n    )\n    time_elapsed = time.process_time() - start\n    logger.debug(f\"Aggregation is completed in {round(time_elapsed, ndigits=2)} seconds.\")\n\n    return aggregated_params\n</code></pre>"},{"location":"developer/api/common/secagg/#fedbiomed.common.secagg.SecaggLomCrypter.encrypt","title":"encrypt","text":"<pre><code>encrypt(current_round, node_id, params, pairwise_secrets, node_ids, clipping_range=None, weight=None)\n</code></pre> <p>Encrypts model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>current_round</code> <code>int</code> <p>Current round of federated training</p> required <code>node_id</code> <code>str</code> <p>ID of the node applies encryption</p> required <code>params</code> <code>List[float]</code> <p>List of flatten parameters</p> required <code>pairwise_secrets</code> <code>Dict[str, bytes]</code> <p>DH agreed secrets between node that applies encryption and others</p> required <code>node_ids</code> <code>List[str]</code> <p>All nodes that participates secure aggregation</p> required <code>weight</code> <code>Optional[int]</code> <p>Weight for the params</p> <code>None</code> <code>clipping_range</code> <code>Union[int, None]</code> <p>Clipping-range for quantization of float model parameters. Clipping range must grater than minimum model parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>List[int]</code> <p>List of encrypted parameters</p> <p>Raises:</p> Type Description <code>FedbiomedSecaggCrypterError</code> <p>bad parameters</p> <code>FedbiomedSecaggCrypterError</code> <p>encryption issue</p> Source code in <code>fedbiomed/common/secagg/_secagg_crypter.py</code> <pre><code>def encrypt(\n    self,\n    current_round: int,\n    node_id: str,\n    params: List[float],\n    pairwise_secrets: Dict[str, bytes],\n    node_ids: List[str],\n    clipping_range: Union[int, None] = None,\n    weight: Optional[int] = None,\n) -&gt; List[int]:\n    \"\"\"Encrypts model parameters.\n\n    Args:\n        current_round: Current round of federated training\n        node_id: ID of the node applies encryption\n        params: List of flatten parameters\n        pairwise_secrets: DH agreed secrets between node that applies encryption and others\n        node_ids: All nodes that participates secure aggregation\n        weight: Weight for the params\n        clipping_range: Clipping-range for quantization of float model parameters. Clipping range\n            must grater than minimum model parameters\n\n    Returns:\n        List of encrypted parameters\n\n    Raises:\n        FedbiomedSecaggCrypterError: bad parameters\n        FedbiomedSecaggCrypterError: encryption issue\n    \"\"\"\n\n    start = time.process_time()\n\n    if not isinstance(params, list):\n        raise FedbiomedSecaggCrypterError(\n            f\"{ErrorNumbers.FB624.value}: Expected argument `params` type list but got {type(params)}\"\n        )\n\n    if not all(isinstance(p, float) for p in params):\n        raise FedbiomedSecaggCrypterError(\n            f\"{ErrorNumbers.FB624.value}: The parameters to encrypt should list of floats. \"\n            f\"There are one or more than a value that is not type of float.\"\n        )\n\n    params = quantize(weights=params,\n                      clipping_range=clipping_range)\n\n    if weight is not None:\n        if 2**weight.bit_length() &gt; SAParameters.WEIGHT_RANGE:\n            raise FedbiomedSecaggCrypterError(\n                f\"{ErrorNumbers.FB624.value}: The weight is too large. The weight should be less than \"\n                f\"{SAParameters.WEIGHT_RANGE}.\"\n            )\n        params = self._apply_weighting(params, weight)\n\n    try:\n        # Encrypt parameters\n        encrypted_params: List[int] = self._lom.protect(\n            pairwise_secrets=pairwise_secrets,\n            node_id=node_id,\n            tau=current_round,\n            x_u_tau=params,\n            node_ids=node_ids\n        )\n    except (TypeError, ValueError) as exp:\n        raise FedbiomedSecaggCrypterError(\n            f\"{ErrorNumbers.FB624.value} Error during parameter encryption. {exp}\") from exp\n\n\n    time_elapsed = time.process_time() - start\n    logger.debug(f\"Encryption of the parameters took {time_elapsed} seconds.\")\n\n    return encrypted_params\n</code></pre>"},{"location":"developer/api/common/secagg_manager/","title":"Secagg Manager","text":"<p>Interface with the component secure aggregation element database</p>"},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager-attributes","title":"Attributes","text":""},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager-classes","title":"Classes","text":""},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.BaseSecaggManager","title":"BaseSecaggManager","text":"<pre><code>BaseSecaggManager(db_path)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Manage a component secagg element database</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>path to the component's secagg database</p> required <p>Raises:</p> Type Description <code>FedbiomedSecaggError</code> <p>failed to access the database</p> Source code in <code>fedbiomed/common/secagg_manager.py</code> <pre><code>def __init__(self, db_path: str):\n    \"\"\"Constructor of the class\n\n    Args:\n        db_path: path to the component's secagg database\n\n    Raises:\n        FedbiomedSecaggError: failed to access the database\n    \"\"\"\n    try:\n        self._db = TinyDB(db_path)\n        self._db.table_class = DBTable\n    except Exception as e:\n        errmess = f'{ErrorNumbers.FB623.value}: failed to access the database with error: {e}'\n        logger.error(errmess)\n        raise FedbiomedSecaggError(errmess)\n\n    self._query = Query()\n    self._table = _SecaggTableSingleton(self._db).table\n</code></pre>"},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.BaseSecaggManager-functions","title":"Functions","text":""},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.BaseSecaggManager.add","title":"add  <code>abstractmethod</code>","text":"<pre><code>add(secagg_id, parties, context, experiment_id)\n</code></pre> <p>Add a new data entry in component secagg element database</p> Source code in <code>fedbiomed/common/secagg_manager.py</code> <pre><code>@abstractmethod\ndef add(self, secagg_id: str, parties: List[str], context: Dict[str, int], experiment_id: str):\n    \"\"\"Add a new data entry in component secagg element database\"\"\"\n</code></pre>"},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.BaseSecaggManager.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(secagg_id, experiment_id)\n</code></pre> <p>Search for a data entry in component secagg element database</p> Source code in <code>fedbiomed/common/secagg_manager.py</code> <pre><code>@abstractmethod\ndef get(self, secagg_id: str, experiment_id: str):\n    \"\"\"Search for a data entry in component secagg element database\"\"\"\n</code></pre>"},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.BaseSecaggManager.remove","title":"remove  <code>abstractmethod</code>","text":"<pre><code>remove(secagg_id, experiment_id)\n</code></pre> <p>Remove a data entry from component secagg element database</p> Source code in <code>fedbiomed/common/secagg_manager.py</code> <pre><code>@abstractmethod\ndef remove(self, secagg_id: str, experiment_id: str) -&gt; bool:\n    \"\"\"Remove a data entry from component secagg element database\"\"\"\n</code></pre>"},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.SecaggDhManager","title":"SecaggDhManager","text":"<pre><code>SecaggDhManager(db_path)\n</code></pre> <p>               Bases: <code>BaseSecaggManager</code></p> <p>Manage the secagg table elements for Diffie Hellman components</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>path to the component's secagg database</p> required <p>Raises:</p> Type Description <code>FedbiomedSecaggError</code> <p>failed to access the database</p> Source code in <code>fedbiomed/common/secagg_manager.py</code> <pre><code>def __init__(self, db_path: str):\n    \"\"\"Constructor of the class\n\n    Args:\n        db_path: path to the component's secagg database\n\n    Raises:\n        FedbiomedSecaggError: failed to access the database\n    \"\"\"\n    try:\n        self._db = TinyDB(db_path)\n        self._db.table_class = DBTable\n    except Exception as e:\n        errmess = f'{ErrorNumbers.FB623.value}: failed to access the database with error: {e}'\n        logger.error(errmess)\n        raise FedbiomedSecaggError(errmess)\n\n    self._query = Query()\n    self._table = _SecaggTableSingleton(self._db).table\n</code></pre>"},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.SecaggDhManager-functions","title":"Functions","text":""},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.SecaggDhManager.add","title":"add","text":"<pre><code>add(secagg_id, parties, context, experiment_id)\n</code></pre> <p>Add a new data entry for a context element in the secagg table</p> <p>Check that no entry exists yet for this <code>secagg_id</code> in the table.</p> <p>Parameters:</p> Name Type Description Default <code>secagg_id</code> <code>str</code> <p>secure aggregation ID key of the entry</p> required <code>parties</code> <code>List[str]</code> <p>list of parties participating in this secagg context element</p> required <code>experiment_id</code> <code>str</code> <p>ID of the experiment to which this secagg context element is attached</p> required <code>context</code> <code>Dict[str, bytes]</code> <p>server key part held by this party</p> required Source code in <code>fedbiomed/common/secagg_manager.py</code> <pre><code>def add(self, secagg_id: str, parties: List[str], context: Dict[str, bytes], experiment_id: str):\n    \"\"\"Add a new data entry for a context element in the secagg table\n\n    Check that no entry exists yet for this `secagg_id` in the table.\n\n    Args:\n        secagg_id: secure aggregation ID key of the entry\n        parties: list of parties participating in this secagg context element\n        experiment_id: ID of the experiment to which this secagg context element is attached\n        context: server key part held by this party\n    \"\"\"\n    # Save key pairs as `str`` since it is the format support by JSON. Need to convert to `base64` first\n    context_json = {node_id: str(base64.b64encode(key), 'utf-8') for node_id, key in context.items()}\n\n    self._add_generic(\n        SecaggElementTypes.DIFFIE_HELLMAN,\n        secagg_id,\n        parties,\n        {'experiment_id': experiment_id, 'context': context_json}\n    )\n</code></pre>"},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.SecaggDhManager.get","title":"get","text":"<pre><code>get(secagg_id, experiment_id)\n</code></pre> <p>Search for data entry with given <code>secagg_id</code></p> <p>Check that there is at most one entry with this unique secagg ID.</p> <p>If there is an entry for this <code>secagg_id</code>, check it is associated with experiment <code>experiment_id</code></p> <p>Parameters:</p> Name Type Description Default <code>secagg_id</code> <code>str</code> <p>secure aggregation ID key to search</p> required <code>experiment_id</code> <code>str</code> <p>the experiment ID associated with the secagg entry</p> required <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>A dict containing all values for the secagg element for this <code>secagg_id</code> if it exists, or None if no element exists for this <code>secagg_id</code></p> Source code in <code>fedbiomed/common/secagg_manager.py</code> <pre><code>def get(self, secagg_id: str, experiment_id: str) -&gt; Union[dict, None]:\n    \"\"\"Search for data entry with given `secagg_id`\n\n    Check that there is at most one entry with this unique secagg ID.\n\n    If there is an entry for this `secagg_id`, check it is associated with experiment `experiment_id`\n\n    Args:\n        secagg_id: secure aggregation ID key to search\n        experiment_id: the experiment ID associated with the secagg entry\n\n    Returns:\n        A dict containing all values for the secagg element for this `secagg_id` if it exists,\n            or None if no element exists for this `secagg_id`\n    \"\"\"\n    # Trust argument type and value check from calling class (`SecaggSetup`, `Node`)\n    element = self._get_generic(secagg_id)\n    self._raise_error_incompatible_requested_entry(element,\n                                                   SecaggElementTypes.DIFFIE_HELLMAN,\n                                                   secagg_id,\n                                                   experiment_id,\n                                                   'getting')\n\n    if element:\n        # Need to convert to keys as bytes\n        context_bytes = {\n            node_id: bytes(base64.b64decode(key)) \\\n                for node_id, key in element['context'].items()}\n        element['context'] = context_bytes\n\n    return element\n</code></pre>"},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.SecaggDhManager.remove","title":"remove","text":"<pre><code>remove(secagg_id, experiment_id)\n</code></pre> <p>Remove data entry for this <code>secagg_id</code> from the secagg table</p> <p>Check that the experiment ID for the table entry and the current experiment match</p> <p>Parameters:</p> Name Type Description Default <code>secagg_id</code> <code>str</code> <p>secure aggregation ID key of the entry</p> required <code>experiment_id</code> <code>str</code> <p>experiment ID of the current experiment</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if an entry existed (and was removed) for this <code>secagg_id</code>, False if no entry existed for this <code>secagg_id</code></p> <p>Raises:</p> Type Description <code>FedbiomedSecaggError</code> <p>database entry does not belong to <code>experiment_id</code></p> Source code in <code>fedbiomed/common/secagg_manager.py</code> <pre><code>def remove(self, secagg_id: str, experiment_id: str) -&gt; bool:\n    \"\"\"Remove data entry for this `secagg_id` from the secagg table\n\n    Check that the experiment ID for the table entry and the current experiment match\n\n    Args:\n        secagg_id: secure aggregation ID key of the entry\n        experiment_id: experiment ID of the current experiment\n\n    Returns:\n        True if an entry existed (and was removed) for this `secagg_id`,\n            False if no entry existed for this `secagg_id`\n\n    Raises:\n        FedbiomedSecaggError: database entry does not belong to `experiment_id`\n    \"\"\"\n    # Trust argument type and value check from calling class for `secagg_id` (`SecaggSetup`, but not `Node`)\n    # Don't trust `Node` for `experiment_id` type (may give `None`) but this is not an issue\n    element = self._get_generic(secagg_id)\n    self._raise_error_incompatible_requested_entry(element,\n                                                   SecaggElementTypes.DIFFIE_HELLMAN,\n                                                   secagg_id,\n                                                   experiment_id,\n                                                   'removing')\n    return self._remove_generic(secagg_id, SecaggElementTypes.DIFFIE_HELLMAN)\n</code></pre>"},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.SecaggServkeyManager","title":"SecaggServkeyManager","text":"<pre><code>SecaggServkeyManager(db_path)\n</code></pre> <p>               Bases: <code>BaseSecaggManager</code></p> <p>Manage the component server key secagg element database table</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>path to the component's secagg database</p> required Source code in <code>fedbiomed/common/secagg_manager.py</code> <pre><code>def __init__(self, db_path: str):\n    \"\"\"Constructor of the class\n\n    Args:\n        db_path: path to the component's secagg database\n    \"\"\"\n    super().__init__(db_path)\n</code></pre>"},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.SecaggServkeyManager-functions","title":"Functions","text":""},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.SecaggServkeyManager.add","title":"add","text":"<pre><code>add(secagg_id, parties, context, experiment_id)\n</code></pre> <p>Add a new data entry for a context element in the secagg table</p> <p>Check that no entry exists yet for this <code>secagg_id</code> in the table.</p> <p>Parameters:</p> Name Type Description Default <code>secagg_id</code> <code>str</code> <p>secure aggregation ID key of the entry</p> required <code>parties</code> <code>List[str]</code> <p>list of parties participating in this secagg context element</p> required <code>context</code> <code>Dict[str, int]</code> <p>server key part held by this party</p> required <code>experiment_id</code> <code>str</code> <p>ID of the experiment to which this secagg context element is attached</p> required Source code in <code>fedbiomed/common/secagg_manager.py</code> <pre><code>def add(self, secagg_id: str, parties: List[str], context: Dict[str, int], experiment_id: str):\n    \"\"\"Add a new data entry for a context element in the secagg table\n\n    Check that no entry exists yet for this `secagg_id` in the table.\n\n    Args:\n        secagg_id: secure aggregation ID key of the entry\n        parties: list of parties participating in this secagg context element\n        context: server key part held by this party\n        experiment_id: ID of the experiment to which this secagg context element is attached\n    \"\"\"\n\n    # Trust argument type and value check from calling class (`SecaggSetup`, but not `Node`)\n    self._add_generic(\n        SecaggElementTypes.SERVER_KEY,\n        secagg_id,\n        parties,\n        {'experiment_id': experiment_id, 'context': context}\n    )\n</code></pre>"},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.SecaggServkeyManager.get","title":"get","text":"<pre><code>get(secagg_id, experiment_id)\n</code></pre> <p>Search for data entry with given <code>secagg_id</code></p> <p>Check that there is at most one entry with this unique secagg ID.</p> <p>If there is an entry for this <code>secagg_id</code>, check it is associated with experiment <code>experiment_id</code></p> <p>Parameters:</p> Name Type Description Default <code>secagg_id</code> <code>str</code> <p>secure aggregation ID key to search</p> required <code>experiment_id</code> <code>str</code> <p>the experiment ID associated with the secagg entry</p> required <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>A dict containing all values for the secagg element for this <code>secagg_id</code> if it exists, or None if no element exists for this <code>secagg_id</code></p> Source code in <code>fedbiomed/common/secagg_manager.py</code> <pre><code>def get(self, secagg_id: str, experiment_id: str) -&gt; Union[dict, None]:\n    \"\"\"Search for data entry with given `secagg_id`\n\n    Check that there is at most one entry with this unique secagg ID.\n\n    If there is an entry for this `secagg_id`, check it is associated with experiment `experiment_id`\n\n    Args:\n        secagg_id: secure aggregation ID key to search\n        experiment_id: the experiment ID associated with the secagg entry\n\n    Returns:\n        A dict containing all values for the secagg element for this `secagg_id` if it exists,\n            or None if no element exists for this `secagg_id`\n    \"\"\"\n\n    # Trust argument type and value check from calling class (`SecaggSetup`, `Node`)\n    element = self._get_generic(secagg_id)\n    self._raise_error_incompatible_requested_entry(element,\n                                                   SecaggElementTypes.SERVER_KEY,\n                                                   secagg_id,\n                                                   experiment_id,\n                                                   'getting')\n\n    return element\n</code></pre>"},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager.SecaggServkeyManager.remove","title":"remove","text":"<pre><code>remove(secagg_id, experiment_id)\n</code></pre> <p>Remove data entry for this <code>secagg_id</code> from the secagg table</p> <p>Check that the experiment ID for the table entry and the current experiment match</p> <p>Parameters:</p> Name Type Description Default <code>secagg_id</code> <code>str</code> <p>secure aggregation ID key of the entry</p> required <code>experiment_id</code> <code>str</code> <p>experiment ID of the current experiment</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if an entry existed (and was removed) for this <code>secagg_id</code>, False if no entry existed for this <code>secagg_id</code></p> Source code in <code>fedbiomed/common/secagg_manager.py</code> <pre><code>def remove(self, secagg_id: str, experiment_id: str) -&gt; bool:\n    \"\"\"Remove data entry for this `secagg_id` from the secagg table\n\n    Check that the experiment ID for the table entry and the current experiment match\n\n    Args:\n        secagg_id: secure aggregation ID key of the entry\n        experiment_id: experiment ID of the current experiment\n\n    Returns:\n        True if an entry existed (and was removed) for this `secagg_id`,\n            False if no entry existed for this `secagg_id`\n    \"\"\"\n\n    # Trust argument type and value check from calling class for `secagg_id` (`SecaggSetup`, but not `Node`)\n    # Don't trust `Node` for `experiment_id` type (may give `None`) but this is not an issue\n    element = self._get_generic(secagg_id)\n    self._raise_error_incompatible_requested_entry(element,\n                                                   SecaggElementTypes.SERVER_KEY,\n                                                   secagg_id,\n                                                   experiment_id,\n                                                   'removing')\n    return self._remove_generic(secagg_id, SecaggElementTypes.SERVER_KEY)\n</code></pre>"},{"location":"developer/api/common/secagg_manager/#fedbiomed.common.secagg_manager-functions","title":"Functions","text":""},{"location":"developer/api/common/serializer/","title":"Serializer","text":"<p>MsgPack serialization utils, wrapped into a namespace class.</p>"},{"location":"developer/api/common/serializer/#fedbiomed.common.serializer-attributes","title":"Attributes","text":""},{"location":"developer/api/common/serializer/#fedbiomed.common.serializer-classes","title":"Classes","text":""},{"location":"developer/api/common/serializer/#fedbiomed.common.serializer.Serializer","title":"Serializer","text":"<p>MsgPack-based (de)serialization utils, wrapped into a namespace class.</p> <p>This class has no value being instantiated: it merely acts as a namespace to pack together encoding and decoding utils to convert data to and from MsgPack dump bytes or binary files.</p> <p>The MsgPack encoding and decoding capabilities are enhanced to add support for the following non-standard object types:     - numpy arrays and scalars     - torch tensors (that are always loaded on CPU)     - tuples (which would otherwise be converted to lists)</p>"},{"location":"developer/api/common/serializer/#fedbiomed.common.serializer.Serializer-functions","title":"Functions","text":""},{"location":"developer/api/common/serializer/#fedbiomed.common.serializer.Serializer.dump","title":"dump  <code>classmethod</code>","text":"<pre><code>dump(obj, path)\n</code></pre> <p>Serialize data into a MsgPack binary dump file.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Data that needs encoding.</p> required <code>path</code> <code>str</code> <p>Path to the created dump file.</p> required Source code in <code>fedbiomed/common/serializer.py</code> <pre><code>@classmethod\ndef dump(cls, obj: Any, path: str) -&gt; None:\n    \"\"\"Serialize data into a MsgPack binary dump file.\n\n    Args:\n        obj: Data that needs encoding.\n        path: Path to the created dump file.\n    \"\"\"\n    with open(path, \"wb\") as file:\n        msgpack.pack(obj, file, default=cls._default, strict_types=True)\n</code></pre>"},{"location":"developer/api/common/serializer/#fedbiomed.common.serializer.Serializer.dumps","title":"dumps  <code>classmethod</code>","text":"<pre><code>dumps(obj, write_to=None)\n</code></pre> <p>Serialize data into MsgPack-encoded bytes.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Data that needs encoding.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>MsgPack-encoded bytes that contains the input data.</p> Source code in <code>fedbiomed/common/serializer.py</code> <pre><code>@classmethod\ndef dumps(cls, obj: Any, write_to: Optional[str] = None) -&gt; bytes:\n    \"\"\"Serialize data into MsgPack-encoded bytes.\n\n    Args:\n        obj: Data that needs encoding.\n\n    Returns:\n        MsgPack-encoded bytes that contains the input data.\n    \"\"\"\n    ser = msgpack.packb(obj, default=cls._default, strict_types=True)\n\n    if write_to:\n        with open(write_to, \"wb\") as file:\n            file.write(ser)\n            file.close()\n\n    return ser\n</code></pre>"},{"location":"developer/api/common/serializer/#fedbiomed.common.serializer.Serializer.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(path)\n</code></pre> <p>Load serialized data from a MsgPack dump file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a MsgPack file, the contents of which to decode.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Data loaded and decoded from the target file.</p> Source code in <code>fedbiomed/common/serializer.py</code> <pre><code>@classmethod\ndef load(cls, path: str) -&gt; Any:\n    \"\"\"Load serialized data from a MsgPack dump file.\n\n    Args:\n        path: Path to a MsgPack file, the contents of which to decode.\n\n    Returns:\n        Data loaded and decoded from the target file.\n    \"\"\"\n    with open(path, \"rb\") as file:\n        return msgpack.unpack(\n            file, object_hook=cls._object_hook, strict_map_key=False\n        )\n</code></pre>"},{"location":"developer/api/common/serializer/#fedbiomed.common.serializer.Serializer.loads","title":"loads  <code>classmethod</code>","text":"<pre><code>loads(data)\n</code></pre> <p>Load serialized data from a MsgPack-encoded string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>MsgPack-encoded bytes that needs decoding.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Data loaded and decoded from the input bytes.</p> Source code in <code>fedbiomed/common/serializer.py</code> <pre><code>@classmethod\ndef loads(cls, data: bytes) -&gt; Any:\n    \"\"\"Load serialized data from a MsgPack-encoded string.\n\n    Args:\n        data: MsgPack-encoded bytes that needs decoding.\n\n    Returns:\n        Data loaded and decoded from the input bytes.\n    \"\"\"\n    return msgpack.unpackb(data, object_hook=cls._object_hook, strict_map_key=False)\n</code></pre>"},{"location":"developer/api/common/singleton/","title":"Singleton","text":"<p>Singleton metaclass. used to easily create thread safe singleton classes</p>"},{"location":"developer/api/common/singleton/#fedbiomed.common.singleton-classes","title":"Classes","text":""},{"location":"developer/api/common/singleton/#fedbiomed.common.singleton.SingletonABCMeta","title":"SingletonABCMeta","text":"<p>               Bases: <code>ABCMeta</code>, <code>type</code></p>"},{"location":"developer/api/common/singleton/#fedbiomed.common.singleton.SingletonMeta","title":"SingletonMeta","text":"<p>               Bases: <code>type</code></p> <p>This (meta) class is a thread safe singleton implementation. It should be used as a metaclass of a new class (NC), which will then provide a singleton-like class (i.e. an instance of the class NC will be a singleton)</p> <p>This metaclass is used in several fedbiomed classes (Request, FedLogger,...)</p>"},{"location":"developer/api/common/synchro/","title":"Synchro","text":""},{"location":"developer/api/common/synchro/#fedbiomed.common.synchro-attributes","title":"Attributes","text":""},{"location":"developer/api/common/synchro/#fedbiomed.common.synchro.GRACE_TRIGGERED_EVENT","title":"GRACE_TRIGGERED_EVENT  <code>module-attribute</code>","text":"<pre><code>GRACE_TRIGGERED_EVENT = 10\n</code></pre>"},{"location":"developer/api/common/synchro/#fedbiomed.common.synchro.MAX_TRIGGERED_EVENT_TIMEOUT","title":"MAX_TRIGGERED_EVENT_TIMEOUT  <code>module-attribute</code>","text":"<pre><code>MAX_TRIGGERED_EVENT_TIMEOUT = 60\n</code></pre>"},{"location":"developer/api/common/synchro/#fedbiomed.common.synchro-classes","title":"Classes","text":""},{"location":"developer/api/common/synchro/#fedbiomed.common.synchro.EventWaitExchange","title":"EventWaitExchange","text":"<pre><code>EventWaitExchange(remove_delivered)\n</code></pre> <p>Provides thread safe synchronized data exchange object.</p> <p>Object users can wait on one or more events registered with an <code>event_id</code> that must be chosen to ensure it is unique in the object.</p> <p>Object users can trigger events using an <code>event_id</code> that may already be waited (or not). Arbitrary data can be associated to the event and will be transmitted to the receiving waiter(s).</p> <p>One or more waiter can wait on the same event. If the event is removed when delivered, to a waiter then only one waiter receives it. If the event is not removed when delivered, then multiple waiters can receive it until it is cleaned on timeout.</p> <p>Parameters:</p> Name Type Description Default <code>remove_delivered</code> <p>if True, remove from an event from the triggered event when they are delivered</p> required Source code in <code>fedbiomed/common/synchro.py</code> <pre><code>def __init__(self, remove_delivered):\n    \"\"\"Constructor of the class.\n\n    Args:\n        remove_delivered: if True, remove from an event from the triggered event when they are delivered\n    \"\"\"\n    self._remove_delivered = remove_delivered\n\n    # table for storing the triggered event received, still waiting to be consumed\n    self._triggered_events = {}\n    # lock for accessing self._triggered_events\n    self._triggered_events_lock = threading.Lock()\n\n    # table for tracking pending requests of the listeners waiting for events\n    # Key is an internal unique `id_counter`, value is a list of `event_id`\n    self._pending_listeners = {}\n    # lock for accessing self._pending_listeners\n    self._pending_listeners_lock = threading.Lock()\n\n    self._id_counter = 0\n</code></pre>"},{"location":"developer/api/common/synchro/#fedbiomed.common.synchro.EventWaitExchange-functions","title":"Functions","text":""},{"location":"developer/api/common/synchro/#fedbiomed.common.synchro.EventWaitExchange.event","title":"event","text":"<pre><code>event(event_id, event_data)\n</code></pre> <p>Add an entry to the table of triggered event</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>str</code> <p>unique ID of the event</p> required <code>event_data</code> <code>Any</code> <p>arbitrary data to transmit to the event receiver</p> required Source code in <code>fedbiomed/common/synchro.py</code> <pre><code>def event(self, event_id: str, event_data: Any) -&gt; None:\n    \"\"\"Add an entry to the table of triggered event\n\n    Args:\n        event_id: unique ID of the event\n        event_data: arbitrary data to transmit to the event receiver\n    \"\"\"\n\n    with self._triggered_events_lock:\n        # Remove obsolete triggered events. We could additionally call at other times.\n        self._clean_triggered_events()\n\n        # In case a event already exists for this ID, overwrite it with the newer one\n        self._triggered_events[event_id] = {\n            \"start_time\": time.time(),\n            \"data\": event_data,\n        }\n    # check if added event completes some listeners\n    completed_listeners = self._all_events()\n    with self._pending_listeners_lock:\n        for completed_listener in completed_listeners:\n            # check: listener may have been removed since tested `_all_events`\n            # as we didn't keep the lock\n            if completed_listener in self._pending_listeners:\n                # wake up waiting listener\n                self._pending_listeners[completed_listener][\"event\"].set()\n</code></pre>"},{"location":"developer/api/common/synchro/#fedbiomed.common.synchro.EventWaitExchange.wait","title":"wait","text":"<pre><code>wait(event_ids, timeout)\n</code></pre> <p>Wait for a registered listener to complete.</p> <p>Blocks until all events for this listener are triggered or until timeout is reached.</p> <p>Parameters:</p> Name Type Description Default <code>event_ids</code> <code>list[str]</code> <p>list of the unique IDs for all events to wait</p> required <code>timeout</code> <code>float</code> <p>maximum time to wait for replies in seconds</p> required <p>Returns:</p> Type Description <code>Tuple[bool, List[Any]]</code> <p>A tuple consisting of - a bool set to <code>True</code> if all events for this listener were delivered to this     request, <code>False</code> if not - a list of the arbitrary data associated with each delivered event</p> <p>Raises:</p> Type Description <code>FedbiomedNodeToNodeError</code> <p>timeout has incorrect type or value</p> Source code in <code>fedbiomed/common/synchro.py</code> <pre><code>def wait(self, event_ids: list[str], timeout: float) -&gt; Tuple[bool, List[Any]]:\n    \"\"\"Wait for a registered listener to complete.\n\n    Blocks until all events for this listener are triggered or until timeout is reached.\n\n    Args:\n        event_ids: list of the unique IDs for all events to wait\n        timeout: maximum time to wait for replies in seconds\n\n    Returns:\n        A tuple consisting of\n            - a bool set to `True` if all events for this listener were delivered to this\n                request, `False` if not\n            - a list of the arbitrary data associated with each delivered event\n\n    Raises:\n           FedbiomedNodeToNodeError: timeout has incorrect type or value\n    \"\"\"\n\n    # Check value for timeout, as bad value may cause hard to detect problems\n    if (\n        not isinstance(timeout, (float, int))\n        or timeout &lt; 0\n        or timeout &gt; MAX_TRIGGERED_EVENT_TIMEOUT\n    ):\n        raise FedbiomedSynchroError(\n            f\"{ErrorNumbers.FB324}: Cannot wait {timeout} seconds. \"\n            f\"Should be int or float between 0 and {MAX_TRIGGERED_EVENT_TIMEOUT}\"\n        )\n\n    time_initial = time.time()\n    with self._pending_listeners_lock:\n        self._id_counter += 1\n        listener_id = self._id_counter\n        event = threading.Event()\n        self._pending_listeners[listener_id] = {\n            \"start_time\": time_initial,\n            \"event\": event,\n            \"event_ids\": event_ids,\n        }\n\n    # wait until all events are triggered or timeout is reached\n    while not self._all_events(listener_id) and (\n        time.time() &lt; time_initial + timeout\n    ):\n        # be sure not to hold any lock when waiting !\n        event.wait(time_initial + timeout - time.time())\n        event.clear()\n\n    with self._pending_listeners_lock:\n        with self._triggered_events_lock:\n            # check if all events were received (and are still available for delivery)\n            all_received = set(\n                self._pending_listeners[listener_id][\"event_ids\"]\n            ).issubset(set(self._triggered_events.keys()))\n\n            # create list of delivered data\n            events_data = [\n                self._triggered_events[reqid][\"data\"]\n                for reqid in self._pending_listeners[listener_id][\"event_ids\"]\n                if reqid in self._triggered_events\n            ]\n\n            if self._remove_delivered:\n                # remove all events for this request from the triggered events available\n                # for delivery\n                for reqid in self._pending_listeners[listener_id][\"event_ids\"]:\n                    if reqid in self._triggered_events:\n                        self._triggered_events.pop(reqid)\n\n            # remove expired listener\n            del self._pending_listeners[listener_id]\n\n    return all_received, events_data\n</code></pre>"},{"location":"developer/api/common/tasks_queue/","title":"TasksQueue","text":"<p>Queue module that contains task queue class that is a wrapper to the persistqueue python library.</p>"},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue-attributes","title":"Attributes","text":""},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue-classes","title":"Classes","text":""},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue","title":"TasksQueue","text":"<pre><code>TasksQueue(messages_queue_dir, tmp_dir)\n</code></pre> <p>A disk-persistent Queue object, ensuring queue will remain on disk even if program crashes.</p> <p>Relies on <code>persistqueue</code> package.</p> <p>Parameters:</p> Name Type Description Default <code>messages_queue_dir</code> <code>str</code> <p>directory where enqueued data should be persisted.</p> required <code>tmp_dir</code> <code>str</code> <p>indicates where temporary files should be stored.</p> required Source code in <code>fedbiomed/common/tasks_queue.py</code> <pre><code>def __init__(self, messages_queue_dir: str, tmp_dir: str):\n    \"\"\"Construct disk-persistent Queue.\n\n    Args:\n        messages_queue_dir: directory where enqueued data should be persisted.\n        tmp_dir: indicates where temporary files should be stored.\n    \"\"\"\n    try:\n        # small chunksize to limit un-needed use of disk space\n        self.queue = persistqueue.Queue(messages_queue_dir, tempdir=tmp_dir, chunksize=1)\n    except ValueError as e:\n        msg = ErrorNumbers.FB603.value + \": cannot create queue (\" + str(e) + \")\"\n        logger.critical(msg)\n        raise FedbiomedTaskQueueError(msg)\n</code></pre>"},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue-attributes","title":"Attributes","text":""},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue.queue","title":"queue  <code>instance-attribute</code>","text":"<pre><code>queue = Queue(messages_queue_dir, tempdir=tmp_dir, chunksize=1)\n</code></pre>"},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue-functions","title":"Functions","text":""},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue.add","title":"add","text":"<pre><code>add(task)\n</code></pre> <p>Adds a task to the queue</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>dict</code> <p>a dict describing the task to be added</p> required Source code in <code>fedbiomed/common/tasks_queue.py</code> <pre><code>def add(self, task: dict):\n    \"\"\"Adds a task to the queue\n\n    Args:\n        task: a dict describing the task to be added\n    \"\"\"\n    try:\n        self.queue.put(task)\n    except persistqueue.exceptions.Full:\n        msg = ErrorNumbers.FB603.value + \": queue is full\"\n        logger.critical(msg)\n        raise FedbiomedTaskQueueError(msg)\n</code></pre>"},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue.get","title":"get","text":"<pre><code>get(block=True)\n</code></pre> <p>Get the current task in the queue</p> <p>Parameters:</p> Name Type Description Default <code>block</code> <code>Optional[bool]</code> <p>if True, block if necessary until an item is available. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary object stored in queue</p> <p>Raises:</p> Type Description <code>FedbiomedTaskQueueError</code> <p>If queue is empty</p> Source code in <code>fedbiomed/common/tasks_queue.py</code> <pre><code>def get(self, block: Optional[bool] = True) -&gt; dict:\n    \"\"\"Get the current task in the queue\n\n    Args:\n        block: if True, block if necessary until an item is available. Defaults to True.\n\n    Returns:\n        Dictionary object stored in queue\n\n    Raises:\n        FedbiomedTaskQueueError: If queue is empty\n    \"\"\"\n    try:\n        return self.queue.get(block)\n    except persistqueue.exceptions.Empty:\n        msg = ErrorNumbers.FB603.value + \": queue is empty\"\n        logger.debug(msg)\n        raise FedbiomedTaskQueueError(msg)\n</code></pre>"},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue.qsize","title":"qsize","text":"<pre><code>qsize()\n</code></pre> <p>Retrieve the size of the queue</p> <p>Returns:</p> Type Description <code>int</code> <p>size of the queue</p> Source code in <code>fedbiomed/common/tasks_queue.py</code> <pre><code>def qsize(self) -&gt; int:\n    \"\"\"Retrieve the size of the queue\n\n    Returns:\n        size of the queue\n    \"\"\"\n    return self.queue.qsize()\n</code></pre>"},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue.task_done","title":"task_done","text":"<pre><code>task_done()\n</code></pre> <p>Indicate whether a formerly enqueued task is complete</p> <p>Returns:</p> Type Description <code>Any</code> <p>True if task is complete</p> Source code in <code>fedbiomed/common/tasks_queue.py</code> <pre><code>def task_done(self) -&gt; Any:\n    \"\"\"Indicate whether a formerly enqueued task is complete\n\n    Returns:\n        True if task is complete\n    \"\"\"\n    try:\n        return self.queue.task_done()\n    except ValueError:\n        # persistqueue raises it if task_done called too many times we can ignore it\n        return\n</code></pre>"},{"location":"developer/api/common/training_args/","title":"TrainingArgs","text":"<p>Provide a way to easily to manage training arguments.</p>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args-attributes","title":"Attributes","text":""},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.DPArgsValidator","title":"DPArgsValidator  <code>module-attribute</code>","text":"<pre><code>DPArgsValidator = SchemeValidator({'type': {'rules': [str, _validate_dp_type], 'required': True, 'default': 'central'}, 'sigma': {'rules': [float], 'required': True}, 'clip': {'rules': [float], 'required': True}})\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args-classes","title":"Classes","text":""},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs","title":"TrainingArgs","text":"<pre><code>TrainingArgs(ta=None, extra_scheme=None, only_required=True)\n</code></pre> <p>Provide a container to manage training arguments.</p> <p>This class uses the Validator and SchemeValidator classes and provides a default scheme, which describes the arguments necessary to train/validate a TrainingPlan.</p> <p>It also permits to extend the TrainingArgs then testing new features by supplying an extra_scheme at TrainingArgs instantiation.</p> <p>Parameters:</p> Name Type Description Default <code>ta</code> <code>Dict</code> <p>dictionary describing the TrainingArgs scheme.     if empty dict or None, a minimal instance of TrainingArgs     will be initialized with default values for required keys</p> <code>None</code> <code>extra_scheme</code> <code>Dict</code> <p>user provided scheme extension, which add new rules or     update the scheme of the default training args.     Warning: this is a dangerous feature, provided to     developers, to ease the test of future Fed-Biomed features</p> <code>None</code> <code>only_required</code> <code>bool</code> <p>if True, the object is initialized only with required     values defined in the default_scheme (+ extra_scheme).     If False, then all default values will also be returned     (not only the required key/value pairs).</p> <code>True</code> <p>Raises:</p> Type Description <code>FedbiomedUserInputError</code> <p>in case of bad value or bad extra_scheme</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def __init__(self, ta: Dict = None, extra_scheme: Dict = None, only_required: bool = True):\n    \"\"\"\n    Create a TrainingArgs from a Dict with input validation.\n\n    Args:\n        ta:     dictionary describing the TrainingArgs scheme.\n                if empty dict or None, a minimal instance of TrainingArgs\n                will be initialized with default values for required keys\n        extra_scheme: user provided scheme extension, which add new rules or\n                update the scheme of the default training args.\n                Warning: this is a dangerous feature, provided to\n                developers, to ease the test of future Fed-Biomed features\n        only_required: if True, the object is initialized only with required\n                values defined in the default_scheme (+ extra_scheme).\n                If False, then all default values will also be returned\n                (not only the required key/value pairs).\n\n    Raises:\n        FedbiomedUserInputError: in case of bad value or bad extra_scheme\n    \"\"\"\n\n    self._scheme = TrainingArgs.default_scheme()\n\n    if not isinstance(extra_scheme, dict):\n        extra_scheme = {}\n\n    for k in extra_scheme:\n        self._scheme[k] = extra_scheme[k]\n\n    try:\n        self._sc = SchemeValidator(self._scheme)\n    except RuleError as e:\n        #\n        # internal error (invalid scheme)\n        msg = ErrorNumbers.FB414.value + f\": {e}\"\n        logger.critical(msg)\n        raise FedbiomedUserInputError(msg)\n\n    # scheme is validated from here\n    if ta is None:\n        ta = {}\n\n    try:\n        self._ta = self._sc.populate_with_defaults(ta, only_required=only_required)\n    except ValidatorError as e:\n        # scheme has required keys without defined default value\n        msg = ErrorNumbers.FB414.value + f\": {e}\"\n        logger.critical(msg)\n        raise FedbiomedUserInputError(msg)\n\n    try:\n        self._sc.validate(self._ta)\n    except ValidateError as e:\n        # transform to a Fed-BioMed error\n        msg = ErrorNumbers.FB414.value + f\": {e}\"\n        logger.critical(msg)\n        raise FedbiomedUserInputError(msg)\n\n    # Validate DP arguments if it is existing in training arguments\n    if self._ta[\"dp_args\"] is not None:\n        try:\n            self._ta[\"dp_args\"] = DPArgsValidator.populate_with_defaults(self._ta[\"dp_args\"], only_required=False)\n            DPArgsValidator.validate(self._ta[\"dp_args\"])\n        except ValidateError as e:\n            msg = f\"{ErrorNumbers.FB414.value}: {e}\"\n            logger.critical(msg)\n            raise FedbiomedUserInputError(msg)\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs-functions","title":"Functions","text":""},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.default_scheme","title":"default_scheme  <code>classmethod</code>","text":"<pre><code>default_scheme()\n</code></pre> <p>Returns the default (base) scheme for TrainingArgs.</p> <p>A summary of the semantics of each argument is given below. Please refer to the source code of this function for additional information on typing and constraints.</p> argument meaning optimizer_args supplemental arguments for initializing the optimizer loader_args supplemental arguments passed to the data loader epochs the number of epochs performed during local training on each node num_updates the number of model updates performed during local training on each node. Supersedes epochs if both are specified use_gpu toggle requesting the use of GPUs for local training on the node when available, propagates to `declearn's GPU dry_run perform a single model update for testing on each node and correctly handle GPU execution batch_maxnum prematurely break after batch_maxnum model updates for each epoch (useful for testing) test_ratio the proportion of validation samples to total number of samples in the dataset test_batch_size batch size used for testing trained model wrt a set of metric test_on_local_updates toggles validation after local training test_on_global_updates toggles validation before local training shuffle_testing_dataset whether reset or not the testing (and training) dataset for this <code>Round</code> test_metric metric to be used for validation test_metric_args supplemental arguments for the validation metric log_interval output a training logging entry every log_interval model updates fedprox_mu set the value of mu and enable FedProx correction dp_args arguments for Differential Privacy share_persistent_buffers toggle whether nodes share the full state_dict (when True) or only trainable parameters (False) in a TorchTrainingPlan random_seed set random seed at the beginning of each round Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>@classmethod\ndef default_scheme(cls) -&gt; Dict:\n    \"\"\"\n    Returns the default (base) scheme for TrainingArgs.\n\n    A summary of the semantics of each argument is given below. Please refer to the source code of this function\n    for additional information on typing and constraints.\n\n    | argument | meaning |\n    | -------- | ------- |\n    | optimizer_args | supplemental arguments for initializing the optimizer |\n    | loader_args | supplemental arguments passed to the data loader |\n    | epochs | the number of epochs performed during local training on each node |\n    | num_updates | the number of model updates performed during local training on each node. Supersedes epochs if both are specified |\n    | use_gpu | toggle requesting the use of GPUs for local training on the node when available, propagates to `declearn's GPU |\n    | dry_run | perform a single model update for testing on each node and correctly handle GPU execution |\n    | batch_maxnum | prematurely break after batch_maxnum model updates for each epoch (useful for testing) |\n    | test_ratio | the proportion of validation samples to total number of samples in the dataset |\n    | test_batch_size | batch size used for testing trained model wrt a set of metric |\n    | test_on_local_updates | toggles validation after local training |\n    | test_on_global_updates | toggles validation before local training |\n    | shuffle_testing_dataset | whether reset or not the testing (and training) dataset for this `Round` |\n    | test_metric | metric to be used for validation |\n    | test_metric_args | supplemental arguments for the validation metric |\n    | log_interval | output a training logging entry every log_interval model updates |\n    | fedprox_mu | set the value of mu and enable FedProx correction |\n    | dp_args | arguments for Differential Privacy |\n    | share_persistent_buffers | toggle whether nodes share the full state_dict (when True) or only trainable parameters (False) in a TorchTrainingPlan |\n    | random_seed | set random seed at the beginning of each round |\n\n    \"\"\"\n    return {\n        \"optimizer_args\": {\n            \"rules\": [dict], \"required\": True, \"default\": {}\n        },\n        \"loader_args\": {\n            \"rules\": [dict], \"required\": True, \"default\": {}\n        },\n        \"epochs\": {\n            \"rules\": [cls._nonnegative_integer_value_validator_hook('epochs')], \"required\": True, \"default\": None\n        },\n        \"num_updates\": {\n            \"rules\": [cls._nonnegative_integer_value_validator_hook('num_updates')],\n            \"required\": True, \"default\": None\n        },\n        \"dry_run\": {\n            \"rules\": [bool], \"required\": True, \"default\": False\n        },\n        \"batch_maxnum\": {\n            \"rules\": [cls._nonnegative_integer_value_validator_hook('batch_maxnum')],\n            \"required\": True, \"default\": None\n        },\n        \"test_ratio\": {\n            \"rules\": [float, cls._test_ratio_hook], \"required\": False, \"default\": 0.0\n        },\n        \"test_batch_size\": {\n            \"rules\": [cls.optional_type(typespec=int, argname='test_batch_size')],\n            \"required\": False,\n            \"default\": 0\n        },\n        \"test_on_local_updates\": {\n            \"rules\": [bool], \"required\": False, \"default\": False\n        },\n        \"test_on_global_updates\": {\n            \"rules\": [bool], \"required\": False, \"default\": False\n        },\n        \"shuffle_testing_dataset\": {\n            \"rules\": [bool], \"required\": False, \"default\": False\n        },\n        \"test_metric\": {\n            \"rules\": [cls._metric_validation_hook], \"required\": False, \"default\": None\n        },\n        \"test_metric_args\": {\n            \"rules\": [dict], \"required\": False, \"default\": {}\n        },\n        \"log_interval\": {\n            \"rules\": [int], \"required\": False, \"default\": 10\n        },\n        \"fedprox_mu\": {\n            \"rules\": [cls._fedprox_mu_validator], 'required': False, \"default\": None\n        },\n        \"use_gpu\": {\n            \"rules\": [bool], 'required': False, \"default\": False\n        },\n        \"dp_args\": {\n            \"rules\": [cls._validate_dp_args], \"required\": True, \"default\": None\n        },\n        \"share_persistent_buffers\": {\n            \"rules\": [bool], \"required\": False, \"default\": True\n        },\n        \"random_seed\": {\n            \"rules\": [cls.optional_type(typespec=int, argname='random_seed')], \"required\": True, \"default\": None\n        }\n    }\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.default_value","title":"default_value","text":"<pre><code>default_value(key)\n</code></pre> <p>Returns the default value for the key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key</p> required <p>Returns:</p> Name Type Description <code>value</code> <code>Any</code> <p>the default value associated to the key</p> <p>Raises:</p> Type Description <code>FedbiomedUserInputError</code> <p>in case of problem (invalid key or value)</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def default_value(self, key: str) -&gt; Any:\n    \"\"\"\n    Returns the default value for the key.\n\n    Args:\n        key:  key\n\n    Returns:\n        value: the default value associated to the key\n\n    Raises:\n        FedbiomedUserInputError: in case of problem (invalid key or value)\n    \"\"\"\n    if key in self._sc.scheme():\n        if \"default\" in self._sc.scheme()[key]:\n            return deepcopy(self._sc.scheme()[key][\"default\"])\n        else:\n            msg = ErrorNumbers.FB410.value + \\\n                  f\"no default value defined for key: {key}\"\n            logger.critical(msg)\n            raise FedbiomedUserInputError(msg)\n    else:\n        msg = ErrorNumbers.FB410.value + \\\n              f\"no such key: {key}\"\n        logger.critical(msg)\n        raise FedbiomedUserInputError(msg)\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.dict","title":"dict","text":"<pre><code>dict()\n</code></pre> <p>Returns a copy of the training_args as a dictionary.</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def dict(self) -&gt; dict:\n    \"\"\"Returns a copy of the training_args as a dictionary.\"\"\"\n\n    ta = deepcopy(self._ta)\n    return ta\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.dp_arguments","title":"dp_arguments","text":"<pre><code>dp_arguments()\n</code></pre> <p>Extracts the arguments for differential privacy</p> <p>Returns:</p> Type Description <p>Contains differential privacy arguments</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def dp_arguments(self):\n    \"\"\"Extracts the arguments for differential privacy\n\n    Returns:\n        Contains differential privacy arguments\n    \"\"\"\n    return self[\"dp_args\"]\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.get","title":"get","text":"<pre><code>get(key, default=None)\n</code></pre> <p>Mimics the get() method of dict, provided for backward compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>a key for retrieving data fro the dictionary</p> required <code>default</code> <code>Any</code> <p>default value to return if key does not belong to dictionary</p> <code>None</code> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def get(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Mimics the get() method of dict, provided for backward compatibility.\n\n    Args:\n        key: a key for retrieving data fro the dictionary\n        default: default value to return if key does not belong to dictionary\n    \"\"\"\n    try:\n        return deepcopy(self._ta[key])\n    except KeyError:\n        # TODO: test if provided default value is compliant with the scheme\n        return default\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.get_state_breakpoint","title":"get_state_breakpoint","text":"<pre><code>get_state_breakpoint()\n</code></pre> <p>Returns JSON serializable dict as state for breakpoints</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def get_state_breakpoint(self):\n    \"\"\"Returns JSON serializable dict as state for breakpoints\"\"\"\n\n    # TODO: This method is a temporary solution for JSON\n    # serialize error during breakpoint save operation\n    args = self.dict()\n    test_metric = args.get('test_metric')\n\n    if test_metric and isinstance(test_metric, MetricTypes):\n        args['test_metric'] = test_metric.name\n\n    return args\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.load_state_breakpoint","title":"load_state_breakpoint  <code>classmethod</code>","text":"<pre><code>load_state_breakpoint(state)\n</code></pre> <p>Loads training arguments state</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>@classmethod\ndef load_state_breakpoint(cls, state: Dict) -&gt; 'TrainingArgs':\n    \"\"\"Loads training arguments state\"\"\"\n    if state.get('test_metric'):\n        state.update(\n            {'test_metric': MetricTypes.get_metric_type_by_name(\n                                            state.get('test_metric'))})\n\n    return cls(state)\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.loader_arguments","title":"loader_arguments","text":"<pre><code>loader_arguments()\n</code></pre> <p>Extracts data loader arguments</p> <p>Returns:</p> Type Description <code>Dict</code> <p>The dictionary of arguments for dataloader</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def loader_arguments(self) -&gt; Dict:\n    \"\"\" Extracts data loader arguments\n\n    Returns:\n        The dictionary of arguments for dataloader\n    \"\"\"\n    return self[\"loader_args\"]\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.optimizer_arguments","title":"optimizer_arguments","text":"<pre><code>optimizer_arguments()\n</code></pre> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def optimizer_arguments(self) -&gt; Dict:\n\n    return self[\"optimizer_args\"]\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.optional_type","title":"optional_type  <code>staticmethod</code>","text":"<pre><code>optional_type(typespec, argname)\n</code></pre> <p>Utility factory function to generate functions that check for an optional type(s).</p> <p>Parameters:</p> Name Type Description Default <code>typespec</code> <code>Union[Type, Tuple[Type, ...]]</code> <p>type specification which will be passed to the <code>isinstance</code> function</p> required <code>argname</code> <code>str</code> <p>the name of the training argument for outputting meaningful error messages</p> required <p>Returns:</p> Name Type Description <code>type_check</code> <p>a callable that takes a single argument and checks whether it is either None or the required type(s)</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>@staticmethod\ndef optional_type(typespec: Union[Type, Tuple[Type, ...]], argname: str):\n    \"\"\"Utility factory function to generate functions that check for an optional type(s).\n\n    Args:\n        typespec: type specification which will be passed to the `isinstance` function\n        argname: the name of the training argument for outputting meaningful error messages\n\n    Returns:\n        type_check: a callable that takes a single argument and checks whether it is either None\n            or the required type(s)\n    \"\"\"\n    @validator_decorator\n    def type_check(v):\n        if v is not None and not isinstance(v, typespec):\n            return False, f\"Invalid type: {argname} must be {typespec} or None\"\n        return True\n    return type_check\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.pure_training_arguments","title":"pure_training_arguments","text":"<pre><code>pure_training_arguments()\n</code></pre> <p>Extracts the arguments that are only necessary for training_routine</p> <p>Returns:</p> Type Description <p>Contains training argument for training routine</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def pure_training_arguments(self):\n    \"\"\" Extracts the arguments that are only necessary for training_routine\n\n    Returns:\n        Contains training argument for training routine\n    \"\"\"\n\n    keys = [\"batch_maxnum\",\n            \"fedprox_mu\",\n            \"log_interval\",\n            \"share_persistent_buffers\",\n            \"dry_run\",\n            \"epochs\",\n            \"use_gpu\",\n            \"num_updates\"]\n    return self._extract_args(keys)\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.scheme","title":"scheme","text":"<pre><code>scheme()\n</code></pre> <p>Returns the scheme of a TrainingArgs instance.</p> <p>The scheme is not necessarily the default_scheme (returned by TrainingArgs.default_scheme().</p> <p>Returns:</p> Name Type Description <code>scheme</code> <code>Dict</code> <p>the current scheme used for validation</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def scheme(self) -&gt; Dict:\n    \"\"\"\n    Returns the scheme of a TrainingArgs instance.\n\n    The scheme is not necessarily the default_scheme (returned by TrainingArgs.default_scheme().\n\n    Returns:\n        scheme:  the current scheme used for validation\n    \"\"\"\n    return deepcopy(self._scheme)\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.testing_arguments","title":"testing_arguments","text":"<pre><code>testing_arguments()\n</code></pre> <p>Extract testing arguments from training arguments</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Testing arguments as dictionary</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def testing_arguments(self) -&gt; Dict:\n    \"\"\" Extract testing arguments from training arguments\n\n    Returns:\n        Testing arguments as dictionary\n    \"\"\"\n    keys = ['test_ratio', 'test_on_local_updates', 'test_on_global_updates',\n            'test_metric', 'test_metric_args', 'test_batch_size', 'shuffle_testing_dataset']\n    return self._extract_args(keys)\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.update","title":"update","text":"<pre><code>update(values)\n</code></pre> <p>Update multiple keys of the training arguments.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict</code> <p>a dictionnary of (key, value) to validate/update</p> required <p>Returns:</p> Type Description <code>TypeVar(TrainingArgs)</code> <p>the object itself after modification</p> <p>Raises:</p> Type Description <code>FedbiomedUserInputError</code> <p>in case of bad key or value in values</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def update(self, values: Dict) -&gt; TypeVar(\"TrainingArgs\"):\n    \"\"\"\n    Update multiple keys of the training arguments.\n\n    Args:\n        values:  a dictionnary of (key, value) to validate/update\n\n    Returns:\n        the object itself after modification\n\n    Raises:\n        FedbiomedUserInputError: in case of bad key or value in values\n    \"\"\"\n    for k in values:\n        self.__setitem__(k, values[k])\n    return self\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args-functions","title":"Functions","text":""},{"location":"developer/api/common/training_plans/","title":"TrainingPlans","text":"<p>The <code>fedbiomed.common.training_plans</code> module includes training plan classes that are used for federated training</p>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans-classes","title":"Classes","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan","title":"BaseTrainingPlan","text":"<pre><code>BaseTrainingPlan()\n</code></pre> <p>Base class for training plan</p> <p>All concrete, framework- and/or model-specific training plans should inherit from this class, and implement:     * the <code>post_init</code> method:         to process model and training hyper-parameters     * the <code>training_routine</code> method:         to train the model for one round     * the <code>predict</code> method:         to compute predictions over a given batch     * (opt.) the <code>testing_step</code> method:         to override the evaluation behavior and compute         a batch-wise (set of) metric(s)</p> <p>Attributes:</p> Name Type Description <code>dataset_path</code> <code>Union[str, None]</code> <p>The path that indicates where dataset has been stored</p> <code>pre_processes</code> <code>Dict[str, PreProcessDict]</code> <p>Preprocess functions that will be applied to the training data at the beginning of the training routine.</p> <code>training_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader used in the training routine.</p> <code>testing_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader used in the validation routine.</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Construct the base training plan.\"\"\"\n    self._dependencies: List[str] = []\n    self.dataset_path: Union[str, None] = None\n    self.pre_processes: Dict[str, PreProcessDict] = OrderedDict()\n    self.training_data_loader: Union[DataLoader, NPDataLoader, None] = None\n    self.testing_data_loader: Union[DataLoader, NPDataLoader, None] = None\n\n    # Arguments provided by the researcher; they will be populated by post_init\n    self._model_args: Dict[str, Any] = None\n    self._aggregator_args: Dict[str, Any] = None\n    self._optimizer_args: Dict[str, Any] = None\n    self._loader_args: Dict[str, Any] = None\n    self._training_args: Dict[str, Any] = None\n\n    self._error_msg_import_model: str = f\"{ErrorNumbers.FB605.value}: Training Plan's Model is not initialized.\\n\" +\\\n                                        \"To %s a model, you should do it through `fedbiomed.researcher.federated_workflows.Experiment`'s interface\" +\\\n                                        \" and not directly from Training Plan\"\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan-attributes","title":"Attributes","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.dataset_path","title":"dataset_path  <code>instance-attribute</code>","text":"<pre><code>dataset_path = None\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.dependencies","title":"dependencies  <code>property</code>","text":"<pre><code>dependencies\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.pre_processes","title":"pre_processes  <code>instance-attribute</code>","text":"<pre><code>pre_processes = OrderedDict()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.testing_data_loader","title":"testing_data_loader  <code>instance-attribute</code>","text":"<pre><code>testing_data_loader = None\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.training_data_loader","title":"training_data_loader  <code>instance-attribute</code>","text":"<pre><code>training_data_loader = None\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan-functions","title":"Functions","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.add_preprocess","title":"add_preprocess","text":"<pre><code>add_preprocess(method, process_type)\n</code></pre> <p>Register a pre-processing method to be executed on training data.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>Pre-processing method to be run before training.</p> required <code>process_type</code> <code>ProcessTypes</code> <p>Type of pre-processing that will be run. The expected signature of <code>method</code> and the arguments passed to it depend on this parameter.</p> required Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def add_preprocess(\n        self,\n        method: Callable,\n        process_type: ProcessTypes\n    ) -&gt; None:\n    \"\"\"Register a pre-processing method to be executed on training data.\n\n    Args:\n        method: Pre-processing method to be run before training.\n        process_type: Type of pre-processing that will be run.\n            The expected signature of `method` and the arguments\n            passed to it depend on this parameter.\n    \"\"\"\n    if not callable(method):\n        msg = (\n            f\"{ErrorNumbers.FB605.value}: error while adding \"\n            \"preprocess, `method` should be callable.\"\n        )\n        logger.critical(msg)\n        raise FedbiomedTrainingPlanError(msg)\n    if not isinstance(process_type, ProcessTypes):\n        msg = (\n            f\"{ErrorNumbers.FB605.value}: error while adding \"\n            \"preprocess, `process_type` should be an instance \"\n            \"of `fedbiomed.common.constants.ProcessTypes`.\"\n        )\n        logger.critical(msg)\n        raise FedbiomedTrainingPlanError(msg)\n    # NOTE: this may be revised into a list rather than OrderedDict\n    self.pre_processes[method.__name__] = {\n        'method': method,\n        'process_type': process_type\n    }\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.after_training_params","title":"after_training_params","text":"<pre><code>after_training_params(flatten=False)\n</code></pre> <p>Return the wrapped model's parameters for aggregation.</p> <p>This method returns a dict containing parameters that need to be reported back and aggregated in a federated learning setting.</p> <p>It may also implement post-processing steps to make these parameters suitable for sharing with the researcher after training - hence its being used over <code>get_model_params</code> at the end of training rounds.</p> <p>Returns:</p> Type Description <code>Union[Dict[str, Any], List[float]]</code> <p>The trained parameters to aggregate.</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def after_training_params(\n    self,\n    flatten: bool = False,\n) -&gt; Union[Dict[str, Any], List[float]]:\n    \"\"\"Return the wrapped model's parameters for aggregation.\n\n    This method returns a dict containing parameters that need to be\n    reported back and aggregated in a federated learning setting.\n\n    It may also implement post-processing steps to make these parameters\n    suitable for sharing with the researcher after training - hence its\n    being used over `get_model_params` at the end of training rounds.\n\n    Returns:\n        The trained parameters to aggregate.\n    \"\"\"\n    exclude_buffers = not self._training_args['share_persistent_buffers']\n    if flatten:\n        return self._model.flatten(exclude_buffers=exclude_buffers)\n    return self.get_model_params(exclude_buffers=exclude_buffers)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.export_model","title":"export_model","text":"<pre><code>export_model(filename)\n</code></pre> <p>Export the wrapped model to a dump file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to the file where the model will be saved.</p> required <p>Raises:</p> Type Description <code>FedBiomedTrainingPlanError</code> <p>raised if model has not be initialized through the</p> <p>!!! info \"Notes\":     This method is designed to save the model to a local dump     file for easy re-use by the same user, possibly outside of     Fed-BioMed. It is not designed to produce trustworthy data     dumps and is not used to exchange models and their weights     as part of the federated learning process.</p> <pre><code>To save the model parameters for sharing as part of the FL process,\nuse the `after_training_params` method (or `get_model_params` one\noutside of a training context) and export results using\n[`Serializer`][fedbiomed.common.serializer.Serializer].\n</code></pre> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def export_model(self, filename: str) -&gt; None:\n    \"\"\"Export the wrapped model to a dump file.\n\n    Args:\n        filename: path to the file where the model will be saved.\n\n    Raises:\n        FedBiomedTrainingPlanError: raised if model has not be initialized through the\n        `post_init` method. If you need to export the model, you must do it through\n        [`Experiment`][`fedbiomed.researcher.federated_workflows.Experiment`]'s interface.\n\n    !!! info \"Notes\":\n        This method is designed to save the model to a local dump\n        file for easy re-use by the same user, possibly outside of\n        Fed-BioMed. It is not designed to produce trustworthy data\n        dumps and is not used to exchange models and their weights\n        as part of the federated learning process.\n\n        To save the model parameters for sharing as part of the FL process,\n        use the `after_training_params` method (or `get_model_params` one\n        outside of a training context) and export results using\n        [`Serializer`][fedbiomed.common.serializer.Serializer].\n    \"\"\"\n    if self._model is None:\n        raise FedbiomedTrainingPlanError(self._error_msg_import_model % \"export\")\n    self._model.export(filename)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.get_model_params","title":"get_model_params","text":"<pre><code>get_model_params(only_trainable=False, exclude_buffers=True)\n</code></pre> <p>Return a copy of the model's trainable weights.</p> <p>The type of data structure used to store weights depends on the actual framework of the wrapped model.</p> <p>Parameters:</p> Name Type Description Default <code>only_trainable</code> <code>bool</code> <p>Whether to ignore non-trainable model parameters from outputs (e.g. frozen neural network layers' parameters), or include all model parameters (the default).</p> <code>False</code> <code>exclude_buffers</code> <code>bool</code> <p>Whether to ignore buffers (the default), or include them.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Model weights, as a dictionary mapping parameters' names to their value.</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def get_model_params(self,\n                     only_trainable: bool = False,\n                     exclude_buffers: bool = True) -&gt; Dict[str, Any]:\n    \"\"\"Return a copy of the model's trainable weights.\n\n    The type of data structure used to store weights depends on the actual\n    framework of the wrapped model.\n\n    Args:\n        only_trainable: Whether to ignore non-trainable model parameters\n            from outputs (e.g. frozen neural network layers' parameters),\n            or include all model parameters (the default).\n        exclude_buffers: Whether to ignore buffers (the default), or\n            include them.\n\n    Returns:\n        Model weights, as a dictionary mapping parameters' names to their value.\n    \"\"\"\n    return self._model.get_weights(only_trainable=only_trainable, exclude_buffers=exclude_buffers)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.get_model_wrapper_class","title":"get_model_wrapper_class","text":"<pre><code>get_model_wrapper_class()\n</code></pre> <p>Gets training plan's model wrapper class.</p> <p>Returns:</p> Type Description <code>Optional[Model]</code> <p>the wrapper class for the model, or None</p> <code>Optional[Model]</code> <p>if model is not instantiated.</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def get_model_wrapper_class(self) -&gt; Optional[Model]:\n    \"\"\"Gets training plan's model wrapper class.\n\n    Returns:\n        the wrapper class for the model, or None\n        if model is not instantiated.\n    \"\"\"\n    return self._model\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.import_model","title":"import_model","text":"<pre><code>import_model(filename)\n</code></pre> <p>Import and replace the wrapped model from a dump file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to the file where the model has been exported.</p> required <p>Raises:</p> Type Description <code>FedBiomedTrainingPlanError</code> <p>raised if model has not be initialized through the</p> <p>!!! info \"Notes\":     This method is designed to load the model from a local dump     file, that might not be in a trustworthy format. It should     therefore only be used to re-load data exported locally and     not received from someone else, including other FL peers.</p> <pre><code>To load model parameters shared as part of the FL process, use the\n[`Serializer`][fedbiomed.common.serializer.Serializer] to read the\nnetwork-exchanged file, and the `set_model_params` method to assign\nthe loaded values into the wrapped model.\n</code></pre> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def import_model(self, filename: str) -&gt; None:\n    \"\"\"Import and replace the wrapped model from a dump file.\n\n    Args:\n        filename: path to the file where the model has been exported.\n\n    Raises:\n        FedBiomedTrainingPlanError: raised if model has not be initialized through the\n        `post_init` method. If you need to export the model from the Training Plan, you\n        must do it through [`Experiment`][`fedbiomed.researcher.federated_workflows.Experiment`]'s\n        interface.\n\n    !!! info \"Notes\":\n        This method is designed to load the model from a local dump\n        file, that might not be in a trustworthy format. It should\n        therefore only be used to re-load data exported locally and\n        not received from someone else, including other FL peers.\n\n        To load model parameters shared as part of the FL process, use the\n        [`Serializer`][fedbiomed.common.serializer.Serializer] to read the\n        network-exchanged file, and the `set_model_params` method to assign\n        the loaded values into the wrapped model.\n    \"\"\"\n    if self._model is None:\n        raise FedbiomedTrainingPlanError(self._error_msg_import_model % \"import\")\n    try:\n        self._model.reload(filename)\n    except FedbiomedModelError as exc:\n        msg = (\n            f\"{ErrorNumbers.FB304.value}: failed to import a model from \"\n            f\"a dump file: {exc}\"\n        )\n        logger.critical(msg)\n        raise FedbiomedTrainingPlanError(msg) from exc\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.init_dependencies","title":"init_dependencies","text":"<pre><code>init_dependencies()\n</code></pre> <p>Default method where dependencies are returned</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>Empty list as default</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def init_dependencies(self) -&gt; List[str]:\n    \"\"\"Default method where dependencies are returned\n\n    Returns:\n        Empty list as default\n    \"\"\"\n    return []\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.init_optimizer","title":"init_optimizer  <code>abstractmethod</code>","text":"<pre><code>init_optimizer()\n</code></pre> <p>Method for declaring optimizer by default</p> <p>Returns:</p> Type Description <code>Any</code> <p>either framework specific optimizer (or None) or</p> <code>Any</code> <p>FedBiomed [<code>Optimizers</code>][<code>fedbiomed.common.optimizers.Optimizer</code>]</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>@abstractmethod\ndef init_optimizer(self) -&gt; Any:\n    \"\"\"Method for declaring optimizer by default\n\n    Returns:\n        either framework specific optimizer (or None) or\n        FedBiomed [`Optimizers`][`fedbiomed.common.optimizers.Optimizer`]\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.loader_args","title":"loader_args","text":"<pre><code>loader_args()\n</code></pre> <p>Retrieve loader arguments</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Loader arguments</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def loader_args(self) -&gt; Dict[str, Any]:\n    \"\"\"Retrieve loader arguments\n\n    Returns:\n        Loader arguments\n    \"\"\"\n    return self._loader_args\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.model","title":"model  <code>abstractmethod</code>","text":"<pre><code>model()\n</code></pre> <p>Gets model instance of the training plan</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>@abstractmethod\ndef model(self):\n    \"\"\"Gets model instance of the training plan\"\"\"\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.model_args","title":"model_args","text":"<pre><code>model_args()\n</code></pre> <p>Retrieve model arguments.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Model arguments</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def model_args(self) -&gt; Dict[str, Any]:\n    \"\"\"Retrieve model arguments.\n\n    Returns:\n        Model arguments\n    \"\"\"\n    return self._model_args\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.optimizer","title":"optimizer","text":"<pre><code>optimizer()\n</code></pre> <p>Get the BaseOptimizer wrapped by this training plan.</p> <p>Returns:</p> Type Description <code>Optional[BaseOptimizer]</code> <p>BaseOptimizer wrapped by this training plan, or None if</p> <code>Optional[BaseOptimizer]</code> <p>it has not been initialized yet.</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def optimizer(self) -&gt; Optional[BaseOptimizer]:\n    \"\"\"Get the BaseOptimizer wrapped by this training plan.\n\n    Returns:\n        BaseOptimizer wrapped by this training plan, or None if\n        it has not been initialized yet.\n    \"\"\"\n    # FUTURE: return `self._optimizer.optimizer` instead?\n    # Currently, the legacy Scaffold implem. needs the BaseOptimizer,\n    # but IMHO it really should remain a private backend component.\n    return self._optimizer\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.optimizer_args","title":"optimizer_args","text":"<pre><code>optimizer_args()\n</code></pre> <p>Retrieves optimizer arguments</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Optimizer arguments</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def optimizer_args(self) -&gt; Dict[str, Any]:\n    \"\"\"Retrieves optimizer arguments\n\n    Returns:\n        Optimizer arguments\n    \"\"\"\n    return self._optimizer_args\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.post_init","title":"post_init  <code>abstractmethod</code>","text":"<pre><code>post_init(model_args, training_args, aggregator_args=None, initialize_optimizer=True)\n</code></pre> <p>Process model, training and optimizer arguments.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict[str, Any]</code> <p>Arguments defined to instantiate the wrapped model.</p> required <code>training_args</code> <code>Dict[str, Any]</code> <p>Arguments that are used in training routines such as epoch, dry_run etc. Please see <code>TrainingArgs</code></p> required <code>aggregator_args</code> <code>Optional[Dict[str, Any]]</code> <p>Arguments managed by and shared with the researcher-side aggregator.</p> <code>None</code> <code>initialize_optimizer</code> <code>bool</code> <p>whether to initialize the optimizer or not. Defaults to True.</p> <code>True</code> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>@abstractmethod\ndef post_init(\n        self,\n        model_args: Dict[str, Any],\n        training_args: Dict[str, Any],\n        aggregator_args: Optional[Dict[str, Any]] = None,\n        initialize_optimizer: bool = True\n    ) -&gt; None:\n    \"\"\"Process model, training and optimizer arguments.\n\n    Args:\n        model_args: Arguments defined to instantiate the wrapped model.\n        training_args: Arguments that are used in training routines\n            such as epoch, dry_run etc.\n            Please see [`TrainingArgs`][fedbiomed.common.training_args.TrainingArgs]\n        aggregator_args: Arguments managed by and shared with the\n            researcher-side aggregator.\n        initialize_optimizer: whether to initialize the optimizer or not. Defaults\n            to True.\n    \"\"\"\n\n    # Store various arguments provided by the researcher\n    self._model_args = model_args\n    self._aggregator_args = aggregator_args or {}\n    self._optimizer_args = training_args.optimizer_arguments() or {}\n    self._loader_args = training_args.loader_arguments() or {}\n    self._training_args = training_args.pure_training_arguments()\n\n    # Set random seed: the seed can be either None or an int provided by the researcher.\n    # when it is None, both random.seed and np.random.seed rely on the OS to generate a random seed.\n    rseed = training_args['random_seed']\n    random.seed(rseed)\n    np.random.seed(rseed)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.save_code","title":"save_code","text":"<pre><code>save_code(filepath, from_code=None)\n</code></pre> <p>Saves the class source/codes of the training plan class that is created byuser.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>path to the destination file</p> required <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanError</code> <p>raised when source of the model class cannot be assessed</p> <code>FedbiomedTrainingPlanError</code> <p>raised when model file cannot be created/opened/edited</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def save_code(self, filepath: str, from_code: Union[str, None] = None) -&gt; None:\n    \"\"\"Saves the class source/codes of the training plan class that is created byuser.\n\n    Args:\n        filepath: path to the destination file\n\n    Raises:\n        FedbiomedTrainingPlanError: raised when source of the model class cannot be assessed\n        FedbiomedTrainingPlanError: raised when model file cannot be created/opened/edited\n    \"\"\"\n    if from_code is None:\n        content = self.source()\n    else:\n        if not isinstance(from_code, str):\n            raise FedbiomedTrainingPlanError(f\"{ErrorNumbers.FB605}: Expected type str for `from_code`, \"\n                                             \"got: {type(from_code)}\")\n        content = from_code\n\n    try:\n        # should we write it in binary (for the sake of space optimization)?\n        with open(filepath, \"w\", encoding=\"utf-8\") as file:\n            file.write(content)\n        logger.debug(\"Model file has been saved: \" + filepath)\n    except PermissionError as exc:\n        _msg = ErrorNumbers.FB605.value + f\" : Unable to read {filepath} due to unsatisfactory privileges\" + \\\n               \", can't write the model content into it\"\n        logger.critical(_msg)\n        raise FedbiomedTrainingPlanError(_msg) from exc\n    except MemoryError as exc:\n        _msg = ErrorNumbers.FB605.value + f\" : Can't write model file on {filepath}: out of memory!\"\n        logger.critical(_msg)\n        raise FedbiomedTrainingPlanError(_msg) from exc\n    except OSError as exc:\n        _msg = ErrorNumbers.FB605.value + f\" : Can't open file {filepath} to write model content\"\n        logger.critical(_msg)\n        raise FedbiomedTrainingPlanError(_msg) from exc\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.set_aggregator_args","title":"set_aggregator_args","text":"<pre><code>set_aggregator_args(aggregator_args)\n</code></pre> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def set_aggregator_args(self, aggregator_args: Dict[str, Any]):\n    raise FedbiomedTrainingPlanError(\"method not implemented and needed\")\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.set_data_loaders","title":"set_data_loaders","text":"<pre><code>set_data_loaders(train_data_loader, test_data_loader)\n</code></pre> <p>Sets data loaders</p> <p>Parameters:</p> Name Type Description Default <code>train_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader for training routine/loop</p> required <code>test_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader for validation routine</p> required Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def set_data_loaders(\n        self,\n        train_data_loader: Union[DataLoader, NPDataLoader, None],\n        test_data_loader: Union[DataLoader, NPDataLoader, None]\n    ) -&gt; None:\n    \"\"\"Sets data loaders\n\n    Args:\n        train_data_loader: Data loader for training routine/loop\n        test_data_loader: Data loader for validation routine\n    \"\"\"\n    self.training_data_loader = train_data_loader\n    self.testing_data_loader = test_data_loader\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.set_dataset_path","title":"set_dataset_path","text":"<pre><code>set_dataset_path(dataset_path)\n</code></pre> <p>Dataset path setter for TrainingPlan</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path where data is saved on the node. This method is called by the node that executes the training.</p> required Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def set_dataset_path(self, dataset_path: str) -&gt; None:\n    \"\"\"Dataset path setter for TrainingPlan\n\n    Args:\n        dataset_path: The path where data is saved on the node.\n            This method is called by the node that executes the training.\n    \"\"\"\n    self.dataset_path = dataset_path\n    logger.debug(f\"Dataset path has been set as {self.dataset_path}\")\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.set_model_params","title":"set_model_params","text":"<pre><code>set_model_params(params)\n</code></pre> <p>Assign new values to the model's trainable parameters.</p> <p>The type of data structure used to store weights depends on the actual framework of the wrapped model.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict[str, Any]</code> <p>model weights, as a dictionary mapping parameters' names to their value.</p> required Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def set_model_params(self, params: Dict[str, Any]) -&gt; None:\n    \"\"\"Assign new values to the model's trainable parameters.\n\n    The type of data structure used to store weights depends on the actual\n    framework of the wrapped model.\n\n    Args:\n        params: model weights, as a dictionary mapping parameters' names\n            to their value.\n    \"\"\"\n    self._model.set_weights(params)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.set_optimizer","title":"set_optimizer","text":"<pre><code>set_optimizer(optimizer)\n</code></pre> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def set_optimizer(self, optimizer: BaseOptimizer):\n    self._optimizer = optimizer\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.source","title":"source","text":"<pre><code>source()\n</code></pre> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def source(self) -&gt; str:\n\n    try:\n        class_source = get_class_source(self.__class__)\n    except FedbiomedError as exc:\n        msg = f\"{ErrorNumbers.FB605.value}: error while getting source of the model class: {exc}\"\n        logger.critical(msg)\n        raise FedbiomedTrainingPlanError(msg) from exc\n\n    # Preparing content of the module\n    content = \"\\n\".join(self._dependencies)\n    content += \"\\n\"\n    content += class_source\n\n    return content\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.testing_routine","title":"testing_routine","text":"<pre><code>testing_routine(metric, metric_args, history_monitor, before_train)\n</code></pre> <p>Evaluation routine, to be called once per round.</p> <p>Note</p> <p>If the training plan implements a <code>testing_step</code> method (the signature of which is func(data, target) -&gt; metrics) then it will be used rather than the input metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Optional[MetricTypes]</code> <p>The metric used for validation. If None, use MetricTypes.ACCURACY.</p> required <code>metric_args</code> <code>Dict[str, Any]</code> <p>dicitonary containing additinal arguments for setting up metric, that maps  ad that will be passed to the metric function as positinal arguments. required <code>history_monitor</code> <code>Optional[HistoryMonitor]</code> <p>HistoryMonitor instance, used to record computed metrics and communicate them to the researcher (server).</p> required <code>before_train</code> <code>bool</code> <p>Whether the evaluation is being performed before local training occurs, of afterwards. This is merely reported back through <code>history_monitor</code>.</p> required Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def testing_routine(\n        self,\n        metric: Optional[MetricTypes],\n        metric_args: Dict[str, Any],\n        history_monitor: Optional['HistoryMonitor'],\n        before_train: bool,\n    ) -&gt; None:\n    \"\"\"Evaluation routine, to be called once per round.\n\n    !!! info \"Note\"\n        If the training plan implements a `testing_step` method\n        (the signature of which is func(data, target) -&gt; metrics)\n        then it will be used rather than the input metric.\n\n    Args:\n        metric: The metric used for validation.\n            If None, use MetricTypes.ACCURACY.\n        metric_args: dicitonary containing additinal arguments for setting up metric,\n            that maps &lt;argument_name; argument_value&gt; ad that will be passed to the\n            metric function as positinal arguments.\n        history_monitor: HistoryMonitor instance,\n            used to record computed metrics and communicate them to\n            the researcher (server).\n        before_train: Whether the evaluation is being performed\n            before local training occurs, of afterwards. This is merely\n            reported back through `history_monitor`.\n    \"\"\"\n    # TODO: Add preprocess option for testing_data_loader.\n    if self.testing_data_loader is None:\n        msg = f\"{ErrorNumbers.FB605.value}: no validation dataset was set.\"\n        logger.critical(msg)\n        raise FedbiomedTrainingPlanError(msg)\n\n    n_samples = len(self.testing_data_loader.dataset)\n    n_batches = max(len(self.testing_data_loader) , 1)\n\n    # Set up a batch-wise metrics-computation function.\n    # Either use an optionally-implemented custom training routine.\n    if hasattr(self, \"testing_step\"):\n        evaluate = getattr(self, \"testing_step\")\n        metric_name = \"Custom\"\n    # Or use the provided `metric` (or its default value).\n    else:\n        if metric is None:\n            metric = MetricTypes.ACCURACY\n        metric_controller = Metrics()\n        def evaluate(data, target):\n            nonlocal metric, metric_args, metric_controller\n            output = self._model.predict(data)\n            if isinstance(target, torch.Tensor):\n                target = target.numpy()\n\n            return metric_controller.evaluate(\n                target, output, metric=metric, **metric_args\n            )\n        metric_name = metric.name\n    # Iterate over the validation dataset and run the defined routine.\n    num_samples_observed_till_now: int = 0\n\n\n    for idx, (data, target) in enumerate(self.testing_data_loader, 1):\n\n        num_samples_observed_till_now += self._infer_batch_size(data)\n        # Run the evaluation step; catch and raise exceptions.\n        try:\n            m_value = evaluate(data, target)\n        except Exception as exc:\n            msg = (\n                f\"{ErrorNumbers.FB605.value}: An error occurred \"\n                f\"while computing the {metric_name} metric: {exc}\"\n            )\n            logger.critical(msg)\n            raise FedbiomedTrainingPlanError(msg) from exc\n        # Log the computed value.\n        # Reporting\n\n        if idx % self.training_args()['log_interval'] == 0 or idx == 1 or idx == n_batches:\n            logger.debug(\n                f\"Validation: Batch {idx}/{n_batches} \"\n                f\"| Samples {num_samples_observed_till_now}/{n_samples} \"\n                f\"| Metric[{metric_name}]: {m_value}\"\n            )\n            # Further parse, and report it (provided a monitor is set).\n            if history_monitor is not None:\n                m_dict = self._create_metric_result_dict(m_value, metric_name)\n                history_monitor.add_scalar(\n                    metric=m_dict,\n                    iteration=idx,\n                    epoch=None,\n                    test=True,\n                    test_on_local_updates=(not before_train),\n                    test_on_global_updates=before_train,\n                    total_samples=n_samples,\n                    batch_samples=num_samples_observed_till_now,\n                    num_batches=n_batches\n                )\n</code></pre> <p>Accounting class for keeping track of training iterations.</p> <p>This class has the following responsibilities:</p> <pre><code>- manage iterators for epochs and batches\n- provide up-to-date values for reporting\n- handle different semantics in case the researcher asked for num_updates or epochs\n</code></pre> <p>We assume that the underlying implementation for the training loop is always made in terms of epochs and batches. So the primary purpose of this class is to provide a way to correctly convert the number of updates into epochs and batches.</p> <p>For reporting purposes, in the case of num_updates then we think of the training as a single big loop, while in the case of epochs and batches we think of it as two nested loops. This changes the meaning of the values outputted by the reporting functions (see their docstrings for more details).</p> <p>Attributes:</p> Name Type Description <code>_training_plan</code> <p>a reference to the training plan executing the training iterations</p> <code>cur_epoch</code> <code>int</code> <p>the index of the current epoch during iterations</p> <code>cur_batch</code> <code>int</code> <p>the index of the current batch during iterations</p> <code>epochs</code> <code>int</code> <p>the total number of epochs to be performed (we always perform one additional -- possibly empty -- epoch</p> <code>num_batches_per_epoch</code> <code>int</code> <p>the number of iterations per epoch</p> <code>num_batches_in_last_epoch</code> <code>int</code> <p>the number of iterations in the last epoch (can be zero)</p> <code>num_samples_observed_in_epoch</code> <code>int</code> <p>a counter for the number of samples observed in the current epoch, for reporting</p> <code>num_samples_observed_in_total</code> <code>int</code> <p>a counter for the number of samples observed total, for reporting</p> <p>Parameters:</p> Name Type Description Default <code>training_plan</code> <code>TBaseTrainingPlan</code> <p>a reference to the training plan that is executing the training iterations</p> required Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def __init__(self, training_plan: TBaseTrainingPlan):\n    \"\"\"Initialize the class.\n\n    Arguments:\n        training_plan: a reference to the training plan that is executing the training iterations\n    \"\"\"\n    self._training_plan = training_plan\n    self.cur_epoch: int = 0\n    self.cur_batch: int = 0\n    self.epochs: int = 0\n    self.num_batches_per_epoch: int = 0\n    self.num_batches_in_last_epoch: int = 0\n    self.num_samples_observed_in_epoch: int = 0\n    self.num_samples_observed_in_total: int = 0\n    self._n_training_iterations()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.training_args","title":"training_args","text":"<pre><code>training_args()\n</code></pre> <p>Retrieve training arguments</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Training arguments</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def training_args(self) -&gt; Dict[str, Any]:\n    \"\"\"Retrieve training arguments\n\n    Returns:\n        Training arguments\n    \"\"\"\n    return self._training_args\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.training_data","title":"training_data","text":"<pre><code>training_data()\n</code></pre> <p>All subclasses must provide a training_data routine the purpose of this actual code is to detect that it has been provided</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanError</code> <p>if called and not inherited</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def training_data(self):\n    \"\"\"All subclasses must provide a training_data routine the purpose of this actual code is to detect\n    that it has been provided\n\n    Raises:\n        FedbiomedTrainingPlanError: if called and not inherited\n    \"\"\"\n    msg = f\"{ErrorNumbers.FB303.value}: training_data must be implemented\"\n    logger.critical(msg)\n    raise FedbiomedTrainingPlanError(msg)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan.training_routine","title":"training_routine  <code>abstractmethod</code>","text":"<pre><code>training_routine(history_monitor=None, node_args=None)\n</code></pre> <p>Training routine, to be called once per round.</p> <p>Parameters:</p> Name Type Description Default <code>history_monitor</code> <code>Optional[HistoryMonitor]</code> <p>optional HistoryMonitor instance, recording training metadata.</p> <code>None</code> <code>node_args</code> <code>Optional[Dict[str, Any]]</code> <p>Command line arguments for node. These arguments can specify GPU use; however, this is not supported for scikit-learn models and thus will be ignored.</p> <code>None</code> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>@abstractmethod\ndef training_routine(\n        self,\n        history_monitor: Optional['HistoryMonitor'] = None,\n        node_args: Optional[Dict[str, Any]] = None\n    ) -&gt; None:\n    \"\"\"Training routine, to be called once per round.\n\n    Args:\n        history_monitor: optional HistoryMonitor\n            instance, recording training metadata.\n        node_args: Command line arguments for node.\n            These arguments can specify GPU use; however, this is not\n            supported for scikit-learn models and thus will be ignored.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.FedPerceptron","title":"FedPerceptron","text":"<pre><code>FedPerceptron()\n</code></pre> <p>               Bases: <code>FedSGDClassifier</code></p> <p>Fed-BioMed training plan for scikit-learn Perceptron models.</p> <p>This class inherits from FedSGDClassifier, and forces the wrapped scikit-learn SGDClassifier model to use a \"perceptron\" loss, that makes it equivalent to an actual scikit-learn Perceptron model.</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_models.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Class constructor.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.FedPerceptron-functions","title":"Functions","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.FedPerceptron.post_init","title":"post_init","text":"<pre><code>post_init(model_args, training_args, aggregator_args=None, **kwargs)\n</code></pre> Source code in <code>fedbiomed/common/training_plans/_sklearn_models.py</code> <pre><code>def post_init(\n        self,\n        model_args: Dict[str, Any],\n        training_args: Dict[str, Any],\n        aggregator_args: Optional[Dict[str, Any]] = None,\n        **kwargs\n    ) -&gt; None:\n    # get default values of Perceptron model (different from SGDClassifier model default values)\n    perceptron_default_values = Perceptron().get_params()\n    sgd_classifier_default_values = SGDClassifier().get_params()\n    # make sure loss used is perceptron loss - can not be changed by user\n    model_args[\"loss\"] = \"perceptron\"\n    super().post_init(model_args, training_args)\n    self._model.set_params(loss=\"perceptron\")\n\n    # collect default values of Perceptron and set it to the model FedPerceptron\n    model_hyperparameters = self._model.get_params()\n    for hyperparameter_name, val in perceptron_default_values.items():\n        if model_hyperparameters[hyperparameter_name] == sgd_classifier_default_values[hyperparameter_name]:\n            # this means default parameter of SGDClassifier has not been changed by user\n            self._model.set_params(**{hyperparameter_name: val})\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.FedSGDClassifier","title":"FedSGDClassifier","text":"<pre><code>FedSGDClassifier()\n</code></pre> <p>               Bases: <code>SKLearnTrainingPlanPartialFit</code></p> <p>Fed-BioMed training plan for scikit-learn SGDClassifier models.</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_models.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the sklearn SGDClassifier training plan.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.FedSGDRegressor","title":"FedSGDRegressor","text":"<pre><code>FedSGDRegressor()\n</code></pre> <p>               Bases: <code>SKLearnTrainingPlanPartialFit</code></p> <p>Fed-BioMed training plan for scikit-learn SGDRegressor models.</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_models.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the sklearn SGDRegressor training plan.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.SKLearnTrainingPlan","title":"SKLearnTrainingPlan","text":"<pre><code>SKLearnTrainingPlan()\n</code></pre> <p>               Bases: <code>BaseTrainingPlan</code></p> <p>Base class for Fed-BioMed wrappers of sklearn classes.</p> <p>Classes that inherit from this abstract class must: - Specify a <code>_model_cls</code> class attribute that defines the type   of scikit-learn model being wrapped for training. - Implement a <code>set_init_params</code> method that:   - sets and assigns the model's initial trainable weights attributes.   - populates the <code>_param_list</code> attribute with names of these attributes. - Implement a <code>_training_routine</code> method that performs a training round   based on <code>self.train_data_loader</code> (which is a <code>NPDataLoader</code>).</p> <p>Attributes:</p> Name Type Description <code>dataset_path</code> <code>Optional[str]</code> <p>The path that indicates where dataset has been stored</p> <code>pre_processes</code> <code>Dict[str, PreProcessDict]</code> <p>Preprocess functions that will be applied to the training data at the beginning of the training routine.</p> <code>training_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader used in the training routine.</p> <code>testing_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader used in the validation routine.</p> <p>Notes</p> <p>The trained model may be exported via the <code>export_model</code> method, resulting in a dump file that may be reloded using <code>joblib.load</code> outside of Fed-BioMed.</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the SKLearnTrainingPlan.\"\"\"\n    super().__init__()\n    self._model: Union[SkLearnModel, None] = None\n    self._training_args = {}  # type: Dict[str, Any]\n    self.__type = TrainingPlans.SkLearnTrainingPlan\n    self._batch_maxnum = 0\n    self.dataset_path: Optional[str] = None\n    self._optimizer: Optional[BaseOptimizer] = None\n    self._add_dependency([\n        \"import inspect\",\n        \"import numpy as np\",\n        \"import pandas as pd\",\n        \"from fedbiomed.common.training_plans import SKLearnTrainingPlan\",\n        \"from fedbiomed.common.data import DataManager\",\n    ])\n    self._add_dependency(list(self._model_dep))\n\n    # Add dependencies\n    self._configure_dependencies()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.SKLearnTrainingPlan-attributes","title":"Attributes","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.SKLearnTrainingPlan.dataset_path","title":"dataset_path  <code>instance-attribute</code>","text":"<pre><code>dataset_path = None\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.SKLearnTrainingPlan-functions","title":"Functions","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.SKLearnTrainingPlan.init_optimizer","title":"init_optimizer","text":"<pre><code>init_optimizer()\n</code></pre> <p>Creates and configures optimizer. By default, returns None (meaning native inner scikit learn optimization SGD based will be used).</p> <p>In the case a Declearn Optimizer is used, this method should be overridden in the Training Plan and return a Fedbiomed <code>Optimizer</code></p> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def init_optimizer(self) -&gt; Optional[FedOptimizer]:\n    \"\"\"Creates and configures optimizer. By default, returns None (meaning native inner scikit\n    learn optimization SGD based will be used).\n\n    In the case a Declearn Optimizer is used, this method should be overridden in the Training Plan and return\n    a Fedbiomed [`Optimizer`][fedbiomed.common.optimizers.optimizer.Optimizer]\"\"\"\n    pass\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.SKLearnTrainingPlan.model","title":"model","text":"<pre><code>model()\n</code></pre> <p>Retrieve the wrapped scikit-learn model instance.</p> <p>Returns:</p> Type Description <code>Optional[BaseEstimator]</code> <p>Scikit-learn model instance</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def model(self) -&gt; Optional[BaseEstimator]:\n    \"\"\"Retrieve the wrapped scikit-learn model instance.\n\n    Returns:\n        Scikit-learn model instance\n    \"\"\"\n    if self._model is not None:\n        return self._model.model\n    else:\n        return self._model\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.SKLearnTrainingPlan.post_init","title":"post_init","text":"<pre><code>post_init(model_args, training_args, aggregator_args=None, initialize_optimizer=True)\n</code></pre> <p>Process model, training and optimizer arguments.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict[str, Any]</code> <p>Arguments defined to instantiate the wrapped model.</p> required <code>training_args</code> <code>TrainingArgs</code> <p>Arguments that are used in training routines such as epoch, dry_run etc. Please see <code>TrainingArgs</code></p> required <code>aggregator_args</code> <code>Optional[Dict[str, Any]]</code> <p>Arguments managed by and shared with the researcher-side aggregator.</p> <code>None</code> <code>initialize_optimizer</code> <code>bool</code> <p>Unused.</p> <code>True</code> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def post_init(\n        self,\n        model_args: Dict[str, Any],\n        training_args: TrainingArgs,\n        aggregator_args: Optional[Dict[str, Any]] = None,\n        initialize_optimizer: bool = True\n) -&gt; None:\n    \"\"\"Process model, training and optimizer arguments.\n\n    Args:\n        model_args: Arguments defined to instantiate the wrapped model.\n        training_args: Arguments that are used in training routines\n            such as epoch, dry_run etc.\n            Please see [`TrainingArgs`][fedbiomed.common.training_args.TrainingArgs]\n        aggregator_args: Arguments managed by and shared with the\n            researcher-side aggregator.\n        initialize_optimizer: Unused.\n    \"\"\"\n    model_args.setdefault(\"verbose\", 1)\n    super().post_init(model_args, training_args, aggregator_args)\n    self._model = SkLearnModel(self._model_cls)\n    self._batch_maxnum = self._training_args.get('batch_maxnum', self._batch_maxnum)\n    self._warn_about_training_args()\n\n    # configure optimizer (if provided in the TrainingPlan)\n    self._configure_optimizer()\n\n    # FIXME: should we do that in `_configure_optimizer`\n    # from now on, `self._optimizer`` is not None\n    # Override default model parameters based on `self._model_args`.\n    params = {\n        key: model_args.get(key, val)\n        for key, val in self._model.get_params().items()\n    }\n    self._model.set_params(**params)\n    # Set up additional parameters (normally created by `self._model.fit`).\n    self._model.set_init_params(model_args)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.SKLearnTrainingPlan.set_data_loaders","title":"set_data_loaders","text":"<pre><code>set_data_loaders(train_data_loader, test_data_loader)\n</code></pre> <p>Sets data loaders</p> <p>Parameters:</p> Name Type Description Default <code>train_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader for training routine/loop</p> required <code>test_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader for validation routine</p> required Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def set_data_loaders(\n        self,\n        train_data_loader: Union[DataLoader, NPDataLoader, None],\n        test_data_loader: Union[DataLoader, NPDataLoader, None]\n) -&gt; None:\n    \"\"\"Sets data loaders\n\n    Args:\n        train_data_loader: Data loader for training routine/loop\n        test_data_loader: Data loader for validation routine\n    \"\"\"\n    args = (train_data_loader, test_data_loader)\n    if not all(isinstance(data, NPDataLoader) for data in args):\n        msg = (\n            f\"{ErrorNumbers.FB310.value}: SKLearnTrainingPlan expects \"\n            \"NPDataLoader instances as training and testing data \"\n            f\"loaders, but received {type(train_data_loader)} \"\n            f\"and {type(test_data_loader)} respectively.\"\n        )\n        logger.error(msg)\n        raise FedbiomedTrainingPlanError(msg)\n    self.training_data_loader = train_data_loader\n    self.testing_data_loader = test_data_loader\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.SKLearnTrainingPlan.testing_routine","title":"testing_routine","text":"<pre><code>testing_routine(metric, metric_args, history_monitor, before_train)\n</code></pre> <p>Evaluation routine, to be called once per round.</p> <p>Note</p> <p>If the training plan implements a <code>testing_step</code> method (the signature of which is func(data, target) -&gt; metrics) then it will be used rather than the input metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Optional[MetricTypes]</code> <p>The metric used for validation. If None, use MetricTypes.ACCURACY.</p> required <code>history_monitor</code> <code>Optional[HistoryMonitor]</code> <p>HistoryMonitor instance, used to record computed metrics and communicate them to the researcher (server).</p> required <code>before_train</code> <code>bool</code> <p>Whether the evaluation is being performed before local training occurs, of afterwards. This is merely reported back through <code>history_monitor</code>.</p> required Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def testing_routine(\n        self,\n        metric: Optional[MetricTypes],\n        metric_args: Dict[str, Any],\n        history_monitor: Optional['HistoryMonitor'],\n        before_train: bool\n) -&gt; None:\n    \"\"\"Evaluation routine, to be called once per round.\n\n    !!! info \"Note\"\n        If the training plan implements a `testing_step` method\n        (the signature of which is func(data, target) -&gt; metrics)\n        then it will be used rather than the input metric.\n\n    Args:\n        metric: The metric used for validation.\n            If None, use MetricTypes.ACCURACY.\n        history_monitor: HistoryMonitor instance,\n            used to record computed metrics and communicate them to\n            the researcher (server).\n        before_train: Whether the evaluation is being performed\n            before local training occurs, of afterwards. This is merely\n            reported back through `history_monitor`.\n    \"\"\"\n    # Check that the testing data loader is of proper type.\n    if not isinstance(self.testing_data_loader, NPDataLoader):\n        msg = (\n            f\"{ErrorNumbers.FB310.value}: SKLearnTrainingPlan cannot be \"\n            \"evaluated without a NPDataLoader as `testing_data_loader`.\"\n        )\n        logger.error(msg)\n        raise FedbiomedTrainingPlanError(msg)\n    # If required, make up for the lack of specifications regarding target\n    # classification labels.\n    if self._model.is_classification and not hasattr(self.model(), 'classes_'):\n        classes = self._classes_from_concatenated_train_test()\n        setattr(self.model(), 'classes_', classes)\n    # If required, select the default metric (accuracy or mse).\n    if metric is None:\n        if self._model.is_classification:\n            metric = MetricTypes.ACCURACY\n        else:\n            metric = MetricTypes.MEAN_SQUARE_ERROR\n    # Delegate the actual evalation routine to the parent class.\n    super().testing_routine(\n        metric, metric_args, history_monitor, before_train\n    )\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.SKLearnTrainingPlan.training_routine","title":"training_routine","text":"<pre><code>training_routine(history_monitor=None, node_args=None)\n</code></pre> <p>Training routine, to be called once per round.</p> <p>Parameters:</p> Name Type Description Default <code>history_monitor</code> <code>Optional[HistoryMonitor]</code> <p>optional HistoryMonitor instance, recording training metadata. Defaults to None.</p> <code>None</code> <code>node_args</code> <code>Optional[Dict[str, Any]]</code> <p>command line arguments for node. These arguments can specify GPU use; however, this is not supported for scikit-learn models and thus will be ignored.</p> <code>None</code> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def training_routine(\n        self,\n        history_monitor: Optional['HistoryMonitor'] = None,\n        node_args: Optional[Dict[str, Any]] = None\n) -&gt; None:\n    \"\"\"Training routine, to be called once per round.\n\n    Args:\n        history_monitor: optional HistoryMonitor\n            instance, recording training metadata. Defaults to None.\n        node_args: command line arguments for node.\n            These arguments can specify GPU use; however, this is not\n            supported for scikit-learn models and thus will be ignored.\n    \"\"\"\n    if self._optimizer is None:\n        raise FedbiomedTrainingPlanError('Optimizer is None, please run `post_init` beforehand')\n\n    # Run preprocesses\n    self._preprocess()\n\n    if not isinstance(self.training_data_loader, NPDataLoader):\n        msg = (\n            f\"{ErrorNumbers.FB310.value}: SKLearnTrainingPlan cannot \"\n            \"be trained without a NPDataLoader as `training_data_loader`.\"\n        )\n        logger.critical(msg)\n        raise FedbiomedTrainingPlanError(msg)\n\n    # Warn if GPU-use was expected (as it is not supported).\n    if node_args is not None and node_args.get('gpu_only', False):\n\n        self._optimizer.send_to_device(False)  # disable GPU, avoid `declearn` triggering warning messages\n        logger.warning(\n            'Node would like to force GPU usage, but sklearn training '\n            'plan does not support it. Training on CPU.'\n        )\n    # Run the model-specific training routine.\n    try:\n        return self._training_routine(history_monitor)\n    except Exception as exc:\n        msg = (\n            f\"{ErrorNumbers.FB605.value}: error while fitting \"\n            f\"the model: {exc}\"\n        )\n        logger.critical(msg)\n        raise FedbiomedTrainingPlanError(msg)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.SKLearnTrainingPlan.type","title":"type","text":"<pre><code>type()\n</code></pre> <p>Getter for training plan type</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def type(self) -&gt; TrainingPlans:\n    \"\"\"Getter for training plan type \"\"\"\n    return self.__type\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan","title":"TorchTrainingPlan","text":"<pre><code>TorchTrainingPlan()\n</code></pre> <p>               Bases: <code>BaseTrainingPlan</code></p> <p>Implements  TrainingPlan for torch NN framework</p> <p>An abstraction over pytorch module to run pytorch models and scripts on node side. Researcher model (resp. params) will be:</p> <ol> <li>saved  on a '.py' (resp. '.mpk') files,</li> <li>uploaded on a HTTP server (network layer),</li> <li>then Downloaded from the HTTP server on node side,</li> <li>finally, read and executed on node side.</li> </ol> <p>Researcher must define/override: - a <code>training_data()</code> function - a <code>training_step()</code> function</p> <p>Researcher may have to add extra dependencies/python imports, by using <code>init_dependencies</code> method.</p> <p>Attributes:</p> Name Type Description <code>dataset_path</code> <code>Union[str, None]</code> <p>The path that indicates where dataset has been stored</p> <code>pre_processes</code> <code>Dict[str, PreProcessDict]</code> <p>Preprocess functions that will be applied to the training data at the beginning of the training routine.</p> <code>training_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader used in the training routine.</p> <code>testing_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader used in the validation routine.</p> <code>correction_state</code> <code>OrderedDict</code> <p>an OrderedDict of {'parameter name': torch.Tensor} where the keys correspond to the names of the model parameters contained in self._model.named_parameters(), and the values correspond to the correction to be applied to that parameter.</p> <p>Notes</p> <p>The trained model may be exported via the <code>export_model</code> method, resulting in a dump file that may be reloded using <code>torch.save</code> outside of Fed-BioMed.</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def __init__(self):\n    \"\"\" Construct training plan \"\"\"\n\n    super().__init__()\n\n    self.__type = TrainingPlans.TorchTrainingPlan\n\n    # Differential privacy support\n    self._dp_controller: Optional[DPController] = None\n    self._optimizer: Union[BaseOptimizer, None] = None\n    self._model: Union[TorchModel, None] = None\n\n    self._use_gpu: bool = False\n    self._share_persistent_buffers = None\n\n    self._batch_maxnum: int = 100\n    self._fedprox_mu: Optional[float] = None\n    self._log_interval: int = 10\n    self._epochs: int = 1\n    self._dry_run = False\n    self._num_updates: Optional[int] = None\n\n    self.correction_state: OrderedDict = OrderedDict()\n    self.aggregator_name: str = None\n\n    # TODO : add random seed init\n    # self.random_seed_params = None\n    # self.random_seed_shuffling_data = None\n\n    # device to use: cpu/gpu\n    # - all operations except training only use cpu\n    # - researcher doesn't request to use gpu by default\n    self._device_init: str = \"cpu\"\n    self._device = self._device_init\n\n    # list dependencies of the model\n    self._add_dependency([\"import torch\",\n                         \"import torch.nn as nn\",\n                         \"import torch.nn.functional as F\",\n                         \"from fedbiomed.common.training_plans import TorchTrainingPlan\",\n                         \"from fedbiomed.common.data import DataManager\",\n                         \"from fedbiomed.common.constants import ProcessTypes\",\n                         \"from torch.utils.data import DataLoader\",\n                         \"from torchvision import datasets, transforms\"\n                         ])\n\n    # Aggregated model parameters\n    #self._init_params: List[torch.Tensor] = None\n\n    # Add dependencies\n    self._configure_dependencies()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan-attributes","title":"Attributes","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.aggregator_name","title":"aggregator_name  <code>instance-attribute</code>","text":"<pre><code>aggregator_name = None\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.correction_state","title":"correction_state  <code>instance-attribute</code>","text":"<pre><code>correction_state = OrderedDict()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan-functions","title":"Functions","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.after_training_params","title":"after_training_params","text":"<pre><code>after_training_params(flatten=False)\n</code></pre> <p>Return the wrapped model's parameters for aggregation.</p> <p>This method returns a dict containing parameters that need to be reported back and aggregated in a federated learning setting.</p> <p>If the <code>postprocess</code> method exists (i.e. has been defined by end-users) it is called in the context of this method. DP-required adjustments are also set to happen as part of this method.</p> <p>If the researcher specified <code>share_persistent_buffers: False</code> in the training arguments, then we return only the output of Model.get_weights, which considers only the trainable parameters. Otherwise, the default behaviour is to return the complete <code>state_dict</code>.</p> <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>The trained parameters to aggregate.</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def after_training_params(self, flatten: bool = False) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Return the wrapped model's parameters for aggregation.\n\n    This method returns a dict containing parameters that need to be\n    reported back and aggregated in a federated learning setting.\n\n    If the `postprocess` method exists (i.e. has been defined by end-users)\n    it is called in the context of this method. DP-required adjustments are\n    also set to happen as part of this method.\n\n    If the researcher specified `share_persistent_buffers: False` in the\n    training arguments, then we return only the output of\n    [Model.get_weights][fedbiomed.common.models.TorchModel.get_weights],\n    which considers only the trainable parameters.\n    Otherwise, the default behaviour is to return the complete `state_dict`.\n\n    Returns:\n        The trained parameters to aggregate.\n    \"\"\"\n    # Either include non-parameter buffers to the outputs or not.\n    # Note: this is mostly about sharing statistics from BatchNorm layers.\n    params = super().after_training_params()\n    # Check whether postprocess method exists, and use it.\n    if hasattr(self, 'postprocess'):\n        logger.debug(\"running model.postprocess() method\")\n        try:\n            params = self.postprocess(self._model.model.state_dict())  # Post process\n        except Exception as e:\n            raise FedbiomedTrainingPlanError(f\"{ErrorNumbers.FB605.value}: Error while running post-process \"\n                                             f\"{e}\") from e\n\n    # Run (optional) DP controller adjustments as well.\n    params = self._dp_controller.after_training(params)\n    if flatten:\n        params = self._model.flatten(exclude_buffers=not self._share_persistent_buffers)\n    return params\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.init_model","title":"init_model  <code>abstractmethod</code>","text":"<pre><code>init_model()\n</code></pre> <p>Abstract method where model should be defined.</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>@abstractmethod\ndef init_model(self):\n    \"\"\"Abstract method where model should be defined.\"\"\"\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.init_optimizer","title":"init_optimizer","text":"<pre><code>init_optimizer()\n</code></pre> <p>Abstract method for declaring optimizer by default</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def init_optimizer(self) -&gt; Union[FedOptimizer, torch.optim.Optimizer]:\n    \"\"\"Abstract method for declaring optimizer by default \"\"\"\n    try:\n        self._optimizer = torch.optim.Adam(self._model.model.parameters(), **self._optimizer_args)\n    except AttributeError as e:\n        raise FedbiomedTrainingPlanError(f\"{ErrorNumbers.FB605.value}: Invalid argument for default \"\n                                         f\"optimizer Adam. Error: {e}\") from e\n\n    return self._optimizer\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.initial_parameters","title":"initial_parameters","text":"<pre><code>initial_parameters()\n</code></pre> <p>Returns initial parameters without DP or training applied</p> <p>Returns:</p> Type Description <code>Dict</code> <p>State dictionary of torch Module</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def initial_parameters(self) -&gt; Dict:\n    \"\"\"Returns initial parameters without DP or training applied\n\n    Returns:\n        State dictionary of torch Module\n    \"\"\"\n    return self._model.init_params\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.model","title":"model","text":"<pre><code>model()\n</code></pre> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def model(self) -&gt; Optional[torch.nn.Module]:\n    if self._model is None:\n        return None\n    return self._model.model\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.optimizer_args","title":"optimizer_args","text":"<pre><code>optimizer_args()\n</code></pre> <p>Retrieves optimizer arguments</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Optimizer arguments</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def optimizer_args(self) -&gt; Dict[str, Any]:\n    \"\"\"Retrieves optimizer arguments\n\n    Returns:\n        Optimizer arguments\n    \"\"\"\n    self.update_optimizer_args()  # update `optimizer_args` (eg after training)\n    return super().optimizer_args()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.post_init","title":"post_init","text":"<pre><code>post_init(model_args, training_args, aggregator_args=None, initialize_optimizer=True)\n</code></pre> <p>Process model, training and optimizer arguments.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict[str, Any]</code> <p>Arguments defined to instantiate the wrapped model.</p> required <code>training_args</code> <code>TrainingArgs</code> <p>Arguments that are used in training routines such as epoch, dry_run etc. Please see <code>TrainingArgs</code></p> required <code>aggregator_args</code> <code>Optional[Dict[str, Any]]</code> <p>Arguments managed by and shared with the researcher-side aggregator.</p> <code>None</code> <code>initialize_optimizer</code> <code>bool</code> <p>If True, configures optimizer. It has to be True for node side configuration to prepare optimizer for the training.</p> <code>True</code> <p>Raises:     FedbiomedTrainingPlanError: If the provided arguments do not         match expectations, or if the optimizer, model and dependencies         configuration goes wrong.</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def post_init(\n        self,\n        model_args: Dict[str, Any],\n        training_args: TrainingArgs,\n        aggregator_args: Optional[Dict[str, Any]] = None,\n        initialize_optimizer: bool = True\n) -&gt; None:\n    \"\"\"Process model, training and optimizer arguments.\n\n    Args:\n        model_args: Arguments defined to instantiate the wrapped model.\n        training_args: Arguments that are used in training routines\n            such as epoch, dry_run etc.\n            Please see [`TrainingArgs`][fedbiomed.common.training_args.TrainingArgs]\n        aggregator_args: Arguments managed by and shared with the\n            researcher-side aggregator.\n        initialize_optimizer: If True, configures optimizer. It has to be True for node\n            side configuration to prepare optimizer for the training.\n    Raises:\n        FedbiomedTrainingPlanError: If the provided arguments do not\n            match expectations, or if the optimizer, model and dependencies\n            configuration goes wrong.\n    \"\"\"\n    super().post_init(model_args, training_args, aggregator_args)\n    # Assign scalar attributes.\n    self._use_gpu = self._training_args.get('use_gpu')\n    self._batch_maxnum = self._training_args.get('batch_maxnum')\n    self._log_interval = self._training_args.get('log_interval')\n    self._epochs = self._training_args.get('epochs')\n    self._num_updates = self._training_args.get('num_updates', 1)\n    self._dry_run = self._training_args.get('dry_run')\n    self._share_persistent_buffers = training_args.get('share_persistent_buffers', True)\n    # Set random seed (Pytorch-specific)\n    rseed = training_args['random_seed']\n\n    if rseed is not None:\n        torch.manual_seed(rseed)\n    # Optionally set up differential privacy.\n    self._dp_controller = DPController(training_args.dp_arguments() or None)\n    # Configure aggregator-related arguments\n    # TODO: put fedprox mu inside strategy_args\n    self._fedprox_mu = self._training_args.get('fedprox_mu')\n    self.set_aggregator_args(aggregator_args or {})\n    # Configure the model and optimizer.\n\n    self._configure_model_and_optimizer(initialize_optimizer)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.send_to_device","title":"send_to_device","text":"<pre><code>send_to_device(to_send, device)\n</code></pre> <p>Send inputs to correct device for training.</p> <p>Recursively traverses lists, tuples and dicts until it meets a torch Tensor, then sends the Tensor to the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>Union[Tensor, list, tuple, dict]</code> <p>the data to be sent to the device.</p> required <code>device</code> <code>device</code> <p>the device to send the data to.</p> required <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanError</code> <p>when to_send is not the correct type</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def send_to_device(self,\n                   to_send: Union[torch.Tensor, list, tuple, dict],\n                   device: torch.device\n                   ):\n    \"\"\"Send inputs to correct device for training.\n\n    Recursively traverses lists, tuples and dicts until it meets a torch Tensor, then sends the Tensor\n    to the specified device.\n\n    Args:\n        to_send: the data to be sent to the device.\n        device: the device to send the data to.\n\n    Raises:\n       FedbiomedTrainingPlanError: when to_send is not the correct type\n    \"\"\"\n    if isinstance(to_send, torch.Tensor):\n        return to_send.to(device)\n    elif isinstance(to_send, dict):\n        return {key: self.send_to_device(val, device) for key, val in to_send.items()}\n    elif isinstance(to_send, tuple):\n        return tuple(self.send_to_device(d, device) for d in to_send)\n    elif isinstance(to_send, list):\n        return [self.send_to_device(d, device) for d in to_send]\n    else:\n        raise FedbiomedTrainingPlanError(f'{ErrorNumbers.FB310.value} cannot send data to device. '\n                                         f'Data must be a torch Tensor or a list, tuple or dict '\n                                         f'ultimately containing Tensors.')\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.set_aggregator_args","title":"set_aggregator_args","text":"<pre><code>set_aggregator_args(aggregator_args)\n</code></pre> <p>Handles and loads aggregators arguments to received from the researcher</p> <p>Parameters:</p> Name Type Description Default <code>aggregator_args</code> <code>Dict[str, Any]</code> <p>dictionary mapping aggregator argument name with its value (eg 'aggregator_correction' with correction states)</p> required Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def set_aggregator_args(self, aggregator_args: Dict[str, Any]):\n    \"\"\"Handles and loads aggregators arguments to received from the researcher\n\n    Args:\n        aggregator_args (Dict[str, Any]): dictionary mapping aggregator argument\n            name with its value (eg 'aggregator_correction' with correction states)\n    \"\"\"\n\n    self.aggregator_name = aggregator_args.get('aggregator_name', self.aggregator_name)\n    # FIXME: this is too specific to Scaffold. Should be redesigned, or handled\n    # by an aggregator handler that contains all keys for all strategies\n    # implemented in fedbiomed\n    # here we ae loading all args that have been sent from file exchange system\n    for arg_name, aggregator_arg in aggregator_args.items():\n        if arg_name == 'aggregator_correction':\n            if not isinstance(aggregator_arg, dict):\n                raise FedbiomedTrainingPlanError(\n                    f\"{ErrorNumbers.FB309.value}: TorchTrainingPlan received \"\n                    \"invalid 'aggregator_correction' aggregator args.\"\n                )\n            self.correction_state = aggregator_arg\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.testing_routine","title":"testing_routine","text":"<pre><code>testing_routine(metric, metric_args, history_monitor, before_train)\n</code></pre> <p>Evaluation routine, to be called once per round.</p> <p>Note</p> <p>If the training plan implements a <code>testing_step</code> method (the signature of which is func(data, target) -&gt; metrics) then it will be used rather than the input metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Optional[MetricTypes]</code> <p>The metric used for validation. If None, use MetricTypes.ACCURACY.</p> required <code>history_monitor</code> <code>Optional[HistoryMonitor]</code> <p>HistoryMonitor instance, used to record computed metrics and communicate them to the researcher (server).</p> required <code>before_train</code> <code>bool</code> <p>Whether the evaluation is being performed before local training occurs, of afterwards. This is merely reported back through <code>history_monitor</code>.</p> required Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def testing_routine(\n        self,\n        metric: Optional[MetricTypes],\n        metric_args: Dict[str, Any],\n        history_monitor: Optional['HistoryMonitor'],\n        before_train: bool\n) -&gt; None:\n    \"\"\"Evaluation routine, to be called once per round.\n\n    !!! info \"Note\"\n        If the training plan implements a `testing_step` method\n        (the signature of which is func(data, target) -&gt; metrics)\n        then it will be used rather than the input metric.\n\n    Args:\n        metric: The metric used for validation.\n            If None, use MetricTypes.ACCURACY.\n        history_monitor: HistoryMonitor instance,\n            used to record computed metrics and communicate them to\n            the researcher (server).\n        before_train: Whether the evaluation is being performed\n            before local training occurs, of afterwards. This is merely\n            reported back through `history_monitor`.\n    \"\"\"\n    if not isinstance(self.model(), torch.nn.Module):\n        msg = (\n            f\"{ErrorNumbers.FB320.value}: model should be a torch \"\n            f\"nn.Module, but is of type {type(self.model())}\"\n        )\n        logger.critical(msg)\n        raise FedbiomedTrainingPlanError(msg)\n    try:\n\n        with torch.no_grad():\n            super().testing_routine(\n                metric, metric_args, history_monitor, before_train\n            )\n    finally:\n        self.model().train()  # restore training behaviors\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.training_data","title":"training_data  <code>abstractmethod</code>","text":"<pre><code>training_data()\n</code></pre> <p>Abstract method to return training data</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>@abstractmethod\ndef training_data(self):\n    \"\"\"Abstract method to return training data\"\"\"\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.training_routine","title":"training_routine","text":"<pre><code>training_routine(history_monitor=None, node_args=None)\n</code></pre> <p>Training routine procedure.</p> <p>End-user should define;</p> <ul> <li>a <code>training_data()</code> function defining how sampling / handling data in node's dataset is done. It should     return a generator able to output tuple (batch_idx, (data, targets)) that is iterable for each batch.</li> <li>a <code>training_step()</code> function defining how cost is computed. It should output loss values for backpropagation.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>history_monitor</code> <code>Any</code> <p>Monitor handler for real-time feed. Defined by the Node and can't be overwritten</p> <code>None</code> <code>node_args</code> <code>Union[dict, None]</code> <p>command line arguments for node. Can include: - <code>gpu (bool)</code>: propose use a GPU device if any is available. Default False. - <code>gpu_num (Union[int, None])</code>: if not None, use the specified GPU device instead of default     GPU device if this GPU device is available. Default None. - <code>gpu_only (bool)</code>: force use of a GPU device if any available, even if researcher     doesn't request for using a GPU. Default False.</p> <code>None</code> <p>Returns:     Total number of samples observed during the training.</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def training_routine(self,\n                     history_monitor: Any = None,\n                     node_args: Union[dict, None] = None,\n                     ) -&gt; int:\n    # FIXME: add betas parameters for ADAM solver + momentum for SGD\n    # FIXME 2: remove parameters specific for validation specified in the\n    # training routine\n    \"\"\"Training routine procedure.\n\n    End-user should define;\n\n    - a `training_data()` function defining how sampling / handling data in node's dataset is done. It should\n        return a generator able to output tuple (batch_idx, (data, targets)) that is iterable for each batch.\n    - a `training_step()` function defining how cost is computed. It should output loss values for backpropagation.\n\n    Args:\n        history_monitor: Monitor handler for real-time feed. Defined by the Node and can't be overwritten\n        node_args: command line arguments for node. Can include:\n            - `gpu (bool)`: propose use a GPU device if any is available. Default False.\n            - `gpu_num (Union[int, None])`: if not None, use the specified GPU device instead of default\n                GPU device if this GPU device is available. Default None.\n            - `gpu_only (bool)`: force use of a GPU device if any available, even if researcher\n                doesn't request for using a GPU. Default False.\n    Returns:\n        Total number of samples observed during the training.\n    \"\"\"\n\n    #self.model().train()  # pytorch switch for training\n    self._optimizer.init_training()\n    # set correct type for node args\n    node_args = {} if not isinstance(node_args, dict) else node_args\n\n    # send all model to device, ensures having all the requested tensors\n    self._set_device(self._use_gpu, node_args)\n    self._model.send_to_device(self._device)\n\n    # Run preprocess when everything is ready before the training\n    self._preprocess()\n\n    # # initial aggregated model parameters\n    # self._init_params = deepcopy(list(self.model().parameters()))\n\n    # DP actions\n    self._optimizer, self.training_data_loader = \\\n        self._dp_controller.before_training(optimizer= self._optimizer, loader=self.training_data_loader)\n\n    # set number of training loop iterations\n    iterations_accountant = MiniBatchTrainingIterationsAccountant(self)\n\n    # Training loop iterations\n    for epoch in iterations_accountant.iterate_epochs():\n        training_data_iter: Iterator = iter(self.training_data_loader)\n\n        for batch_idx in iterations_accountant.iterate_batches():\n            # retrieve data and target\n            data, target = next(training_data_iter)\n\n            # update accounting for number of observed samples\n            batch_size = self._infer_batch_size(data)\n            iterations_accountant.increment_sample_counters(batch_size)\n\n            # handle training on accelerator devices\n            data, target = self.send_to_device(data, self._device), self.send_to_device(target, self._device)\n\n            # train this batch\n            corrected_loss, loss = self._train_over_batch(data, target)\n\n            # Reporting\n            if iterations_accountant.should_log_this_batch():\n                # Retrieve reporting information: semantics differ whether num_updates or epochs were specified\n                num_samples, num_samples_max = iterations_accountant.reporting_on_num_samples()\n                num_iter, num_iter_max = iterations_accountant.reporting_on_num_iter()\n                epoch_to_report = iterations_accountant.reporting_on_epoch()\n\n                logger.debug('Train {}| '\n                             'Iteration {}/{} | '\n                             'Samples {}/{} ({:.0f}%)\\tLoss: {:.6f}'.format(\n                                 f'Epoch: {epoch_to_report} ' if epoch_to_report is not None else '',\n                                 num_iter,\n                                 num_iter_max,\n                                 num_samples,\n                                 num_samples_max,\n                                 100. * num_iter / num_iter_max,\n                                 loss.item())\n                             )\n\n                # Send scalar values via general/feedback topic\n                if history_monitor is not None:\n                    # the researcher only sees the average value of samples observed until now\n                    history_monitor.add_scalar(metric={'Loss': loss.item()},\n                                               iteration=num_iter,\n                                               epoch=epoch_to_report,\n                                               train=True,\n                                               num_samples_trained=num_samples,\n                                               num_batches=num_iter_max,\n                                               total_samples=num_samples_max,\n                                               batch_samples=batch_size)\n\n            # Handle dry run mode\n            if self._dry_run:\n                self._model.send_to_device(self._device_init)\n                torch.cuda.empty_cache()\n                return iterations_accountant.num_samples_observed_in_total\n\n    # release gpu usage as much as possible though:\n    # - it should be done by deleting the object\n    # - and some gpu memory remains used until process (cuda kernel ?) finishes\n\n    self._model.send_to_device(self._device_init)\n    torch.cuda.empty_cache()\n\n    # # test (to be removed)\n    # assert id(self._optimizer.model.model) == id(self._model.model)\n\n    # assert id(self._optimizer.model) == id(self._model)\n\n    # for (layer, val), (layer2, val2) in zip(self._model.model.state_dict().items(), self._optimizer.model.model.state_dict().items()):\n    #     assert layer == layer2\n    #     print(val, layer2)\n    #     assert torch.isclose(val, val2).all()\n\n    # for attributes, values in self._model.__dict__.items():\n    #     print(\"ATTRIBUTES\", values)\n    #     assert values == getattr(self._optimizer.model, attributes)\n\n    # for attributes, values in self._model.model.__dict__.items():\n    #     print(\"ATTRIBUTES\", values)\n    #     assert values == getattr(self._optimizer.model.model, attributes)\n    return iterations_accountant.num_samples_observed_in_total\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.training_step","title":"training_step  <code>abstractmethod</code>","text":"<pre><code>training_step()\n</code></pre> <p>Abstract method, all subclasses must provide a training_step.</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>@abstractmethod\ndef training_step(self):\n    \"\"\"Abstract method, all subclasses must provide a training_step.\"\"\"\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.type","title":"type","text":"<pre><code>type()\n</code></pre> <p>Gets training plan type</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def type(self) -&gt; TrainingPlans.TorchTrainingPlan:\n    \"\"\" Gets training plan type\"\"\"\n    return self.__type\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan.update_optimizer_args","title":"update_optimizer_args","text":"<pre><code>update_optimizer_args()\n</code></pre> <p>Updates <code>_optimizer_args</code> variable. Can prove useful to retrieve optimizer parameters after having trained a model, parameters which may have changed during training (eg learning rate).</p> Updated arguments <ul> <li>learning_rate</li> </ul> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>updated <code>_optimizer_args</code></p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def update_optimizer_args(self) -&gt; Dict:\n    \"\"\"\n    Updates `_optimizer_args` variable. Can prove useful\n    to retrieve optimizer parameters after having trained a\n    model, parameters which may have changed during training (eg learning rate).\n\n    Updated arguments:\n     - learning_rate\n\n    Returns:\n        Dict: updated `_optimizer_args`\n    \"\"\"\n    if self._optimizer_args is None:\n        self._optimizer_args = {}\n    if self.aggregator_name is not None and self.aggregator_name.lower() == \"scaffold\":\n        self._optimizer_args['lr'] = self._optimizer.get_learning_rate()\n    return self._optimizer_args\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant-attributes","title":"Attributes","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.cur_batch","title":"cur_batch  <code>instance-attribute</code>","text":"<pre><code>cur_batch = 0\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.cur_epoch","title":"cur_epoch  <code>instance-attribute</code>","text":"<pre><code>cur_epoch = 0\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.epochs","title":"epochs  <code>instance-attribute</code>","text":"<pre><code>epochs = 0\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.num_batches_in_last_epoch","title":"num_batches_in_last_epoch  <code>instance-attribute</code>","text":"<pre><code>num_batches_in_last_epoch = 0\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.num_batches_per_epoch","title":"num_batches_per_epoch  <code>instance-attribute</code>","text":"<pre><code>num_batches_per_epoch = 0\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.num_samples_observed_in_epoch","title":"num_samples_observed_in_epoch  <code>instance-attribute</code>","text":"<pre><code>num_samples_observed_in_epoch = 0\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.num_samples_observed_in_total","title":"num_samples_observed_in_total  <code>instance-attribute</code>","text":"<pre><code>num_samples_observed_in_total = 0\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant-classes","title":"Classes","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.BatchIter","title":"BatchIter","text":"<pre><code>BatchIter(accountant)\n</code></pre> <p>Iterator over batches.</p> <p>Attributes:</p> Name Type Description <code>_accountant</code> <p>an instance of the class that created this iterator</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def __init__(self, accountant: TTrainingIterationsAccountant):\n    self._accountant = accountant\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.EpochsIter","title":"EpochsIter","text":"<pre><code>EpochsIter(accountant)\n</code></pre> <p>Iterator over epochs.</p> <p>Attributes:</p> Name Type Description <code>_accountant</code> <p>an instance of the class that created this iterator</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def __init__(self, accountant: TTrainingIterationsAccountant):\n    self._accountant = accountant\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant-functions","title":"Functions","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.increment_sample_counters","title":"increment_sample_counters","text":"<pre><code>increment_sample_counters(n_samples)\n</code></pre> <p>Increments internal counter for numbers of observed samples</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def increment_sample_counters(self, n_samples: int):\n    \"\"\"Increments internal counter for numbers of observed samples\"\"\"\n    self.num_samples_observed_in_epoch += n_samples\n    self.num_samples_observed_in_total += n_samples\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.iterate_batches","title":"iterate_batches","text":"<pre><code>iterate_batches()\n</code></pre> <p>Returns an instance of a batches iterator.</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def iterate_batches(self):\n    \"\"\"Returns an instance of a batches iterator.\"\"\"\n    return MiniBatchTrainingIterationsAccountant.BatchIter(self)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.iterate_epochs","title":"iterate_epochs","text":"<pre><code>iterate_epochs()\n</code></pre> <p>Returns an instance of an epochs iterator.</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def iterate_epochs(self):\n    \"\"\"Returns an instance of an epochs iterator.\"\"\"\n    return MiniBatchTrainingIterationsAccountant.EpochsIter(self)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.num_batches_in_this_epoch","title":"num_batches_in_this_epoch","text":"<pre><code>num_batches_in_this_epoch()\n</code></pre> <p>Returns the number of iterations to be performed in the current epoch</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def num_batches_in_this_epoch(self) -&gt; int:\n    \"\"\"Returns the number of iterations to be performed in the current epoch\"\"\"\n    if self.cur_epoch == self.epochs:\n        return self.num_batches_in_last_epoch\n    else:\n        return self.num_batches_per_epoch\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.reporting_on_epoch","title":"reporting_on_epoch","text":"<pre><code>reporting_on_epoch()\n</code></pre> <p>Returns the optional index of the current epoch, for reporting.</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def reporting_on_epoch(self) -&gt; Optional[int]:\n    \"\"\"Returns the optional index of the current epoch, for reporting.\"\"\"\n    if self._training_plan.training_args()['num_updates'] is not None:\n        return None\n    else:\n        return self.cur_epoch\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.reporting_on_num_iter","title":"reporting_on_num_iter","text":"<pre><code>reporting_on_num_iter()\n</code></pre> <p>Outputs useful reporting information about the number of iterations</p> <p>If the researcher specified num_updates, then the iteration number will be the cumulated total, and similarly the maximum number of iterations will be equal to the requested number of updates. If the researcher specified epochs, then the iteration number will be the batch index in the current epoch, while the maximum number of iterations will be computed specifically for the current epoch.</p> <p>Returns:</p> Type Description <code>int</code> <p>the iteration number</p> <code>int</code> <p>the maximum number of iterations to be reported</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def reporting_on_num_iter(self) -&gt; Tuple[int, int]:\n    \"\"\"Outputs useful reporting information about the number of iterations\n\n    If the researcher specified num_updates, then the iteration number will be the cumulated total, and\n    similarly the maximum number of iterations will be equal to the requested number of updates.\n    If the researcher specified epochs, then the iteration number will be the batch index in the current epoch,\n    while the maximum number of iterations will be computed specifically for the current epoch.\n\n    Returns:\n        the iteration number\n        the maximum number of iterations to be reported\n    \"\"\"\n    if self._training_plan.training_args()['num_updates'] is not None:\n        num_iter = (self.cur_epoch - 1) * self.num_batches_per_epoch + self.cur_batch\n        total_batches_to_be_observed = (self.epochs - 1) * self.num_batches_per_epoch + \\\n            self.num_batches_in_last_epoch\n        num_iter_max = total_batches_to_be_observed\n    else:\n        num_iter = self.cur_batch\n        num_iter_max = self.num_batches_per_epoch\n    return num_iter, num_iter_max\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.reporting_on_num_samples","title":"reporting_on_num_samples","text":"<pre><code>reporting_on_num_samples()\n</code></pre> <p>Outputs useful reporting information about the number of observed samples</p> <p>If the researcher specified num_updates, then the number of observed samples will be the grand total, and similarly the maximum number of samples will be the grand total over all iterations. If the researcher specified epochs, then both values will be specific to the current epoch.</p> <p>Returns:</p> Type Description <code>int</code> <p>the number of samples observed until the current iteration</p> <code>int</code> <p>the maximum number of samples to be observed</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def reporting_on_num_samples(self) -&gt; Tuple[int, int]:\n    \"\"\"Outputs useful reporting information about the number of observed samples\n\n    If the researcher specified num_updates, then the number of observed samples will be the grand total, and\n    similarly the maximum number of samples will be the grand total over all iterations.\n    If the researcher specified epochs, then both values will be specific to the current epoch.\n\n    Returns:\n        the number of samples observed until the current iteration\n        the maximum number of samples to be observed\n    \"\"\"\n    # get batch size\n    if 'batch_size' in self._training_plan.loader_args():\n        batch_size = self._training_plan.loader_args()['batch_size']\n    else:\n        raise FedbiomedUserInputError('Missing required key `batch_size` in `loader_args`.')\n    # compute number of observed samples\n    if self._training_plan.training_args()['num_updates'] is not None:\n        num_samples = self.num_samples_observed_in_total\n        total_batches_to_be_observed = (self.epochs - 1) * self.num_batches_per_epoch + \\\n            self.num_batches_in_last_epoch\n        total_n_samples_to_be_observed = batch_size * total_batches_to_be_observed\n        num_samples_max = total_n_samples_to_be_observed\n    else:\n        num_samples = self.num_samples_observed_in_epoch\n        num_samples_max = batch_size*self.num_batches_in_this_epoch() if \\\n            self.cur_batch &lt; self.num_batches_in_this_epoch() else num_samples\n    return num_samples, num_samples_max\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.should_log_this_batch","title":"should_log_this_batch","text":"<pre><code>should_log_this_batch()\n</code></pre> <p>Whether the current batch should be logged or not.</p> <p>A batch shall be logged if at least one of the following conditions is True:</p> <pre><code>- the cumulative batch index is a multiple of the logging interval\n- the dry_run condition was specified by the researcher\n- it is the last batch of the epoch\n- it is the first batch of the epoch\n</code></pre> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def should_log_this_batch(self) -&gt; bool:\n    \"\"\"Whether the current batch should be logged or not.\n\n    A batch shall be logged if at least one of the following conditions is True:\n\n        - the cumulative batch index is a multiple of the logging interval\n        - the dry_run condition was specified by the researcher\n        - it is the last batch of the epoch\n        - it is the first batch of the epoch\n    \"\"\"\n    current_iter = (self.cur_epoch - 1) * self.num_batches_per_epoch + self.cur_batch\n    return (current_iter % self._training_plan.training_args()['log_interval'] == 0 or\n            self._training_plan.training_args()['dry_run'] or\n            self.cur_batch &gt;= self.num_batches_in_this_epoch() or  # last batch\n            self.cur_batch == 1)  # first batch\n</code></pre>"},{"location":"developer/api/common/utils/","title":"Utils","text":""},{"location":"developer/api/common/utils/#fedbiomed.common.utils-attributes","title":"Attributes","text":""},{"location":"developer/api/common/utils/#fedbiomed.common.utils.FBM_Component_Version","title":"FBM_Component_Version  <code>module-attribute</code>","text":"<pre><code>FBM_Component_Version = Version\n</code></pre> <p>The Type of objects representing version numbers in Fed-BioMed</p>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.ROOT_DIR","title":"ROOT_DIR  <code>module-attribute</code>","text":"<pre><code>ROOT_DIR = _get_fedbiomed_root()\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.SHARE_DIR","title":"SHARE_DIR  <code>module-attribute</code>","text":"<pre><code>SHARE_DIR = _get_shared_dir()\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils-functions","title":"Functions","text":""},{"location":"developer/api/common/utils/#fedbiomed.common.utils.compute_dot_product","title":"compute_dot_product","text":"<pre><code>compute_dot_product(model, params, device=None)\n</code></pre> <p>Compute the dot product between model and input parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>OrderedDict representing model state</p> required <code>params</code> <code>dict</code> <p>OrderedDict containing correction parameters</p> required <p>Returns:</p> Type Description <code>tensor</code> <p>A tensor containing a single numerical value which is the dot product.</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def compute_dot_product(model: dict, params: dict, device: Optional[str] = None) -&gt; torch.tensor:\n    \"\"\"Compute the dot product between model and input parameters.\n\n    Args:\n        model: OrderedDict representing model state\n        params: OrderedDict containing correction parameters\n\n    Returns:\n        A tensor containing a single numerical value which is the dot product.\n    \"\"\"\n    model_p = model.values()\n    correction_state = params.values()\n    if device is None:\n        if model_p:\n            device = list(model_p)[0].device\n        else:\n            # if device is not found, set it to `cpu`\n            device = 'cpu'\n    dot_prod = sum([torch.sum(m * torch.tensor(p).float().to(device)) for m, p in zip(model_p, correction_state)])\n    return dot_prod\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.convert_iterator_to_list_of_python_floats","title":"convert_iterator_to_list_of_python_floats","text":"<pre><code>convert_iterator_to_list_of_python_floats(iterator)\n</code></pre> <p>Converts numerical values of array-like object to float</p> <p>Parameters:</p> Name Type Description Default <code>iterator</code> <code>Iterator</code> <p>Array-like numeric object to convert numerics to float</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>Numerical elements as converted to List of floats</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def convert_iterator_to_list_of_python_floats(iterator: Iterator) -&gt; List[float]:\n    \"\"\"Converts numerical values of array-like object to float\n\n    Args:\n        iterator: Array-like numeric object to convert numerics to float\n\n    Returns:\n        Numerical elements as converted to List of floats\n    \"\"\"\n\n    if not isinstance(iterator, Iterable):\n        raise FedbiomedError(f\"{ErrorNumbers.FB627.value}: object {type(iterator)} is not iterable\")\n\n    list_of_floats = []\n    if isinstance(iterator, dict):\n        # specific processing for dictionaries\n        for val in iterator.values():\n            list_of_floats.append(convert_to_python_float(val))\n    else:\n        for it in iterator:\n            list_of_floats.append(convert_to_python_float(it))\n    return list_of_floats\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.convert_to_python_float","title":"convert_to_python_float","text":"<pre><code>convert_to_python_float(value)\n</code></pre> <p>Convert numeric types to float</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[Tensor, integer, floating, float, int]</code> <p>value to convert python type float</p> required <p>Returns:</p> Type Description <code>float</code> <p>Python float</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def convert_to_python_float(value: Union[torch.Tensor, np.integer, np.floating, float, int]) -&gt; float:\n    \"\"\" Convert numeric types to float\n\n    Args:\n        value: value to convert python type float\n\n    Returns:\n        Python float\n    \"\"\"\n\n    if not isinstance(value, (torch.Tensor, np.integer, np.floating, float, int)):\n        raise FedbiomedError(\n            f\"{ErrorNumbers.FB627.value}: Converting {type(value)} to python to float is not supported.\")\n\n    # if the result is a tensor, convert it back to numpy\n    if isinstance(value, torch.Tensor):\n        value = value.numpy()\n\n    if isinstance(value, Iterable) and value.size &gt; 1:\n        raise FedbiomedError(f\"{ErrorNumbers.FB627.value}: Can not convert array-type objects to float.\")\n\n    return float(value)\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.create_fedbiomed_setup_folders","title":"create_fedbiomed_setup_folders","text":"<pre><code>create_fedbiomed_setup_folders(root)\n</code></pre> <p>Creates folders reequired by Fed-BioMed component setup</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory of Fed-BioMed component setup</p> required Source code in <code>fedbiomed/common/utils/_config_utils.py</code> <pre><code>def create_fedbiomed_setup_folders(root: str):\n    \"\"\"Creates folders reequired by Fed-BioMed component setup\n\n    Args:\n        root: Root directory of Fed-BioMed component setup\n    \"\"\"\n\n    etc_config_dir = os.path.join(root, CONFIG_FOLDER_NAME)\n    var_dir = os.path.join(root, VAR_FOLDER_NAME)\n    cache_dir = os.path.join(var_dir, CACHE_FOLDER_NAME)\n    tmp_dir = os.path.join(var_dir, TMP_FOLDER_NAME)\n\n    for dir_ in [etc_config_dir, var_dir, cache_dir, tmp_dir]:\n        if not os.path.isdir(dir_):\n            os.makedirs(dir_)\n\n    return etc_config_dir, var_dir, cache_dir, tmp_dir\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.divide","title":"divide","text":"<pre><code>divide(xs, k)\n</code></pre> <p>Divides a list of integers by a constant</p> <p>Parameters:</p> Name Type Description Default <code>xs</code> <code>List[int]</code> <p>List of integers</p> required <code>k</code> <code>int</code> <p>Constant to divide by</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List of divided integers</p> Source code in <code>fedbiomed/common/utils/_secagg_utils.py</code> <pre><code>def divide(xs: List[int], k: int) -&gt; List[int]:\n    \"\"\"\n    Divides a list of integers by a constant\n\n    Args:\n        xs: List of integers\n        k: Constant to divide by\n\n    Returns:\n        List of divided integers\n    \"\"\"\n    # Quicker than converting to/from numpy\n    return [e / k for e in xs]\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_all_existing_certificates","title":"get_all_existing_certificates","text":"<pre><code>get_all_existing_certificates()\n</code></pre> <p>Gets all existing certificates from Fed-BioMed <code>etc</code> directory.</p> <p>This method parse all available configs in <code>etc</code> directory.</p> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List of certificate objects that contain  component type as <code>component</code>, party id <code>id</code>, public key content (not path)  as <code>certificate</code>.</p> Source code in <code>fedbiomed/common/utils/_config_utils.py</code> <pre><code>def get_all_existing_certificates() -&gt; List[Dict[str, str]]:\n    \"\"\"Gets all existing certificates from Fed-BioMed `etc` directory.\n\n    This method parse all available configs in `etc` directory.\n\n    Returns:\n        List of certificate objects that contain  component type as `component`,\n            party id `id`, public key content (not path)  as `certificate`.\n    \"\"\"\n\n    config_files = get_all_existing_config_files()\n\n    certificates = []\n    for config in config_files:\n        certificates.append(get_component_certificate_from_config(config))\n\n    return certificates\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_all_existing_config_files","title":"get_all_existing_config_files","text":"<pre><code>get_all_existing_config_files()\n</code></pre> <p>Gets all existing config files from Fed-BioMed <code>etc</code> directory</p> Source code in <code>fedbiomed/common/utils/_config_utils.py</code> <pre><code>def get_all_existing_config_files():\n    \"\"\"Gets all existing config files from Fed-BioMed `etc` directory\"\"\"\n    etc = os.path.join(ROOT_DIR, CONFIG_FOLDER_NAME, \"\")\n    return [file for file in glob.glob(f\"{etc}*.ini\")]\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_class_source","title":"get_class_source","text":"<pre><code>get_class_source(cls)\n</code></pre> <p>Get source of the class.</p> <p>It uses different methods for getting the class source based on shell type; IPython,Notebook shells or Python shell</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Callable</code> <p>The class to extract the source code from</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Source code of the given class</p> <p>Raises:</p> Type Description <code>FedbiomedError</code> <p>if argument is not a class</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def get_class_source(cls: Callable) -&gt; str:\n    \"\"\"Get source of the class.\n\n    It uses different methods for getting the class source based on shell type; IPython,Notebook\n    shells or Python shell\n\n    Args:\n        cls: The class to extract the source code from\n\n    Returns:\n        str: Source code of the given class\n\n    Raises:\n        FedbiomedError: if argument is not a class\n    \"\"\"\n\n    if not inspect.isclass(cls):\n        raise FedbiomedError(f'{ErrorNumbers.FB627.value}: The argument `cls` must be a python class')\n\n    # Check ipython status\n    status = is_ipython()\n\n    if status:\n        file = get_ipython_class_file(cls)\n        codes = \"\".join(inspect.linecache.getlines(file))\n\n        # Import only on IPython interface\n        module = importlib.import_module(\"IPython.core.magics.code\")\n        extract_symbols = module.extract_symbols\n        print(extract_symbols)\n        class_code = extract_symbols(codes, cls.__name__)[0][0]\n        return class_code\n\n    return inspect.getsource(cls)\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_component_certificate_from_config","title":"get_component_certificate_from_config","text":"<pre><code>get_component_certificate_from_config(config_path)\n</code></pre> <p>Gets component certificate, id and component type by given config file path.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path where config file is located.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Certificate object that contains  component type as <code>component</code>, party id <code>id</code>, public key content (not path)  as <code>certificate</code></p> <p>Raises:</p> Type Description <code>FedbiomedError</code> <ul> <li>If config file does not contain <code>node_id</code> or <code>researcher_id</code> under <code>default</code> section.</li> <li>If config file does not contain <code>public_key</code> under <code>ssl</code> section.</li> <li>If certificate file is not found or not readable</li> </ul> Source code in <code>fedbiomed/common/utils/_config_utils.py</code> <pre><code>def get_component_certificate_from_config(config_path: str) -&gt; Dict[str, str]:\n    \"\"\"Gets component certificate, id and component type by given config file path.\n\n    Args:\n        config_path: Path where config file is located.\n\n    Returns:\n        Certificate object that contains  component type as `component`, party id `id`, public key content\n            (not path)  as `certificate`\n\n    Raises:\n        FedbiomedError:\n            - If config file does not contain `node_id` or `researcher_id` under `default` section.\n            - If config file does not contain `public_key` under `ssl` section.\n            - If certificate file is not found or not readable\n    \"\"\"\n\n    config = get_component_config(config_path)\n    component_id = config.get(\"default\", \"id\")\n    component_type = config.get(\"default\", \"component\")\n\n    certificate = config.get(\"certificate\", \"public_key\")\n    certificate_path = os.path.join(os.path.dirname(config_path), certificate)\n\n    if not os.path.isfile(certificate_path):\n        raise FedbiomedError(\n            f\"The certificate for component '{component_id}' not found in {certificate_path}\"\n        )\n\n    certificate = read_file(certificate_path)\n\n    return {\n        \"party_id\": component_id,\n        \"certificate\": certificate,\n        \"component\": component_type,\n    }\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_component_config","title":"get_component_config","text":"<pre><code>get_component_config(config_path)\n</code></pre> <p>Gets config object from given config path.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path where config file is stored.</p> required <p>Returns:</p> Type Description <code>ConfigParser</code> <p>Configuration object.</p> <p>Raises:</p> Type Description <code>FedbiomedError</code> <p>If config file is not readable or not existing.</p> Source code in <code>fedbiomed/common/utils/_config_utils.py</code> <pre><code>def get_component_config(config_path: str) -&gt; configparser.ConfigParser:\n    \"\"\"Gets config object from given config path.\n\n    Args:\n        config_path: The path where config file is stored.\n\n    Returns:\n        Configuration object.\n\n    Raises:\n        FedbiomedError: If config file is not readable or not existing.\n    \"\"\"\n    config = configparser.ConfigParser()\n\n    try:\n        config.read(config_path)\n    except Exception:\n        raise FedbiomedError(\n            f\"Can not read config file. Please make sure it is existing or it has valid format. \"\n            f\"{config_path}\"\n        )\n\n    return config\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_existing_component_db_names","title":"get_existing_component_db_names","text":"<pre><code>get_existing_component_db_names()\n</code></pre> <p>Gets DB_PATHs of all existing components in Fed-BioMed root</p> Source code in <code>fedbiomed/common/utils/_config_utils.py</code> <pre><code>def get_existing_component_db_names():\n    \"\"\"Gets DB_PATHs of all existing components in Fed-BioMed root\"\"\"\n\n    config_files = get_all_existing_config_files()\n    db_names = {}\n\n    for _config in config_files:\n        config = get_component_config(_config)\n        component_id = config[\"default\"][\"id\"]\n\n        db_name = f\"{DB_PREFIX}{component_id}\"\n        db_names = {**db_names, component_id: db_name}\n\n    return db_names\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_ipython_class_file","title":"get_ipython_class_file","text":"<pre><code>get_ipython_class_file(cls)\n</code></pre> <p>Get source file/cell-id of the class which is defined in ZMQInteractiveShell or TerminalInteractiveShell</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Callable</code> <p>Python class defined on the IPython kernel</p> required <p>Returns:</p> Type Description <code>str</code> <p>File path or id of Jupyter cell. On IPython's interactive shell, it returns cell ID</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def get_ipython_class_file(cls: Callable) -&gt; str:\n    \"\"\"Get source file/cell-id of the class which is defined in ZMQInteractiveShell or TerminalInteractiveShell\n\n    Args:\n        cls: Python class defined on the IPython kernel\n\n    Returns:\n        File path or id of Jupyter cell. On IPython's interactive shell, it returns cell ID\n    \"\"\"\n\n    # Lookup by parent module\n    if hasattr(cls, '__module__'):\n        object_ = sys.modules.get(cls.__module__)\n        # If module has `__file__` attribute\n        if hasattr(object_, '__file__'):\n            return object_.__file__\n\n        # If parent module is __main__\n        for name, member in inspect.getmembers(cls):\n            if inspect.isfunction(member) and cls.__qualname__ + '.' + member.__name__ == member.__qualname__:\n                return inspect.getfile(member)\n    else:\n        raise FedbiomedError(f'{ErrorNumbers.FB627.value}: {cls} has no attribute `__module__`, source is not found.')\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_method_spec","title":"get_method_spec","text":"<pre><code>get_method_spec(method)\n</code></pre> <p>Helper to get argument specification</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>The function/method to extract argument specification from</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Specification of the method</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def get_method_spec(method: Callable) -&gt; dict:\n    \"\"\" Helper to get argument specification\n\n    Args:\n        method: The function/method to extract argument specification from\n\n    Returns:\n         Specification of the method\n    \"\"\"\n\n    method_spec = {}\n    parameters = inspect.signature(method).parameters\n    for (key, val) in parameters.items():\n        method_spec[key] = {\n            'name': val.name,\n            'default': None if val.default is inspect._empty else val.default,\n            'annotation': None if val.default is inspect._empty else val.default\n        }\n\n    return method_spec\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.import_class_from_file","title":"import_class_from_file","text":"<pre><code>import_class_from_file(module_path, class_name)\n</code></pre> <p>Import a module from a file and return a specified class of the module.</p> <p>Parameters:</p> Name Type Description Default <code>module_path</code> <code>str</code> <p>path to python module file</p> required <code>class_name</code> <code>str</code> <p>name of the class</p> required <p>Returns:</p> Type Description <code>Tuple[Any, Any]</code> <p>Tuple of the module created and the training plan class loaded</p> <p>Raises:</p> Type Description <code>FedbiomedError</code> <p>bad argument type</p> <code>FedbiomedError</code> <p>cannot load module or class</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def import_class_from_file(module_path: str, class_name: str) -&gt; Tuple[Any, Any]:\n    \"\"\"Import a module from a file and return a specified class of the module.\n\n    Args:\n        module_path: path to python module file\n        class_name: name of the class\n\n    Returns:\n        Tuple of the module created and the training plan class loaded\n\n    Raises:\n        FedbiomedError: bad argument type\n        FedbiomedError: cannot load module or class\n    \"\"\"\n    if not os.path.isfile(module_path):\n        raise FedbiomedError(f\"{ErrorNumbers.FB627}: Given path for importing {class_name} is not existing\")\n\n    module_base_name = os.path.basename(module_path)\n    pattern = re.compile(\"(.*).py$\")\n\n    match = pattern.match(module_base_name)\n    if not match:\n        raise FedbiomedError(f\"{ErrorNumbers.FB627}: File is not a python file.\")\n\n    module = match.group(1)\n    sys.path.insert(0, os.path.dirname(module_path))\n\n\n    class_ = import_object(module, class_name)\n    sys.path.pop(0)\n\n    return module, class_\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.import_class_from_spec","title":"import_class_from_spec","text":"<pre><code>import_class_from_spec(code, class_name)\n</code></pre> <p>Import a module from a code and extract the code of a specified class of the module.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>code of the module</p> required <code>class_name</code> <code>str</code> <p>name of the class</p> required <p>Returns:</p> Type Description <code>Tuple[Any, Any]</code> <p>Tuple of the module created and the extracted class</p> <p>Raises:</p> Type Description <code>FedbiomedError</code> <p>bad argument type</p> <code>FedbiomedError</code> <p>cannot load module or extract clas</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def import_class_from_spec(code: str, class_name: str) -&gt; Tuple[Any, Any] :\n    \"\"\"Import a module from a code and extract the code of a specified class of the module.\n\n    Args:\n        code: code of the module\n        class_name: name of the class\n\n    Returns:\n        Tuple of the module created and the extracted class\n\n    Raises:\n        FedbiomedError: bad argument type\n        FedbiomedError: cannot load module or extract clas\n    \"\"\"\n\n    for arg in [code, class_name]:\n        if not isinstance(arg, str):\n            raise FedbiomedError(f\"{ErrorNumbers.FB627.value}: Expected argument type is string but got '{type(arg)}'\")\n\n    try:\n        spec = importlib.util.spec_from_loader(\"module_\", loader=None)\n        module = importlib.util.module_from_spec(spec)\n        exec(code, module.__dict__)\n    except Exception as e:\n        raise FedbiomedError(f\"{ErrorNumbers.FB627.value}: Can not load module from given code: {e}\")\n\n    try:\n        class_ = getattr(module, class_name)\n    except AttributeError:\n        raise FedbiomedError(f\"{ErrorNumbers.FB627.value}: Can not import {class_name} from given code\")\n\n    return module, class_\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.import_class_object_from_file","title":"import_class_object_from_file","text":"<pre><code>import_class_object_from_file(module_path, class_name)\n</code></pre> <p>Import a module from a file and create an instance of a specified class of the module.</p> <p>Parameters:</p> Name Type Description Default <code>module_path</code> <code>str</code> <p>path to python module file</p> required <code>class_name</code> <code>str</code> <p>name of the class</p> required <p>Returns:</p> Type Description <code>Tuple[Any, Any]</code> <p>Tuple of the module created and the training plan object created</p> <p>Raises:</p> Type Description <code>FedbiomedError</code> <p>bad argument type</p> <code>FedbiomedError</code> <p>cannot instantiate object</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def import_class_object_from_file(module_path: str, class_name: str) -&gt; Tuple[Any, Any]:\n    \"\"\"Import a module from a file and create an instance of a specified class of the module.\n\n    Args:\n        module_path: path to python module file\n        class_name: name of the class\n\n    Returns:\n        Tuple of the module created and the training plan object created\n\n    Raises:\n        FedbiomedError: bad argument type\n        FedbiomedError: cannot instantiate object\n    \"\"\"\n    for arg in [module_path, class_name]:\n        if not isinstance(arg, str):\n            raise FedbiomedError(f\"{ErrorNumbers.FB627.value}: Expected argument type is string but got '{type(arg)}'\")\n\n    module, train_class = import_class_from_file(module_path, class_name)\n\n    try:\n        train_class_instance = train_class()\n    except Exception as e:\n        raise FedbiomedError(f\"{ErrorNumbers.FB627.value}: Cannot instantiate training plan object: {e}\")\n\n    return module, train_class_instance\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.import_object","title":"import_object","text":"<pre><code>import_object(module, obj_name)\n</code></pre> <p>Imports given object/class from given module</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>str</code> <p>Module that the object will be imported from</p> required <code>obj_name</code> <code>str</code> <p>Name of the object or class defined in the module</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Python object</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def import_object(module: str, obj_name: str) -&gt; Any:\n    \"\"\"Imports given object/class from given module\n\n\n    Args:\n        module: Module that the object will be imported from\n        obj_name: Name of the object or class defined in the module\n\n    Returns:\n        Python object\n    \"\"\"\n    try:\n        module = importlib.import_module(module)\n    except ModuleNotFoundError as exp:\n        raise FedbiomedError(f\"Specified module is not existing. {exp}\") from exp\n\n    try:\n        obj_ = getattr(module, obj_name)\n    except AttributeError as exp:\n        raise FedbiomedError(f\"{ErrorNumbers.FB627}, Attribute error while loading the class \"\n                             f\"{obj_name} from {module}. Error: {exp}\") from exp\n\n\n    return obj_\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.matching_parties_dh","title":"matching_parties_dh","text":"<pre><code>matching_parties_dh(context, parties)\n</code></pre> <p>Check if parties of given context are compatible with the parties of a secagg Diffie Hellman element.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>dict</code> <p>context to be compared with the secagg servkey element parties</p> required <code>parties</code> <code>list</code> <p>the secagg servkey element parties</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if this context can be used with this element, False if not.</p> Source code in <code>fedbiomed/common/utils/_secagg_utils.py</code> <pre><code>def matching_parties_dh(context: dict, parties: list) -&gt; bool:\n    \"\"\"Check if parties of given context are compatible with the parties\n        of a secagg Diffie Hellman element.\n\n        Args:\n            context: context to be compared with the secagg servkey element parties\n            parties: the secagg servkey element parties\n\n        Returns:\n            True if this context can be used with this element, False if not.\n        \"\"\"\n    # Need to ensure that:\n    # - no check on first party (no cryptographic material attached to the researcher).\n    #   The context is established for a given experiment, thus a given researcher but this should\n    #   be tested prior to this call.\n    # - existing element was established for the same node parties or a superset of the node parties\n    #   (order can differ, as nodes are ordered by the cipher code)\n    #\n    # eg: [ 'un', 'deux', 'trois' ] parties compatible with [ 'un', 'trois', 'deux' ] context\n    # but not with [ 'deux', 'un', 'trois' ]\n    # eg: [ 'un', 'deux', 'trois' ] parties compatible with [ 'un', 'trois', 'quatre', 'deux' ] context\n    # but not with [ 'un', 'deux', 'quatre' ]\n    return (\n        # Commented tests can be assumed from calling functions\n        #\n        # isinstance(context, dict) and\n        # 'parties' in context and\n        # isinstance(context['parties'], list) and\n        # len(context['parties']) &gt;= 1 and\n        # isinstance(parties, list) and\n        set(parties[1:]).issubset(set(context['parties'][1:])))\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.matching_parties_servkey","title":"matching_parties_servkey","text":"<pre><code>matching_parties_servkey(context, parties)\n</code></pre> <p>Check if parties of given context are compatible with the parties of a secagg servkey element.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>dict</code> <p>context to be compared with the secagg servkey element parties</p> required <code>parties</code> <code>list</code> <p>the secagg servkey element parties</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if this context can be used with this element, False if not.</p> Source code in <code>fedbiomed/common/utils/_secagg_utils.py</code> <pre><code>def matching_parties_servkey(context: dict, parties: list) -&gt; bool:\n    \"\"\"Check if parties of given context are compatible with the parties\n        of a secagg servkey element.\n\n        Args:\n            context: context to be compared with the secagg servkey element parties\n            parties: the secagg servkey element parties\n\n        Returns:\n            True if this context can be used with this element, False if not.\n        \"\"\"\n    # Need to ensure that:\n    # - existing element was established for the same parties\n    # - first party needs to be the same for both\n    # - set of other parties needs to be the same for both (order can differ)\n    #\n    # eg: [ 'un', 'deux', 'trois' ] compatible with [ 'un', 'trois', 'deux' ]\n    # but not with [ 'deux', 'un', 'trois' ]\n    return (\n        # Commented tests can be assumed from calling functions\n        #\n        # isinstance(context, dict) and\n        # 'parties' in context and\n        # isinstance(context['parties'], list) and\n        # len(context['parties']) &gt;= 1 and\n        # isinstance(parties, list) and\n        parties[0] == context['parties'][0] and\n        set(parties[1:]) == set(context['parties'][1:]))\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.multiply","title":"multiply","text":"<pre><code>multiply(xs, k)\n</code></pre> <p>Multiplies a list of integers by a constant</p> <p>Parameters:</p> Name Type Description Default <code>xs</code> <code>List[int]</code> <p>List of integers</p> required <code>k</code> <code>int</code> <p>Constant to multiply by</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List of multiplied integers</p> Source code in <code>fedbiomed/common/utils/_secagg_utils.py</code> <pre><code>def multiply(xs: List[int], k: int) -&gt; List[int]:\n    \"\"\"\n    Multiplies a list of integers by a constant\n\n    Args:\n        xs: List of integers\n        k: Constant to multiply by\n\n    Returns:\n        List of multiplied integers\n    \"\"\"\n    # Quicker than converting to/from numpy\n    return [e * k for e in xs]\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.quantize","title":"quantize","text":"<pre><code>quantize(weights, clipping_range=None, target_range=SAParameters.TARGET_RANGE)\n</code></pre> <p>Quantization step implemented by: https://dl.acm.org/doi/pdf/10.1145/3488659.3493776</p> <p>This function returns a vector in the range [0, target_range-1].</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>List[float]</code> <p>List of model weight values</p> required <code>clipping_range</code> <code>Union[int, None]</code> <p>Clipping range</p> <code>None</code> <code>target_range</code> <code>int</code> <p>Target range</p> <code>TARGET_RANGE</code> <p>Returns:</p> Type Description <code>List[int]</code> <p>Quantized model weights as numpy array.</p> Source code in <code>fedbiomed/common/utils/_secagg_utils.py</code> <pre><code>def quantize(\n    weights: List[float],\n    clipping_range: Union[int, None] = None,\n    target_range: int = SAParameters.TARGET_RANGE,\n) -&gt; List[int]:\n    \"\"\"Quantization step implemented by: https://dl.acm.org/doi/pdf/10.1145/3488659.3493776\n\n    This function returns a vector in the range [0, target_range-1].\n\n    Args:\n        weights: List of model weight values\n        clipping_range: Clipping range\n        target_range: Target range\n\n    Returns:\n        Quantized model weights as numpy array.\n\n    \"\"\"\n    if clipping_range is None:\n        clipping_range = SAParameters.CLIPPING_RANGE\n\n    _check_clipping_range(weights, clipping_range)\n\n    # CAVEAT: ensure to be converting from `float` to `uint64` (no intermediate `int64`)\n    # Process ensures to compute an `int`` in the range [0, target_range -1]\n    # This enables to use at most 2**64 as target_range (max value of `uint` - 1)\n    f = np.vectorize(\n        lambda x: min(\n            target_range - 1,\n            (sorted((-clipping_range, x, clipping_range))[1] +\n            clipping_range) *\n            target_range / (2 * clipping_range),\n        ),\n        otypes=[np.uint64]\n    )\n    quantized_list = f(weights)\n\n    return quantized_list.tolist()\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.raise_for_version_compatibility","title":"raise_for_version_compatibility","text":"<pre><code>raise_for_version_compatibility(their_version, our_version, error_msg=None)\n</code></pre> <p>Check version compatibility and behave accordingly.</p> <p>Raises an exception if the versions are incompatible, otherwise outputs a warning or info message.</p> <p>Parameters:</p> Name Type Description Default <code>their_version</code> <code>Union[FBM_Component_Version, str]</code> <p>the version that we detected in the component</p> required <code>our_version</code> <code>Union[FBM_Component_Version, str]</code> <p>the version of the component within the current runtime</p> required <code>error_msg</code> <code>Optional[str]</code> <p>an optional error message. It may contain two %s placeholders which will be substituted with the values of their_version and our_version.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedVersionError</code> <p>if the versions are incompatible</p> Source code in <code>fedbiomed/common/utils/_versions.py</code> <pre><code>def raise_for_version_compatibility(their_version: Union[FBM_Component_Version, str],\n                                    our_version: Union[FBM_Component_Version, str],\n                                    error_msg: Optional[str] = None) -&gt; None:\n    \"\"\"Check version compatibility and behave accordingly.\n\n    Raises an exception if the versions are incompatible, otherwise outputs a warning or info message.\n\n    Args:\n        their_version: the version that we detected in the component\n        our_version: the version of the component within the current runtime\n        error_msg: an optional error message. It may contain two %s placeholders which will be substituted with\n            the values of their_version and our_version.\n\n    Raises:\n        FedbiomedVersionError: if the versions are incompatible\n    \"\"\"\n    if isinstance(our_version, str):\n        our_version = FBM_Component_Version(our_version)\n    if isinstance(their_version, str):\n        their_version = FBM_Component_Version(their_version)\n    if not isinstance(our_version, FBM_Component_Version):\n        msg = f\"{ErrorNumbers.FB625.value}: Component version has incorrect type `our_version` type={str(type(our_version))} value={our_version}\"\n        logger.critical(msg)\n        raise FedbiomedVersionError(msg)\n    if not isinstance(their_version, FBM_Component_Version):\n        msg = f\"{ErrorNumbers.FB625.value}: Component version has incorrect type `their_version` type={str(type(their_version))} value={their_version}\"\n        logger.critical(msg)\n        raise FedbiomedVersionError(msg)\n    if our_version != their_version:\n        # note: the checks below rely on the short-circuiting behaviour of the or operator\n        # (e.g. when checking our_version.minor &lt; their_version.minor we have the guarantee that\n        # our_version.major == their_version.major\n        if our_version.major != their_version.major or \\\n                our_version.minor &lt; their_version.minor or \\\n                (our_version.minor == their_version.minor and our_version.micro &lt; their_version.micro):\n            msg = _create_msg_for_version_check(\n                f\"{ErrorNumbers.FB625.value}: Found incompatible version %s, expected version %s\" if error_msg is None else error_msg,\n                their_version,\n                our_version\n            )\n            logger.critical(msg)\n            raise FedbiomedVersionError(msg)\n        else:\n            msg = _create_msg_for_version_check(\n                \"Found version %s, expected version %s\",\n                their_version,\n                our_version\n            )\n            logger.warning(msg)\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.read_file","title":"read_file","text":"<pre><code>read_file(path)\n</code></pre> <p>Read given file</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Path to file to be read</p> required <p>Raises:</p> Type Description <code>FedbiomedError</code> <p>If the file is not existing or readable.</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def read_file(path):\n    \"\"\"Read given file\n\n    Args:\n        path: Path to file to be read\n\n    Raises:\n        FedbiomedError: If the file is not existing or readable.\n    \"\"\"\n    try:\n        with open(path, \"r\", encoding=\"UTF-8\") as file:\n            content = file.read()\n            file.close()\n    except Exception as e:\n        raise FedbiomedError(\n            f\"{ErrorNumbers.FB627.value}: Can not read file {path}. Error: {e}\"\n        ) from e\n\n    return content\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.reverse_quantize","title":"reverse_quantize","text":"<pre><code>reverse_quantize(weights, clipping_range=None, target_range=SAParameters.TARGET_RANGE)\n</code></pre> <p>Reverse quantization step implemented by: https://dl.acm.org/doi/pdf/10.1145/3488659.3493776</p> <p>Args:     weights: List of quantized model weights     clipping_range: Clipping range used for quantization     target_range: Target range used for quantization</p> <p>Returns:</p> Type Description <code>List[float]</code> <p>Reversed quantized model weights as numpy array.</p> Source code in <code>fedbiomed/common/utils/_secagg_utils.py</code> <pre><code>def reverse_quantize(\n    weights: List[int],\n    clipping_range: Union[int, None] = None,\n    target_range: int = SAParameters.TARGET_RANGE,\n) -&gt; List[float]:\n    \"\"\"Reverse quantization step implemented by: https://dl.acm.org/doi/pdf/10.1145/3488659.3493776\n\n     Args:\n        weights: List of quantized model weights\n        clipping_range: Clipping range used for quantization\n        target_range: Target range used for quantization\n\n    Returns:\n        Reversed quantized model weights as numpy array.\n    \"\"\"\n\n    if clipping_range is None:\n        clipping_range = SAParameters.CLIPPING_RANGE\n\n    # CAVEAT: there should not be any weight received that does not fit in `uint64`\n    max_val = np.iinfo(np.uint64).max\n    if any([v &gt; max_val or v &lt; 0 for v in weights]):\n        raise FedbiomedSecaggCrypterError(\n            f\"{ErrorNumbers.FB624.value}: Cannot reverse quantize, received values exceed maximum number\"\n        )\n\n    max_range = clipping_range\n    min_range = -clipping_range\n    step_size = (max_range - min_range) / (target_range - 1)\n    # Compute as input type (`np.uint64` then convert to `np.float64`)\n    f = np.vectorize(\n        lambda x: (min_range + step_size * x),\n        otypes=[np.float64]\n    )\n\n    # TODO: we could check that received values are in the range\n    weights = np.array(weights, dtype=np.uint64)\n    reverse_quantized_list = f(weights)\n\n    return reverse_quantized_list.tolist()\n</code></pre>"},{"location":"developer/api/common/validator/","title":"Validator","text":"<p>Provide Validator ans SchemeValidator classes for validating parameters against a set of validation rules.</p> <p>This module provides two \"validation\" classes and two Error classes (exceptions):</p> <p>Validator:</p> <p>This class manage a rulebook of rules which can afterwards be accessed by their (registered) name.</p> <p>Values can be checked against the rules.</p> <p>Typical example:</p> <pre><code>  def my_validation_funct( value ):\n      if some_python_code:\n          return False\n      else:\n          return True\n\n  v = Validator()\n  v.register( \"funky_name\", my_validation_funct)\n  v.register( \"float_type\", float)\n\n  val = 3.14\n  v.validate( val, \"funky_name\")\n  v.validate( val, \"float_type\")\n  v.validate( val, float)\n\n  v.validate( \"{ 'a': 1 }\", dict)\n  ...\n</code></pre> <p>SchemeValidator:</p> <p>This class provides json validation against a scheme describing the expected json content.</p> <p>The scheme needs to follow a specific format, which describes each allowed fields and their characteristics: - a list of associated validators to check against (aka Validator instances) - the field requirement (required on not) - a default value (which will be used if the field is required but not provided)</p> <p>A SchemeValidator is accepted by the Validator class.</p> <p>Typical example:</p> <pre><code>  # direct use\n  scheme = { \"a\" : { \"rules\" : [float], \"required\": True } }\n\n  sc = SchemeValidator(scheme)\n\n  value =  { \"a\": 3.14 }\n  sc.validate(value)\n\n\n  # use also the Validator class\n  v = Validator()\n\n  v.register( \"message_a\", sc )\n  v.validate( value, \"message_a\" )\n\n  # remark: all these lines are equivalent\n  v.register( \"message_a\", sc )\n  v.register( \"message_a\", SchemeValidator( scheme) )\n  v.register( \"message_a\", scheme )\n</code></pre> <p>RuleError:</p> <p>This error is raised then the provided value is badly specified.</p> <p>ValidateError:</p> <p>This error is raised then a value does not comply to defined rule(s)</p>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator-classes","title":"Classes","text":""},{"location":"developer/api/common/validator/#fedbiomed.common.validator.RuleError","title":"RuleError","text":"<p>               Bases: <code>ValidatorError</code></p> <p>Error raised then the rule is badly defined.</p>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.SchemeValidator","title":"SchemeValidator","text":"<pre><code>SchemeValidator(scheme)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Validation class for scheme (grammar) which describes a json content.</p> <p>this class uses Validator's class base functions</p> <p>it requires a json grammar as argument and validate it's again the requested json description scheme</p> <p>A valid json description is also a dictionary with the following grammar:</p> <pre><code>{\n  \"var_name\": {\n                \"rules\": [ validator1, validator2, ...] ,\n                \"default\": a_default_value,\n                \"required\": True/False\n              },\n  ...\n}\n</code></pre> <p>the \"rules\" field is mandatory \"default\" and \"required\" fields are optional.</p> <p>Example:</p> <p>This is a valid scheme: <pre><code>{ \"a\" : { \"rules\" : [float], \"required\": True } }\n</code></pre></p> <p>The following json complies to this scheme: <pre><code>{ \"a\": 3.14 }\n</code></pre></p> <p>The following does not: <pre><code>{ \"a\": True }\n{ \"b\": 3.14 }\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>scheme</code> <code>Dict[str, Dict]</code> <p>scheme to validate</p> required <p>Raises:</p> Type Description <code>RuleError</code> <p>if provided scheme is invalid</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def __init__(self, scheme: Dict[str, Dict]):\n    \"\"\"\n    Create a SchemeValidator instance, and validate the scheme passed as input.\n\n    it requires a json grammar as argument and validate\n    it's again the requested json description scheme\n\n    A valid json description is also a dictionary\n    with the following grammar:\n\n    ```python\n    {\n      \"var_name\": {\n                    \"rules\": [ validator1, validator2, ...] ,\n                    \"default\": a_default_value,\n                    \"required\": True/False\n                  },\n      ...\n    }\n    ```\n\n    the \"rules\" field is mandatory\n    \"default\" and \"required\" fields are optional.\n\n    Example:\n\n    This is a valid scheme:\n    ```python\n    { \"a\" : { \"rules\" : [float], \"required\": True } }\n    ```\n\n    The following json complies to this scheme:\n    ```python\n    { \"a\": 3.14 }\n    ```\n\n    The following does not:\n    ```python\n    { \"a\": True }\n    { \"b\": 3.14 }\n    ```\n\n    Args:\n        scheme:     scheme to validate\n\n    Raises:\n        RuleError: if provided scheme is invalid\n    \"\"\"\n    status = self.__validate_scheme(scheme)\n\n    if isinstance(status, bool) and status:\n        self._scheme = scheme\n        self._is_valid = True\n\n    else:\n        self._scheme = None   # type: ignore\n        self._is_valid = False\n        raise RuleError(f\"scheme is not valid: {status}\")\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.SchemeValidator-functions","title":"Functions","text":""},{"location":"developer/api/common/validator/#fedbiomed.common.validator.SchemeValidator.is_valid","title":"is_valid","text":"<pre><code>is_valid()\n</code></pre> <p>Status of the scheme passed at creation time.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if scheme is valid, False instead</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def is_valid(self) -&gt; bool:\n    \"\"\"\n    Status of the scheme passed at creation time.\n\n    Returns:\n        True if scheme is valid, False instead\n    \"\"\"\n    return (self._scheme is not None) or self._is_valid\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.SchemeValidator.populate_with_defaults","title":"populate_with_defaults","text":"<pre><code>populate_with_defaults(value, only_required=True)\n</code></pre> <p>Inject default values defined in the rule to a given dictionary.</p> <p>Parse the given json value and add default value if key was required but not provided. Of course, the default values must be provided in the scheme.</p> <p>Warning: this does not parse the result against the scheme. It has to be done by the user.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>dict</code> <p>a json data to verify/populate</p> required <code>only_required</code> <code>bool</code> <p>if True, only force required key. If False, update all            keys with default values in the scheme. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict</code> <p>(dict) a json populated with default values, returns an empty dict if something is wrong</p> <p>Raises:</p> Type Description <code>RuleError</code> <p>if scheme provided at init contains a required rules without default value</p> <code>ValidatorError</code> <p>if input value was not a dict</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def populate_with_defaults(self, value: Dict, only_required: bool = True) -&gt; Dict:\n    \"\"\"\n    Inject default values defined in the rule to a given dictionary.\n\n    Parse the given json value and add default value if key was required\n    but not provided.\n    Of course, the default values must be provided in the scheme.\n\n    Warning: this does not parse the result against the scheme. It has\n    to be done by the user.\n\n    Args:\n        value (dict):   a json data to verify/populate\n        only_required (bool): if True, only force required key. If False, update all\n                       keys with default values in the scheme. Defaults to True.\n\n    Returns:\n        (dict) a json populated with default values, returns an empty dict if something is wrong\n\n    Raises:\n        RuleError: if scheme provided at init contains a required rules without default value\n        ValidatorError: if input value was not a dict\n    \"\"\"\n    if not self.is_valid():  # pragma: no cover\n        return {}\n\n    # check the value against the scheme\n    if isinstance(value, dict):\n        result = deepcopy(value)\n    else:\n        raise ValidatorError(\"input value is not a dict\")\n\n\n    for k, v in self._scheme.items():\n        if 'required' in v and v['required'] is True:\n\n            if k in value:\n                result[k] = value[k]\n            else:\n                if 'default' in v:\n                    result[k] = v['default']\n                else:\n                    raise RuleError(f\"scheme does not define a default value for required key: {k}\")\n\n        else:\n            if not only_required:\n                if k in value:\n                    result[k] = value[k]\n                else:\n                    if 'default' in v:\n                        result[k] = v['default']\n\n\n    return result\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.SchemeValidator.scheme","title":"scheme","text":"<pre><code>scheme()\n</code></pre> <p>Scheme getter.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict]</code> <p>scheme passed at init if valid, None instead</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def scheme(self) -&gt; Dict[str, Dict]:\n    \"\"\"\n    Scheme getter.\n\n    Returns:\n        scheme passed at __init__ if valid, None instead\n    \"\"\"\n    return self._scheme\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.SchemeValidator.validate","title":"validate","text":"<pre><code>validate(value)\n</code></pre> <p>Validate a value against the scheme passed at creation time.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>dict</code> <p>value (dict) to validate against the scheme passed    at init</p> required <p>Returns:     True if value is valid</p> <p>Raises:</p> Type Description <code>ValidateError</code> <p>if provided value is invalid</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def validate(self, value: Dict) -&gt; bool:\n    \"\"\"\n    Validate a value against the scheme passed at creation time.\n\n    Args:\n         value (dict):  value (dict) to validate against the scheme passed\n                 at __init__\n    Returns:\n        True if value is valid\n\n    Raises:\n        ValidateError: if provided value is invalid\n    \"\"\"\n    # TODO: raises error messages\n    # or store error string in self._error and provide a error() method\n    if not self.is_valid():  # pragma: no cover\n        return False\n\n    if not isinstance(value, dict):\n        raise ValidateError(\"value is not a dict\")\n\n    # check the value against the scheme\n    for k, v in self._scheme.items():\n        if 'required' in v and v['required'] is True and k not in value:\n            raise ValidateError(f\"{k} key is required\")\n\n    for k in value:\n        if k not in self._scheme:\n            raise ValidateError(f\"undefined key ({k}) in scheme\")\n\n        for hook in self._scheme[k]['rules']:\n            if not Validator().validate(value[k], hook):\n                # this should already have raised an error\n                raise ValidateError(f\"invalid value ({value[k]}) for key: {k}\")  # pragma: nocover\n\n    return True\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.ValidateError","title":"ValidateError","text":"<p>               Bases: <code>ValidatorError</code></p> <p>Error raised then validating a value against a rule.</p>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.Validator","title":"Validator","text":"<pre><code>Validator()\n</code></pre> <p>               Bases: <code>object</code></p> <p>Container class for validation functions accessible via their names.</p> <p>this class: - manages a catalog of tuples  ( \"name\", validation_hook )   The validation_hook is validated at registration phase. - permit to validate a value against     - a (named) registered hook     - a direct validation hook passed as argument to validate()     - a SchemeValidator for json validation     - typechecking</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Create an instance of Validator. For now, nothing to do.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.Validator-functions","title":"Functions","text":""},{"location":"developer/api/common/validator/#fedbiomed.common.validator.Validator.delete","title":"delete","text":"<pre><code>delete(rule)\n</code></pre> <p>Delete a rule from the rulebook.</p> <p>Parameters:</p> Name Type Description Default <code>rule</code> <code>str</code> <p>name (string) of a possibly registered hook</p> required Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def delete(self, rule: str) -&gt; None:\n    \"\"\"\n    Delete a rule from the rulebook.\n\n    Args:\n        rule:   name (string) of a possibly registered hook\n    \"\"\"\n    if rule in self._validation_rulebook:\n        del self._validation_rulebook[rule]\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.Validator.is_known_rule","title":"is_known_rule","text":"<pre><code>is_known_rule(rule)\n</code></pre> <p>Information about rule registration.</p> <p>Parameters:</p> Name Type Description Default <code>rule</code> <code>str</code> <p>name <code>str</code> of a possibly registered hook</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if rule is registered, False instead</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def is_known_rule(self, rule: str) -&gt; bool:\n    \"\"\"\n    Information about rule registration.\n\n    Args:\n        rule:   name [`str`][str] of a possibly registered hook\n\n    Returns:\n        True if rule is registered, False instead\n    \"\"\"\n    return rule in self._validation_rulebook\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.Validator.register","title":"register","text":"<pre><code>register(rule, hook, override=False)\n</code></pre> <p>Add a rule/validation_function to the rulebook.</p> <p>if the rule (entry of the catalog) was already registered, it will be rejected, except if override is True</p> <p>Parameters:</p> Name Type Description Default <code>rule</code> <code>str</code> <p>registration name (string)</p> required <code>hook</code> <code>Any</code> <p>validation hook to register (the hook is checked against        the accepted hook types)</p> required <code>override</code> <code>bool</code> <p>if True, still register the rule even if it existed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if rule is accepted, False instead if rule exists and overrride is False</p> <p>Raises:</p> Type Description <code>RuleError</code> <p>if provided rule name or hook is invalid</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def register(self, rule: str, hook: Any, override: bool = False) -&gt; bool:\n    \"\"\"\n    Add a rule/validation_function to the rulebook.\n\n    if the rule (entry of the catalog) was already registered,\n    it will be rejected, except if override is True\n\n    Args:\n        rule:      registration name (string)\n        hook:      validation hook to register (the hook is checked against\n                   the accepted hook types)\n        override:  if True, still register the rule even if it existed. Defaults to False.\n\n    Returns:\n        True if rule is accepted, False instead if rule exists and overrride is False\n\n    Raises:\n        RuleError: if provided rule name or hook is invalid\n\n    \"\"\"\n    if not isinstance(rule, str):\n        raise RuleError(\"rule name must be a string\")\n\n    if not override and rule in self._validation_rulebook:\n        sys.stdout.write(f\"WARNING - Validator: rule is already registered: {rule} \\n\")\n        return False\n\n    if not Validator._is_hook_type_valid(hook):\n        raise RuleError(\"action associated to the rule is not allowed\")\n\n    # hook is a dict, we transform it to a SchemeValidator\n    if isinstance(hook, dict):\n        try:\n            sv = SchemeValidator(hook)\n        except RuleError as e:\n            raise RuleError(f\"validator is an invalid dict: {e}\")\n        hook = sv\n\n    # rule description is valid -&gt; register it\n    self._validation_rulebook[rule] = hook\n    return True\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.Validator.rule","title":"rule","text":"<pre><code>rule(rule)\n</code></pre> <p>Return a presumably stored rule.</p> <p>Parameters:</p> Name Type Description Default <code>rule</code> <code>str</code> <p>name (<code>str</code>) of a possibly registered hook</p> required <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>the hook associated to the rule name if registered, None if not registered</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def rule(self, rule: str) -&gt; Union[str, None]:\n    \"\"\"\n    Return a presumably stored rule.\n\n    Args:\n        rule:   name ([`str`][str]) of a possibly registered hook\n\n    Returns:\n        the hook associated to the rule name if registered, None if not registered\n    \"\"\"\n    if rule in self._validation_rulebook:\n        return self._validation_rulebook[rule]\n    else:\n        return None\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.Validator.validate","title":"validate","text":"<pre><code>validate(value, rule, strict=True)\n</code></pre> <p>Validate a value against a validation rule.</p> <p>The rule may be one of: - (registered) rule - a provided function, - a simple type checking - a SchemeValidator</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>value to check</p> required <code>rule</code> <code>Any</code> <p>validation hook (registered name, typecheck, direct hook, ...)</p> required <code>strict</code> <code>bool</code> <p>Raises error if rule is not defined. Otherwise, prints a warning message.</p> <code>True</code> <p>Returns:     True if rule exists and value is compliant.</p> <p>Raises:</p> Type Description <code>ValidateError</code> <p>if provided value does not comply to the rule</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def validate(self, value: Any, rule: Any, strict: bool = True) -&gt; bool:\n    \"\"\"\n    Validate a value against a validation rule.\n\n    The rule may be one of:\n    - (registered) rule\n    - a provided function,\n    - a simple type checking\n    - a SchemeValidator\n\n    Args:\n        value:   value to check\n        rule:    validation hook (registered name, typecheck, direct hook, ...)\n        strict:  Raises error if rule is not defined. Otherwise, prints a warning message.\n    Returns:\n        True if rule exists and value is compliant.\n\n    Raises:\n        ValidateError: if provided value does not comply to the rule\n    \"\"\"\n    # rule is in the rulebook -&gt; execute the rule associated function\n    if isinstance(rule, str) and rule in self._validation_rulebook:\n\n        status, error = Validator._hook_execute(value,\n                                                self._validation_rulebook[rule])\n        if not status:\n            raise ValidateError(error)\n\n        return status\n\n    # rule is an unknown string\n    if isinstance(rule, str):\n        if strict:\n            raise ValidateError(f\"unknown rule: {rule}\")\n        else:\n            sys.stdout.write(f\"WARNING - Validator(): unknown rule: {rule} \\n\")\n            return True\n\n    # consider the rule as a direct rule definition\n    status, error = Validator._hook_execute(value, rule)\n\n    if not status:\n        raise ValidateError(error)\n\n    return status\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.ValidatorError","title":"ValidatorError","text":"<p>               Bases: <code>Exception</code></p> <p>Top class of all Validator/SchemaValidator exception.</p>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator-functions","title":"Functions","text":""},{"location":"developer/api/common/validator/#fedbiomed.common.validator.validator_decorator","title":"validator_decorator","text":"<pre><code>validator_decorator(func)\n</code></pre> <p>Ease the writing of validation function/hooks.</p> <p>The decorator catches the output of the validator hook and replace it with a tuple(<code>bool</code>, <code>str</code>) as expected by the Validator class.</p> <p>It creates an error message if not provided by the decorated function. The error message is forced to if the decorated function returns True</p> <p>If the validator is not used to decorate a validation function/hook, then the user feedback will be less precise for the end-user but this will not change the accuracy (True/False) of the feedback.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>function to decorate</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>decorated function</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def validator_decorator(func: Callable) -&gt; Callable:\n    \"\"\"\n    Ease the writing of validation function/hooks.\n\n    The decorator catches the output of the validator hook and replace\n    it with a tuple([`bool`][bool], [`str`][str]) as expected by\n    the Validator class.\n\n    It creates an error message if not provided by the decorated function.\n    The error message is forced to if the decorated function returns True\n\n    If the validator is not used to decorate a validation function/hook,\n    then the user feedback will be less precise for the end-user but this\n    will not change the accuracy (True/False) of the feedback.\n\n    Args:\n       func:  function to decorate\n\n    Returns:\n       decorated function\n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n\n        # execute the wrapped function\n        status = func(*args, **kwargs)\n\n        # we expect a tuple [boolean, str] as output of func()\n        # but we try to be resilient to function that simply return boolean\n        # and create the tuple in case that it is not provided\n        error = f\"validation error then calling: {func.__name__}\"\n        if isinstance(status, tuple):\n            status, *error = status\n\n        if status:\n            return status, None\n        else:\n            error = ''.join(error)\n            return status, error\n\n    return wrapper\n</code></pre>"},{"location":"developer/api/node/cli/","title":"CLI","text":"<p>Command line user interface for the node component</p>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli-attributes","title":"Attributes","text":""},{"location":"developer/api/node/cli/#fedbiomed.node.cli.cli","title":"cli  <code>module-attribute</code>","text":"<pre><code>cli = NodeCLI()\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli-classes","title":"Classes","text":""},{"location":"developer/api/node/cli/#fedbiomed.node.cli.DatasetArgumentParser","title":"DatasetArgumentParser","text":"<pre><code>DatasetArgumentParser(subparser, parser=None)\n</code></pre> <p>               Bases: <code>CLIArgumentParser</code></p> <p>Initializes CLI options for dataset actions</p> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def __init__(self, subparser: argparse.ArgumentParser, parser = None):\n\n    self._subparser = subparser\n    # Parser that is going to be add using subparser\n    self._parser = None\n\n    self._main_parser = parser\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.DatasetArgumentParser-functions","title":"Functions","text":""},{"location":"developer/api/node/cli/#fedbiomed.node.cli.DatasetArgumentParser.add","title":"add","text":"<pre><code>add(args)\n</code></pre> <p>Adds datasets</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def add(self, args):\n    \"\"\"Adds datasets\"\"\"\n\n    if args.mnist != \"\":\n        if args.mnist is None:\n            mnist_path = os.path.join(self._node.config.root, NODE_DATA_FOLDER)\n        else:\n            mnist_path = args.mnist\n        return add_database(\n            self._node.dataset_manager,\n            interactive=False,\n            path=mnist_path\n        )\n\n    if args.file:\n        return self._add_dataset_from_file(path=args.file)\n\n    # All operation is handled by CLI utils add_database\n    return add_database(self._node.dataset_manager)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.DatasetArgumentParser.delete","title":"delete","text":"<pre><code>delete(args)\n</code></pre> <p>Deletes datasets</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def delete(self, args):\n    \"\"\"Deletes datasets\"\"\"\n\n    if args.all:\n        return delete_all_database(self._node.dataset_manager)\n\n    if args.mnist:\n        return delete_database(self._node.dataset_manager, interactive=False)\n\n    return delete_database(self._node.dataset_manager)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.DatasetArgumentParser.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initializes dataset options for the node CLI</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def initialize(self):\n    \"\"\"Initializes dataset options for the node CLI\"\"\"\n\n    self._parser = self._subparser.add_parser(\n        \"dataset\",\n        help=\"Dataset operations\"\n    )\n    self._parser.set_defaults(func=self.default)\n\n\n    # Creates subparser of dataset option\n    dataset_subparsers = self._parser.add_subparsers()\n\n\n    # Add option\n    add = dataset_subparsers.add_parser(\n        \"add\",\n        help=\"Adds dataset\"\n    )\n    add.set_defaults(func=self.add)\n\n    # List option\n    list_ = dataset_subparsers.add_parser(\n        \"list\",\n        help=\"List datasets that are deployed in the node.\")\n\n    # Delete option\n    delete = dataset_subparsers.add_parser(\n        \"delete\",\n        help=\"Deletes dataset that are deployed in the node.\")\n\n\n    add.add_argument(\n        \"--mnist\",\n        \"-m\",\n        metavar=\"MNIST_DATA_PATH\",\n        help=\"Deploys MNIST dataset by downloading form default source to given path.\",\n        nargs='?',\n        type=str,\n        required=False,\n        default=\"\"\n    )\n    self._mnist_path = add\n\n    add.add_argument(\n        \"--file\",\n        \"-fl\",\n        required=False,\n        metavar=\"File that describes the dataset\",\n        help=\"File path the dataset file description. This option adds dataset by given file which has\"\n             \"custom format that describes the dataset.\")\n\n    delete.add_argument(\n        \"--all\",\n        '-a',\n        required=False,\n        action=\"store_true\",\n        help=\"Removes entire dataset database.\")\n\n    delete.add_argument(\n        \"--mnist\",\n        '-m',\n        required=False,\n        action=\"store_true\",\n        help=\"Removes MNIST dataset.\")\n\n\n    list_.set_defaults(func=self.list)\n    delete.set_defaults(func=self.delete)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.DatasetArgumentParser.list","title":"list","text":"<pre><code>list(unused_args)\n</code></pre> <p>List datasets</p> <p>Parameters:</p> Name Type Description Default <code>unused_args</code> <p>Empty arguments since <code>list</code> command no positional args.</p> required Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def list(self, unused_args):\n    \"\"\"List datasets\n\n    Args:\n      unused_args: Empty arguments since `list` command no positional args.\n    \"\"\"\n    print('Listing your data available')\n    data = self._node.dataset_manager.list_my_data(verbose=True)\n    if len(data) == 0:\n        print('No data has been set up.')\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.GUIControl","title":"GUIControl","text":"<pre><code>GUIControl(subparser, parser=None)\n</code></pre> <p>               Bases: <code>CLIArgumentParser</code></p> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def __init__(self, subparser: argparse.ArgumentParser, parser = None):\n\n    self._subparser = subparser\n    # Parser that is going to be add using subparser\n    self._parser = None\n\n    self._main_parser = parser\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.GUIControl-functions","title":"Functions","text":""},{"location":"developer/api/node/cli/#fedbiomed.node.cli.GUIControl.forward","title":"forward","text":"<pre><code>forward(args, extra_args)\n</code></pre> <p>Launches Fed-BioMed Node GUI</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>parser argument's namespace</p> required Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def forward(self, args: argparse.Namespace, extra_args):\n    \"\"\"Launches Fed-BioMed Node GUI\n\n    Args:\n        args: parser argument's namespace\n    \"\"\"\n\n    fedbiomed_root = os.path.abspath(args.path)\n\n    if args.data_folder == '':\n        data_folder = os.path.join(self._node.config.root, NODE_DATA_FOLDER)\n    else:\n        data_folder = os.path.abspath(args.data_folder)\n    if not os.path.isdir(data_folder):\n        raise FedbiomedError(f\"path {data_folder} is not a folder. Aborting\")\n    os.environ.update({\n        \"DATA_PATH\": data_folder,\n        \"FBM_NODE_COMPONENT_ROOT\": fedbiomed_root,\n    })\n    current_env = os.environ.copy()\n\n    if args.key_file and args.cert_file:\n        certificate = [\"--keyfile\", args.key_file, \"--certfile\", args.cert_file ]\n    else:\n        certificate = []\n\n\n    fedbiomed_gui = importlib.import_module(\"fedbiomed_gui\")\n    server_app = Path(fedbiomed_gui.__file__).parent\n    print(\"path to server\", server_app)\n\n    host_port = [\"--host\", args.host, \"--port\", args.port]\n    if args.development:\n        command = [\n            \"FLASK_ENV=development\",\n            f\"FLASK_APP=\"\n            f\"{os.path.join(server_app, 'server', 'wsgi.py')}\",\n            \"flask\",\n            \"run\",\n            *host_port,\n            *certificate\n        ]\n    else:\n        command = [\n            \"gunicorn\",\n            \"--workers\",\n            \"1\",\n            # str(os.cpu_count()),\n            *certificate,\n            \"-b\",\n            f\"{args.host}:{args.port}\",\n            \"--access-logfile\",\n            \"-\",\n            \"fedbiomed_gui.server.wsgi:app\"\n        ]\n\n    try:\n        with subprocess.Popen(\" \".join(command), env=current_env, shell=True) as proc:\n            proc.wait()\n    except Exception as e:\n        print(e)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.GUIControl.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initializes GUI commands</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def initialize(self):\n    \"\"\"Initializes GUI commands\"\"\"\n    self._parser = self._subparser.add_parser(\n        \"gui\", #add_help=False,\n         help=\"Action to manage Node user interface\"\n    )\n\n    gui_subparsers = self._parser.add_subparsers(title='start GUI')\n    start = gui_subparsers.add_parser(\n        'start',\n        help='Launch the server (defaults on localhost:8484)')\n\n    start.set_defaults(func=self.forward)\n\n\n    start.add_argument(\n        \"--data-folder\",\n        \"-df\",\n        type=str,\n        nargs=\"?\",\n        default=\"\",  # data folder in root directory\n        required=False)\n\n    start.add_argument(\n        \"--cert-file\",\n        \"-cf\",\n        type=str,\n        nargs=\"?\",\n        required=False,\n        help=\"Name of the certificate to use in order to enable HTTPS. \"\n             \"If cert file doesn't exist script will raise an error.\")\n\n    start.add_argument(\n        \"--key-file\",\n        \"-kf\",\n        type=str,\n        nargs=\"?\",\n        required=False,\n        help=\"Name of the private key for the SSL certificate. \"\n             \"If the key file doesn't exist, the script will raise an error.\")\n\n    start.add_argument(\n        \"--port\",\n        \"-p\",\n        type=str,\n        nargs=\"?\",\n        default=\"8484\",\n        required=False,\n        help=\"HTTP port that GUI will be served. Default is `8484`\")\n\n    start.add_argument(\n        \"--host\",\n        \"-ho\",\n        type=str,\n        default=\"localhost\",\n        nargs=\"?\",\n        required=False,\n        help=\"HTTP port that GUI will be served. Default is `127.0.0.1` (localhost)\")\n\n    start.add_argument(\n        \"--debug\",\n        \"-dbg\",\n        action=\"store_true\",\n        required=False,\n        help=\"HTTP port that GUI will be served. Default is `8484`\")\n\n    start.add_argument(\n        \"--recreate\",\n        \"-rc\",\n        action=\"store_true\",\n        required=False,\n        help=\"Re-creates gui build\")\n\n    start.add_argument(\n        \"--development\",\n        \"-dev\",\n        action=\"store_true\",\n        required=False,\n        help=\"If it is set, GUI will start in development mode.\"\n    )\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.NodeCLI","title":"NodeCLI","text":"<pre><code>NodeCLI()\n</code></pre> <p>               Bases: <code>CommonCLI</code></p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def __init__(self):\n    super().__init__()\n\n    self._parser.prog = \"fedbiomed node\"\n    self.description = f\"{__intro__} \\nA CLI app for fedbiomed node component.\"\n    # Parent parser for parameters that are common for Node CLI actions\n    self.initialize()\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.NodeCLI-attributes","title":"Attributes","text":""},{"location":"developer/api/node/cli/#fedbiomed.node.cli.NodeCLI.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description = f'{__intro__} \nA CLI app for fedbiomed node component.'\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.NodeCLI-functions","title":"Functions","text":""},{"location":"developer/api/node/cli/#fedbiomed.node.cli.NodeCLI.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initializes node module</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def initialize(self):\n    \"\"\"Initializes node module\"\"\"\n\n    class ComponentDirectoryActionNode(ComponentDirectoryAction):\n\n        _this = self\n        _component = ComponentType.NODE\n\n        def set_component(self, component_dir: str | None = None) -&gt; None:\n            \"\"\"Create node instance\"\"\"\n\n            if component_dir:\n                component_dir = os.path.abspath(component_dir)\n                os.environ[\"FBM_NODE_COMPONENT_ROOT\"] = component_dir\n            else:\n                print(\"Component is not specified: Using 'fbm-node' in current working directory...\")\n                component_dir =  os.path.join(os.getcwd(), 'fbm-node')\n                os.environ[\"FBM_NODE_COMPONENT_ROOT\"] = component_dir\n\n            config = node_component.initiate(component_dir)\n            self._this.config = config\n            node = Node(config)\n\n            # Set node object to make it accessible\n            setattr(ComponentDirectoryActionNode._this, '_node', node)\n            os.environ[f\"FEDBIOMED_ACTIVE_{self._component.name}_ID\"] = \\\n                config.get(\"default\", \"id\")\n\n            # Set node in all subparsers\n            for _, parser in ComponentDirectoryActionNode._this._arg_parsers.items():\n                setattr(parser, '_node', node)\n\n    super().initialize()\n\n    self._parser.add_argument(\n        \"--path\",\n        \"-p\",\n        nargs=\"?\",\n        action=ComponentDirectoryActionNode,\n        default=\"fbm-node\",\n        help=\"The path were component is located. It can be absolute or \"\n            \"realtive to the path where CLI is executed.\"\n    )\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.NodeControl","title":"NodeControl","text":"<pre><code>NodeControl(subparser, parser=None)\n</code></pre> <p>               Bases: <code>CLIArgumentParser</code></p> <p>CLI argument parser for starting the node</p> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def __init__(self, subparser: argparse.ArgumentParser, parser = None):\n\n    self._subparser = subparser\n    # Parser that is going to be add using subparser\n    self._parser = None\n\n    self._main_parser = parser\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.NodeControl-functions","title":"Functions","text":""},{"location":"developer/api/node/cli/#fedbiomed.node.cli.NodeControl.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initializes missinon control argument parser</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def initialize(self):\n    \"\"\"Initializes missinon control argument parser\"\"\"\n    start = self._subparser.add_parser(\"start\", help=\"Starts the node\")\n    start.set_defaults(func=self.start)\n\n    start.add_argument(\n        \"--gpu\",\n        action=\"store_true\",\n        help=\"Activate GPU usage if the flag is present\")\n\n    start.add_argument(\n        \"--gpu-num\",\n        \"-gn\",\n        type=int,\n        nargs=\"?\",\n        required=False,\n        default=1,\n        help=\"Number of GPU that is going to be used\")\n\n    start.add_argument(\n        \"--gpu-only\",\n        \"-go\",\n        action=\"store_true\",\n        help=\"Node performs training only using GPU resources.\"\n             \"This flag automatically activate GPU.\")\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.NodeControl.start","title":"start","text":"<pre><code>start(args)\n</code></pre> <p>Starts the node</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def start(self, args):\n    \"\"\"Starts the node\"\"\"\n    intro()\n\n    # Define arguments\n    node_args = {\n        \"gpu\": (args.gpu is True) or (args.gpu_only is True),\n        \"gpu_num\": args.gpu_num,\n        \"gpu_only\": True if args.gpu_only else False}\n\n    # Node instance has to be re-instantiated in start_node\n    # It is because Process can only pickle pure python objects\n    p = Process(\n        target=start_node,\n        name=f'node-{self._node.config.get(\"default\", \"id\")}',\n        args=(self._node.config, node_args)\n    )\n    p.deamon = True\n    p.start()\n\n    logger.info(\"Node started as process with pid = \" + str(p.pid))\n    try:\n        print('To stop press Ctrl + C.')\n        p.join()\n    except KeyboardInterrupt:\n        p.terminate()\n        time.sleep(1)\n        while p.is_alive():\n            logger.info(\"Terminating process id =\" + str(p.pid))\n            time.sleep(1)\n        logger.info('Exited with code ' + str(p.exitcode))\n        sys.exit(0)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.TrainingPlanArgumentParser","title":"TrainingPlanArgumentParser","text":"<pre><code>TrainingPlanArgumentParser(subparser, parser=None)\n</code></pre> <p>               Bases: <code>CLIArgumentParser</code></p> <p>Argument parser for training-plan operations</p> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def __init__(self, subparser: argparse.ArgumentParser, parser = None):\n\n    self._subparser = subparser\n    # Parser that is going to be add using subparser\n    self._parser = None\n\n    self._main_parser = parser\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.TrainingPlanArgumentParser-functions","title":"Functions","text":""},{"location":"developer/api/node/cli/#fedbiomed.node.cli.TrainingPlanArgumentParser.approve","title":"approve","text":"<pre><code>approve(args)\n</code></pre> <p>Approves training plan</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def approve(self, args):\n    \"\"\"Approves training plan\"\"\"\n    approve_training_plan(self._node.tp_security_manager, id=args.id)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.TrainingPlanArgumentParser.delete","title":"delete","text":"<pre><code>delete(args)\n</code></pre> <p>Deletes training plan</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def delete(self, args):\n    \"\"\"Deletes training plan\"\"\"\n    delete_training_plan(self._node.tp_security_manager, id=args.id)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.TrainingPlanArgumentParser.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def initialize(self):\n\n    self._parser = self._subparser.add_parser(\n        \"training-plan\",\n        help=\"CLI operations for TrainingPlans register/list/delete/approve/reject etc.\"\n    )\n\n    training_plan_suparsers = self._parser.add_subparsers()\n    self._parser.set_defaults(func=self.default)\n\n\n    common_reject_approve = argparse.ArgumentParser(add_help=False)\n    common_reject_approve.add_argument(\n        '--id',\n        type=str,\n        nargs='?',\n        required=False,\n        help='ID of the training plan that will be processed.'\n    )\n\n\n    update = training_plan_suparsers.add_parser(\n        \"update\", help=\"Updates training plan\"\n    )\n    update.set_defaults(func=self.update)\n\n    register = training_plan_suparsers.add_parser(\n        \"register\", help=\"Registers training plans manually by selected file thorugh interactive browser.\"\n    )\n    register.set_defaults(func=self.register)\n\n    list = training_plan_suparsers.add_parser(\n        \"list\", help=\"Lists all saved/registered training plans with their status.\")\n    list.set_defaults(func=self.list)\n\n    delete = training_plan_suparsers.add_parser(\n        \"delete\",\n        parents=[common_reject_approve],\n        help=\"Deletes interactively selected training plan from the database.\")\n    delete.set_defaults(func=self.delete)\n\n    approve = training_plan_suparsers.add_parser(\n        \"approve\",\n        parents=[common_reject_approve],\n        help=\"Approves interactively selected training plans.\")\n    approve.set_defaults(func=self.approve)\n\n    reject = training_plan_suparsers.add_parser(\n        \"reject\",\n        parents=[common_reject_approve],\n        help=\"Rejects interactively selected training plans.\")\n\n    reject.add_argument(\n        \"--notes\",\n        type=str,\n        nargs=\"?\",\n        required=False,\n        default=\"No notes provided.\",\n        help=\"Note to explain why training plan is rejected.\"\n    )\n    reject.set_defaults(func=self.reject)\n\n    view = training_plan_suparsers.add_parser(\n        \"view\", help=\"View interactively selected training plans.\")\n    view.set_defaults(func=self.view)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.TrainingPlanArgumentParser.list","title":"list","text":"<pre><code>list()\n</code></pre> <p>Lists training plans</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def list(self):\n    \"\"\"Lists training plans\"\"\"\n    self._node.tp_security_manager.list_training_plans(verbose=True)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.TrainingPlanArgumentParser.register","title":"register","text":"<pre><code>register()\n</code></pre> <p>Registers training plan</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def register(self):\n    \"\"\"Registers training plan\"\"\"\n    register_training_plan(self._node.tp_security_manager)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.TrainingPlanArgumentParser.reject","title":"reject","text":"<pre><code>reject(args)\n</code></pre> <p>Approves training plan</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def reject(self, args):\n    \"\"\"Approves training plan\"\"\"\n    reject_training_plan(self._node.tp_security_manager, id=args.id, notes=args.notes)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.TrainingPlanArgumentParser.update","title":"update","text":"<pre><code>update()\n</code></pre> <p>Updates training plan</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def update(self):\n    \"\"\"Updates training plan\"\"\"\n    update_training_plan(self._node.tp_security_manager)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.TrainingPlanArgumentParser.view","title":"view","text":"<pre><code>view()\n</code></pre> <p>Views training plan</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def view(self):\n    \"\"\"Views training plan\"\"\"\n    view_training_plan(self._node.tp_security_manager)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli-functions","title":"Functions","text":""},{"location":"developer/api/node/cli/#fedbiomed.node.cli.intro","title":"intro","text":"<pre><code>intro()\n</code></pre> <p>Prints intro for the CLI</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def intro():\n    \"\"\"Prints intro for the CLI\"\"\"\n\n    print(__intro__)\n    print('\\t- \ud83c\udd94 Your node ID:', os.environ['FEDBIOMED_ACTIVE_NODE_ID'], '\\n')\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.start_node","title":"start_node","text":"<pre><code>start_node(config, node_args)\n</code></pre> <p>Starts the node</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Config name for the node</p> required <code>node_args</code> <p>Arguments for the node</p> required Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def start_node(config, node_args):\n    \"\"\"Starts the node\n\n    Args:\n        name: Config name for the node\n        node_args: Arguments for the node\n    \"\"\"\n\n    _node = Node(config, node_args)\n\n    print(_node)\n\n    def _node_signal_handler(signum: int, frame: Union[FrameType, None]):\n        \"\"\"Signal handler that terminates the process.\n\n        Args:\n            signum: Signal number received.\n            frame: Frame object received. Currently unused\n\n        Raises:\n           SystemExit: Always raised.\n        \"\"\"\n\n        # get the (running) Node object\n\n        try:\n            if _node and _node.is_connected():\n                _node.send_error(ErrorNumbers.FB312,\n                                 extra_msg = \"Node is stopped\",\n                                 broadcast=True)\n                time.sleep(2)\n                logger.critical(\"Node stopped in signal_handler, probably node exit on error or user decision (Ctrl C)\")\n            else:\n                # take care of logger level used because message cannot be sent to node\n                logger.info(\"Cannot send error message to researcher (node not initialized yet)\")\n                logger.info(\"Node stopped in signal_handler, probably node exit on error or user decision (Ctrl C)\")\n        finally:\n            # give some time to send messages to the researcher\n            time.sleep(0.5)\n            sys.exit(signum)\n\n    logger.setLevel(\"DEBUG\")\n\n\n    try:\n        signal.signal(signal.SIGTERM, _node_signal_handler)\n        logger.info('Launching node...')\n\n        # Register default training plans and update hashes\n        if _node.config.getbool('security', 'training_plan_approval'):\n            # This methods updates hashes if hashing algorithm has changed\n            _node.tp_security_manager.check_hashes_for_registered_training_plans()\n            if _node.config.getbool('security', 'allow_default_training_plans'):\n                logger.info('Loading default training plans')\n                _node.tp_security_manager.register_update_default_training_plans()\n        else:\n            logger.warning('Training plan approval for train request is not activated. ' +\n                           'This might cause security problems. Please, consider to enable training plan approval.')\n\n        logger.info('Starting communication channel with network')\n\n        _node.start_messaging(_node_signal_trigger_term)\n        logger.info('Starting node to node router')\n        _node.start_protocol()\n        logger.info('Starting task manager')\n        _node.task_manager()  # handling training tasks in queue\n\n    except FedbiomedError as exp:\n        logger.critical(f\"Node stopped. {exp}\")\n        # we may add extra information for the user depending on the error\n\n    except Exception as exp:\n        # must send info to the researcher (no mqqt should be handled\n        # by the previous FedbiomedError)\n        _node.send_error(ErrorNumbers.FB300, extra_msg=\"Error = \" + str(exp))\n        logger.critical(f\"Node stopped. {exp}\")\n</code></pre>"},{"location":"developer/api/node/cli_utils/","title":"CLI Utils","text":"<p>to simplify imports from fedbiomed.node.cli_utils</p>"},{"location":"developer/api/node/cli_utils/#fedbiomed.node.cli_utils-functions","title":"Functions","text":""},{"location":"developer/api/node/cli_utils/#fedbiomed.node.cli_utils.add_database","title":"add_database","text":"<pre><code>add_database(dataset_manager, interactive=True, path=None, name=None, tags=None, description=None, data_type=None, dataset_parameters=None)\n</code></pre> <p>Adds a dataset to the node database.</p> <p>Also queries interactively the user on the command line (and file browser) for dataset parameters if needed.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_manager</code> <code>DatasetManager</code> <p>Object for managing the dataset</p> required <code>interactive</code> <code>bool</code> <p>Whether to query interactively for dataset parameters even if they are all passed as arguments. Defaults to <code>True</code>.</p> <code>True</code> <code>path</code> <code>str</code> <p>Path to the dataset.</p> <code>None</code> <code>name</code> <code>str</code> <p>Keyword for the dataset.</p> <code>None</code> <code>tags</code> <code>str</code> <p>Comma separated list of tags for the dataset.</p> <code>None</code> <code>description</code> <code>str</code> <p>Human readable description of the dataset.</p> <code>None</code> <code>data_type</code> <code>str</code> <p>Keyword for the data type of the dataset.</p> <code>None</code> <code>dataset_parameters</code> <code>dict</code> <p>Parameters for the dataset manager</p> <code>None</code> Source code in <code>fedbiomed/node/cli_utils/_database.py</code> <pre><code>def add_database(\n    dataset_manager: DatasetManager,\n    interactive: bool = True,\n    path: str = None,\n    name: str = None,\n    tags: str = None,\n    description: str = None,\n    data_type: str = None,\n    dataset_parameters: dict = None\n) -&gt; None:\n    \"\"\"Adds a dataset to the node database.\n\n    Also queries interactively the user on the command line (and file browser)\n    for dataset parameters if needed.\n\n    Args:\n        dataset_manager: Object for managing the dataset\n        interactive: Whether to query interactively for dataset parameters\n            even if they are all passed as arguments. Defaults to `True`.\n        path: Path to the dataset.\n        name: Keyword for the dataset.\n        tags: Comma separated list of tags for the dataset.\n        description: Human readable description of the dataset.\n        data_type: Keyword for the data type of the dataset.\n        dataset_parameters: Parameters for the dataset manager\n    \"\"\"\n\n    dataset_parameters = dataset_parameters or None\n    data_loading_plan = None\n\n    # if all args are provided, just try to load the data\n    # if not, ask the user more informations\n    if interactive or \\\n            path is None or \\\n            name is None or \\\n            tags is None or \\\n            description is None or \\\n            data_type is None :\n\n\n        print('Welcome to the Fed-BioMed CLI data manager')\n\n        if interactive is True:\n            data_type = validated_data_type_input()\n        else:\n            data_type = 'default'\n\n        if data_type == 'default':\n            tags = ['#MNIST', \"#dataset\"]\n            if interactive is True:\n                while input(f'MNIST will be added with tags {tags} [y/N]').lower() != 'y':\n                    pass\n                path = validated_path_input(data_type)\n            name = 'MNIST'\n            description = 'MNIST database'\n\n        elif data_type == 'mednist':\n            tags = ['#MEDNIST', \"#dataset\"]\n            if interactive is True:\n                while input(f'MEDNIST will be added with tags {tags} [y/N]').lower() != 'y':\n                    pass\n                path = validated_path_input(data_type)\n            name = 'MEDNIST'\n            description = 'MEDNIST dataset'\n        else:\n\n            name = input('Name of the database: ')\n\n            tags = input('Tags (separate them by comma and no spaces): ')\n            tags = tags.replace(' ', '').split(',')\n\n            description = input('Description: ')\n\n            if data_type == 'medical-folder':\n                path, dataset_parameters, data_loading_plan = add_medical_folder_dataset_from_cli(interactive,\n                                                                                                  dataset_parameters,\n                                                                                                  data_loading_plan)\n            elif data_type == 'flamby':\n                from fedbiomed.common.data.flamby_dataset import discover_flamby_datasets, FlambyDatasetMetadataBlock, \\\n                    FlambyLoadingBlockTypes\n                # Select the type of dataset (fed_ixi, fed_heart, etc...)\n                available_flamby_datasets = discover_flamby_datasets()\n                msg = \"Please select the FLamby dataset that you're configuring:\\n\"\n                msg += \"\\n\".join([f\"\\t{i}) {val}\" for i, val in available_flamby_datasets.items()])\n                msg += \"\\nselect: \"\n                keep_asking_for_input = True\n                while keep_asking_for_input:\n                    try:\n                        flamby_dataset_index = input(msg)\n                        flamby_dataset_index = int(flamby_dataset_index)\n                        # check that the user inserted a number within the valid range\n                        if flamby_dataset_index in available_flamby_datasets.keys():\n                            keep_asking_for_input = False\n                        else:\n                            warnings.warn(f\"Please pick a number in the range {list(available_flamby_datasets.keys())}\")\n                    except ValueError:\n                        warnings.warn('Please input a numeric value (integer)')\n                path = available_flamby_datasets[flamby_dataset_index]  # flamby datasets not identified by their path\n                # Select the center id\n                module = import_module(f\".{available_flamby_datasets[flamby_dataset_index]}\", package='flamby.datasets')\n                n_centers = module.NUM_CLIENTS\n                keep_asking_for_input = True\n                while keep_asking_for_input:\n                    try:\n                        center_id = int(input(f\"Give a center id between 0 and {str(n_centers-1)}: \"))\n                        if 0 &lt;= center_id &lt; n_centers:\n                            keep_asking_for_input = False\n                    except ValueError:\n                        warnings.warn(f'Please input a numeric value (integer) between 0 and {str(n_centers-1)}')\n\n                # Build the DataLoadingPlan with the selected dataset type and center id\n                data_loading_plan = DataLoadingPlan()\n                metadata_dlb = FlambyDatasetMetadataBlock()\n                metadata_dlb.metadata = {\n                    'flamby_dataset_name': available_flamby_datasets[flamby_dataset_index],\n                    'flamby_center_id': center_id\n                }\n                data_loading_plan[FlambyLoadingBlockTypes.FLAMBY_DATASET_METADATA] = metadata_dlb\n            else:\n                path = validated_path_input(data_type)\n\n        # if a data loading plan was specified, we now ask for the description\n        if interactive and data_loading_plan is not None:\n            keep_asking_for_input = True\n            while keep_asking_for_input:\n                desc = input('Please input a short name/description for your data loading plan:')\n                if len(desc) &lt; 4:\n                    print('Description must be at least 4 characters long.')\n                else:\n                    keep_asking_for_input = False\n            data_loading_plan.desc = desc\n\n    else:\n        # all data have been provided at call\n        # check few things\n\n        # transform a string with coma(s) as a string list\n        tags = str(tags).split(',')\n\n        name = str(name)\n        description = str(description)\n\n        data_type = str(data_type).lower()\n        if data_type not in [ 'csv', 'default', 'mednist', 'images', 'medical-folder']:\n            data_type = 'default'\n\n        if not os.path.exists(path):\n            logger.critical(\"provided path does not exists: \" + path)\n\n        path = os.path.abspath(path)\n\n    path = os.path.abspath(path)\n    logger.info(f\"Dataset absolute path: {path}\")\n\n    try:\n        dataset_manager.add_database(name=name,\n                                     tags=tags,\n                                     data_type=data_type,\n                                     description=description,\n                                     path=path,\n                                     dataset_parameters=dataset_parameters,\n                                     data_loading_plan=data_loading_plan)\n    except (AssertionError, FedbiomedDatasetManagerError) as e:\n        if interactive is True:\n            try:\n                tkinter.messagebox.showwarning(title='Warning', message=str(e))\n            except ModuleNotFoundError:\n                warnings.warn(f'[ERROR]: {e}')\n        else:\n            warnings.warn(f'[ERROR]: {e}')\n        exit(1)\n    except FedbiomedDatasetError as err:\n        warnings.warn(f'[ERROR]: {err} ... Aborting'\n                      \"\\nHint: are you sure you have selected the correct index in Demographic file?\")\n    print('\\nGreat! Take a look at your data:')\n    dataset_manager.list_my_data(verbose=True)\n</code></pre>"},{"location":"developer/api/node/cli_utils/#fedbiomed.node.cli_utils.approve_training_plan","title":"approve_training_plan","text":"<pre><code>approve_training_plan(tp_security_manager, id=None, *, sort_by_date=True)\n</code></pre> <p>Approves a given training plan that has either Pending or Rejected status</p> <p>Parameters:</p> Name Type Description Default <code>tp_security_manager</code> <code>TrainingPlanSecurityManager</code> <p>Object for managing the training plan approval</p> required <code>id</code> <code>Optional[str]</code> <p>unique if of the training plan to be approved. Providing an id will trigger non-interactive approval.</p> <code>None</code> <code>sort_by_date</code> <code>bool</code> <p>whether to sort by last modification date. Defaults to True.</p> <code>True</code> Source code in <code>fedbiomed/node/cli_utils/_training_plan_management.py</code> <pre><code>def approve_training_plan(\n    tp_security_manager: TrainingPlanSecurityManager,\n    id: Optional[str] = None,\n    *,\n    sort_by_date: bool = True\n) -&gt; None:\n    \"\"\"Approves a given training plan that has either Pending or Rejected status\n\n    Args:\n        tp_security_manager: Object for managing the training plan approval\n        id: unique if of the training plan to be approved. Providing an id will trigger non-interactive approval.\n        sort_by_date: whether to sort by last modification date. Defaults to True.\n    \"\"\"\n\n    def approve(training_plan_id):\n        tp_security_manager.approve_training_plan(training_plan_id)\n        logger.info(f\"Training plan {training_plan_id} has been approved. \"\n                    \"Researchers can now train the Training Plan \"\n                    \"on this node.\")\n\n    # If id is already provided\n    if id:\n        return approve(id)\n\n    if sort_by_date:\n        sort_by = 'date_modified'\n    else:\n        sort_by = None\n\n    non_approved_training_plans = tp_security_manager.list_training_plans(\n        sort_by=sort_by,\n        select_status=[TrainingPlanApprovalStatus.PENDING,\n                       TrainingPlanApprovalStatus.REJECTED],\n        verbose=False)\n    if not non_approved_training_plans:\n        logger.warning(\"All training_plans have been approved or no training plan has been registered... aborting\")\n        return\n\n    options = [m['name'] + '\\t Training plan ID ' + m['training_plan_id'] + '\\t training plan status ' +\n               m['training_plan_status'] + '\\tdate_last_action ' +\n               str(m['date_last_action']) for m in non_approved_training_plans]\n\n    msg = \"Select the training plan to approve:\\n\"\n    msg += \"\\n\".join([f'{i}) {d}' for i, d in enumerate(options, 1)])\n    msg += \"\\nSelect: \"\n\n    while True:\n        try:\n            opt_idx = int(input(msg)) - 1\n            assert opt_idx in range(len(non_approved_training_plans))\n            training_plan_id = non_approved_training_plans[opt_idx]['training_plan_id']\n            return approve(training_plan_id)\n        except (ValueError, IndexError, AssertionError):\n            logger.error('Invalid option. Please, try again.')\n</code></pre>"},{"location":"developer/api/node/cli_utils/#fedbiomed.node.cli_utils.delete_all_database","title":"delete_all_database","text":"<pre><code>delete_all_database(dataset_manager)\n</code></pre> <p>Deletes all datasets from the node's database.</p> <p>Does not modify the dataset's files.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_manager</code> <p>Object for managing the dataset</p> required Source code in <code>fedbiomed/node/cli_utils/_database.py</code> <pre><code>def delete_all_database(dataset_manager):\n    \"\"\"Deletes all datasets from the node's database.\n\n    Does not modify the dataset's files.\n\n    Args:\n        dataset_manager: Object for managing the dataset\n    \"\"\"\n    my_data = dataset_manager.list_my_data(verbose=False)\n\n    if not my_data:\n        logger.warning('No dataset to delete')\n        return\n\n    for ds in my_data:\n        d_id = ds['dataset_id']\n        dataset_manager.remove_database(d_id)\n        logger.info('Dataset removed for dataset_id:' + str(d_id))\n\n    return\n</code></pre>"},{"location":"developer/api/node/cli_utils/#fedbiomed.node.cli_utils.delete_database","title":"delete_database","text":"<pre><code>delete_database(dataset_manager, interactive=True)\n</code></pre> <p>Removes one or more dataset from the node's database.</p> <p>Does not modify the dataset's files.</p> <p>Parameters:</p> Name Type Description Default <code>interactive</code> <code>bool</code> <ul> <li>if <code>True</code> interactively queries (repeatedly) from the command line     for a dataset to delete</li> <li>if <code>False</code> delete MNIST dataset if it exists in the database</li> </ul> <code>True</code> Source code in <code>fedbiomed/node/cli_utils/_database.py</code> <pre><code>def delete_database(\n    dataset_manager: DatasetManager,\n    interactive: bool = True\n) -&gt; None:\n    \"\"\"Removes one or more dataset from the node's database.\n\n    Does not modify the dataset's files.\n\n    Args:\n        interactive:\n\n            - if `True` interactively queries (repeatedly) from the command line\n                for a dataset to delete\n            - if `False` delete MNIST dataset if it exists in the database\n    \"\"\"\n    my_data = dataset_manager.list_my_data(verbose=False)\n    if not my_data:\n        logger.warning('No dataset to delete')\n        return\n\n    msg: str = ''\n    d_id: Union[str, None] = None\n\n    if interactive is True:\n        options = [d['name'] for d in my_data]\n        msg = \"Select the dataset to delete:\\n\"\n        msg += \"\\n\".join([f'{i}) {d}' for i, d in enumerate(options, 1)])\n        msg += \"\\nSelect: \"\n\n    while True:\n        try:\n            if interactive is True:\n                opt_idx = int(input(msg)) - 1\n                assert opt_idx in range(len(my_data))\n\n                d_id = my_data[opt_idx]['dataset_id']\n            else:\n                for ds in my_data:\n                    if ds['name'] == 'MNIST':\n                        d_id = ds['dataset_id']\n                        break\n\n            if not d_id:\n                logger.warning('No matching dataset to delete')\n                return\n            dataset_manager.remove_database(d_id)\n            logger.info('Dataset removed. Here your available datasets')\n            dataset_manager.list_my_data()\n            return\n        except (ValueError, IndexError, AssertionError):\n            logger.error('Invalid option. Please, try again.')\n</code></pre>"},{"location":"developer/api/node/cli_utils/#fedbiomed.node.cli_utils.delete_training_plan","title":"delete_training_plan","text":"<pre><code>delete_training_plan(tp_security_manager, id=None)\n</code></pre> <p>Deletes an authorized training plan in the database interactively from the CLI.</p> <p>Does not modify or delete training plan file.</p> <p>Deletes only registered and requested training_plans. For default training plans, files should be removed directly from the file system.</p> <p>Parameters:</p> Name Type Description Default <code>tp_security_manager</code> <code>TrainingPlanSecurityManager</code> <p>Object for managing the training plan approval</p> required <code>id</code> <code>Optional[str]</code> <p>ID of the training plan that will be removed.</p> <code>None</code> Source code in <code>fedbiomed/node/cli_utils/_training_plan_management.py</code> <pre><code>def delete_training_plan(\n    tp_security_manager: TrainingPlanSecurityManager,\n    id: Optional[str] = None\n) -&gt; None:\n    \"\"\"Deletes an authorized training plan in the database interactively from the CLI.\n\n    Does not modify or delete training plan file.\n\n    Deletes only registered and requested training_plans. For default training plans, files\n    should be removed directly from the file system.\n\n    Args:\n        tp_security_manager: Object for managing the training plan approval\n        id: ID of the training plan that will be removed.\n    \"\"\"\n\n    def delete(training_plan_id):\n        # Delete training plan\n        tp_security_manager.delete_training_plan(training_plan_id)\n        logger.info('Training plan has been removed. Here your other training plans')\n        tp_security_manager.list_training_plans(verbose=True)\n\n    training_plans = tp_security_manager.list_training_plans(verbose=False)\n    training_plans = [m for m in training_plans if m['training_plan_type'] in [TrainingPlanStatus.REGISTERED.value,\n                                                       TrainingPlanStatus.REQUESTED.value]]\n    if not training_plans:\n        logger.warning('No training plans to delete')\n        return\n\n    if id:\n        return delete(id)\n\n\n    options = [m['name'] + '\\t Training plan ID ' + m['training_plan_id'] + '\\t Training plan type ' +\n               m['training_plan_type'] + '\\tTraining plan status ' + m['training_plan_status'] for m in training_plans]\n    msg = \"Select the training plan to delete:\\n\"\n    msg += \"\\n\".join([f'{i}) {d}' for i, d in enumerate(options, 1)])\n    msg += \"\\nSelect: \"\n\n    while True:\n        try:\n            opt_idx = int(input(msg)) - 1\n            assert opt_idx in range(len(training_plans))\n            training_plan_id = training_plans[opt_idx]['training_plan_id']\n\n            if not training_plan_id:\n                logger.warning('No matching training plan to delete')\n                return\n            return delete(training_plan_id)\n\n        except (ValueError, IndexError, AssertionError):\n            logger.error('Invalid option. Please, try again.')\n</code></pre>"},{"location":"developer/api/node/cli_utils/#fedbiomed.node.cli_utils.register_training_plan","title":"register_training_plan","text":"<pre><code>register_training_plan(tp_security_manager)\n</code></pre> <p>Registers an authorized training plan in the database interactively through the CLI.</p> <p>Does not modify training plan file.</p> <p>Parameters:</p> Name Type Description Default <code>tp_security_manager</code> <code>TrainingPlanSecurityManager</code> <p>Object for managing the training plan approval</p> required Source code in <code>fedbiomed/node/cli_utils/_training_plan_management.py</code> <pre><code>def register_training_plan(\n    tp_security_manager: TrainingPlanSecurityManager\n):\n    \"\"\"Registers an authorized training plan in the database interactively through the CLI.\n\n    Does not modify training plan file.\n\n    Args:\n        tp_security_manager: Object for managing the training plan approval\n    \"\"\"\n\n    print('Welcome to the Fed-BioMed CLI data manager')\n    name = input('Please enter a training plan name: ')\n    description = input('Please enter a description for the training plan: ')\n\n    # Allow files saved as txt\n    path = validated_path_input(type=\"txt\")\n\n    # Register training plan\n    try:\n        tp_security_manager.register_training_plan(name=name,\n                                     description=description,\n                                     path=path)\n\n    except AssertionError as e:\n        try:\n            tkinter.messagebox.showwarning(title='Warning', message=str(e))\n        except ModuleNotFoundError:\n            warnings.warn(f'[ERROR]: {e}')\n        exit(1)\n\n    print('\\nGreat! Take a look at your data:')\n    tp_security_manager.list_training_plans(verbose=True)\n</code></pre>"},{"location":"developer/api/node/cli_utils/#fedbiomed.node.cli_utils.reject_training_plan","title":"reject_training_plan","text":"<pre><code>reject_training_plan(tp_security_manager, id=None, notes=None)\n</code></pre> <p>Rejects a given training plan that has either Pending or Approved status</p> <p>Parameters:</p> Name Type Description Default <code>tp_security_manager</code> <code>TrainingPlanSecurityManager</code> <p>Object for managing the training plan approval</p> required <code>id</code> <code>Optional[str]</code> <p>Training plan ID</p> <code>None</code> <code>notes</code> <code>Optional[str]</code> <p>Comment about rejection reason</p> <code>None</code> Source code in <code>fedbiomed/node/cli_utils/_training_plan_management.py</code> <pre><code>def reject_training_plan(\n    tp_security_manager: TrainingPlanSecurityManager,\n    id: Optional[str] = None,\n    notes: Optional[str] = None\n) -&gt; None:\n    \"\"\"Rejects a given training plan that has either Pending or Approved status\n\n    Args:\n        tp_security_manager: Object for managing the training plan approval\n        id: Training plan ID\n        notes: Comment about rejection reason\n    \"\"\"\n\n    def reject(training_plan_id, notes):\n        tp_security_manager.reject_training_plan(training_plan_id, notes)\n        logger.info(f\"Training plan {training_plan_id} has been rejected. \"\n                     \"Researchers can not train training plan \"\n                     \"on this node anymore\")\n\n\n    approved_training_plans = tp_security_manager.list_training_plans(\n        select_status=[TrainingPlanApprovalStatus.APPROVED,\n                       TrainingPlanApprovalStatus.PENDING],\n        verbose=False)\n\n    if not approved_training_plans:\n        logger.warning(\"All training plans have already been rejected or no training plan has been registered... aborting\")\n        return\n\n    if id:\n        reject(id, notes)\n        return\n\n    options = [m['name'] + '\\t Training plan ID ' + m['training_plan_id'] + '\\t training plan status ' +\n               m['training_plan_status'] + '\\tTraining plan Type ' + m['training_plan_type'] for m in approved_training_plans]\n\n    msg = \"Select the training plan to reject (this will prevent Researcher to run training plan on Node):\\n\"\n    msg += \"\\n\".join([f'{i}) {d}' for i, d in enumerate(options, 1)])\n    msg += \"\\nSelect: \"\n\n    while True:\n        try:\n            opt_idx = int(input(msg)) - 1\n            assert opt_idx in range(len(approved_training_plans))\n            training_plan_id = approved_training_plans[opt_idx]['training_plan_id']\n            notes = input(\"Please give a note to explain why training plan has been rejected: \\n\")\n            reject(training_plan_id, notes)\n            return\n\n        except (ValueError, IndexError, AssertionError):\n            logger.error('Invalid option. Please, try again.')\n</code></pre>"},{"location":"developer/api/node/cli_utils/#fedbiomed.node.cli_utils.update_training_plan","title":"update_training_plan","text":"<pre><code>update_training_plan(tp_security_manager)\n</code></pre> <p>Updates an authorized training plan in the database interactively through the CLI.</p> <p>Does not modify training plan file.</p> <p>User can either choose different training plan file (different path) to update training plan or same training plan file.</p> <p>Parameters:</p> Name Type Description Default <code>tp_security_manager</code> <code>TrainingPlanSecurityManager</code> <p>Object for managing the training plan approval</p> required Source code in <code>fedbiomed/node/cli_utils/_training_plan_management.py</code> <pre><code>def update_training_plan(\n    tp_security_manager: TrainingPlanSecurityManager\n):\n    \"\"\"Updates an authorized training plan in the database interactively through the CLI.\n\n    Does not modify training plan file.\n\n    User can either choose different training plan file (different path)\n    to update training plan or same training plan file.\n\n    Args:\n        tp_security_manager: Object for managing the training plan approval\n    \"\"\"\n    training_plans = tp_security_manager.list_training_plans(verbose=False)\n\n    # Select only registered training plan to update\n    training_plans = [m for m in training_plans if m['training_plan_type'] == TrainingPlanStatus.REGISTERED.value]\n    if not training_plans:\n        logger.warning('No registered training plans has been found to update')\n        return\n\n    options = [m['name'] + '\\t Training plan ID ' + m['training_plan_id'] for m in training_plans]\n    msg = \"Select the training plan to update:\\n\"\n    msg += \"\\n\".join([f'{i}) {d}' for i, d in enumerate(options, 1)])\n    msg += \"\\nSelect: \"\n\n    while True:\n        try:\n\n            # Get the selection\n            opt_idx = int(input(msg)) - 1\n            assert opt_idx in range(len(training_plans))\n            training_plan_id = training_plans[opt_idx]['training_plan_id']\n\n            if not training_plan_id:\n                logger.warning('No matching training plan to update')\n                return\n\n            # Get the new file or same file.  User can provide same training plan file\n            # with updated content or new training plan file.\n            path = validated_path_input(type=\"txt\")\n\n            # Update training plan through training plan manager\n            tp_security_manager.update_training_plan_hash(training_plan_id, path)\n\n            logger.info('Training plan has been updated. Here all your training plans')\n            tp_security_manager.list_training_plans(verbose=True)\n\n            return\n\n        except (ValueError, IndexError, AssertionError):\n            logger.error('Invalid option. Please, try again.')\n</code></pre>"},{"location":"developer/api/node/cli_utils/#fedbiomed.node.cli_utils.view_training_plan","title":"view_training_plan","text":"<pre><code>view_training_plan(tp_security_manager)\n</code></pre> <p>Views source code for a training plan in the database</p> <p>If training plan cannot be displayed to the logger, then abort.</p> <p>Parameters:</p> Name Type Description Default <code>tp_security_manager</code> <code>TrainingPlanSecurityManager</code> <p>Object for managing the training plan approval</p> required Source code in <code>fedbiomed/node/cli_utils/_training_plan_management.py</code> <pre><code>def view_training_plan(tp_security_manager: TrainingPlanSecurityManager) -&gt; None:\n    \"\"\"Views source code for a training plan in the database\n\n    If training plan cannot be displayed to the logger, then abort.\n\n    Args:\n        tp_security_manager: Object for managing the training plan approval\n    \"\"\"\n    training_plans = tp_security_manager.list_training_plans(verbose=False)\n    if not training_plans:\n        logger.warning(\"No training plan has been registered... aborting\")\n        return\n\n    options = [m['name'] + '\\t Training plan ID ' + m['training_plan_id'] + '\\t training plan status ' +\n               m['training_plan_status'] for m in training_plans]\n\n    msg = \"Select the training plan to view:\\n\"\n    msg += \"\\n\".join([f'{i}) {d}' for i, d in enumerate(options, 1)])\n    msg += \"\\n\\nDon't try to modify the training plan with this viewer, modifications will be dropped.\"\n    msg += \"\\nSelect: \"\n\n    while True:\n        try:\n            opt_idx = int(input(msg)) - 1\n            assert opt_idx in range(len(training_plans))\n            training_plan_name = training_plans[opt_idx]['name']\n        except (ValueError, IndexError, AssertionError):\n            logger.error('Invalid option. Please, try again.')\n            continue\n\n        # TODO: more robust (when refactor whole CLI)\n        # - check `training_plan` though it should never be None, as we just checked for it\n        # - check after file copy though it should work\n        # - etc.\n        training_plan = tp_security_manager.get_training_plan_by_name(training_plan_name)\n\n        try:\n            training_plan_source = highlight(training_plan[\"training_plan\"], PythonLexer(), Terminal256Formatter())\n            logger.info(f'\\n\\n{training_plan_source}\\n\\n')\n        except Exception as err:\n            logger.critical(f'Cannot display training plan via logger. Aborting. Error message is: {err}')\n\n        return\n</code></pre>"},{"location":"developer/api/node/config/","title":"Config","text":""},{"location":"developer/api/node/config/#fedbiomed.node.config-attributes","title":"Attributes","text":""},{"location":"developer/api/node/config/#fedbiomed.node.config.component_root","title":"component_root  <code>module-attribute</code>","text":"<pre><code>component_root = get('FBM_NODE_COMPONENT_ROOT', None)\n</code></pre>"},{"location":"developer/api/node/config/#fedbiomed.node.config.node_component","title":"node_component  <code>module-attribute</code>","text":"<pre><code>node_component = NodeComponent()\n</code></pre>"},{"location":"developer/api/node/config/#fedbiomed.node.config-classes","title":"Classes","text":""},{"location":"developer/api/node/config/#fedbiomed.node.config.NodeComponent","title":"NodeComponent","text":"<pre><code>NodeComponent()\n</code></pre> <p>               Bases: <code>Component</code></p> <p>Fed-BioMed Node Component Class</p> <p>This class is used for creating and validating components by given component root directory</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def __init__(self):\n    \"\"\"Test\"\"\"\n    self._reference = '.fedbiomed'\n</code></pre>"},{"location":"developer/api/node/config/#fedbiomed.node.config.NodeComponent-attributes","title":"Attributes","text":""},{"location":"developer/api/node/config/#fedbiomed.node.config.NodeComponent.config_cls","title":"config_cls  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config_cls = NodeConfig\n</code></pre>"},{"location":"developer/api/node/config/#fedbiomed.node.config.NodeComponent-functions","title":"Functions","text":""},{"location":"developer/api/node/config/#fedbiomed.node.config.NodeComponent.initiate","title":"initiate","text":"<pre><code>initiate(root=None)\n</code></pre> Source code in <code>fedbiomed/node/config.py</code> <pre><code>def initiate(self, root: Optional[str] = None) -&gt; NodeConfig:\n    config = super().initiate(root)\n    node_data_path = os.path.join(config.root, NODE_DATA_FOLDER)\n    os.makedirs(node_data_path, exist_ok=True)\n    return config\n</code></pre>"},{"location":"developer/api/node/config/#fedbiomed.node.config.NodeConfig","title":"NodeConfig","text":"<pre><code>NodeConfig(root)\n</code></pre> <p>               Bases: <code>Config</code></p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory for the component</p> required Source code in <code>fedbiomed/common/config.py</code> <pre><code>def __init__(\n    self, root: str\n) -&gt; None:\n    \"\"\"Initializes configuration\n\n    Args:\n        root: Root directory for the component\n    \"\"\"\n    self._cfg = configparser.ConfigParser()\n    self.load(root)\n</code></pre>"},{"location":"developer/api/node/config/#fedbiomed.node.config.NodeConfig-attributes","title":"Attributes","text":""},{"location":"developer/api/node/config/#fedbiomed.node.config.NodeConfig.COMPONENT_TYPE","title":"COMPONENT_TYPE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>COMPONENT_TYPE = 'NODE'\n</code></pre>"},{"location":"developer/api/node/config/#fedbiomed.node.config.NodeConfig-functions","title":"Functions","text":""},{"location":"developer/api/node/config/#fedbiomed.node.config.NodeConfig.add_parameters","title":"add_parameters","text":"<pre><code>add_parameters()\n</code></pre> <p>Generate <code>Node</code> config</p> Source code in <code>fedbiomed/node/config.py</code> <pre><code>def add_parameters(self):\n    \"\"\"Generate `Node` config\"\"\"\n\n    # Security variables\n    self._cfg['security'] = {\n        'hashing_algorithm': HashingAlgorithms.SHA256.value,\n        'allow_default_training_plans': os.getenv('FBM_SECURITY_ALLOW_DEFAULT_TRAINING_PLANS', 'True'),\n        'training_plan_approval': os.getenv('FBM_SECURITY_TRAINING_PLAN_APPROVAL', 'False'),\n        'secure_aggregation': os.getenv('FBM_SECURITY_SECURE_AGGREGATION', 'True'),\n        'force_secure_aggregation': os.getenv('FBM_SECURITY_FORCE_SECURE_AGGREGATION', 'False'),\n        'secagg_insecure_validation': os.getenv('FBM_SECURITY_SECAGG_INSECURE_VALIDATION', 'True'),\n    }\n    # Generate self-signed certificates\n    key_file, pem_file = generate_certificate(\n        root=self.root, component_id=self._cfg[\"default\"][\"id\"], prefix=DEFAULT_CERT_NAME\n    )\n\n    self._cfg[\"certificate\"] = {\n        \"private_key\": os.path.relpath(key_file, os.path.join(self.root, \"etc\")),\n        \"public_key\": os.path.relpath(pem_file, os.path.join(self.root, \"etc\"))\n    }\n\n    # gRPC server host and port\n    self._cfg[\"researcher\"] = {\n        'ip': os.getenv('FBM_RESEARCHER_IP', 'localhost'),\n        'port': os.getenv('FBM_RESEARCHER_PORT', '50051')\n    }\n</code></pre>"},{"location":"developer/api/node/config/#fedbiomed.node.config-functions","title":"Functions","text":""},{"location":"developer/api/node/dataset_manager/","title":"DatasetManager","text":"<p>Interfaces with the node component database.</p>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager-attributes","title":"Attributes","text":""},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager-classes","title":"Classes","text":""},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager","title":"DatasetManager","text":"<pre><code>DatasetManager(db)\n</code></pre> <p>Interfaces with the node component database.</p> <p>Facility for storing data, retrieving data and getting data info for the node. Currently uses TinyDB.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>str</code> <p>Path to the database file</p> required Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def __init__(self, db: str):\n    \"\"\"Constructor of the class.\n\n    Args:\n        db: Path to the database file\n    \"\"\"\n    self._db = TinyDB(db)\n    self._database = Query()\n\n    # don't use DB read cache to ensure coherence\n    # (eg when mixing CLI commands with a GUI session)\n    self._dataset_table = DBTable(self._db.storage, name='Datasets', cache_size=0)\n    self._dlp_table = DBTable(self._db.storage, name='Data_Loading_Plans', cache_size=0)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager-functions","title":"Functions","text":""},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.add_database","title":"add_database","text":"<pre><code>add_database(name, data_type, tags, description, path=None, dataset_id=None, dataset_parameters=None, data_loading_plan=None, save_dlp=True)\n</code></pre> <p>Adds a new dataset contained in a file to node's database.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the dataset</p> required <code>data_type</code> <code>str</code> <p>File extension/format of the dataset (*.csv, images, ...)</p> required <code>tags</code> <code>Union[tuple, list]</code> <p>Tags of the dataset.</p> required <code>description</code> <code>str</code> <p>Human readable description of the dataset.</p> required <code>path</code> <code>Optional[str]</code> <p>Path to the dataset. Defaults to None.</p> <code>None</code> <code>dataset_id</code> <code>Optional[str]</code> <p>Id of the dataset. Defaults to None.</p> <code>None</code> <code>dataset_parameters</code> <code>Optional[dict]</code> <p>a dictionary of additional (customized) parameters, or None</p> <code>None</code> <code>data_loading_plan</code> <code>Optional[DataLoadingPlan]</code> <p>a DataLoadingPlan to be linked to this dataset, or None</p> <code>None</code> <code>save_dlp</code> <code>bool</code> <p>if True, save the <code>data_loading_plan</code></p> <code>True</code> <p>Returns:</p> Name Type Description <code>dataset_id</code> <p>id of the dataset stored in database. Returns <code>dataset_id</code> if provided (non-None) or a new id if not.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p><code>data_type</code> is not supported.</p> <code>FedbiomedDatasetManagerError</code> <p>path does not exist or dataset was not saved properly.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def add_database(self,\n                 name: str,\n                 data_type: str,\n                 tags: Union[tuple, list],\n                 description: str,\n                 path: Optional[str] = None,\n                 dataset_id: Optional[str] = None,\n                 dataset_parameters : Optional[dict] = None,\n                 data_loading_plan: Optional[DataLoadingPlan] = None,\n                 save_dlp: bool = True):\n    \"\"\"Adds a new dataset contained in a file to node's database.\n\n    Args:\n        name: Name of the dataset\n        data_type: File extension/format of the\n            dataset (*.csv, images, ...)\n        tags: Tags of the dataset.\n        description: Human readable description of the dataset.\n        path: Path to the dataset. Defaults to None.\n        dataset_id: Id of the dataset. Defaults to None.\n        dataset_parameters: a dictionary of additional (customized) parameters, or None\n        data_loading_plan: a DataLoadingPlan to be linked to this dataset, or None\n        save_dlp: if True, save the `data_loading_plan`\n\n    Returns:\n        dataset_id: id of the dataset stored in database. Returns `dataset_id`\n            if provided (non-None) or a new id if not.\n\n    Raises:\n        NotImplementedError: `data_type` is not supported.\n        FedbiomedDatasetManagerError: path does not exist or dataset was not saved properly.\n    \"\"\"\n    # Accept tilde as home folder\n    if path is not None:\n        path = os.path.expanduser(path)\n\n    # Check that there are not existing dataset with conflicting tags\n    conflicting = self.search_conflicting_tags(tags)\n    if len(conflicting) &gt; 0:\n        msg = f\"{ErrorNumbers.FB322.value}, one or more registered dataset has conflicting tags: \" \\\n            f\" {' '.join([ c['name'] for c in conflicting ])}\"\n        logger.critical(msg)\n        raise FedbiomedDatasetManagerError(msg)\n\n    dtypes = []  # empty list for Image datasets\n    data_types = ['csv', 'default', 'mednist', 'images', 'medical-folder', 'flamby']\n\n    if data_type not in data_types:\n        raise NotImplementedError(f'Data type {data_type} is not'\n                                  ' a compatible data type. '\n                                  f'Compatible data types are: {data_types}')\n\n    elif data_type == 'flamby':\n        from fedbiomed.common.data.flamby_dataset import FlambyLoadingBlockTypes, FlambyDataset\n        # check that data loading plan is present and well formed\n        if data_loading_plan is None or \\\n                FlambyLoadingBlockTypes.FLAMBY_DATASET_METADATA not in data_loading_plan:\n            msg = f\"{ErrorNumbers.FB316.value}. A DataLoadingPlan containing \" \\\n                  f\"{FlambyLoadingBlockTypes.FLAMBY_DATASET_METADATA.value} is required for adding a FLamby dataset \" \\\n                  f\"to the database.\"\n            logger.critical(msg)\n            raise FedbiomedDatasetManagerError(msg)\n\n        # initialize a dataset and link to the flamby data. If all goes well, compute shape.\n        try:\n            dataset = FlambyDataset()\n            dataset.set_dlp(data_loading_plan)  # initializes fed_class as a side effect\n        except FedbiomedError as e:\n            raise FedbiomedDatasetManagerError(f\"Can not create FLamby dataset. {e}\")\n        else:\n            shape = dataset.shape()\n\n    if data_type == 'default':\n        assert os.path.isdir(path), f'Folder {path} for Default Dataset does not exist.'\n        shape = self.load_default_database(name, path)\n\n    elif data_type == 'mednist':\n        assert os.path.isdir(path), f'Folder {path} for MedNIST Dataset does not exist.'\n        shape, path = self.load_mednist_database(path)\n\n    elif data_type == 'csv':\n        assert os.path.isfile(path), f'Path provided ({path}) does not correspond to a CSV file.'\n        dataset = self.load_csv_dataset(path)\n        shape = dataset.shape\n        dtypes = self.get_csv_data_types(dataset)\n\n    elif data_type == 'images':\n        assert os.path.isdir(path), f'Folder {path} for Images Dataset does not exist.'\n        shape = self.load_images_dataset(path)\n\n    elif data_type == 'medical-folder':\n        if not os.path.isdir(path):\n            raise FedbiomedDatasetManagerError(f'Folder {path} for Medical Folder Dataset does not exist.')\n\n        if \"tabular_file\" not in dataset_parameters:\n            logger.info(\"Medical Folder Dataset will be loaded without reference/demographics data.\")\n        else:\n            if not os.path.isfile(dataset_parameters['tabular_file']):\n                raise FedbiomedDatasetManagerError(f'Path {dataset_parameters[\"tabular_file\"]} does not '\n                                                   f'correspond a file.')\n            if \"index_col\" not in dataset_parameters:\n                raise FedbiomedDatasetManagerError('Index column is not provided')\n\n        try:\n            # load using the MedicalFolderController to ensure all available modalities are inspected\n            controller = MedicalFolderController(root=path)\n            if data_loading_plan is not None:\n                controller.set_dlp(data_loading_plan)\n            dataset = controller.load_MedicalFolder(tabular_file=dataset_parameters.get('tabular_file', None),\n                                                    index_col=dataset_parameters.get('index_col', None))\n\n        except FedbiomedError as e:\n            raise FedbiomedDatasetManagerError(f\"Can not create Medical Folder dataset. {e}\")\n        else:\n            shape = dataset.shape()\n\n        # try to read one sample and raise if it doesn't work\n        try:\n            _ = dataset.get_nontransformed_item(0)\n        except Exception as e:\n            raise FedbiomedDatasetManagerError(f'Medical Folder Dataset was not saved properly and '\n                                               f'cannot be read. {e}')\n\n    if not dataset_id:\n        dataset_id = 'dataset_' + str(uuid.uuid4())\n\n    new_database = dict(name=name, data_type=data_type, tags=tags,\n                        description=description, shape=shape,\n                        path=path, dataset_id=dataset_id, dtypes=dtypes,\n                        dataset_parameters=dataset_parameters)\n    if save_dlp:\n        dlp_id = self.save_data_loading_plan(data_loading_plan)\n    elif isinstance(data_loading_plan, DataLoadingPlan):\n        dlp_id = data_loading_plan.dlp_id\n    else:\n        dlp_id = None\n    if dlp_id is not None:\n        new_database['dlp_id'] = dlp_id\n\n    self._dataset_table.insert(new_database)\n\n    return dataset_id\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.get_by_id","title":"get_by_id","text":"<pre><code>get_by_id(dataset_id)\n</code></pre> <p>Searches for a dataset with given dataset_id.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>A dataset id</p> required <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>A <code>dict</code> containing the dataset's description if a dataset with this <code>dataset_id</code></p> <code>Union[dict, None]</code> <p>exists in the database. <code>None</code> if no such dataset exists in the database.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def get_by_id(self, dataset_id: str) -&gt; Union[dict, None]:\n    \"\"\"Searches for a dataset with given dataset_id.\n\n    Args:\n        dataset_id:  A dataset id\n\n    Returns:\n        A `dict` containing the dataset's description if a dataset with this `dataset_id`\n        exists in the database. `None` if no such dataset exists in the database.\n    \"\"\"\n    return self._dataset_table.get(self._database.dataset_id == dataset_id)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.get_csv_data_types","title":"get_csv_data_types","text":"<pre><code>get_csv_data_types(dataset)\n</code></pre> <p>Gets data types of each variable in dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>A Pandas dataset.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings containing data types.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def get_csv_data_types(self, dataset: pd.DataFrame) -&gt; List[str]:\n    \"\"\"Gets data types of each variable in dataset.\n\n    Args:\n        dataset: A Pandas dataset.\n\n    Returns:\n        A list of strings containing data types.\n    \"\"\"\n    types = [str(t) for t in dataset.dtypes]\n\n    return types\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.get_data_loading_blocks_by_ids","title":"get_data_loading_blocks_by_ids","text":"<pre><code>get_data_loading_blocks_by_ids(dlb_ids)\n</code></pre> <p>Search for a list of DataLoadingBlockTypes, each corresponding to one given id.</p> <p>Note that in case of conflicting ids (which should not happen), this function will silently return a random one with the sought id.</p> <p>DataLoadingBlock IDs always start with 'serialized_data_loading_block_' and should be unique in the database.</p> <p>Parameters:</p> Name Type Description Default <code>dlb_ids</code> <code>Union[str, List[str]]</code> <p>(List[str]) a list of DataLoadingBlock IDs</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>A list of dictionaries, each one containing the DataLoadingBlock metadata corresponding to one given id.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def get_data_loading_blocks_by_ids(self, dlb_ids: Union[str, List[str]]) -&gt; List[dict]:\n    \"\"\"Search for a list of DataLoadingBlockTypes, each corresponding to one given id.\n\n    Note that in case of conflicting ids (which should not happen), this function will silently return a random\n    one with the sought id.\n\n    DataLoadingBlock IDs always start with 'serialized_data_loading_block_' and should be unique in the database.\n\n    Args:\n        dlb_ids: (List[str]) a list of DataLoadingBlock IDs\n\n    Returns:\n        A list of dictionaries, each one containing the DataLoadingBlock metadata corresponding to one given id.\n    \"\"\"\n    return self._dlp_table.search(self._database.dlb_id.one_of(dlb_ids))\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.get_dlp_by_id","title":"get_dlp_by_id","text":"<pre><code>get_dlp_by_id(dlp_id)\n</code></pre> <p>Search for a DataLoadingPlan with a given id.</p> <p>Note that in case of conflicting ids (which should not happen), this function will silently return a random one with the sought id.</p> <p>DataLoadingPlan IDs always start with 'dlp_' and should be unique in the database.</p> <p>Parameters:</p> Name Type Description Default <code>dlp_id</code> <code>str</code> <p>(str) the DataLoadingPlan id</p> required <p>Returns:</p> Type Description <code>Tuple[dict, List[dict]]</code> <p>A Tuple containing a dictionary with the DataLoadingPlan metadata corresponding to the given id.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def get_dlp_by_id(self, dlp_id: str) -&gt; Tuple[dict, List[dict]]:\n    \"\"\"Search for a DataLoadingPlan with a given id.\n\n    Note that in case of conflicting ids (which should not happen), this function will silently return a random\n    one with the sought id.\n\n    DataLoadingPlan IDs always start with 'dlp_' and should be unique in the database.\n\n    Args:\n        dlp_id: (str) the DataLoadingPlan id\n\n    Returns:\n        A Tuple containing a dictionary with the DataLoadingPlan metadata corresponding to the given id.\n    \"\"\"\n    dlp_metadata = self._dlp_table.get(self._database.dlp_id == dlp_id)\n\n    # TODO: This exception should be removed once non-existing DLP situation is\n    # handled by higher layers in Round or Node classes\n    if dlp_metadata is None:\n        raise FedbiomedDatasetManagerError(\n            f\"{ErrorNumbers.FB315.value}: Non-existing DLP for the dataset.\"\n        )\n\n    return dlp_metadata, self._dlp_table.search(\n        self._database.dlb_id.one_of(dlp_metadata['loading_blocks'].values()))\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.get_torch_dataset_shape","title":"get_torch_dataset_shape","text":"<pre><code>get_torch_dataset_shape(dataset)\n</code></pre> <p>Gets info about dataset shape.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>A Pytorch dataset</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>A list of int containing [, ]. Example for MNIST: [60000, 1, 28, 28], where =60000 and =1, 28, 28 Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def get_torch_dataset_shape(self, dataset: torch.utils.data.Dataset) -&gt; List[int]:\n    \"\"\"Gets info about dataset shape.\n\n    Args:\n        dataset: A Pytorch dataset\n\n    Returns:\n        A list of int containing\n            [&lt;nb_of_data&gt;, &lt;dimension_of_first_input_data&gt;].\n            Example for MNIST: [60000, 1, 28, 28], where &lt;nb_of_data&gt;=60000\n            and &lt;dimension_of_first_input_data&gt;=1, 28, 28\n    \"\"\"\n    return [len(dataset)] + list(dataset[0][0].shape)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.list_dlp","title":"list_dlp","text":"<pre><code>list_dlp(target_dataset_type=None)\n</code></pre> <p>Return all existing DataLoadingPlans.</p> <p>Parameters:</p> Name Type Description Default <code>target_dataset_type</code> <code>Optional[str]</code> <p>(str or None) if specified, return only dlps matching the requested target type.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[dict]</code> <p>An array of dict, each dict is a DataLoadingPlan</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def list_dlp(self, target_dataset_type: Optional[str] = None) -&gt; List[dict]:\n    \"\"\"Return all existing DataLoadingPlans.\n\n    Args:\n        target_dataset_type: (str or None) if specified, return only dlps matching the requested target type.\n\n    Returns:\n        An array of dict, each dict is a DataLoadingPlan\n    \"\"\"\n    if target_dataset_type is not None:\n        if not isinstance(target_dataset_type, str):\n            raise FedbiomedDatasetManagerError(f\"Wrong input type for target_dataset_type. \"\n                                               f\"Expected str, got {type(target_dataset_type)} instead.\")\n        if target_dataset_type not in [t.value for t in DatasetTypes]:\n            raise FedbiomedDatasetManagerError(\"target_dataset_type should be of the values defined in \"\n                                               \"fedbiomed.common.constants.DatasetTypes\")\n\n        return self._dlp_table.search(\n            (self._database.dlp_id.exists()) &amp;\n            (self._database.dlp_name.exists()) &amp;\n            (self._database.target_dataset_type == target_dataset_type))\n    else:\n        return self._dlp_table.search(\n            (self._database.dlp_id.exists()) &amp; (self._database.dlp_name.exists()))\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.list_my_data","title":"list_my_data","text":"<pre><code>list_my_data(verbose=True)\n</code></pre> <p>Lists all datasets on the node.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>Give verbose output. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[dict]</code> <p>All datasets in the node's database.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def list_my_data(self, verbose: bool = True) -&gt; List[dict]:\n    \"\"\"Lists all datasets on the node.\n\n    Args:\n        verbose: Give verbose output. Defaults to True.\n\n    Returns:\n        All datasets in the node's database.\n    \"\"\"\n    my_data = self._dataset_table.all()\n\n    # Do not display dtypes\n    for doc in my_data:\n        doc.pop('dtypes')\n\n    if verbose:\n        print(tabulate(my_data, headers='keys'))\n\n    return my_data\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.load_as_dataloader","title":"load_as_dataloader","text":"<pre><code>load_as_dataloader(dataset)\n</code></pre> <p>Loads content of an image dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>dict</code> <p>Description of the dataset.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>Content of the dataset.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def load_as_dataloader(self, dataset: dict) -&gt; torch.utils.data.Dataset:\n    \"\"\"Loads content of an image dataset.\n\n    Args:\n        dataset: Description of the dataset.\n\n    Returns:\n        Content of the dataset.\n    \"\"\"\n    name = dataset['data_type']\n    if name == 'default':\n        return self.load_default_database(name=dataset['name'],\n                                          path=dataset['path'],\n                                          as_dataset=True)\n    elif name == 'images':\n        return self.load_images_dataset(folder_path=dataset['path'],\n                                        as_dataset=True)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.load_csv_dataset","title":"load_csv_dataset","text":"<pre><code>load_csv_dataset(path)\n</code></pre> <p>Loads a CSV dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the CSV file.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pandas DataFrame with the content of the file.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def load_csv_dataset(self, path: str) -&gt; pd.DataFrame:\n    \"\"\"Loads a CSV dataset.\n\n    Args:\n        path: Path to the CSV file.\n\n    Returns:\n        Pandas DataFrame with the content of the file.\n    \"\"\"\n    return self.read_csv(path)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.load_default_database","title":"load_default_database","text":"<pre><code>load_default_database(name, path, as_dataset=False)\n</code></pre> <p>Loads a default dataset.</p> <p>Currently, only MNIST dataset is used as the default dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the default dataset. Currently, only MNIST is accepted.</p> required <code>path</code> <code>str</code> <p>Pathfile to MNIST dataset.</p> required <code>as_dataset</code> <code>bool</code> <p>Whether to return the complete dataset (True) or dataset dimensions (False). Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Name is not matching with the name of a default dataset.</p> <p>Returns:</p> Type Description <code>Union[List[int], Dataset]</code> <p>Depends on the value of the parameter <code>as_dataset</code>: If</p> <code>Union[List[int], Dataset]</code> <p>set to True,  returns dataset (type: torch.utils.data.Dataset).</p> <code>Union[List[int], Dataset]</code> <p>If set to False, returns the size of the dataset stored inside</p> <code>Union[List[int], Dataset]</code> <p>a list (type: List[int]).</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def load_default_database(self,\n                          name: str,\n                          path: str,\n                          as_dataset: bool = False) -&gt; Union[List[int],\n                                                             torch.utils.data.Dataset]:\n    \"\"\"Loads a default dataset.\n\n    Currently, only MNIST dataset is used as the default dataset.\n\n    Args:\n        name: Name of the default dataset. Currently,\n            only MNIST is accepted.\n        path: Pathfile to MNIST dataset.\n        as_dataset: Whether to return\n            the complete dataset (True) or dataset dimensions (False).\n            Defaults to False.\n\n    Raises:\n        NotImplementedError: Name is not matching with\n            the name of a default dataset.\n\n    Returns:\n        Depends on the value of the parameter `as_dataset`: If\n        set to True,  returns dataset (type: torch.utils.data.Dataset).\n        If set to False, returns the size of the dataset stored inside\n        a list (type: List[int]).\n    \"\"\"\n    kwargs = dict(root=path, download=True, transform=transforms.ToTensor())\n\n    if 'mnist' in name.lower():\n        dataset = datasets.MNIST(**kwargs)\n    else:\n        raise NotImplementedError(f'Default dataset `{name}` has'\n                                  'not been implemented.')\n    if as_dataset:\n        return dataset\n    else:\n        return self.get_torch_dataset_shape(dataset)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.load_images_dataset","title":"load_images_dataset","text":"<pre><code>load_images_dataset(folder_path, as_dataset=False)\n</code></pre> <p>Loads an image dataset.</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>Path to the directory containing the images.</p> required <code>as_dataset</code> <code>bool</code> <p>Whether to return the complete dataset (True) or dataset dimensions (False). Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List[int], Dataset]</code> <p>Depends on the value of the parameter <code>as_dataset</code>: If</p> <code>Union[List[int], Dataset]</code> <p>set to True,  returns dataset (type: torch.utils.data.Dataset).</p> <code>Union[List[int], Dataset]</code> <p>If set to False, returns the size of the dataset stored inside</p> <code>Union[List[int], Dataset]</code> <p>a list (type: List[int])</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def load_images_dataset(self,\n                        folder_path: str,\n                        as_dataset: bool = False) -&gt; Union[List[int],\n                                                           torch.utils.data.Dataset]:\n    \"\"\"Loads an image dataset.\n\n    Args:\n        folder_path: Path to the directory containing the images.\n        as_dataset: Whether to return\n            the complete dataset (True) or dataset dimensions (False).\n            Defaults to False.\n\n    Returns:\n        Depends on the value of the parameter `as_dataset`: If\n        set to True,  returns dataset (type: torch.utils.data.Dataset).\n        If set to False, returns the size of the dataset stored inside\n        a list (type: List[int])\n    \"\"\"\n    try:\n        dataset = datasets.ImageFolder(folder_path,\n                                       transform=transforms.ToTensor())\n    except Exception as e:\n        _msg = ErrorNumbers.FB315.value +\\\n            \"\\nThe following error was raised while loading dataset from the selected\" \\\n            \" path:  \" + str(e) + \"\\nPlease make sure that the selected folder is not empty \\\n            and doesn't have any empty class folder\"\n        logger.error(_msg)\n        raise FedbiomedDatasetManagerError(_msg)\n\n    if as_dataset:\n        return dataset\n    else:\n        return self.get_torch_dataset_shape(dataset)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.load_mednist_database","title":"load_mednist_database","text":"<pre><code>load_mednist_database(path, as_dataset=False)\n</code></pre> <p>Loads the MedNist dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Pathfile to save a local copy of the MedNist dataset.</p> required <code>as_dataset</code> <code>bool</code> <p>Whether to return the complete dataset (True) or dataset dimensions (False). Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>FedbiomedDatasetManagerError</code> <p>One of the following cases:</p> <ul> <li>tarfile cannot be downloaded</li> <li>downloaded tarfile cannot     be extracted</li> <li>MedNIST path is empty</li> <li>one of the classes path is empty</li> </ul> <p>Returns:</p> Type Description <code>Union[List[int], Dataset]</code> <p>Tuple of 2 items:</p> <code>str</code> <p>First item Depends on the value of the parameter <code>as_dataset</code>: If</p> <code>Tuple[Union[List[int], Dataset], str]</code> <p>set to True,  returns dataset (type: torch.utils.data.Dataset).</p> <code>Tuple[Union[List[int], Dataset], str]</code> <p>If set to False, returns the size of the dataset stored inside</p> <code>Tuple[Union[List[int], Dataset], str]</code> <p>a list (type: List[int])</p> <code>Tuple[Union[List[int], Dataset], str]</code> <p>Second item is the path used to download the MedNIST dataset, that needs to be saved as an</p> <code>Tuple[Union[List[int], Dataset], str]</code> <p>entry in the dataset</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def load_mednist_database(self,\n                          path: str,\n                          as_dataset: bool = False) -&gt; Tuple[Union[List[int],\n                                                        torch.utils.data.Dataset], str]:\n    \"\"\"Loads the MedNist dataset.\n\n    Args:\n        path: Pathfile to save a local copy of the MedNist dataset.\n        as_dataset: Whether to return\n            the complete dataset (True) or dataset dimensions (False).\n            Defaults to False.\n\n    Raises:\n        FedbiomedDatasetManagerError: One of the following cases:\n\n            - tarfile cannot be downloaded\n            - downloaded tarfile cannot\n                be extracted\n            - MedNIST path is empty\n            - one of the classes path is empty\n\n    Returns:\n        Tuple of 2 items:\n        First item Depends on the value of the parameter `as_dataset`: If\n        set to True,  returns dataset (type: torch.utils.data.Dataset).\n        If set to False, returns the size of the dataset stored inside\n        a list (type: List[int])\n        Second item is the path used to download the MedNIST dataset, that needs to be saved as an\n        entry in the dataset\n    \"\"\"\n    download_path = os.path.join(path, 'MedNIST')\n    if not os.path.isdir(download_path):\n        url = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz\"\n        filepath = os.path.join(path, 'MedNIST.tar.gz')\n        try:\n            logger.info(\"Now downloading MEDNIST...\")\n            urlretrieve(url, filepath)\n            with tarfile.open(filepath) as tar_file:\n                logger.info(\"Now extracting MEDNIST...\")\n                tar_file.extractall(path)\n            os.remove(filepath)\n\n        except (URLError, HTTPError, ContentTooShortError, OSError, tarfile.TarError,\n                MemoryError) as e:\n            _msg = ErrorNumbers.FB315.value + \"\\nThe following error was raised while downloading MedNIST dataset\"\\\n                + \"from the MONAI repo:  \" + str(e)\n            logger.error(_msg)\n            raise FedbiomedDatasetManagerError(_msg)\n\n    try:\n        dataset = datasets.ImageFolder(download_path,\n                                       transform=transforms.ToTensor())\n\n    except (FileNotFoundError, RuntimeError) as e:\n        _msg = ErrorNumbers.FB315.value + \"\\nThe following error was raised while loading MedNIST dataset from\"\\\n            \"the selected path:  \" + str(e) + \"\\nPlease make sure that the selected MedNIST folder is not empty \\\n               or choose another path.\"\n        logger.error(_msg)\n        raise FedbiomedDatasetManagerError(_msg)\n\n    except Exception as e:\n        _msg = ErrorNumbers.FB315.value + \"\\nThe following error was raised while loading MedNIST dataset\" + str(e)\n        logger.error(_msg)\n        raise FedbiomedDatasetManagerError(_msg)\n\n    if as_dataset:\n        return dataset, download_path\n    else:\n        return self.get_torch_dataset_shape(dataset), download_path\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.modify_database_info","title":"modify_database_info","text":"<pre><code>modify_database_info(dataset_id, modified_dataset)\n</code></pre> <p>Modifies a dataset in the database.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>ID of the dataset to modify.</p> required <code>modified_dataset</code> <code>dict</code> <p>New dataset description to replace the existing one.</p> required <p>Raises:</p> Type Description <code>FedbiomedDatasetManagerError</code> <p>conflicting tags with existing dataset</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def modify_database_info(self,\n                         dataset_id: str,\n                         modified_dataset: dict):\n    \"\"\"Modifies a dataset in the database.\n\n    Args:\n        dataset_id: ID of the dataset to modify.\n        modified_dataset: New dataset description to replace the existing one.\n\n    Raises:\n        FedbiomedDatasetManagerError: conflicting tags with existing dataset\n    \"\"\"\n    # Check that there are not existing dataset with conflicting tags\n    if 'tags' in modified_dataset:\n        conflicting = self.search_conflicting_tags(modified_dataset['tags'])\n\n        conflicting_ids = [ c['dataset_id'] for c in conflicting ]\n        # the dataset to modify is ignored (can conflict with its previous tags)\n        if dataset_id in conflicting_ids:\n            conflicting_ids.remove(dataset_id)\n\n        if len(conflicting_ids) &gt; 0:\n            msg = f\"{ErrorNumbers.FB322.value}, one or more registered dataset has conflicting tags: \" \\\n                f\" {' '.join([ c['name'] for c in conflicting if c['dataset_id'] != dataset_id ])}\"\n            logger.critical(msg)\n            raise FedbiomedDatasetManagerError(msg)\n\n    self._dataset_table.update(modified_dataset, self._database.dataset_id == dataset_id)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.obfuscate_private_information","title":"obfuscate_private_information  <code>staticmethod</code>","text":"<pre><code>obfuscate_private_information(database_metadata)\n</code></pre> <p>Remove privacy-sensitive information, to prepare for sharing with a researcher.</p> <p>Removes any information that could be considered privacy-sensitive by the node. The typical use-case is to prevent sharing this information with a researcher through a reply message.</p> <p>Parameters:</p> Name Type Description Default <code>database_metadata</code> <code>Iterable[dict]</code> <p>an iterable of metadata information objects, one per dataset. Each metadata object should be in the format af key-value pairs, such as e.g. a dict.</p> required <p>Returns:      the updated iterable of metadata information objects without privacy-sensitive information</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>@staticmethod\ndef obfuscate_private_information(database_metadata: Iterable[dict]) -&gt; Iterable[dict]:\n    \"\"\"Remove privacy-sensitive information, to prepare for sharing with a researcher.\n\n    Removes any information that could be considered privacy-sensitive by the node. The typical use-case is to\n    prevent sharing this information with a researcher through a reply message.\n\n    Args:\n        database_metadata: an iterable of metadata information objects, one per dataset. Each metadata object\n            should be in the format af key-value pairs, such as e.g. a dict.\n    Returns:\n         the updated iterable of metadata information objects without privacy-sensitive information\n    \"\"\"\n    for d in database_metadata:\n        try:\n            # common obfuscations\n            d.pop('path', None)\n            # obfuscations specific for each data type\n            if 'data_type' in d:\n                if d['data_type'] == 'medical-folder':\n                    if 'dataset_parameters' in d:\n                        d['dataset_parameters'].pop('tabular_file', None)\n        except AttributeError:\n            raise FedbiomedDatasetManagerError(f\"Object of type {type(d)} does not support pop or getitem method \"\n                                               f\"in obfuscate_private_information.\")\n    return database_metadata\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.read_csv","title":"read_csv","text":"<pre><code>read_csv(csv_file, index_col=None)\n</code></pre> <p>Gets content of a CSV file.</p> <p>Reads a *.csv file and outputs its data into a pandas DataFrame. Finds automatically the CSV delimiter by parsing the first line.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>File name / path</p> required <code>index_col</code> <code>Union[int, None]</code> <p>Column that contains CSV file index. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pandas DataFrame with data contained in CSV file.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def read_csv(self, csv_file: str, index_col: Union[int, None] = None) -&gt; pd.DataFrame:\n    \"\"\"Gets content of a CSV file.\n\n    Reads a *.csv file and outputs its data into a pandas DataFrame.\n    Finds automatically the CSV delimiter by parsing the first line.\n\n    Args:\n        csv_file: File name / path\n        index_col: Column that contains CSV file index.\n            Defaults to None.\n\n    Returns:\n        Pandas DataFrame with data contained in CSV file.\n    \"\"\"\n\n    # Automatically identify separator and header\n    sniffer = csv.Sniffer()\n    with open(csv_file, 'r') as file:\n        delimiter = sniffer.sniff(file.readline()).delimiter\n        file.seek(0)\n        header = 0 if sniffer.has_header(file.read()) else None\n\n    return pd.read_csv(csv_file, index_col=index_col, sep=delimiter, header=header)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.remove_database","title":"remove_database","text":"<pre><code>remove_database(dataset_id)\n</code></pre> <p>Removes a dataset from database.</p> <p>Only the dataset matching the <code>dataset_id</code> should be removed.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset unique ID.</p> required Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def remove_database(self, dataset_id: str):\n    \"\"\"Removes a dataset from database.\n\n    Only the dataset matching the `dataset_id` should be removed.\n\n    Args:\n        dataset_id: Dataset unique ID.\n    \"\"\"\n    # TODO: check that there is no more than one dataset with `dataset_id` (consistency, should not happen)\n    _, dataset_document = self._dataset_table.get(self._database.dataset_id == dataset_id, add_docs=True)\n\n    if dataset_document:\n        self._dataset_table.remove(doc_ids=[dataset_document.doc_id])\n    else:\n        _msg = ErrorNumbers.FB322.value + f\": No dataset found with id {dataset_id}\"\n        logger.error(_msg)\n        raise FedbiomedDatasetManagerError(_msg)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.remove_dlp_by_id","title":"remove_dlp_by_id","text":"<pre><code>remove_dlp_by_id(dlp_id)\n</code></pre> <p>Removes a data loading plan (DLP) from the database.</p> <p>Only DLP with matching ID is removed from the database. There should be at most one.</p> <p>If <code>remove_dlbs</code> is True, also remove the attached DLBs. You should ensure they are not used by another DLP, no verification is made.</p> <p>Parameters:</p> Name Type Description Default <code>dlp_id</code> <code>str</code> <p>the DataLoadingPlan id</p> required Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def remove_dlp_by_id(self, dlp_id: str):\n    \"\"\"Removes a data loading plan (DLP) from the database.\n\n    Only DLP with matching ID is removed from the database. There should be at most one.\n\n    If `remove_dlbs` is True, also remove the attached DLBs. You should ensure\n    they are not used by another DLP, no verification is made.\n\n    Args:\n        dlp_id: the DataLoadingPlan id\n    \"\"\"\n    if not isinstance(dlp_id, str):\n        _msg = ErrorNumbers.FB316.value + f\": Bad type for dlp '{type(dlp_id)}', expecting str\"\n        logger.error(_msg)\n        raise FedbiomedDatasetManagerError(_msg)\n    if not str:\n        _msg = ErrorNumbers.FB316.value + \": Bad value for dlp, expecting non empty str\"\n        logger.error(_msg)\n        raise FedbiomedDatasetManagerError(_msg)\n\n    _ , dlbs = self.get_dlp_by_id(dlp_id)\n    try:\n        self._dlp_table.remove(self._database.dlp_id == dlp_id)\n        for dlb in dlbs:\n            self._dlp_table.remove(self._database.dlb_id == dlb['dlb_id'])\n    except Exception as e:\n        _msg = ErrorNumbers.FB316.value + f\": Error during remove of DLP {dlp_id}: {e}\"\n        logger.error(_msg)\n        raise FedbiomedDatasetManagerError(_msg)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.save_data_loading_block","title":"save_data_loading_block","text":"<pre><code>save_data_loading_block(dlb)\n</code></pre> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def save_data_loading_block(self, dlb: DataLoadingBlock) -&gt; None:\n    # seems unused\n    self._dlp_table.insert(dlb.serialize())\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.save_data_loading_plan","title":"save_data_loading_plan","text":"<pre><code>save_data_loading_plan(data_loading_plan)\n</code></pre> <p>Save a DataLoadingPlan to the database.</p> <p>This function saves a DataLoadingPlan to the database, and returns its ID.</p> <p>Raises:</p> Type Description <code>FedbiomedDatasetManagerError</code> <p>bad data loading plan name (size, not unique)</p> <p>Parameters:</p> Name Type Description Default <code>data_loading_plan</code> <code>Optional[DataLoadingPlan]</code> <p>the DataLoadingPlan to be saved, or None.</p> required <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>The <code>dlp_id</code> if a DLP was saved, or None</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def save_data_loading_plan(self,\n                           data_loading_plan: Optional[DataLoadingPlan]\n                           ) -&gt; Union[str, None]:\n    \"\"\"Save a DataLoadingPlan to the database.\n\n    This function saves a DataLoadingPlan to the database, and returns its ID.\n\n    Raises:\n        FedbiomedDatasetManagerError: bad data loading plan name (size, not unique)\n\n    Args:\n        data_loading_plan: the DataLoadingPlan to be saved, or None.\n\n    Returns:\n        The `dlp_id` if a DLP was saved, or None\n    \"\"\"\n    if data_loading_plan is None:\n        return None\n\n    if len(data_loading_plan.desc) &lt; 4:\n        _msg = ErrorNumbers.FB316.value + \": Cannot save data loading plan, \" + \\\n            \"DLP name needs to have at least 4 characters.\"\n        logger.error(_msg)\n        raise FedbiomedDatasetManagerError(_msg)\n\n    _dlp_same_name = self._dlp_table.search(\n        (self._database.dlp_id.exists()) &amp; (self._database.dlp_name.exists()) &amp;\n        (self._database.dlp_name == data_loading_plan.desc))\n    if _dlp_same_name:\n        _msg = ErrorNumbers.FB316.value + \": Cannot save data loading plan, \" + \\\n            \"DLP name needs to be unique.\"\n        logger.error(_msg)\n        raise FedbiomedDatasetManagerError(_msg)\n\n    dlp_metadata, loading_blocks_metadata = data_loading_plan.serialize()\n    self._dlp_table.insert(dlp_metadata)\n    self._dlp_table.insert_multiple(loading_blocks_metadata)\n    return data_loading_plan.dlp_id\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.search_by_tags","title":"search_by_tags","text":"<pre><code>search_by_tags(tags)\n</code></pre> <p>Searches for data with given tags.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Union[tuple, list]</code> <p>List of tags</p> required <p>Returns:</p> Type Description <code>list</code> <p>The list of matching datasets</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def search_by_tags(self, tags: Union[tuple, list]) -&gt; list:\n    \"\"\"Searches for data with given tags.\n\n    Args:\n        tags:  List of tags\n\n    Returns:\n        The list of matching datasets\n    \"\"\"\n    return self._dataset_table.search(self._database.tags.all(tags))\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.search_conflicting_tags","title":"search_conflicting_tags","text":"<pre><code>search_conflicting_tags(tags)\n</code></pre> <p>Searches for registered data that have conflicting tags with the given tags</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Union[tuple, list]</code> <p>List of tags</p> required <p>Returns:</p> Type Description <code>list</code> <p>The list of conflicting datasets</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def search_conflicting_tags(self, tags: Union[tuple, list]) -&gt; list:\n    \"\"\"Searches for registered data that have conflicting tags with the given tags\n\n    Args:\n        tags:  List of tags\n\n    Returns:\n        The list of conflicting datasets\n    \"\"\"\n    def _conflicting_tags(val):\n        return all(t in val for t in tags) or all(t in tags for t in val)\n\n\n    return self._dataset_table.search(self._database.tags.test(_conflicting_tags))\n</code></pre>"},{"location":"developer/api/node/history_monitor/","title":"HistoryMonitor","text":""},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor--this-file-is-originally-part-of-fed-biomed","title":"This file is originally part of Fed-BioMed","text":""},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor--spdx-license-identifier-apache-20","title":"SPDX-License-Identifier: Apache-2.0","text":"<p>Send information from node to researcher during the training</p>"},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor-classes","title":"Classes","text":""},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor.HistoryMonitor","title":"HistoryMonitor","text":"<pre><code>HistoryMonitor(node_id, experiment_id, researcher_id, send)\n</code></pre> <p>Send information from node to researcher during the training</p> <p>Parameters:</p> Name Type Description Default <code>nod_id</code> <p>Unique ID of this node</p> required <code>experiment_id</code> <code>str</code> <p>TODO</p> required <code>researcher_id</code> <code>str</code> <p>TODO</p> required <code>client</code> <p>TODO</p> required Source code in <code>fedbiomed/node/history_monitor.py</code> <pre><code>def __init__(self,\n             node_id: str,\n             experiment_id: str,\n             researcher_id: str,\n             send: Callable):\n    \"\"\"Simple constructor for the class.\n\n    Args:\n        nod_id: Unique ID of this node\n        experiment_id: TODO\n        researcher_id: TODO\n        client: TODO\n    \"\"\"\n    self._node_id = node_id\n    self.experiment_id = experiment_id\n    self.researcher_id = researcher_id\n    self.send = send\n</code></pre>"},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor.HistoryMonitor-attributes","title":"Attributes","text":""},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor.HistoryMonitor.experiment_id","title":"experiment_id  <code>instance-attribute</code>","text":"<pre><code>experiment_id = experiment_id\n</code></pre>"},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor.HistoryMonitor.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id = researcher_id\n</code></pre>"},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor.HistoryMonitor.send","title":"send  <code>instance-attribute</code>","text":"<pre><code>send = send\n</code></pre>"},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor.HistoryMonitor-functions","title":"Functions","text":""},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor.HistoryMonitor.add_scalar","title":"add_scalar","text":"<pre><code>add_scalar(metric, iteration, epoch, total_samples, batch_samples, num_batches, num_samples_trained=None, train=False, test=False, test_on_global_updates=False, test_on_local_updates=False)\n</code></pre> <p>Adds a scalar value to the monitor, and sends an 'AddScalarReply'     response to researcher.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Dict[str, Union[int, float]]</code> <p>recorded value</p> required <code>iteration</code> <code>int</code> <p>current epoch iteration.</p> required <code>epoch</code> <code>int</code> <p>current epoch</p> required <code>total_samples</code> <code>int</code> <p>TODO</p> required <code>batch_samples</code> <code>int</code> <p>TODO</p> required <code>num_batches</code> <code>int</code> <p>TODO</p> required <code>num_samples_trained</code> <code>int</code> <p>TODO</p> <code>None</code> <code>train</code> <code>bool</code> <p>TODO</p> <code>False</code> <code>test</code> <code>bool</code> <p>TODO</p> <code>False</code> <code>test_on_global_updates</code> <code>bool</code> <p>TODO</p> <code>False</code> <code>test_on_local_updates</code> <code>bool</code> <p>TODO</p> <code>False</code> Source code in <code>fedbiomed/node/history_monitor.py</code> <pre><code>def add_scalar(\n        self,\n        metric: Dict[str, Union[int, float]],\n        iteration: int,\n        epoch: int,\n        total_samples: int,\n        batch_samples: int,\n        num_batches: int,\n        num_samples_trained: int = None,\n        train: bool = False,\n        test: bool = False,\n        test_on_global_updates: bool = False,\n        test_on_local_updates: bool = False\n) -&gt; None:\n    \"\"\"Adds a scalar value to the monitor, and sends an 'AddScalarReply'\n        response to researcher.\n\n    Args:\n        metric:  recorded value\n        iteration: current epoch iteration.\n        epoch: current epoch\n        total_samples: TODO\n        batch_samples: TODO\n        num_batches: TODO\n        num_samples_trained: TODO\n        train: TODO\n        test: TODO\n        test_on_global_updates: TODO\n        test_on_local_updates: TODO\n\n    \"\"\"\n    self.send(\n        FeedbackMessage(researcher_id=self.researcher_id,\n                        scalar=Scalar(**{\n                            'node_id': self._node_id,\n                            'experiment_id': self.experiment_id,\n                            'train': train,\n                            'test': test,\n                            'test_on_global_updates': test_on_global_updates,\n                            'test_on_local_updates': test_on_local_updates,\n                            'metric': metric,\n                            'iteration': iteration,\n                            'epoch': epoch,\n                            'num_samples_trained': num_samples_trained,\n                            'total_samples': total_samples,\n                            'batch_samples': batch_samples,\n                            'num_batches': num_batches}\n                        ))\n    )\n</code></pre>"},{"location":"developer/api/node/node/","title":"Node","text":"<p>Core code of the node component.</p>"},{"location":"developer/api/node/node/#fedbiomed.node.node-attributes","title":"Attributes","text":""},{"location":"developer/api/node/node/#fedbiomed.node.node-classes","title":"Classes","text":""},{"location":"developer/api/node/node/#fedbiomed.node.node.Node","title":"Node","text":"<pre><code>Node(config, node_args=None)\n</code></pre> <p>Core code of the node component.</p> <p>Defines the behaviour of the node, while communicating with the researcher through the <code>Messaging</code>, parsing messages from the researcher, etiher treating them instantly or queuing them, executing tasks requested by researcher stored in the queue.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>Node configuration</p> <code>node_args</code> <p>Command line arguments for node.</p> Source code in <code>fedbiomed/node/node.py</code> <pre><code>def __init__(\n    self,\n    config: NodeConfig,\n    node_args: Union[dict, None] = None,\n):\n    \"\"\"Constructor of the class.\n\n    Attributes:\n        config: Node configuration\n        node_args: Command line arguments for node.\n    \"\"\"\n\n    self._config = config\n    self._node_id = self._config.get('default', 'id')\n    self._tasks_queue = TasksQueue(\n        os.path.join(self._config.root, 'var', f'queue_{self._node_id}'),\n        str(os.path.join(self._config.root, 'var', 'tmp'))\n    )\n\n    self._grpc_client = GrpcController(\n        node_id=self._node_id,\n        researchers=[\n            ResearcherCredentials(\n                port=self._config.get('researcher', 'port'),\n                host=self._config.get('researcher', 'ip'),\n                certificate=self._config.get('researcher', 'certificate', fallback=None)\n            )\n        ],\n        on_message=self.on_message,\n    )\n    self._db_path = os.path.abspath(os.path.join(\n        self._config.root, CONFIG_FOLDER_NAME, self._config.get('default', 'db'))\n    )\n\n    self._pending_requests = EventWaitExchange(remove_delivered=True)\n    self._controller_data = EventWaitExchange(remove_delivered=False)\n    self._n2n_router = NodeToNodeRouter(\n        self._node_id,\n        self._db_path,\n        self._grpc_client,\n        self._pending_requests,\n        self._controller_data\n    )\n\n\n    self.dataset_manager = DatasetManager(db=self._db_path)\n    self.tp_security_manager = TrainingPlanSecurityManager(\n        db=self._db_path,\n        node_id=self._node_id,\n        hashing=self._config.get('security', 'hashing_algorithm'),\n        tp_approval=self._config.getbool('security', 'training_plan_approval')\n    )\n\n    self.node_args = node_args\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node-attributes","title":"Attributes","text":""},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.config","title":"config  <code>property</code>","text":"<pre><code>config\n</code></pre> <p>Return node config</p>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.dataset_manager","title":"dataset_manager  <code>instance-attribute</code>","text":"<pre><code>dataset_manager = DatasetManager(db=_db_path)\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.node_args","title":"node_args  <code>instance-attribute</code>","text":"<pre><code>node_args = node_args\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.node_id","title":"node_id  <code>property</code>","text":"<pre><code>node_id\n</code></pre> <p>Returns id of the node</p>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.tp_security_manager","title":"tp_security_manager  <code>instance-attribute</code>","text":"<pre><code>tp_security_manager = TrainingPlanSecurityManager(db=_db_path, node_id=_node_id, hashing=get('security', 'hashing_algorithm'), tp_approval=getbool('security', 'training_plan_approval'))\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node-functions","title":"Functions","text":""},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.add_task","title":"add_task","text":"<pre><code>add_task(task)\n</code></pre> <p>Adds a task to the pending tasks queue.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>dict</code> <p>A <code>Message</code> object describing a training task</p> required Source code in <code>fedbiomed/node/node.py</code> <pre><code>def add_task(self, task: dict):\n    \"\"\"Adds a task to the pending tasks queue.\n\n    Args:\n        task: A `Message` object describing a training task\n    \"\"\"\n    self._tasks_queue.add(task)\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.is_connected","title":"is_connected","text":"<pre><code>is_connected()\n</code></pre> <p>Checks if node is ready for communication with researcher</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if node is ready, False if node is not ready</p> Source code in <code>fedbiomed/node/node.py</code> <pre><code>def is_connected(self) -&gt; bool:\n    \"\"\"Checks if node is ready for communication with researcher\n\n    Returns:\n        True if node is ready, False if node is not ready\n    \"\"\"\n    return self._grpc_client.is_connected()\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.on_message","title":"on_message","text":"<pre><code>on_message(msg)\n</code></pre> <p>Handler to be used with <code>Messaging</code> class (ie the messager).</p> <p>Called when a  message arrives through the <code>Messaging</code>. It reads and triggers instructions received by node from Researcher, mainly: - ping requests, - train requests (then a new task will be added on node's task queue), - search requests (for searching data in node's database).</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict</code> <p>Incoming message from Researcher.</p> required Source code in <code>fedbiomed/node/node.py</code> <pre><code>def on_message(self, msg: dict):\n    \"\"\"Handler to be used with `Messaging` class (ie the messager).\n\n    Called when a  message arrives through the `Messaging`.\n    It reads and triggers instructions received by node from Researcher,\n    mainly:\n    - ping requests,\n    - train requests (then a new task will be added on node's task queue),\n    - search requests (for searching data in node's database).\n\n    Args:\n        msg: Incoming message from Researcher.\n    \"\"\"\n    message: Message\n    try:\n        message = Message.from_dict(msg)\n    except FedbiomedError as e:\n        logger.error(e)  # Message was not properly formatted\n        resid = msg.get(\"researcher_id\", \"unknown_researcher_id\")\n        self.send_error(\n            ErrorNumbers.FB301,\n            extra_msg=\"Message was not properly formatted\",\n            researcher_id=resid,\n        )\n    else:\n        no_print = [\n            \"aggregator_args\",\n            \"optim_aux_var\",\n            \"params\",\n            \"training_plan\",\n            \"overlay\",\n        ]\n        msg_print = {\n            key: value\n            for key, value in message.get_dict().items()\n            if key not in no_print\n        }\n        logger.debug(\"Message received: \" + str(msg_print))\n\n        match message.__name__:\n\n            case (\n                TrainRequest.__name__\n                | SecaggRequest.__name__\n                | AdditiveSSSetupRequest.__name__\n            ):\n                self.add_task(message)\n            case SecaggDeleteRequest.__name__:\n                self._task_secagg_delete(message)\n            case OverlayMessage.__name__:\n                self._n2n_router.submit(message)\n            case SearchRequest.__name__:\n                databases = self.dataset_manager.search_by_tags(message.tags)\n                if len(databases) != 0:\n                    databases = self.dataset_manager.obfuscate_private_information(\n                        databases\n                    )\n                self._grpc_client.send(\n                    SearchReply(\n                        request_id=message.request_id,\n                        node_id=self._node_id,\n                        researcher_id=message.researcher_id,\n                        databases=databases,\n                        count=len(databases),\n                    )\n                )\n            case ListRequest.__name__:\n                # Get list of all datasets\n                databases = self.dataset_manager.list_my_data(verbose=False)\n                databases = self.dataset_manager.obfuscate_private_information(databases)\n                self._grpc_client.send(\n                    ListReply(\n                        success=True,\n                        request_id=message.request_id,\n                        node_id=self._node_id,\n                        researcher_id=message.researcher_id,\n                        databases=databases,\n                        count=len(databases),\n                    )\n                )\n\n            case PingRequest.__name__:\n                self._grpc_client.send(\n                    PingReply(\n                        request_id=message.request_id,\n                        researcher_id=message.researcher_id,\n                        node_id=self._node_id,\n                    )\n                )\n            case ApprovalRequest.__name__:\n                reply = (\n                    self.tp_security_manager.reply_training_plan_approval_request(\n                        message\n                    )\n                )\n                self._grpc_client.send(reply)\n            case TrainingPlanStatusRequest.__name__:\n                reply = self.tp_security_manager.reply_training_plan_status_request(\n                    message\n                )\n                self._grpc_client.send(reply)\n            case _:\n                resid = msg.get(\"researcher_id\", \"unknown_researcher_id\")\n                self.send_error(\n                    ErrorNumbers.FB301,\n                    extra_msg=\"This request handler is not implemented \"\n                    f\"{message.__class__.__name__} is not implemented\",\n                    researcher_id=resid,\n                )\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.parser_task_train","title":"parser_task_train","text":"<pre><code>parser_task_train(msg)\n</code></pre> <p>Parses a given training task message to create a round instance</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>TrainRequest</code> <p><code>TrainRequest</code> message object to parse</p> required <p>Returns:</p> Type Description <code>Union[Round, None]</code> <p>a <code>Round</code> object for the training to perform, or None if no training</p> Source code in <code>fedbiomed/node/node.py</code> <pre><code>def parser_task_train(self, msg: TrainRequest) -&gt; Union[Round, None]:\n    \"\"\"Parses a given training task message to create a round instance\n\n    Args:\n        msg: `TrainRequest` message object to parse\n\n    Returns:\n        a `Round` object for the training to perform, or None if no training\n    \"\"\"\n    round_ = None\n    hist_monitor = HistoryMonitor(\n        node_id=self._node_id,\n        experiment_id=msg.experiment_id,\n        researcher_id=msg.researcher_id,\n        send=self._grpc_client.send,\n    )\n\n    dataset_id = msg.get_param(\"dataset_id\")\n    data = self.dataset_manager.get_by_id(dataset_id)\n\n    if data is None:\n        return self.send_error(\n            extra_msg=\"Did not found proper data in local datasets \"\n            f'on node={self._node_id}',\n            request_id=msg.request_id,\n            researcher_id=msg.researcher_id,\n            errnum=ErrorNumbers.FB313,\n        )\n\n    dlp_and_loading_block_metadata = None\n    if \"dlp_id\" in data:\n        dlp_and_loading_block_metadata = self.dataset_manager.get_dlp_by_id(\n            data[\"dlp_id\"]\n        )\n\n    round_ = Round(\n        root_dir=self._config.root,\n        db=self._db_path,\n        node_id=self._node_id,\n        training_plan=msg.get_param(\"training_plan\"),\n        training_plan_class=msg.get_param(\"training_plan_class\"),\n        model_kwargs=msg.get_param(\"model_args\") or {},\n        training_kwargs=msg.get_param(\"training_args\") or {},\n        training=msg.get_param(\"training\") or False,\n        dataset=data,\n        params=msg.get_param(\"params\"),\n        experiment_id=msg.get_param(\"experiment_id\"),\n        researcher_id=msg.get_param(\"researcher_id\"),\n        history_monitor=hist_monitor,\n        aggregator_args=msg.get_param(\"aggregator_args\") or None,\n        node_args=self.node_args,\n        tp_security_manager=self.tp_security_manager,\n        round_number=msg.get_param(\"round\"),\n        dlp_and_loading_block_metadata=dlp_and_loading_block_metadata,\n        aux_vars=msg.get_param('optim_aux_var'),\n    )\n\n    # the round raises an error if it cannot initialize\n    err_msg = round_.initialize_arguments(msg.get_param(\"state_id\"))\n    if err_msg is not None:\n        self._grpc_client.send(\n            ErrorMessage(\n                node_id=self._node_id,\n                errnum=ErrorNumbers.FB300,\n                researcher_id=msg.researcher_id,\n                extra_msg=\"Could not initialize arguments\",\n            )\n        )\n\n    return round_\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.send_error","title":"send_error","text":"<pre><code>send_error(errnum=ErrorNumbers.FB300, extra_msg='', researcher_id='&lt;unknown&gt;', broadcast=False, request_id=None)\n</code></pre> <p>Sends an error message.</p> <p>Parameters:</p> Name Type Description Default <code>errnum</code> <code>ErrorNumbers</code> <p>Code of the error.</p> <code>FB300</code> <code>extra_msg</code> <code>str</code> <p>Additional human readable error message.</p> <code>''</code> <code>researcher_id</code> <code>str</code> <p>Destination researcher.</p> <code>'&lt;unknown&gt;'</code> <code>broadcast</code> <code>bool</code> <p>Broadcast the message all available researchers regardless of specific researcher.</p> <code>False</code> <code>request_id</code> <code>str</code> <p>Optional request i to reply as error to a request.</p> <code>None</code> Source code in <code>fedbiomed/node/node.py</code> <pre><code>def send_error(\n    self,\n    errnum: ErrorNumbers = ErrorNumbers.FB300,\n    extra_msg: str = \"\",\n    researcher_id: str = \"&lt;unknown&gt;\",\n    broadcast: bool = False,\n    request_id: str = None,\n):\n    \"\"\"Sends an error message.\n\n    Args:\n        errnum: Code of the error.\n        extra_msg: Additional human readable error message.\n        researcher_id: Destination researcher.\n        broadcast: Broadcast the message all available researchers\n            regardless of specific researcher.\n        request_id: Optional request i to reply as error to a request.\n    \"\"\"\n    try:\n        logger.error(extra_msg)\n        self._grpc_client.send(\n            ErrorMessage(\n                request_id=request_id,\n                errnum=errnum.name,\n                node_id=self._node_id,\n                extra_msg=extra_msg,\n                researcher_id=researcher_id,\n            ),\n            broadcast=broadcast,\n        )\n    except Exception as e:\n        logger.error(f\"{ErrorNumbers.FB601.value}: Cannot send error message: {e}\")\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.start_messaging","title":"start_messaging","text":"<pre><code>start_messaging(on_finish=None)\n</code></pre> <p>Calls the start method of messaging class.</p> <p>Parameters:</p> Name Type Description Default <code>on_finish</code> <code>Optional[Callable]</code> <p>Called when the tasks for handling all known researchers have finished. Callable has no argument.</p> <code>None</code> Source code in <code>fedbiomed/node/node.py</code> <pre><code>def start_messaging(self, on_finish: Optional[Callable] = None):\n    \"\"\"Calls the start method of messaging class.\n\n    Args:\n        on_finish: Called when the tasks for handling all known researchers have finished.\n            Callable has no argument.\n    \"\"\"\n    self._grpc_client.start(on_finish)\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.start_protocol","title":"start_protocol","text":"<pre><code>start_protocol()\n</code></pre> <p>Start the node to node router thread, for handling node to node message</p> Source code in <code>fedbiomed/node/node.py</code> <pre><code>def start_protocol(self) -&gt; None:\n    \"\"\"Start the node to node router thread, for handling node to node message\"\"\"\n    self._n2n_router.start()\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.task_manager","title":"task_manager","text":"<pre><code>task_manager()\n</code></pre> <p>Manages training tasks in the queue.</p> Source code in <code>fedbiomed/node/node.py</code> <pre><code>def task_manager(self):\n    \"\"\"Manages training tasks in the queue.\"\"\"\n\n    while True:\n\n        item: TrainRequest = self._tasks_queue.get()\n        # don't want to treat again in case of failure\n        self._tasks_queue.task_done()\n\n        logger.info(\n            f\"[TASKS QUEUE] Task received by task manager: \"\n            f\"Researcher: {item.researcher_id} \"\n            f\"Experiment: {item.experiment_id}\"\n        )\n        try:\n\n            match item.__name__:\n                case TrainRequest.__name__:\n                    round_ = self.parser_task_train(item)\n                    # once task is out of queue, initiate training rounds\n                    if round_ is not None:\n                        msg = round_.run_model_training(\n                            tp_approval=self._config.getbool('security', 'training_plan_approval'),\n                            secagg_insecure_validation=self._config.getbool('security',\n                                \"secagg_insecure_validation\"),\n                            secagg_active=self._config.getbool(\"security\", \"secure_aggregation\"),\n                            force_secagg=self._config.getbool(\n                                \"security\", \"force_secure_aggregation\"),\n                            secagg_arguments=item.get_param(\"secagg_arguments\"),\n                        )\n                        msg.request_id = item.request_id\n                        self._grpc_client.send(msg)\n                        del round_\n\n                case SecaggRequest.__name__ | AdditiveSSSetupRequest.__name__:\n                    self._task_secagg(item)\n                case _:\n                    errmess = f\"{ErrorNumbers.FB319.value}: Undefined request message\"\n                    self.send_error(errnum=ErrorNumbers.FB319, extra_msg=errmess)\n\n        # TODO: Test exception\n        except Exception as e:\n            self.send_error(\n                request_id=item.request_id,\n                researcher_id=item.researcher_id,\n                errnum=ErrorNumbers.FB300,\n                extra_msg=\"Round error: \" + str(e),\n        )\n</code></pre>"},{"location":"developer/api/node/node_state_manager/","title":"NodeStateManager","text":""},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager-attributes","title":"Attributes","text":""},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NODE_STATE_TABLE_NAME","title":"NODE_STATE_TABLE_NAME  <code>module-attribute</code>","text":"<pre><code>NODE_STATE_TABLE_NAME = 'Node_states'\n</code></pre>"},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager-classes","title":"Classes","text":""},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateFileName","title":"NodeStateFileName","text":"<p>               Bases: <code>_BaseEnum</code></p> <p>Collection of File names that composes a <code>Node</code> state. File names should contains 2 <code>%s</code>: one for round number, the second for state_id Example: VALUE = \"some_values_%s_%s\"</p>"},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateFileName-attributes","title":"Attributes","text":""},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateFileName.OPTIMIZER","title":"OPTIMIZER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OPTIMIZER = 'optim_state_%s_%s'\n</code></pre>"},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateManager","title":"NodeStateManager","text":"<pre><code>NodeStateManager(dir, node_id, db_path)\n</code></pre> <p>Node state saving facility: Handles saving and loading Node states from previous <code>Rounds</code>, given a <code>state_id</code>. <code>NodeStateManager</code> ensures that states are not reset from one <code>Round</code> to another. Currently a state is composed of the Optimizer state - only for <code>DeclearnOptimizer</code>, but it will be extended in the future with other components - such as model layers, validation dataset, ...</p> <p>Interfaces with database use to save and load Node State entries.</p> Database Table that handles Node states is built with the following entries <ul> <li>state_id</li> <li>experiment_id</li> <li>version_node_id: state id version, that checks if states can be used regarding FedBioMed versions.</li> <li>state_entries: content of each entry that cmposes a Node State</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Root directory of the component</p> required <code>node_id</code> <code>str</code> <p>Unique ID of the node</p> required <code>db_path</code> <code>str</code> <p>path to the node state database</p> required <p>Raises:</p> Type Description <code>FedbiomedNodeStateManagerError</code> <p>failed to access the database</p> Source code in <code>fedbiomed/node/node_state_manager.py</code> <pre><code>def __init__(self, dir: str, node_id: str, db_path: str):\n    \"\"\"Constructor of the class.\n\n    Args:\n        dir: Root directory of the component\n        node_id: Unique ID of the node\n        db_path: path to the node state database\n\n    Raises:\n        FedbiomedNodeStateManagerError: failed to access the database\n\n\"\"\"\n    # NOTA: constructor has been designed wrt other object handling DataBase\n    self._dir = dir\n    self._node_id = node_id\n    self._query: Query = Query()\n    # node state base directory, where all node state related files are saved\n    self._node_state_base_dir: Optional[str] = None\n    self._state_id: Optional[str] = None\n    self._previous_state_id: Optional[str] = None\n    try:\n        self._connection = TinyDB(db_path)\n        self._connection.table_class = DBTable\n        self._db: Table = self._connection.table(name=NODE_STATE_TABLE_NAME, cache_size=0)\n    except Exception as e:\n        raise FedbiomedNodeStateManagerError(f\"{ErrorNumbers.FB323.value}: \"\n                                             \"Error found when loading database\") from e\n</code></pre>"},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateManager-attributes","title":"Attributes","text":""},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateManager.previous_state_id","title":"previous_state_id  <code>property</code>","text":"<pre><code>previous_state_id\n</code></pre> <p>Getter for previous state ID</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>previous state ID, or None if does not exist</p>"},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateManager.state_id","title":"state_id  <code>property</code>","text":"<pre><code>state_id\n</code></pre> <p>Getter for state ID</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>state ID, or None if not defined yet</p>"},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateManager-functions","title":"Functions","text":""},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateManager.add","title":"add","text":"<pre><code>add(experiment_id, state)\n</code></pre> <p>Adds new <code>Node</code> State into Database.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_id</code> <code>str</code> <p>experiment_id used to save entry in database</p> required <code>state</code> <code>Dict[str, Dict[str, str]]</code> <p>state to be saved in database.</p> required <p>Returns:</p> Type Description <code>str</code> <p>state_id</p> <p>Raises:</p> Type Description <code>FedbiomedNodeStateManagerError</code> <p>state manager is not initialized</p> Source code in <code>fedbiomed/node/node_state_manager.py</code> <pre><code>def add(self, experiment_id: str, state: Dict[str, Dict[str, str]]) -&gt; str:\n    \"\"\"Adds new `Node` State into Database.\n\n    Args:\n        experiment_id: experiment_id used to save entry in database\n        state: state to be saved in database.\n\n    Returns:\n        state_id\n\n    Raises:\n        FedbiomedNodeStateManagerError: state manager is not initialized\n    \"\"\"\n\n    if self._state_id is None:\n        raise FedbiomedNodeStateManagerError(f\"{ErrorNumbers.FB323.value}: Node state manager is not initialized\")\n    header = {\n        \"version_node_id\": str(__node_state_version__),\n        \"state_id\": self._state_id,\n        \"experiment_id\": experiment_id\n    }\n\n    state.update(header)\n    self._save_state(self._state_id, state)\n    return self._state_id\n</code></pre>"},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateManager.generate_folder_and_create_file_name","title":"generate_folder_and_create_file_name","text":"<pre><code>generate_folder_and_create_file_name(experiment_id, round_nb, element)\n</code></pre> <p>Creates folders and file name for each content (Optimizer, model, ...) that composes a Node State.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_id</code> <code>str</code> <p>experiment_id</p> required <code>round_nb</code> <code>int</code> <p>current Round number</p> required <code>element</code> <code>NodeStateFileName</code> <p>a NodeStateFileName object used to create specific file names. For instance, could be a NodeStateFileName.OPTIMIZER</p> required <p>Raises:</p> Type Description <code>FedbiomedNodeStateManagerError</code> <p>raised if node state manager is not initialized</p> <code>FedbiomedNodeStateManagerError</code> <p>raised if folder can not be created.</p> <p>Returns:</p> Type Description <code>str</code> <p>path to the file that corresponds to the object that needs to be saved.</p> Source code in <code>fedbiomed/node/node_state_manager.py</code> <pre><code>def generate_folder_and_create_file_name(self,\n                                         experiment_id: str,\n                                         round_nb: int,\n                                         element: NodeStateFileName) -&gt; str:\n    \"\"\"Creates folders and file name for each content (Optimizer, model, ...) that composes\n    a Node State.\n\n    Args:\n        experiment_id: experiment_id\n        round_nb: current Round number\n        element: a NodeStateFileName object used to create specific file names. For instance,\n            could be a NodeStateFileName.OPTIMIZER\n\n    Raises:\n        FedbiomedNodeStateManagerError: raised if node state manager is not initialized\n        FedbiomedNodeStateManagerError: raised if folder can not be created.\n\n    Returns:\n        path to the file that corresponds to the object that needs to be saved.\n    \"\"\"\n\n    node_state_base_dir = self.get_node_state_base_dir()\n    if node_state_base_dir is None:\n        raise FedbiomedNodeStateManagerError(f\"{ErrorNumbers.FB323.value}: working directory has not been \"\n                                             \"initialized, have you run `initialize` method beforehand ?\")\n\n    if self._state_id is None:\n        raise FedbiomedNodeStateManagerError(f\"{ErrorNumbers.FB323.value}: Node state manager is not initialized\")\n    base_dir = os.path.join(node_state_base_dir, \"experiment_id_%s\" % experiment_id)\n    try:\n        os.makedirs(base_dir, exist_ok=True)\n\n    except Exception as e:\n        raise FedbiomedNodeStateManagerError(f\"{ErrorNumbers.FB323.value}: Failing to create directories \"\n                                             f\"{base_dir}\") from e\n    # TODO catch exception here\n    file_name = element.value % (round_nb, self._state_id)\n    return os.path.join(base_dir, file_name)\n</code></pre>"},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateManager.get","title":"get","text":"<pre><code>get(experiment_id, state_id)\n</code></pre> <p>Returns a state of an experiment on the <code>Node</code>.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_id</code> <code>str</code> <p>the experiment for which a state is requested</p> required <code>state_id</code> <code>str</code> <p>the unique identifier of the experiment</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>dict containing the experiment state</p> <p>Raises:</p> Type Description <code>FedbiomedNodeStateManagerError</code> <p>no matching state in the database</p> Source code in <code>fedbiomed/node/node_state_manager.py</code> <pre><code>def get(self, experiment_id: str, state_id: str) -&gt; Dict:\n    \"\"\"Returns a state of an experiment on the `Node`.\n\n    Args:\n        experiment_id: the experiment for which a state is requested\n        state_id: the unique identifier of the experiment\n\n    Returns:\n        dict containing the experiment state\n\n    Raises:\n        FedbiomedNodeStateManagerError: no matching state in the database\n    \"\"\"\n    state = self._load_state(experiment_id, state_id)\n\n    if state is None:\n        raise FedbiomedNodeStateManagerError(f\"{ErrorNumbers.FB323.value}: no entries matching\"\n                                             f\" experiment_id {experiment_id} and \"\n                                             f\"state_id {state_id} found in the DataBase\")\n\n    # from this point, state should be a dictionary\n    self._check_version(state.get(\"version_node_id\"))\n    return state\n</code></pre>"},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateManager.get_node_state_base_dir","title":"get_node_state_base_dir","text":"<pre><code>get_node_state_base_dir()\n</code></pre> <p>Returns <code>Node</code> State base directory path, in which are saved Node state files and other contents</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>path to <code>Node</code> state base directory, or None if not defined</p> Source code in <code>fedbiomed/node/node_state_manager.py</code> <pre><code>def get_node_state_base_dir(self) -&gt; Optional[str]:\n    \"\"\"Returns `Node` State base directory path, in which are saved Node state files and other contents\n\n    Returns:\n        path to `Node` state base directory, or None if not defined\n    \"\"\"\n    return self._node_state_base_dir\n</code></pre>"},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateManager.initialize","title":"initialize","text":"<pre><code>initialize(previous_state_id=None, testing=False)\n</code></pre> <p>Initializes NodeStateManager, by creating folder that will contains Node state folders.</p> <p>Parameters:</p> Name Type Description Default <code>previous_state_id</code> <code>Optional[str]</code> <p>state_id from previous Round, from whch to reload a Node state</p> <code>None</code> <code>testing</code> <code>Optional[bool]</code> <p>only doing testing, not training</p> <code>False</code> Source code in <code>fedbiomed/node/node_state_manager.py</code> <pre><code>def initialize(self, previous_state_id: Optional[str] = None, testing: Optional[bool] = False) -&gt; None:\n    \"\"\"Initializes NodeStateManager, by creating folder that will contains Node state folders.\n\n    Args:\n        previous_state_id: state_id from previous Round, from whch to reload a Node state\n        testing: only doing testing, not training\n    \"\"\"\n\n    self._previous_state_id = previous_state_id\n    if not testing:\n        self._generate_new_state_id()\n\n        self._node_state_base_dir = os.path.join(\n            self._dir, VAR_FOLDER_NAME, \"node_state_%s\" % self._node_id\n        )\n        # Always create the base folder for saving states for this node\n        try:\n            os.makedirs(self._node_state_base_dir, exist_ok=True)\n        except Exception as e:\n            raise FedbiomedNodeStateManagerError(f\"{ErrorNumbers.FB323.value}: Failing to create\"\n                                                 f\" directories {self._node_state_base_dir}\") from e\n</code></pre>"},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateManager.list_states","title":"list_states","text":"<pre><code>list_states(experiment_id)\n</code></pre> Source code in <code>fedbiomed/node/node_state_manager.py</code> <pre><code>def list_states(self, experiment_id: str):\n    raise NotImplementedError\n</code></pre>"},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager.NodeStateManager.remove","title":"remove","text":"<pre><code>remove(experiment_id, state_id)\n</code></pre> Source code in <code>fedbiomed/node/node_state_manager.py</code> <pre><code>def remove(self, experiment_id: Optional[str], state_id: Optional[str]):\n    raise NotImplementedError\n</code></pre>"},{"location":"developer/api/node/node_state_manager/#fedbiomed.node.node_state_manager-functions","title":"Functions","text":""},{"location":"developer/api/node/requests/","title":"Requests","text":"<p>to simplify imports from fedbiomed.node.requests</p>"},{"location":"developer/api/node/requests/#fedbiomed.node.requests-classes","title":"Classes","text":""},{"location":"developer/api/node/requests/#fedbiomed.node.requests.NodeToNodeController","title":"NodeToNodeController","text":"<pre><code>NodeToNodeController(node_id, grpc_controller, overlay_channel, pending_requests, controller_data)\n</code></pre> <p>Defines the controller for protocol messages processed by the node to node router</p> <p>Each message type must have a handler. A handler receives <code>overlay_msg</code> and <code>inner_msg</code>, returns a dict which will be passed as <code>**kwargs</code> to the <code>final()</code> - types must match ! It may receive an asyncio.CancelledError</p> <p>Each message type optionally has a final. It executes only if the <code>handler()</code> completed without being cancelled It won't be interrupted by an asyncio.CancelledError If no <code>final()</code> exist, no action is taken after cancelling or completing the <code>handler()</code></p> <p>async def _HandlerExample(self, overlay_msg: dict, inner_msg: InnerMessage) -&gt; Any:     logger.debug(\"Normal handler code that can be cancelled\")     return { 'value: 3 } async def _FinalExample(self, value: int) -&gt; None:         logger.debug(f\"Final code than cannot be cancelled. Received {value}\")</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>ID of the node.</p> required <code>grpc_controller</code> <code>GrpcController</code> <p>object managing the communication with other components</p> required <code>overlay_channel</code> <code>OverlayChannel</code> <p>layer for managing overlay message send and receive</p> required <code>pending_requests</code> <code>EventWaitExchange</code> <p>object for receiving overlay node to node messages</p> required <code>controller_data</code> <code>EventWaitExchange</code> <p>object for sharing data</p> required Source code in <code>fedbiomed/node/requests/_n2n_controller.py</code> <pre><code>def __init__(\n        self,\n        node_id: str,\n        grpc_controller: GrpcController,\n        overlay_channel: OverlayChannel,\n        pending_requests: EventWaitExchange,\n        controller_data: EventWaitExchange,\n) -&gt; None:\n    \"\"\"Constructor of the class.\n\n    Args:\n        node_id: ID of the node.\n        grpc_controller: object managing the communication with other components\n        overlay_channel: layer for managing overlay message send and receive\n        pending_requests: object for receiving overlay node to node messages\n        controller_data: object for sharing data\n    \"\"\"\n\n    self._node_id = node_id\n    self._grpc_controller = grpc_controller\n    self._overlay_channel = overlay_channel\n    self._pending_requests = pending_requests\n    self._controller_data = controller_data\n\n    self._command2method = {\n        KeyRequest.__name__: self._HandlerKeyRequest,\n        KeyReply.__name__: self._HandlerKeyReply,\n        AdditiveSSharingRequest.__name__: self._AdditiveSSharingRequest,\n        AdditiveSSharingReply.__name__: self._HandlerAdditiveSSharingReply,\n        ChannelSetupRequest.__name__: self._HandlerChannelRequest,\n        ChannelSetupReply.__name__: self._HandlerKeyReply,\n    }\n\n    self._command2final = {\n        KeyRequest.__name__: self._FinalKeyRequest,\n        KeyReply.__name__: self._FinalKeyReply,\n        AdditiveSSharingRequest.__name__: self._FinalAdditiveSSharingRequest,\n        AdditiveSSharingReply.__name__: self._FinalAdditiveSSharingReply,\n        ChannelSetupRequest.__name__: self._FinalKeyRequest,\n        ChannelSetupReply.__name__: self._FinalChannelReply,\n    }\n</code></pre>"},{"location":"developer/api/node/requests/#fedbiomed.node.requests.NodeToNodeController-functions","title":"Functions","text":""},{"location":"developer/api/node/requests/#fedbiomed.node.requests.NodeToNodeController.final","title":"final  <code>async</code>","text":"<pre><code>final(message, **kwargs)\n</code></pre> <p>Calls the final processing for a received message protocol.</p> <p>This handler is optional, it may not be declared for a message.</p> <p>Should be called only if the handler completed without being interrupted. Cannot be interrupted, thus should not launch treatment that may hang.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Specific arguments for this message final handler</p> <code>{}</code> Source code in <code>fedbiomed/node/requests/_n2n_controller.py</code> <pre><code>async def final(self, message, **kwargs) -&gt; None:\n    \"\"\"Calls the final processing for a received message protocol.\n\n    This handler is optional, it may not be declared for a message.\n\n    Should be called only if the handler completed without being interrupted.\n    Cannot be interrupted, thus should not launch treatment that may hang.\n\n    Args:\n        kwargs: Specific arguments for this message final handler\n    \"\"\"\n    if message in self._command2final:\n        # Useful ? Allow omitting some arguments, automatically add them with None value\n        expected_args = dict(\n            inspect.signature(self._command2final[message]).parameters\n        ).keys()\n        kwargs.update({arg: None for arg in expected_args if arg not in kwargs})\n\n        await self._command2final[message](**kwargs)\n</code></pre>"},{"location":"developer/api/node/requests/#fedbiomed.node.requests.NodeToNodeController.handle","title":"handle  <code>async</code>","text":"<pre><code>handle(overlay_msg, inner_msg)\n</code></pre> <p>Calls the handler for processing a received message protocol.</p> <p>If it does not exist, call the default handler to trigger an error.</p> <p>Main part of the processing which can be interrupted if the processing takes too long.</p> <p>Parameters:</p> Name Type Description Default <code>overlay_msg</code> <code>OverlayMessage</code> <p>Outer message for node to node communication</p> required <code>inner_msg</code> <code>InnerMessage</code> <p>Unpacked inner message from the outer message</p> required <p>Returns:</p> Type Description <code>Optional[dict]</code> <p>A dict of the <code>kwargs</code> expected by the corresponding <code>final()</code> handler for this message. Empty dict or <code>None</code> if no <code>kwargs</code> expected of no final handler</p> Source code in <code>fedbiomed/node/requests/_n2n_controller.py</code> <pre><code>async def handle(\n    self, overlay_msg: OverlayMessage, inner_msg: InnerMessage\n) -&gt; Optional[dict]:\n    \"\"\"Calls the handler for processing a received message protocol.\n\n    If it does not exist, call the default handler to trigger an error.\n\n    Main part of the processing which can be interrupted if the processing takes too long.\n\n    Args:\n        overlay_msg: Outer message for node to node communication\n        inner_msg: Unpacked inner message from the outer message\n\n    Returns:\n        A dict of the `kwargs` expected by the corresponding `final()` handler for this message.\n            Empty dict or `None` if no `kwargs` expected of no final handler\n    \"\"\"\n\n    if inner_msg.__name__ in self._command2method:\n        return await self._command2method[inner_msg.__name__](\n            overlay_msg, inner_msg\n        )\n\n    return await self._HandlerDefault(overlay_msg, inner_msg)\n</code></pre>"},{"location":"developer/api/node/requests/#fedbiomed.node.requests.NodeToNodeRouter","title":"NodeToNodeRouter","text":"<pre><code>NodeToNodeRouter(node_id, db, grpc_controller, pending_requests, controller_data)\n</code></pre> <p>               Bases: <code>_NodeToNodeAsyncRouter</code></p> <p>Handles node to node messages received by a node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>ID of the active node.</p> required <code>db</code> <code>str</code> <p>Path to database file.</p> required <code>grpc_controller</code> <code>GrpcController</code> <p>object managing the communication with other components</p> required <code>pending_requests</code> <code>EventWaitExchange</code> <p>object for receiving overlay node to node messages</p> required <code>controller_data</code> <code>EventWaitExchange</code> <p>object for sharing data with the controller</p> required Source code in <code>fedbiomed/node/requests/_n2n_router.py</code> <pre><code>def __init__(\n    self,\n    node_id: str,\n    db: str,\n    grpc_controller: GrpcController,\n    pending_requests: EventWaitExchange,\n    controller_data: EventWaitExchange,\n) -&gt; None:\n    \"\"\"Class constructor.\n\n    Args:\n        node_id: ID of the active node.\n        db: Path to database file.\n        grpc_controller: object managing the communication with other components\n        pending_requests: object for receiving overlay node to node messages\n        controller_data: object for sharing data with the controller\n    \"\"\"\n    super().__init__(node_id, db, grpc_controller, pending_requests, controller_data)\n\n    self._thread = Thread(target=self._run, args=(), daemon=True)\n</code></pre>"},{"location":"developer/api/node/requests/#fedbiomed.node.requests.NodeToNodeRouter-functions","title":"Functions","text":""},{"location":"developer/api/node/requests/#fedbiomed.node.requests.NodeToNodeRouter.format_outgoing_overlay","title":"format_outgoing_overlay","text":"<pre><code>format_outgoing_overlay(message, researcher_id)\n</code></pre> <p>Creates an overlay message payload from an inner message.</p> <p>Serialize, crypt, sign the inner message</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>InnerMessage</code> <p>Inner message to send as overlay payload</p> required <code>researcher_id</code> <code>str</code> <p>unique ID of researcher connecting the nodes</p> required <code>setup</code> <p>False for sending a message over the channel, True for a message setting up the channel</p> required <p>Returns:</p> Type Description <code>Tuple[bytes, bytes, bytes]</code> <p>A tuple consisting of: payload for overlay message, salt for inner message encryption key, nonce for the inner message encryption</p> Source code in <code>fedbiomed/node/requests/_n2n_router.py</code> <pre><code>def format_outgoing_overlay(self, message: InnerMessage, researcher_id: str) -&gt; \\\n        Tuple[bytes, bytes, bytes]:\n    \"\"\"Creates an overlay message payload from an inner message.\n\n    Serialize, crypt, sign the inner message\n\n    Args:\n        message: Inner message to send as overlay payload\n        researcher_id: unique ID of researcher connecting the nodes\n        setup: False for sending a message over the channel, True for a message\n            setting up the channel\n\n    Returns:\n        A tuple consisting of: payload for overlay message, salt for inner message\n            encryption key, nonce for the inner message encryption\n    \"\"\"\n    future = asyncio.run_coroutine_threadsafe(\n        self._overlay_channel.format_outgoing_overlay(message, researcher_id),\n        self._loop\n    )\n    return future.result()\n</code></pre>"},{"location":"developer/api/node/requests/#fedbiomed.node.requests.NodeToNodeRouter.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starts the node to node router.</p> Source code in <code>fedbiomed/node/requests/_n2n_router.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Starts the node to node router.\"\"\"\n    self._thread.start()\n</code></pre>"},{"location":"developer/api/node/requests/#fedbiomed.node.requests.NodeToNodeRouter.submit","title":"submit","text":"<pre><code>submit(msg)\n</code></pre> <p>Submits a received message to the node to node router for processing.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>OverlayMessage</code> <p>received message</p> required Source code in <code>fedbiomed/node/requests/_n2n_router.py</code> <pre><code>def submit(self, msg: OverlayMessage) -&gt; None:\n    \"\"\"Submits a received message to the node to node router for processing.\n\n    Args:\n        msg: received message\n    \"\"\"\n\n    try:\n        asyncio.run_coroutine_threadsafe(self._submit(msg), self._loop)\n    except Exception as e:\n        logger.critical(\n            \"Failed submitting message to node to node router. \"\n            f\"Exception: {type(e).__name__}. Error message: {e}\"\n        )\n        raise e\n</code></pre>"},{"location":"developer/api/node/requests/#fedbiomed.node.requests.OverlayChannel","title":"OverlayChannel","text":"<pre><code>OverlayChannel(node_id, db, grpc_client)\n</code></pre> <p>Provides asyncio safe layer for sending and receiving overlay messages.</p> <p>This class is not thread safe, all calls must be done within the same thread (except constructor).</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>ID of the active node.</p> required <code>db</code> <code>str</code> <p>Path to database file.</p> required <code>grpc_client</code> <code>GrpcController</code> <p>object managing the communication with other components</p> required Source code in <code>fedbiomed/node/requests/_overlay.py</code> <pre><code>def __init__(\n    self,\n    node_id: str,\n    db: str,\n    grpc_client: GrpcController\n) -&gt; None:\n    \"\"\"Class constructor\n\n    Args:\n        node_id: ID of the active node.\n        db: Path to database file.\n        grpc_client: object managing the communication with other components\n    \"\"\"\n    self._node_id = node_id\n    self._grpc_client = grpc_client\n\n    self._channel_keys = _ChannelKeys(db)\n\n    # Issue #1142 in \"Crypto material management\" will optionally replace current default key published with the\n    # library and used for each node for setup by a keypair generated securely for each node.\n    # Caveat: though encrypted, current implementation does not ensure a secure overlay node2node channel ...\n\n    # Default keys\n    default_private_key, default_public_key = self._load_default_n2n_key()\n    self._default_n2n_key = _N2nKeysEntry(\n        local_key=default_private_key,\n        ready_event=asyncio.Event(),\n        distant_key=default_public_key\n    )\n</code></pre>"},{"location":"developer/api/node/requests/#fedbiomed.node.requests.OverlayChannel-functions","title":"Functions","text":""},{"location":"developer/api/node/requests/#fedbiomed.node.requests.OverlayChannel.format_incoming_overlay","title":"format_incoming_overlay  <code>async</code>","text":"<pre><code>format_incoming_overlay(overlay_msg)\n</code></pre> <p>Retrieves inner message from overlay message payload.</p> <p>Check signature, decrypt, deserialize the inner message</p> <p>Parameters:</p> Name Type Description Default <code>overlay_msg</code> <code>OverlayMessage</code> <p>Overlay message.</p> required <p>Returns:</p> Type Description <code>InnerMessage</code> <p>Inner message retrieved from overlay payload</p> <p>Raises:</p> Type Description <code>FedbiomedNodeToNodeError</code> <p>bad message type</p> <code>FedbiomedNodeToNodeError</code> <p>cannot decrypt payload</p> <code>FedbiomedNodeToNodeError</code> <p>bad inner payload format</p> <code>FedbiomedNodeToNodeError</code> <p>cannot verify payload integrity</p> <code>FedbiomedNodeToNodeError</code> <p>sender/dest node ID don't match in overlay and inner message</p> Source code in <code>fedbiomed/node/requests/_overlay.py</code> <pre><code>async def format_incoming_overlay(self, overlay_msg: OverlayMessage) -&gt; InnerMessage:\n    \"\"\"Retrieves inner message from overlay message payload.\n\n    Check signature, decrypt, deserialize the inner message\n\n    Args:\n        overlay_msg: Overlay message.\n\n    Returns:\n        Inner message retrieved from overlay payload\n\n    Raises:\n        FedbiomedNodeToNodeError: bad message type\n        FedbiomedNodeToNodeError: cannot decrypt payload\n        FedbiomedNodeToNodeError: bad inner payload format\n        FedbiomedNodeToNodeError: cannot verify payload integrity\n        FedbiomedNodeToNodeError: sender/dest node ID don't match in overlay and inner message\n    \"\"\"\n    # robustify from developer error (try to encapsulate a bad message type)\n    if not isinstance(overlay_msg, OverlayMessage):\n        raise FedbiomedNodeToNodeError(f'{ErrorNumbers.FB324.value}: not an overlay message')\n\n\n    _, distant_node_public_key, derived_key = await self._setup_use_channel_keys(\n        overlay_msg.node_id,\n        overlay_msg.researcher_id,\n        overlay_msg.setup,\n        overlay_msg.salt,\n    )\n\n    # decrypt outer payload\n    try:\n        decryptor = Cipher(\n            algorithms.ChaCha20(derived_key, overlay_msg.nonce),\n            mode=None,\n            backend=default_backend()\n        ).decryptor()\n        decrypted_serial = decryptor.update(overlay_msg.overlay) + decryptor.finalize()\n    except ValueError as e:\n        raise FedbiomedNodeToNodeError(\n            f'{ErrorNumbers.FB324.value}: cannot decrypt payload: {e}') from e\n\n    decrypted = Serializer.loads(decrypted_serial)\n    if not isinstance(decrypted, dict) or not set(('message', 'signature')) &lt;= set(decrypted):\n        raise FedbiomedNodeToNodeError(f'{ErrorNumbers.FB324.value}: bad inner payload format '\n                                       f\"in received message\")\n\n    # verify inner payload\n    try:\n        distant_node_public_key.public_key.verify(\n            decrypted['signature'],\n            Serializer.dumps(decrypted['message']),\n            ec.ECDSA(hashes.SHA256()),\n        )\n    except InvalidSignature as e:\n        raise FedbiomedNodeToNodeError(\n            f'{ErrorNumbers.FB324.value}: cannot verify payload integrity: {e}') from e\n\n    inner_msg = Message.from_dict(decrypted['message'])\n\n    # Node ID mismatch reveals either (1) malicious peer forging message (2) application internal error\n    if inner_msg.node_id != overlay_msg.node_id:\n        raise FedbiomedNodeToNodeError(\n            f'{ErrorNumbers.FB324.value}: Source node ID mismatch for overlay message '\n            f'inner_node_id={inner_msg.node_id} overlay_node_id={overlay_msg.node_id}')\n    if inner_msg.dest_node_id != overlay_msg.dest_node_id:\n        raise FedbiomedNodeToNodeError(\n            f'{ErrorNumbers.FB324.value}: Destination node ID mismatch for overlay message '\n            f'inner_node_id={inner_msg.dest_node_id} overlay_node_id={overlay_msg.dest_node_id}')\n\n    return inner_msg\n</code></pre>"},{"location":"developer/api/node/requests/#fedbiomed.node.requests.OverlayChannel.format_outgoing_overlay","title":"format_outgoing_overlay  <code>async</code>","text":"<pre><code>format_outgoing_overlay(message, researcher_id, setup=False)\n</code></pre> <p>Creates an overlay message payload from an inner message.</p> <p>Serialize, crypt, sign the inner message</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>InnerMessage</code> <p>Inner message to send as overlay payload</p> required <code>researcher_id</code> <code>str</code> <p>unique ID of researcher connecting the nodes</p> required <code>setup</code> <code>bool</code> <p>False for sending a message over the channel, True for a message setting up the channel</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[bytes, bytes, bytes]</code> <p>A tuple consisting of: payload for overlay message, salt for inner message encryption key, nonce for the inner message encryption</p> <p>Raises:</p> Type Description <code>FedbiomedNodeToNodeError</code> <p>bad message type</p> Source code in <code>fedbiomed/node/requests/_overlay.py</code> <pre><code>async def format_outgoing_overlay(self, message: InnerMessage, researcher_id: str, setup: bool = False) -&gt; \\\n        Tuple[bytes, bytes, bytes]:\n    \"\"\"Creates an overlay message payload from an inner message.\n\n    Serialize, crypt, sign the inner message\n\n    Args:\n        message: Inner message to send as overlay payload\n        researcher_id: unique ID of researcher connecting the nodes\n        setup: False for sending a message over the channel, True for a message\n            setting up the channel\n\n    Returns:\n        A tuple consisting of: payload for overlay message, salt for inner message\n            encryption key, nonce for the inner message encryption\n\n    Raises:\n        FedbiomedNodeToNodeError: bad message type\n    \"\"\"\n    # robustify from developer error (try to encapsulate a bad message type)\n    if not isinstance(message, InnerMessage):\n        raise FedbiomedNodeToNodeError(f'{ErrorNumbers.FB324.value}: not an inner message')\n\n    # Value for salting the symmetric encryption key generation for this message\n    # Adjust length of `salt` depending on algorithm\n    salt = secrets.token_bytes(32)\n\n    # Value for noncing the symmetric encryption for this message\n    # This is normally not needed as we generate different key for each message due to `salt`\n    # but provides another layer of security\n    # Adjust the length of `nonce` depending on algotrithm\n    nonce = secrets.token_bytes(16)\n\n    local_node_private_key, _, derived_key = await self._setup_use_channel_keys(\n        message.get_param('dest_node_id'),\n        researcher_id,\n        setup,\n        salt\n    )\n\n    # consider encrypt-sign([message,node_id]) or other see\n    # https://theworld.com/~dtd/sign_encrypt/sign_encrypt7.html\n\n    # sign inner payload\n    signed = Serializer.dumps({\n        'message': message.to_dict(),\n        'signature': local_node_private_key.private_key.sign(\n            Serializer.dumps(message.to_dict()),\n            ec.ECDSA(hashes.SHA256()),\n        )\n    })\n\n    encryptor = Cipher(\n        algorithms.ChaCha20(derived_key, nonce),\n        mode=None,\n        backend=default_backend()\n    ).encryptor()\n    return encryptor.update(signed) + encryptor.finalize(), salt, nonce\n</code></pre>"},{"location":"developer/api/node/requests/#fedbiomed.node.requests.OverlayChannel.get_local_public_key","title":"get_local_public_key  <code>async</code>","text":"<pre><code>get_local_public_key(distant_node_id)\n</code></pre> <p>Gets local public key for peering with a given distant peer node</p> <p>Parameters:</p> Name Type Description Default <code>distant_node_id</code> <code>str</code> <p>unique ID of the peer node</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>Local node's public key ID for this peer node</p> Source code in <code>fedbiomed/node/requests/_overlay.py</code> <pre><code>async def get_local_public_key(self, distant_node_id: str) -&gt; bytes:\n    \"\"\"Gets local public key for peering with a given distant peer node\n\n    Args:\n        distant_node_id: unique ID of the peer node\n\n    Returns:\n        Local node's public key ID for this peer node\n    \"\"\"\n    return await self._channel_keys.get_local_public_key(distant_node_id)\n</code></pre>"},{"location":"developer/api/node/requests/#fedbiomed.node.requests.OverlayChannel.send_node_setup","title":"send_node_setup  <code>async</code>","text":"<pre><code>send_node_setup(researcher_id, node, message)\n</code></pre> <p>Send a channel setup message to another node using overlay communications and wait for its reply.</p> <p>Parameters:</p> Name Type Description Default <code>researcher_id</code> <code>str</code> <p>unique ID of researcher connecting the nodes</p> required <code>node</code> <code>str</code> <p>unique node ID of the destination node</p> required <code>message</code> <code>InnerMessage</code> <p>inner message for the destination node</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if channel is ready, False if channel not ready after timeout</p> Source code in <code>fedbiomed/node/requests/_overlay.py</code> <pre><code>async def send_node_setup(\n        self,\n        researcher_id: str,\n        node: str,\n        message: InnerMessage,\n) -&gt; bool:\n    \"\"\"Send a channel setup message to another node using overlay communications and wait for its reply.\n\n        Args:\n            researcher_id: unique ID of researcher connecting the nodes\n            node: unique node ID of the destination node\n            message: inner message for the destination node\n\n        Returns:\n            True if channel is ready, False if channel not ready after timeout\n    \"\"\"\n    overlay, salt, nonce = await self.format_outgoing_overlay(message, researcher_id, True)\n    message_overlay = OverlayMessage(\n        researcher_id=researcher_id,\n        node_id=self._node_id,\n        dest_node_id=node,\n        overlay=overlay,\n        setup=True,\n        salt=salt,\n        nonce=nonce,\n    )\n\n    self._grpc_client.send(message_overlay)\n\n    return await self._channel_keys.wait_ready_channel(node)\n</code></pre>"},{"location":"developer/api/node/requests/#fedbiomed.node.requests.OverlayChannel.set_distant_key","title":"set_distant_key  <code>async</code>","text":"<pre><code>set_distant_key(distant_node_id, public_key_pem, request_id)\n</code></pre> <p>Sets distant (public) key of a channel for peering with a given     distant peer node.</p> <p>Distant key is not set if no channel exists for that <code>distant_node_id</code>     or if the <code>request_id</code> does not match a pending request</p> <p>Parameters:</p> Name Type Description Default <code>distant_node_id</code> <code>str</code> <p>unique ID of the peer node</p> required <code>public_key_pem</code> <code>bytes</code> <p>public key in PEM format</p> required <code>request_id</code> <code>str</code> <p>unique ID of the request</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the distant key was set, False if it was not set</p> Source code in <code>fedbiomed/node/requests/_overlay.py</code> <pre><code>async def set_distant_key(\n        self,\n        distant_node_id: str,\n        public_key_pem: bytes,\n        request_id: str,\n) -&gt; bool:\n    \"\"\"Sets distant (public) key of a channel for peering with a given\n        distant peer node.\n\n    Distant key is not set if no channel exists for that `distant_node_id`\n        or if the `request_id` does not match a pending request\n\n    Args:\n        distant_node_id: unique ID of the peer node\n        public_key_pem: public key in PEM format\n        request_id: unique ID of the request\n\n    Returns:\n        True if the distant key was set, False if it was not set\n    \"\"\"\n    return await self._channel_keys.set_distant_key(\n        distant_node_id,\n        public_key_pem,\n        request_id\n    )\n</code></pre>"},{"location":"developer/api/node/requests/#fedbiomed.node.requests-functions","title":"Functions","text":""},{"location":"developer/api/node/requests/#fedbiomed.node.requests.send_nodes","title":"send_nodes","text":"<pre><code>send_nodes(n2n_router, grpc_client, pending_requests, researcher_id, nodes, messages, raise_if_not_all_received=False)\n</code></pre> <p>Send message to some other nodes using overlay communications and wait for their replies.</p> <p>Parameters:</p> Name Type Description Default <code>n2n_router</code> <code>NodeToNodeRouter</code> <p>object managing node to node messages</p> required <code>grpc_client</code> <code>GrpcController</code> <p>object managing the communication with other components</p> required <code>pending_requests</code> <code>EventWaitExchange</code> <p>object for receiving overlay node to node reply message</p> required <code>researcher_id</code> <code>str</code> <p>unique ID of researcher connecting the nodes</p> required <code>nodes</code> <code>List[str]</code> <p>list of node IDs of the destination nodes</p> required <code>messages</code> <code>List[InnerMessage]</code> <p>list of the inner messages for the destination nodes</p> required <code>raise_if_not_all_received</code> <code>bool</code> <p>if True, raise exception if not all answers from nodes were received. Default to False, return with <code>status</code> to False when not all answers from nodes were received.</p> <code>False</code> <p>Returns:     status: True if all messages are received     replies: List of replies from each node.</p> <p>Raises:</p> Type Description <code>FedbiomedNodeToNodeError</code> <p>not all answers received and raise_if_not_all_received is True</p> Source code in <code>fedbiomed/node/requests/_send_nodes.py</code> <pre><code>def send_nodes(\n        n2n_router: NodeToNodeRouter,\n        grpc_client: GrpcController,\n        pending_requests: EventWaitExchange,\n        researcher_id: str,\n        nodes: List[str],\n        messages: List[InnerMessage],\n        raise_if_not_all_received: bool = False,\n) -&gt; Tuple[bool, List[Any]]:\n    \"\"\"Send message to some other nodes using overlay communications and wait for their replies.\n\n        Args:\n            n2n_router: object managing node to node messages\n            grpc_client: object managing the communication with other components\n            pending_requests: object for receiving overlay node to node reply message\n            researcher_id: unique ID of researcher connecting the nodes\n            nodes: list of node IDs of the destination nodes\n            messages: list of the inner messages for the destination nodes\n            raise_if_not_all_received: if True, raise exception if not all answers from nodes were received.\n                Default to False, return with `status` to False when not all answers from nodes were received.\n        Returns:\n            status: True if all messages are received\n            replies: List of replies from each node.\n\n        Raises:\n            FedbiomedNodeToNodeError: not all answers received and raise_if_not_all_received is True\n    \"\"\"\n    request_ids = []\n\n    for node, message in zip(nodes, messages):\n        overlay, salt, nonce = n2n_router.format_outgoing_overlay(message, researcher_id)\n        message_overlay = OverlayMessage(\n            researcher_id=researcher_id,\n            node_id=n2n_router.node_id,\n            dest_node_id=node,\n            overlay=overlay,\n            setup=False,\n            salt=salt,\n            nonce=nonce,\n        )\n\n        grpc_client.send(message_overlay)\n\n        if isinstance(message, InnerRequestReply):\n            request_ids += [message.get_param('request_id')]\n\n    all_received, replies = pending_requests.wait(request_ids, TIMEOUT_NODE_TO_NODE_REQUEST)\n    if not all_received and raise_if_not_all_received:\n        nodes_no_answer = set(nodes) - set(m.node_id for m in replies)\n        raise FedbiomedNodeToNodeError(\n            f\"{ErrorNumbers.FB318.value}: Some nodes did not answer request \"\n            f\"{nodes_no_answer}\"\n        )\n\n    return all_received, replies\n</code></pre>"},{"location":"developer/api/node/round/","title":"Round","text":"<p>implementation of Round class of the node component</p>"},{"location":"developer/api/node/round/#fedbiomed.node.round-attributes","title":"Attributes","text":""},{"location":"developer/api/node/round/#fedbiomed.node.round-classes","title":"Classes","text":""},{"location":"developer/api/node/round/#fedbiomed.node.round.Round","title":"Round","text":"<pre><code>Round(root_dir, db, node_id, training_plan, training_plan_class, model_kwargs, training_kwargs, training, dataset, params, experiment_id, researcher_id, history_monitor, aggregator_args, node_args, tp_security_manager, round_number=0, dlp_and_loading_block_metadata=None, aux_vars=None)\n</code></pre> <p>This class represents the training part execute by a node in a given round</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Root fedbiomed directory where node instance files will be stored.</p> required <code>db</code> <code>str</code> <p>Path to node database file.</p> required <code>node_id</code> <code>str</code> <p>Node id</p> required <code>training_plan</code> <code>str</code> <p>code of the training plan for this round</p> required <code>training_plan_class</code> <code>str</code> <p>class name of the training plan</p> required <code>model_kwargs</code> <code>dict</code> <p>contains model args. Defaults to None.</p> required <code>training_kwargs</code> <code>dict</code> <p>contains training arguments. Defaults to None.</p> required <code>training</code> <code>bool</code> <p>whether to perform a model training or just to perform a validation check (model infering)</p> required <code>dataset</code> <code>dict</code> <p>dataset details to use in this round. It contains the dataset name, dataset's id, data path, its shape, its description... . Defaults to None.</p> required <code>params</code> <code>str</code> <p>parameters of the model</p> required <code>experiment_id</code> <code>str</code> <p>experiment id</p> required <code>researcher_id</code> <code>str</code> <p>researcher id</p> required <code>history_monitor</code> <code>HistoryMonitor</code> <p>Sends real-time feed-back to end-user during training</p> required <code>aggregator_args</code> <code>Dict[str, Any]</code> <p>Arguments managed by and shared with the researcher-side aggregator.</p> required <code>node_args</code> <code>Dict</code> <p>command line arguments for node. Can include: - <code>gpu (bool)</code>: propose use a GPU device if any is available. - <code>gpu_num (Union[int, None])</code>: if not None, use the specified GPU device instead of default     GPU device if this GPU device is available. - <code>gpu_only (bool)</code>: force use of a GPU device if any available, even if researcher     doesn't request for using a GPU.</p> required <code>tp_security_manager</code> <code>TrainingPlanSecurityManager</code> <p>Training plan security manager instance.</p> required <code>dlp_and_loading_block_metadata</code> <code>Optional[Tuple[dict, List[dict]]]</code> <p>Data loading plan to apply, or None if no DLP for this round.</p> <code>None</code> <code>round_number</code> <code>int</code> <p>number of the iteration for this experiment</p> <code>0</code> <code>aux_vars</code> <code>Optional[Dict[str, AuxVar]]</code> <p>Optional optimizer auxiliary variables.</p> <code>None</code> Source code in <code>fedbiomed/node/round.py</code> <pre><code>def __init__(\n    self,\n    root_dir: str,\n    db: str,\n    node_id: str,\n    training_plan: str,\n    training_plan_class: str,\n    model_kwargs: dict,\n    training_kwargs: dict,\n    training: bool ,\n    dataset: dict,\n    params: str,\n    experiment_id: str,\n    researcher_id: str,\n    history_monitor: HistoryMonitor,\n    aggregator_args: Dict[str, Any],\n    node_args: Dict,\n    tp_security_manager: TrainingPlanSecurityManager,\n    round_number: int = 0,\n    dlp_and_loading_block_metadata: Optional[Tuple[dict, List[dict]]] = None,\n    aux_vars: Optional[Dict[str, AuxVar]] = None,\n) -&gt; None:\n    \"\"\"Constructor of the class\n\n    Args:\n        root_dir: Root fedbiomed directory where node instance files will be stored.\n        db: Path to node database file.\n        node_id: Node id\n        training_plan: code of the training plan for this round\n        training_plan_class: class name of the training plan\n        model_kwargs: contains model args. Defaults to None.\n        training_kwargs: contains training arguments. Defaults to None.\n        training: whether to perform a model training or just to perform a validation check (model infering)\n        dataset: dataset details to use in this round. It contains the dataset name, dataset's id,\n            data path, its shape, its description... . Defaults to None.\n        params: parameters of the model\n        experiment_id: experiment id\n        researcher_id: researcher id\n        history_monitor: Sends real-time feed-back to end-user during training\n        aggregator_args: Arguments managed by and shared with the\n            researcher-side aggregator.\n        node_args: command line arguments for node. Can include:\n            - `gpu (bool)`: propose use a GPU device if any is available.\n            - `gpu_num (Union[int, None])`: if not None, use the specified GPU device instead of default\n                GPU device if this GPU device is available.\n            - `gpu_only (bool)`: force use of a GPU device if any available, even if researcher\n                doesn't request for using a GPU.\n        tp_security_manager: Training plan security manager instance.\n        dlp_and_loading_block_metadata: Data loading plan to apply, or None if no DLP for this round.\n        round_number: number of the iteration for this experiment\n        aux_vars: Optional optimizer auxiliary variables.\n    \"\"\"\n    self._node_id = node_id\n    self._db = db\n    self._dir = root_dir\n\n    self.dataset = dataset\n    self.training_plan_source = training_plan\n    self.training_plan_class = training_plan_class\n    self.params = params\n    self.experiment_id = experiment_id\n    self.researcher_id = researcher_id\n    self.history_monitor = history_monitor\n    self.aggregator_args = aggregator_args\n    self.aux_vars = aux_vars or {}\n    self.node_args = node_args\n    self.training = training\n    self._dlp_and_loading_block_metadata = dlp_and_loading_block_metadata\n    self.training_kwargs = training_kwargs\n    self.model_arguments = model_kwargs\n\n    # Class attributes\n    self.tp_security_manager = tp_security_manager\n    self.training_plan = None\n    self.testing_arguments = None\n    self.loader_arguments = None\n    self.training_arguments = None\n    self._secure_aggregation = None\n    self.is_test_data_shuffled: bool = False\n    self._testing_indexes: Dict = {\n        'testing_index': [],\n        'training_index': [],\n        'test_ratio': None\n    }\n    self._round = round_number\n    self._node_state_manager: NodeStateManager = NodeStateManager(\n        self._dir, self._node_id, self._db\n    )\n    self._temp_dir = tempfile.TemporaryDirectory()\n    self._keep_files_dir = self._temp_dir.name\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round-attributes","title":"Attributes","text":""},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.aggregator_args","title":"aggregator_args  <code>instance-attribute</code>","text":"<pre><code>aggregator_args = aggregator_args\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.aux_vars","title":"aux_vars  <code>instance-attribute</code>","text":"<pre><code>aux_vars = aux_vars or {}\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset = dataset\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.experiment_id","title":"experiment_id  <code>instance-attribute</code>","text":"<pre><code>experiment_id = experiment_id\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.history_monitor","title":"history_monitor  <code>instance-attribute</code>","text":"<pre><code>history_monitor = history_monitor\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.is_test_data_shuffled","title":"is_test_data_shuffled  <code>instance-attribute</code>","text":"<pre><code>is_test_data_shuffled = False\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.loader_arguments","title":"loader_arguments  <code>instance-attribute</code>","text":"<pre><code>loader_arguments = None\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.model_arguments","title":"model_arguments  <code>instance-attribute</code>","text":"<pre><code>model_arguments = model_kwargs\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.node_args","title":"node_args  <code>instance-attribute</code>","text":"<pre><code>node_args = node_args\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params = params\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.researcher_id","title":"researcher_id  <code>instance-attribute</code>","text":"<pre><code>researcher_id = researcher_id\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.testing_arguments","title":"testing_arguments  <code>instance-attribute</code>","text":"<pre><code>testing_arguments = None\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.tp_security_manager","title":"tp_security_manager  <code>instance-attribute</code>","text":"<pre><code>tp_security_manager = tp_security_manager\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.training","title":"training  <code>instance-attribute</code>","text":"<pre><code>training = training\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.training_arguments","title":"training_arguments  <code>instance-attribute</code>","text":"<pre><code>training_arguments = None\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.training_kwargs","title":"training_kwargs  <code>instance-attribute</code>","text":"<pre><code>training_kwargs = training_kwargs\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.training_plan","title":"training_plan  <code>instance-attribute</code>","text":"<pre><code>training_plan = None\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.training_plan_class","title":"training_plan_class  <code>instance-attribute</code>","text":"<pre><code>training_plan_class = training_plan_class\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.training_plan_source","title":"training_plan_source  <code>instance-attribute</code>","text":"<pre><code>training_plan_source = training_plan\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round-functions","title":"Functions","text":""},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.collect_optim_aux_var","title":"collect_optim_aux_var","text":"<pre><code>collect_optim_aux_var()\n</code></pre> <p>Collect auxiliary variables from the wrapped Optimizer, if any.</p> <p>If the TrainingPlan does not use a Fed-BioMed Optimizer, return an empty dict. If it does not hold any BaseOptimizer however, raise a FedbiomedRoundError.</p> <p>Returns:</p> Type Description <code>Dict[str, AuxVar]</code> <p>Auxiliary variables, as a <code>{module_name: module_auxvar}</code> dict.</p> Source code in <code>fedbiomed/node/round.py</code> <pre><code>def collect_optim_aux_var(\n    self,\n) -&gt; Dict[str, AuxVar]:\n    \"\"\"Collect auxiliary variables from the wrapped Optimizer, if any.\n\n    If the TrainingPlan does not use a Fed-BioMed Optimizer, return an\n    empty dict. If it does not hold any BaseOptimizer however, raise a\n    FedbiomedRoundError.\n\n    Returns:\n        Auxiliary variables, as a `{module_name: module_auxvar}` dict.\n    \"\"\"\n    optimizer = self._get_base_optimizer()\n    if isinstance(optimizer.optimizer, Optimizer):\n        return optimizer.optimizer.get_aux()\n    return {}\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.initialize_arguments","title":"initialize_arguments","text":"<pre><code>initialize_arguments(previous_state_id=None)\n</code></pre> <p>Initializes arguments for training and testing and the NodeStateManager, the latter handling Node state loading and saving.</p> <p>Parameters:</p> Name Type Description Default <code>previous_state_id</code> <code>Optional[str]</code> <p>previous Node state id. Defaults to None (which is the state_id default value for the first Round).</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>A dictionary containing the error message if an error is triggered while parsing training and testing</p> <code>Optional[Dict[str, Any]]</code> <p>arguments, None otherwise.</p> <p>!!! \"Note\"     If secure aggregation is activated, model weights will be encrypted as well as the     optimizer's auxiliary variables (only if the optimizer used is a <code>DeclearnOptimizer</code>).</p> Source code in <code>fedbiomed/node/round.py</code> <pre><code>def initialize_arguments(self,\n                         previous_state_id: Optional[str] = None) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Initializes arguments for training and testing and the NodeStateManager, the latter handling\n    Node state loading and saving.\n\n    Args:\n        previous_state_id: previous Node state id. Defaults to None (which is the state_id default value for the first Round).\n\n    Returns:\n        A dictionary containing the error message if an error is triggered while parsing training and testing\n        arguments, None otherwise.\n\n    !!! \"Note\"\n        If secure aggregation is activated, model weights will be encrypted as well as the\n        optimizer's auxiliary variables (only if the optimizer used is a `DeclearnOptimizer`).\n    \"\"\"\n    # initialize Node State Manager\n    self._node_state_manager.initialize(previous_state_id=previous_state_id,\n                                        testing=not self.training)\n    return self._initialize_validate_training_arguments()\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.process_optim_aux_var","title":"process_optim_aux_var","text":"<pre><code>process_optim_aux_var()\n</code></pre> <p>Process researcher-emitted Optimizer auxiliary variables, if any.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Error message, empty if the operation was successful.</p> Source code in <code>fedbiomed/node/round.py</code> <pre><code>def process_optim_aux_var(self) -&gt; Optional[str]:\n    \"\"\"Process researcher-emitted Optimizer auxiliary variables, if any.\n\n    Returns:\n        Error message, empty if the operation was successful.\n    \"\"\"\n    # Early-exit if there are no auxiliary variables to process.\n    if not any(self.aux_vars):\n        return None\n    # Fetch the training plan's BaseOptimizer.\n    try:\n        optimizer = self._get_base_optimizer()\n    except FedbiomedRoundError as exc:\n        return str(exc)\n    # Verify that the BaseOptimizer wraps an Optimizer.\n    if not isinstance(optimizer.optimizer, Optimizer):\n        return (\n            \"Received Optimizer auxiliary variables, but the \"\n            \"TrainingPlan does not manage a compatible Optimizer.\"\n        )\n    # Pass auxiliary variables to the Optimizer.\n    try:\n        optimizer.optimizer.set_aux(self.aux_vars)\n    except FedbiomedOptimizerError as exc:\n        return (\n            \"TrainingPlan Optimizer failed to ingest the provided \"\n            f\"auxiliary variables: {repr(exc)}\"\n        )\n    # early stop if secagg is activated and optimizer has more than one module that accepts\n    # auxiliary variable\n    if optimizer.count_nb_auxvar() &gt; 1 and self._secure_aggregation.use_secagg:\n        return (\n            \"Can not parse more than one `declearn` module requiring auxiliary variables while\"\n            \" Secure Aggregation activated. Aborting...\"\n        )\n    return None\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.run_model_training","title":"run_model_training","text":"<pre><code>run_model_training(tp_approval, secagg_insecure_validation, secagg_active, force_secagg, secagg_arguments=None)\n</code></pre> <p>Runs one round of model training</p> <p>Parameters:</p> Name Type Description Default <code>tp_approval</code> <code>bool</code> <p>True if training plan approval by node is requested</p> required <code>secagg_insecure_validation</code> <code>bool</code> <p>True if (potentially insecure) consistency check is enabled</p> required <code>secagg_active</code> <code>bool</code> <p>True if secure aggregation is enabled on node</p> required <code>force_secagg</code> <code>bool</code> <p>True is secure aggregation is mandatory on node</p> required <code>secagg_arguments</code> <code>Union[Dict[str, Any], None]</code> <p>arguments for secure aggregation, some are specific to the scheme</p> <code>None</code> <p>Returns:</p> Type Description <code>TrainReply</code> <p>Returns the corresponding node message, training reply instance</p> Source code in <code>fedbiomed/node/round.py</code> <pre><code>def run_model_training(\n    self,\n    tp_approval: bool,\n    secagg_insecure_validation: bool,\n    secagg_active: bool,\n    force_secagg: bool,\n    secagg_arguments: Union[Dict[str, Any], None] = None,\n) -&gt; TrainReply:\n    \"\"\"Runs one round of model training\n\n    Args:\n        tp_approval: True if training plan approval by node is requested\n        secagg_insecure_validation: True if (potentially insecure) consistency check is enabled\n        secagg_active: True if secure aggregation is enabled on node\n        force_secagg: True is secure aggregation is mandatory on node\n        secagg_arguments: arguments for secure aggregation, some are specific to the scheme\n\n    Returns:\n        Returns the corresponding node message, training reply instance\n    \"\"\"\n    # Validate secagg status. Raises error if the training request is not compatible with\n    # secure aggregation settings\n\n    try:\n        self._secure_aggregation = SecaggRound(\n            db=self._db,\n            node_id=self._node_id,\n            secagg_arguments=secagg_arguments,\n            secagg_active=secagg_active,\n            force_secagg=force_secagg,\n            experiment_id=self.experiment_id\n        )\n    except FedbiomedSecureAggregationError as e:\n        logger.error(str(e))\n        return self._send_round_reply(\n            success=False,\n            message='Could not configure secure aggregation on node')\n\n    # Validate and load training plan\n    if tp_approval:\n        approved, training_plan_ = self.tp_security_manager.\\\n            check_training_plan_status(\n                self.training_plan_source,\n                TrainingPlanApprovalStatus.APPROVED)\n\n        if not approved:\n            return self._send_round_reply(\n                False,\n                f'Requested training plan is not approved by the node: {self._node_id}')\n        else:\n            logger.info(f'Training plan has been approved by the node {training_plan_[\"name\"]}',\n                        researcher_id=self.researcher_id)\n\n    # Import training plan, save to file, reload, instantiate a training plan\n    try:\n        CurrentTPModule, CurrentTrainingPlan = utils.import_class_from_spec(\n            code=self.training_plan_source, class_name=self.training_plan_class)\n        self.training_plan = CurrentTrainingPlan()\n    except Exception:\n        error_message = \"Cannot instantiate training plan object.\"\n        return self._send_round_reply(success=False, message=error_message)\n\n    # save and load training plan to a file to be sure\n    # 1. a file is associated to training plan so we can read its source, etc.\n    # 2. all dependencies are applied\n    training_plan_module = 'model_' + str(uuid.uuid4())\n    training_plan_file = os.path.join(self._keep_files_dir, training_plan_module + '.py')\n    try:\n        self.training_plan.save_code(training_plan_file, from_code=self.training_plan_source)\n    except Exception as e:\n        error_message = \"Cannot save the training plan to a local tmp dir\"\n        logger.error(f\"Cannot save the training plan to a local tmp dir : {e}\")\n        return self._send_round_reply(success=False, message=error_message)\n\n    del CurrentTrainingPlan\n    del CurrentTPModule\n\n    try:\n        CurrentTPModule, self.training_plan = utils.import_class_object_from_file(\n            training_plan_file, self.training_plan_class)\n    except Exception:\n        error_message = \"Cannot load training plan object from file.\"\n        return self._send_round_reply(success=False, message=error_message)\n\n    try:\n        self.training_plan.post_init(model_args=self.model_arguments,\n                                     training_args=self.training_arguments,\n                                     aggregator_args=self.aggregator_args)\n    except Exception:\n        error_message = \"Can't initialize training plan with the arguments.\"\n        return self._send_round_reply(success=False, message=error_message)\n\n    # load node state\n    previous_state_id = self._node_state_manager.previous_state_id\n    if previous_state_id is not None:\n        try:\n            self._load_round_state(previous_state_id)\n        except Exception:\n            # don't send error details\n            return self._send_round_reply(success=False, message=\"Can't read previous node state.\")\n\n    # Load model parameters received from researcher\n    try:\n        self.training_plan.set_model_params(self.params)\n    except Exception:\n        error_message = \"Cannot initialize model parameters.\"\n        return self._send_round_reply(success=False, message=error_message)\n    # ---------------------------------------------------------------------\n\n    # Process Optimizer auxiliary variables, if any.\n    error_message = self.process_optim_aux_var()\n    if error_message:\n        return self._send_round_reply(success=False, message=error_message)\n\n    # Split training and validation data -------------------------------------\n    try:\n\n        self._set_training_testing_data_loaders()\n\n    except FedbiomedError as fe:\n        error_message = f\"Can not create validation/train data: {repr(fe)}\"\n        return self._send_round_reply(success=False, message=error_message)\n    except Exception as e:\n        error_message = f\"Undetermined error while creating data for training/validation. Can not create \" \\\n                        f\"validation/train data: {repr(e)}\"\n        return self._send_round_reply(success=False, message=error_message)\n    # ------------------------------------------------------------------------\n\n\n    # Validation Before Training\n    if self.testing_arguments.get('test_on_global_updates', False) is not False:\n\n        # Last control to make sure validation data loader is set.\n        if self.training_plan.testing_data_loader is not None:\n            try:\n                self.training_plan.testing_routine(metric=self.testing_arguments.get('test_metric', None),\n                                                   metric_args=self.testing_arguments.get('test_metric_args', {}),\n                                                   history_monitor=self.history_monitor,\n                                                   before_train=True)\n            except FedbiomedError as e:\n                logger.error(f\"{ErrorNumbers.FB314}: During the validation phase on global parameter updates; \"\n                             f\"{repr(e)}\", researcher_id=self.researcher_id)\n            except Exception as e:\n                logger.error(f\"Undetermined error during the testing phase on global parameter updates: \"\n                             f\"{repr(e)}\", researcher_id=self.researcher_id)\n        else:\n            logger.error(f\"{ErrorNumbers.FB314}: Can not execute validation routine due to missing testing dataset\"\n                         f\"Please make sure that `test_ratio` has been set correctly\",\n                         researcher_id=self.researcher_id)\n\n    # If training is activated.\n    if self.training:\n        results = {}  # type: Dict[str, Any]\n\n        # Perform the training round.\n        if self.training_plan.training_data_loader is not None:\n            try:\n                rtime_before = time.perf_counter()\n                ptime_before = time.process_time()\n                self.training_plan.training_routine(history_monitor=self.history_monitor,\n                                                    node_args=self.node_args)\n                rtime_after = time.perf_counter()\n                ptime_after = time.process_time()\n            except Exception as exc:\n                error_message = f\"Cannot train model in round: {repr(exc)}\"\n                return self._send_round_reply(success=False, message=error_message)\n\n        # Collect Optimizer auxiliary variables, if any.\n\n        try:\n            results['optim_aux_var'] = self.collect_optim_aux_var()\n        except (FedbiomedOptimizerError, FedbiomedRoundError) as exc:\n            error_message = f\"Cannot collect Optimizer auxiliary variables: {repr(exc)}\"\n            return self._send_round_reply(success=False, message=error_message)\n\n        # Validation after training\n        if self.testing_arguments.get('test_on_local_updates', False) is not False:\n\n            if self.training_plan.testing_data_loader is not None:\n                try:\n                    self.training_plan.testing_routine(metric=self.testing_arguments.get('test_metric', None),\n                                                       metric_args=self.testing_arguments.get('test_metric_args',\n                                                                                              {}),\n                                                       history_monitor=self.history_monitor,\n                                                       before_train=False)\n                except FedbiomedError as e:\n                    logger.error(\n                        f\"{ErrorNumbers.FB314.value}: During the validation phase on local parameter updates; \"\n                        f\"{repr(e)}\", researcher_id=self.researcher_id)\n                except Exception as e:\n                    logger.error(f\"Undetermined error during the validation phase on local parameter updates\"\n                                 f\"{repr(e)}\", researcher_id=self.researcher_id)\n            else:\n                logger.error(\n                    f\"{ErrorNumbers.FB314.value}: Can not execute validation routine due to missing testing \"\n                    f\"dataset please make sure that test_ratio has been set correctly\",\n                    researcher_id=self.researcher_id)\n\n        # FIXME: this will fail if `self.training_plan.training_data_loader = None` (see issue )\n        results[\"sample_size\"] = len(self.training_plan.training_data_loader.dataset)\n\n        results[\"encrypted\"] = False\n        model_weights = self.training_plan.after_training_params(flatten=self._secure_aggregation.use_secagg)\n\n        if self._secure_aggregation.use_secagg:\n            model_weights, enc_factor, aux_var = self._encrypt_weights_and_auxvar(\n                model_weights=model_weights,\n                optim_aux_var=results[\"optim_aux_var\"],\n                sample_size=results[\"sample_size\"],\n                secagg_insecure_validation=secagg_insecure_validation,\n            )\n            results[\"encrypted\"] = True\n            results[\"encryption_factor\"] = enc_factor\n            if aux_var is not None:\n                results[\"optim_aux_var\"] = aux_var.to_dict()\n        results['params'] = model_weights\n        results['optimizer_args'] = self.training_plan.optimizer_args()\n        results['state_id'] = self._node_state_manager.state_id\n\n        try:\n            self._save_round_state()\n        except Exception:\n            # don't send details to researcher\n            return self._send_round_reply(success=False, message=\"Can't save new node state.\")\n\n        # end : clean the namespace\n        try:\n            del self.training_plan\n            del CurrentTPModule\n        except Exception:\n            logger.debug('Exception raised while deleting training plan instance')\n\n        return self._send_round_reply(success=True,\n                                      timing={'rtime_training': rtime_after - rtime_before,\n                                              'ptime_training': ptime_after - ptime_before},\n                                      extend_with=results)\n    else:\n        # Only for validation\n        return self._send_round_reply(success=True)\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round-functions","title":"Functions","text":""},{"location":"developer/api/node/secagg/","title":"Secagg","text":""},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg-classes","title":"Classes","text":""},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggBaseSetup","title":"SecaggBaseSetup","text":"<pre><code>SecaggBaseSetup(db, node_id, researcher_id, secagg_id, parties, experiment_id)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Sets up a Secure Aggregation context element on the node side.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>str</code> <p>Path to database file the node.</p> required <code>node_id</code> <code>str</code> <p>ID of the node.</p> required <code>researcher_id</code> <code>str</code> <p>ID of the researcher that requests setup</p> required <code>secagg_id</code> <code>str</code> <p>ID of secagg context element for this setup request</p> required <code>parties</code> <code>List[str]</code> <p>List of parties participating in the secagg context element setup</p> required <code>experiment_id</code> <code>Union[str, None]</code> <p>ID of the experiment to which this secagg context element is attached</p> required <p>Raises:</p> Type Description <code>FedbiomedSecaggError</code> <p>bad argument type or value</p> Source code in <code>fedbiomed/node/secagg/_secagg_setups.py</code> <pre><code>def __init__(\n    self,\n    db: str,\n    node_id: str,\n    researcher_id: str,\n    secagg_id: str,\n    parties: List[str],\n    experiment_id: Union[str, None],\n):\n    \"\"\"Constructor of the class.\n\n    Args:\n        db: Path to database file the node.\n        node_id: ID of the node.\n        researcher_id: ID of the researcher that requests setup\n        secagg_id: ID of secagg context element for this setup request\n        parties: List of parties participating in the secagg context element setup\n        experiment_id: ID of the experiment to which this secagg context element\n            is attached\n\n    Raises:\n        FedbiomedSecaggError: bad argument type or value\n    \"\"\"\n    errmess: str = \"\"\n    if len(parties) &lt; self._min_num_parties:\n        errmess = (\n            f\"{ErrorNumbers.FB318.value}: bad parameter `parties` : {parties} : need  \"\n            f\"at least {self._min_num_parties} parties for secure aggregation, but got \"\n            f\"{len(parties)}\"\n        )\n\n    if researcher_id is None:\n        errmess = (\n            f\"{ErrorNumbers.FB318.value}: argument `researcher_id` must be a non-None value\"\n        )\n\n    if errmess:\n        # if one of the above condition is met, raise error\n        logger.error(errmess)\n        raise FedbiomedSecaggError(errmess)\n\n    # assign argument values\n    self._secagg_manager = SecaggManager(db, self._element.value)()\n    self._node_id = node_id\n    self._researcher_id = researcher_id\n    self._secagg_id = secagg_id\n    self._experiment_id = experiment_id\n    self._parties = parties\n</code></pre>"},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggBaseSetup-attributes","title":"Attributes","text":""},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggBaseSetup.element","title":"element  <code>property</code>","text":"<pre><code>element\n</code></pre> <p>Getter for secagg context element type</p> <p>Returns:</p> Type Description <code>Enum</code> <p>secagg context element name</p>"},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggBaseSetup.experiment_id","title":"experiment_id  <code>property</code>","text":"<pre><code>experiment_id\n</code></pre> <p>Getter for <code>experiment_id</code></p> <p>Returns:</p> Type Description <code>str</code> <p>ID of the experiment to which this secagg context element is attached</p>"},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggBaseSetup.researcher_id","title":"researcher_id  <code>property</code>","text":"<pre><code>researcher_id\n</code></pre> <p>Getter for <code>researcher_id</code></p> <p>Returns:</p> Type Description <code>str</code> <p>researcher unique ID</p>"},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggBaseSetup.secagg_id","title":"secagg_id  <code>property</code>","text":"<pre><code>secagg_id\n</code></pre> <p>Getter for <code>secagg_id</code></p> <p>Returns:</p> Type Description <code>str</code> <p>secagg context element unique ID</p>"},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggBaseSetup-functions","title":"Functions","text":""},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggBaseSetup.setup","title":"setup","text":"<pre><code>setup()\n</code></pre> <p>Set up a secagg context element.</p> <p>Returns:</p> Type Description <code>Message</code> <p>message to return to the researcher after the setup</p> Source code in <code>fedbiomed/node/secagg/_secagg_setups.py</code> <pre><code>def setup(self) -&gt; Message:\n    \"\"\"Set up a secagg context element.\n\n    Returns:\n        message to return to the researcher after the setup\n    \"\"\"\n    try:\n        return self._setup_specific()\n    except FedbiomedError as e:\n        logger.debug(f\"{e}\")\n        return self._create_secagg_reply(\n            False,\n            \"Can not setup secure aggregation context \"\n            f\"on node for {self._secagg_id}. {e}\",\n        )\n</code></pre>"},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggDHSetup","title":"SecaggDHSetup","text":"<pre><code>SecaggDHSetup(*args, n2n_router, grpc_client, pending_requests, controller_data, **kwargs)\n</code></pre> <p>               Bases: <code>_SecaggNN</code></p> <p>Sets up a server key Secure Aggregation context element on the node side.</p> <p>Parameters:</p> Name Type Description Default <code>n2n_router</code> <code>NodeToNodeRouter</code> <p>object managing node to node messages</p> required <code>grpc_client</code> <code>GrpcController</code> <p>object managing the communication with other components</p> required <code>pending_requests</code> <code>EventWaitExchange</code> <p>object for receiving overlay node to node messages</p> required <code>controller_data</code> <code>EventWaitExchange</code> <p>object for passing data to the node controller</p> required <code>*args</code> <p>Please see [SecaggBaseSetup]</p> <code>()</code> <code>**kwargs</code> <p>Please see [SecaggBaseSetup]</p> <code>{}</code> <p>Raises:     FedbiomedSecaggError: bad argument type or value</p> Source code in <code>fedbiomed/node/secagg/_secagg_setups.py</code> <pre><code>def __init__(\n        self,\n        *args,\n        n2n_router: NodeToNodeRouter,\n        grpc_client: GrpcController,\n        pending_requests: EventWaitExchange,\n        controller_data: EventWaitExchange,\n        **kwargs,\n):\n    \"\"\"Constructor of the class.\n\n    Args:\n        n2n_router: object managing node to node messages\n        grpc_client: object managing the communication with other components\n        pending_requests: object for receiving overlay node to node messages\n        controller_data: object for passing data to the node controller\n        *args: Please see [SecaggBaseSetup]\n        **kwargs: Please see [SecaggBaseSetup]\n    Raises:\n        FedbiomedSecaggError: bad argument type or value\n    \"\"\"\n\n    super().__init__(*args, **kwargs)\n\n    # self._secagg_manager = SKManager\n    self._n2n_router = n2n_router\n    self._grpc_client = grpc_client\n    self._pending_requests = pending_requests\n    self._controller_data = controller_data\n</code></pre>"},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggRound","title":"SecaggRound","text":"<pre><code>SecaggRound(db, node_id, secagg_arguments, secagg_active, force_secagg, experiment_id)\n</code></pre> <p>This class wraps secure aggregation schemes</p> <p>Attributes:</p> Name Type Description <code>scheme</code> <code>_SecaggSchemeRound | None</code> <p>Secure aggregation scheme</p> <code>use_secagg</code> <code>bool</code> <p>True if secure aggregation is activated for round</p> Source code in <code>fedbiomed/node/secagg/_secagg_round.py</code> <pre><code>def __init__(\n    self,\n    db: str,\n    node_id: str,\n    secagg_arguments: Dict[str, Any] | None,\n    secagg_active: bool,\n    force_secagg: bool,\n    experiment_id: str\n) -&gt; None:\n    \"\"\"Constructor of the class\"\"\"\n\n    self._node_id = node_id\n    self._secagg_active = secagg_active\n    self._force_secagg = force_secagg\n\n    self.use_secagg: bool = False\n    self.scheme: _SecaggSchemeRound | None = None\n\n    if not secagg_arguments and self._force_secagg:\n        raise FedbiomedSecureAggregationError(\n            f\"{ErrorNumbers.FB318.value}: Node requires to apply secure aggregation but \"\n            f\"training request does not define it.\")\n\n    if secagg_arguments:\n        if not self._secagg_active:\n            raise FedbiomedSecureAggregationError(\n                f\"{ErrorNumbers.FB318.value} Requesting secure aggregation while \"\n                \"it's not activated on the node.\"\n            )\n\n        sn = secagg_arguments.get('secagg_scheme')\n\n        if sn is None:\n            raise FedbiomedSecureAggregationError(\n                f\"{ErrorNumbers.FB318.value}: Secagg scheme value missing in \"\n                \"the argument `secagg_arguments`\"\n            )\n        try:\n            _scheme = SecureAggregationSchemes(sn)\n        except ValueError as e:\n            raise FedbiomedSecureAggregationError(\n                f\"{ErrorNumbers.FB318.value}: Bad secagg scheme value in train request: {sn}\"\n            ) from e\n\n        self.scheme = SecaggRound.element2class[_scheme.value](\n            db, node_id, secagg_arguments, experiment_id\n        )\n        self.use_secagg = True\n</code></pre>"},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggRound-attributes","title":"Attributes","text":""},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggRound.element2class","title":"element2class  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>element2class = {value: _JLSRound, value: _LomRound}\n</code></pre>"},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggRound.scheme","title":"scheme  <code>instance-attribute</code>","text":"<pre><code>scheme = None\n</code></pre>"},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggRound.use_secagg","title":"use_secagg  <code>instance-attribute</code>","text":"<pre><code>use_secagg = False\n</code></pre>"},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggServkeySetup","title":"SecaggServkeySetup","text":"<pre><code>SecaggServkeySetup(*args, n2n_router, grpc_client, pending_requests, controller_data, **kwargs)\n</code></pre> <p>               Bases: <code>_SecaggNN</code></p> <p>Secure aggregation setup phase for ServerKey generation on the node side</p> <p>Parameters:</p> Name Type Description Default <code>n2n_router</code> <code>NodeToNodeRouter</code> <p>object managing node to node messages</p> required <code>grpc_client</code> <code>GrpcController</code> <p>object managing the communication with other components</p> required <code>pending_requests</code> <code>EventWaitExchange</code> <p>object for receiving overlay node to node messages</p> required <code>controller_data</code> <code>EventWaitExchange</code> <p>object for passing data to the node controller</p> required <code>*args</code> <p>Please see [SecaggBaseSetup]</p> <code>()</code> <code>**kwargs</code> <p>Please see [SecaggBaseSetup]</p> <code>{}</code> <p>Raises:     FedbiomedSecaggError: bad argument type or value</p> Source code in <code>fedbiomed/node/secagg/_secagg_setups.py</code> <pre><code>def __init__(\n        self,\n        *args,\n        n2n_router: NodeToNodeRouter,\n        grpc_client: GrpcController,\n        pending_requests: EventWaitExchange,\n        controller_data: EventWaitExchange,\n        **kwargs,\n):\n    \"\"\"Constructor of the class.\n\n    Args:\n        n2n_router: object managing node to node messages\n        grpc_client: object managing the communication with other components\n        pending_requests: object for receiving overlay node to node messages\n        controller_data: object for passing data to the node controller\n        *args: Please see [SecaggBaseSetup]\n        **kwargs: Please see [SecaggBaseSetup]\n    Raises:\n        FedbiomedSecaggError: bad argument type or value\n    \"\"\"\n\n    super().__init__(*args, **kwargs)\n\n    # self._secagg_manager = SKManager\n    self._n2n_router = n2n_router\n    self._grpc_client = grpc_client\n    self._pending_requests = pending_requests\n    self._controller_data = controller_data\n</code></pre>"},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggSetup","title":"SecaggSetup","text":"<pre><code>SecaggSetup(element, **kwargs)\n</code></pre> <p>Factory class for instantiating any type of node secagg context element setup class</p> Source code in <code>fedbiomed/node/secagg/_secagg_setups.py</code> <pre><code>def __init__(self, element: int, **kwargs):\n    \"\"\"Constructor of the class\"\"\"\n    self._element = element\n    self.kwargs = kwargs\n</code></pre>"},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggSetup-attributes","title":"Attributes","text":""},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggSetup.element2class","title":"element2class  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>element2class = {name: SecaggServkeySetup, name: SecaggDHSetup}\n</code></pre>"},{"location":"developer/api/node/secagg/#fedbiomed.node.secagg.SecaggSetup.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"developer/api/node/secagg_manager/","title":"Secagg Manager","text":"<p>Interface with the node secure aggregation element database</p>"},{"location":"developer/api/node/secagg_manager/#fedbiomed.node.secagg_manager-attributes","title":"Attributes","text":""},{"location":"developer/api/node/secagg_manager/#fedbiomed.node.secagg_manager-classes","title":"Classes","text":""},{"location":"developer/api/node/secagg_manager/#fedbiomed.node.secagg_manager.SecaggManager","title":"SecaggManager","text":"<pre><code>SecaggManager(db, element)\n</code></pre> <p>Wrapper class for returning any type of node secagg element database manager</p> Source code in <code>fedbiomed/node/secagg_manager.py</code> <pre><code>def __init__(self, db: str, element: int):\n    \"\"\"Constructor of the class\n    \"\"\"\n    self._db = db\n    self._element = element\n</code></pre>"},{"location":"developer/api/node/secagg_manager/#fedbiomed.node.secagg_manager.SecaggManager-attributes","title":"Attributes","text":""},{"location":"developer/api/node/secagg_manager/#fedbiomed.node.secagg_manager.SecaggManager.element2class","title":"element2class  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>element2class = {name: SecaggServkeyManager, name: SecaggDhManager}\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/","title":"TrainingPlanSecurityManager","text":"<p>Manages training plan approval for a node.</p>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager-attributes","title":"Attributes","text":""},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.HASH_FUNCTIONS","title":"HASH_FUNCTIONS  <code>module-attribute</code>","text":"<pre><code>HASH_FUNCTIONS = {value: sha256, value: sha384, value: sha512, value: sha3_256, value: sha3_384, value: sha3_512, value: blake2s, value: blake2s}\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager-classes","title":"Classes","text":""},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager","title":"TrainingPlanSecurityManager","text":"<pre><code>TrainingPlanSecurityManager(db, node_id, hashing, tp_approval=False)\n</code></pre> <p>Manages training plan approval for a node.</p> <p>Creates a DB object for the table named as <code>Training plans</code> and builds a query object to query the database.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>str</code> <p>Path to database file.</p> required <code>node_id</code> <code>str</code> <p>ID of the active node.</p> required <code>hashing</code> <code>str</code> <p>Hashing algorithm</p> required <code>tp_approval</code> <code>bool</code> <p>True if training plan approval is requested</p> <code>False</code> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def __init__(\n    self,\n    db: str,\n    node_id: str,\n    hashing: str,\n    tp_approval: bool = False\n) -&gt; None:\n    \"\"\"Class constructor for TrainingPlanSecurityManager.\n\n    Creates a DB object for the table named as `Training plans` and builds a query object to query\n    the database.\n\n    Args:\n        db: Path to database file.\n        node_id: ID of the active node.\n        hashing: Hashing algorithm\n        tp_approval: True if training plan approval is requested\n    \"\"\"\n\n    self._node_id = node_id\n    self._tp_approval = tp_approval\n    self._default_tps = os.path.join(SHARE_DIR, 'envs', 'common', 'default_training_plans')\n    self._tinydb = TinyDB(db)\n    self._tinydb.table_class = DBTable\n    # dont use DB read cache for coherence when updating from multiple sources (eg: GUI and CLI)\n    self._db = self._tinydb.table(name=\"TrainingPlans\", cache_size=0)\n    self._database = Query()\n\n    self._hashing = hashing\n    self._tags_to_remove = [\"hash\", \"date_modified\", \"date_created\"]\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager-functions","title":"Functions","text":""},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.approve_training_plan","title":"approve_training_plan","text":"<pre><code>approve_training_plan(training_plan_id, extra_notes=None)\n</code></pre> <p>Approves a training plan stored into the database given its [<code>training_plan_id</code>]</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_id</code> <code>str</code> <p>id of the training plan.</p> required <code>extra_notes</code> <code>Union[str, None]</code> <p>notes detailing why training plan has been approved. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>True</code> <p>Currently always returns True</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def approve_training_plan(\n    self, training_plan_id: str, extra_notes: Union[str, None] = None\n) -&gt; True:\n    \"\"\"Approves a training plan stored into the database given its [`training_plan_id`]\n\n    Args:\n        training_plan_id: id of the training plan.\n        extra_notes: notes detailing why training plan has been approved. Defaults to None.\n\n    Returns:\n        Currently always returns True\n    \"\"\"\n    res = self._update_training_plan_status(\n        training_plan_id, TrainingPlanApprovalStatus.APPROVED, extra_notes\n    )\n    return res\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.check_hashes_for_registered_training_plans","title":"check_hashes_for_registered_training_plans","text":"<pre><code>check_hashes_for_registered_training_plans()\n</code></pre> <p>Checks registered training plans (training plans either rejected or approved).</p> <p>Makes sure training plan files exists and hashing algorithm is matched with specified algorithm in the config file.</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>cannot update training plan list in database</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def check_hashes_for_registered_training_plans(self):\n    \"\"\"Checks registered training plans (training plans either rejected or approved).\n\n    Makes sure training plan files exists and hashing algorithm is matched with specified\n    algorithm in the config file.\n\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: cannot update training plan list in database\n    \"\"\"\n\n    try:\n        training_plans, docs = self._db.search(\n            self._database.training_plan_type.all(\n                TrainingPlanStatus.REGISTERED.value\n            ),\n            add_docs=True,\n        )\n    except Exception as e:\n        raise FedbiomedTrainingPlanSecurityManagerError(\n            ErrorNumbers.FB606.value\n            + f\"database search operation failed, with following error: {str(e)}\"\n        )\n    logger.info(\"Checking hashes for registered training plans\")\n    if not training_plans:\n        logger.info(\"There are no training plans registered\")\n    else:\n        for training_plan, doc in zip(training_plans, docs):\n            # If training plan file is exists\n            if training_plan[\"algorithm\"] != self._hashing:\n                logger.info(\n                    f'Recreating hashing for : {training_plan[\"name\"]} \\t {training_plan[\"training_plan_id\"]}'\n                )\n                hashing, algorithm, _ = self._create_hash(\n                    training_plan[\"training_plan\"], from_string=True\n                )\n\n                # Verify no such training plan already exists in DB\n                self._check_training_plan_not_existing(None, hashing, None)\n\n                rtime = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S.%f\")\n                try:\n                    self._db.update(\n                        {\n                            \"hash\": hashing,\n                            \"algorithm\": algorithm,\n                            \"date_last_action\": rtime,\n                        },\n                        self._database.training_plan_id.all(\n                            training_plan[\"training_plan_id\"]\n                        ),\n                    )\n                except Exception as err:\n                    raise FedbiomedTrainingPlanSecurityManagerError(\n                        ErrorNumbers.FB606.value\n                        + \": database update failed, with error \"\n                        f\" {str(err)}\"\n                    )\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.check_training_plan_status","title":"check_training_plan_status","text":"<pre><code>check_training_plan_status(training_plan_source, state)\n</code></pre> <p>Checks whether training plan exists in database and has the specified status.</p> <p>Sends a query to database to search for hash of requested training plan. If the hash matches with one of the training plans hashes in the DB, and if training plan has the specified status {approved, rejected, pending} or training_plan_type {registered, requested, default}.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_source</code> <code>str</code> <p>The source code of requested training plan</p> required <code>state</code> <code>Union[TrainingPlanApprovalStatus, TrainingPlanStatus, None]</code> <p>training plan status or training plan type, to check against training plan. <code>None</code> accepts any training plan status or type.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, Dict[str, Any]]</code> <p>A tuple (is_status, training plan) where</p> <ul> <li>status: Whether training plan exists in database     with specified status (returns True) or not (False)</li> <li>training_plan: Dictionary containing fields     related to the training plan. If database search request failed,     returns None instead.</li> </ul> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>bad argument type or value</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>database access problem</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def check_training_plan_status(\n    self,\n    training_plan_source: str,\n    state: Union[TrainingPlanApprovalStatus, TrainingPlanStatus, None],\n) -&gt; Tuple[bool, Dict[str, Any]]:\n    \"\"\"Checks whether training plan exists in database and has the specified status.\n\n    Sends a query to database to search for hash of requested training plan.\n    If the hash matches with one of the\n    training plans hashes in the DB, and if training plan has the specified status {approved, rejected, pending}\n    or training_plan_type {registered, requested, default}.\n\n    Args:\n        training_plan_source: The source code of requested training plan\n        state: training plan status or training plan type, to check against training plan. `None` accepts\n            any training plan status or type.\n\n    Returns:\n        A tuple (is_status, training plan) where\n\n            - status: Whether training plan exists in database\n                with specified status (returns True) or not (False)\n            - training_plan: Dictionary containing fields\n                related to the training plan. If database search request failed,\n                returns None instead.\n\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: bad argument type or value\n        FedbiomedTrainingPlanSecurityManagerError: database access problem\n    \"\"\"\n\n    # Create hash for requested training plan\n    req_training_plan_hash, *_ = self._create_hash(\n        training_plan_source, from_string=True\n    )\n\n    # If node allows defaults training plans search hash for all training plan types\n    # otherwise search only for `registered` training plans\n\n    if state is None:\n        _all_training_plans_with_status = None\n    elif isinstance(state, TrainingPlanApprovalStatus):\n        _all_training_plans_with_status = (\n            self._database.training_plan_status == state.value\n        )\n    elif isinstance(state, TrainingPlanStatus):\n        _all_training_plans_with_status = (\n            self._database.training_plan_type == state.value\n        )\n    else:\n        raise FedbiomedTrainingPlanSecurityManagerError(\n            f\"{ErrorNumbers.FB606.value} + status should be either TrainingPlanApprovalStatus or \"\n            f\"TrainingPlanStatus, but got {type(state)}\"\n        )\n    _all_training_plans_which_have_req_hash = (\n        self._database.hash == req_training_plan_hash\n    )\n\n    if _all_training_plans_with_status is None:\n        # check only against hash\n        training_plan = self._db.get(_all_training_plans_which_have_req_hash)\n    else:\n        # check against hash and status\n        training_plan = self._db.get(\n            _all_training_plans_with_status\n            &amp; _all_training_plans_which_have_req_hash\n        )\n\n    status = True if training_plan else False\n\n    return status, training_plan\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.delete_training_plan","title":"delete_training_plan","text":"<pre><code>delete_training_plan(training_plan_id)\n</code></pre> <p>Removes training plan file from database.</p> <p>Only removes <code>registered</code> and <code>requested</code> type of training plans from the database. Does not remove the corresponding training plan file from the disk. Default training plans should be removed from the directory</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_id</code> <code>str</code> <p>The id of the registered training plan.</p> required <p>Returns:</p> Type Description <code>True</code> <p>Currently always returns True.</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>bad type for parameter</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>cannot read or remove training plan from the database</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>training plan is not a <code>registered</code> training plan (thus a <code>default</code> training plan)</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def delete_training_plan(self, training_plan_id: str) -&gt; True:\n    \"\"\"Removes training plan file from database.\n\n    Only removes `registered` and `requested` type of training plans from the database.\n    Does not remove the corresponding training plan file from the disk.\n    Default training plans should be removed from the directory\n\n    Args:\n        training_plan_id: The id of the registered training plan.\n\n    Returns:\n        Currently always returns True.\n\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: bad type for parameter\n        FedbiomedTrainingPlanSecurityManagerError: cannot read or remove training plan from the database\n        FedbiomedTrainingPlanSecurityManagerError: training plan is not a `registered` training plan\n            (thus a `default` training plan)\n    \"\"\"\n\n    try:\n        training_plan, doc = self._db.get(\n            self._database.training_plan_id == training_plan_id, add_docs=True\n        )\n    except Exception as err:\n        raise FedbiomedTrainingPlanSecurityManagerError(\n            ErrorNumbers.FB606.value + \": cannot get training plan from database.\"\n            f\"Details: {str(err)}\"\n        )\n\n    if training_plan is None:\n        raise FedbiomedTrainingPlanSecurityManagerError(\n            ErrorNumbers.FB606.value\n            + f\": training plan {training_plan_id} not in database\"\n        )\n\n    if training_plan[\"training_plan_type\"] != TrainingPlanStatus.DEFAULT.value:\n        try:\n            self._db.remove(doc_ids=[doc.doc_id])\n        except Exception as err:\n            raise FedbiomedTrainingPlanSecurityManagerError(\n                ErrorNumbers.FB606.value\n                + f\": cannot remove training plan from database. Details: {str(err)}\"\n            )\n    else:\n        raise FedbiomedTrainingPlanSecurityManagerError(\n            ErrorNumbers.FB606.value\n            + \"For default training plans, please remove training plan file from \"\n            \"`default_training_plans` and restart your node\"\n        )\n\n    return True\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.get_training_plan_by_id","title":"get_training_plan_by_id","text":"<pre><code>get_training_plan_by_id(training_plan_id, secure=True)\n</code></pre> <p>Get a training plan in database given his <code>training_plan_id</code>.</p> <p>Also add a <code>content</code> key to the returned dictionary. This method is not used within the library source code but it is used for Fed-BioMed GUI.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_id</code> <code>str</code> <p>id of the training plan to pick from the database</p> required <code>secure</code> <code>bool</code> <p>if <code>True</code> then strip some security sensitive fields</p> <code>True</code> <code>content</code> <p>if <code>True</code> add content of training plan in <code>content</code> key of returned training plan. If <code>False</code> then <code>content</code> key value is <code>None</code></p> required <p>Returns:</p> Type Description <code>Union[Dict[str, Any], None]</code> <p>training plan entry from database through a query based on the training plan_id.</p> <code>Union[Dict[str, Any], None]</code> <p>If there is no training plan matching [<code>training_plan_id</code>], returns None</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>bad argument type</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>database access problem</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def get_training_plan_by_id(\n    self, training_plan_id: str, secure: bool = True\n) -&gt; Union[Dict[str, Any], None]:\n    \"\"\"Get a training plan in database given his `training_plan_id`.\n\n    Also add a `content` key to the returned dictionary. This method is not used within the\n    library source code but it is used for Fed-BioMed GUI.\n\n    Args:\n        training_plan_id: id of the training plan to pick from the database\n        secure: if `True` then strip some security sensitive fields\n        content: if `True` add content of training plan in `content` key of returned training plan. If `False` then\n            `content` key value is `None`\n\n\n    Returns:\n        training plan entry from database through a query based on the training plan_id.\n        If there is no training plan matching [`training_plan_id`], returns None\n\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: bad argument type\n        FedbiomedTrainingPlanSecurityManagerError: database access problem\n    \"\"\"\n\n    training_plan = self._db.get(\n        self._database.training_plan_id == training_plan_id\n    )\n\n    if training_plan and secure:\n        self._remove_sensible_keys_from_request(training_plan)\n\n    return training_plan\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.get_training_plan_by_name","title":"get_training_plan_by_name","text":"<pre><code>get_training_plan_by_name(training_plan_name)\n</code></pre> <p>Gets training plan from database, by its name</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_name</code> <code>str</code> <p>name of the training plan entry to search in the database</p> required <p>Returns:</p> Type Description <code>Union[Dict[str, Any], None]</code> <p>training plan entry found in the database matching <code>training_plan_name</code>. Otherwise, returns None.</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>bad argument type</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>cannot read database.</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def get_training_plan_by_name(\n    self, training_plan_name: str\n) -&gt; Union[Dict[str, Any], None]:\n    \"\"\"Gets training plan from database, by its name\n\n    Args:\n        training_plan_name: name of the training plan entry to search in the database\n\n    Returns:\n        training plan entry found in the database matching `training_plan_name`. Otherwise, returns None.\n\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: bad argument type\n        FedbiomedTrainingPlanSecurityManagerError: cannot read database.\n    \"\"\"\n\n    training_plan = self._db.get(self._database.name == training_plan_name)\n\n    return training_plan\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.get_training_plan_from_database","title":"get_training_plan_from_database","text":"<pre><code>get_training_plan_from_database(training_plan)\n</code></pre> <p>Gets training plan from database, by its hash</p> <p>Training plan file MUST be a *.txt file.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan</code> <code>str</code> <p>training plan source code, in order to compute its hash.</p> required <p>Returns:</p> Type Description <code>Union[Dict[str, Any], None]</code> <p>Training plan entry found in the dataset if query in database succeed. Otherwise, returns</p> <code>Union[Dict[str, Any], None]</code> <p>None.</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>bad argument type</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>database access problem</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def get_training_plan_from_database(\n    self, training_plan: str\n) -&gt; Union[Dict[str, Any], None]:\n    \"\"\"Gets training plan from database, by its hash\n\n    !!! info \"Training plan file MUST be a *.txt file.\"\n\n    Args:\n        training_plan: training plan source code, in order to compute its hash.\n\n    Returns:\n        Training plan entry found in the dataset if query in database succeed. Otherwise, returns\n        None.\n\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: bad argument type\n        FedbiomedTrainingPlanSecurityManagerError: database access problem\n    \"\"\"\n\n    req_training_plan_hash, *_ = self._create_hash(training_plan, from_string=True)\n    _all_training_plans_which_have_req_hash = (\n        self._database.hash == req_training_plan_hash\n    )\n\n    return self._db.get(_all_training_plans_which_have_req_hash)\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.list_training_plans","title":"list_training_plans","text":"<pre><code>list_training_plans(sort_by=None, select_status=None, verbose=True, search=None)\n</code></pre> <p>Lists approved training plan files</p> <p>Parameters:</p> Name Type Description Default <code>sort_by</code> <code>Union[str, None]</code> <p>when specified, sort results by alphabetical order, provided sort_by is an entry in the database.</p> <code>None</code> <code>select_status</code> <code>Union[None, List[TrainingPlanApprovalStatus]]</code> <p>filter list by training plan status or list of training plan statuses</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>When it is True, print list of training plan in tabular format. Default is True.</p> <code>True</code> <code>search</code> <code>Union[dict, None]</code> <p>Dictionary that contains <code>text</code> property to declare the text that wil be search and <code>by</code> property to declare text will be search on which field</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of training plans that have been found as <code>registered</code>. Each training plan is in fact a dictionary containing fields (note that following fields are removed :'training_plan', 'hash', dates due to privacy reasons).</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>bad type for parameter</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>database access error</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def list_training_plans(\n    self,\n    sort_by: Union[str, None] = None,\n    select_status: Union[None, List[TrainingPlanApprovalStatus]] = None,\n    verbose: bool = True,\n    search: Union[dict, None] = None,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Lists approved training plan files\n\n    Args:\n        sort_by: when specified, sort results by alphabetical order,\n            provided sort_by is an entry in the database.\n        select_status: filter list by training plan status or list of training plan statuses\n        verbose: When it is True, print list of training plan in tabular format.\n            Default is True.\n        search: Dictionary that contains `text` property to declare the text that wil be search and `by`\n            property to declare text will be search on which field\n\n    Returns:\n        A list of training plans that have\n            been found as `registered`. Each training plan is in fact a dictionary\n            containing fields (note that following fields are removed :'training_plan',\n            'hash', dates due to privacy reasons).\n\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: bad type for parameter\n        FedbiomedTrainingPlanSecurityManagerError: database access error\n    \"\"\"\n\n    # Selects all\n    query = self._database.training_plan_id.exists()\n\n    if select_status:\n        # filtering training plan based on their status\n        if not isinstance(select_status, list):\n            # convert everything into a list\n            select_status = [select_status]\n\n        select_status = [\n            x.value\n            for x in select_status\n            if isinstance(x, TrainingPlanApprovalStatus)\n        ]\n        query = self._database.training_plan_status.one_of(select_status)\n        # extract value from TrainingPlanApprovalStatus\n\n    if search:\n        query = query &amp; self._database[search[\"by\"]].matches(\n            search[\"text\"], flags=re.IGNORECASE\n        )\n\n    try:\n        training_plans = self._db.search(query)\n    except Exception as err:\n        raise FedbiomedTrainingPlanSecurityManagerError(\n            f\"{ErrorNumbers.FB606.value}: request failed when looking for a training plan into database with \"\n            f\"error: {err}\"\n        )\n\n    # Drop some keys for security reasons\n    for doc in training_plans:\n        self._remove_sensible_keys_from_request(doc)\n\n    if sort_by is not None:\n        # sorting training plan fields by column attributes\n        is_entry_exists = self._db.search(self._database[sort_by].exists())\n        if is_entry_exists and sort_by not in self._tags_to_remove:\n            training_plans = sorted(\n                training_plans, key=lambda x: (x[sort_by] is None, x[sort_by])\n            )\n        else:\n            logger.warning(f\"Field {sort_by} is not available in dataset\")\n\n    if verbose:\n        training_plans_verbose = training_plans.copy()\n        for tp in training_plans_verbose:\n            tp.pop(\"training_plan\")\n\n        print(tabulate(training_plans_verbose, headers=\"keys\"))\n\n    return training_plans\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.register_training_plan","title":"register_training_plan","text":"<pre><code>register_training_plan(name, description, path, training_plan_type=TrainingPlanStatus.REGISTERED.value, training_plan_id=None, researcher_id=None)\n</code></pre> <p>Approves/registers training plan file through CLI.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Training plan file name. The name should be unique. Otherwise, methods throws an Exception FedbiomedTrainingPlanSecurityManagerError</p> required <code>description</code> <code>str</code> <p>Description for training plan file.</p> required <code>path</code> <code>str</code> <p>Exact path for the training plan that will be registered</p> required <code>training_plan_type</code> <code>str</code> <p>Default is <code>registered</code>. It means that training plan has been registered by a user/hospital. Other value can be <code>default</code> which indicates that training plan is default (training plans for tutorials/examples)</p> <code>value</code> <code>training_plan_id</code> <code>str</code> <p>Pre-defined id for training plan. Default is None. When it is None method creates unique id for the training plan.</p> <code>None</code> <code>researcher_id</code> <code>str</code> <p>ID of the researcher who is owner/requester of the training plan file</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The ID of registered training plan</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p><code>training_plan_type</code> is not <code>registered</code> or <code>default</code></p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>training plan is already registered into database</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>training plan name is already used for saving another training plan</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>database access problem</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def register_training_plan(\n    self,\n    name: str,\n    description: str,\n    path: str,\n    training_plan_type: str = TrainingPlanStatus.REGISTERED.value,\n    training_plan_id: str = None,\n    researcher_id: str = None,\n) -&gt; str:\n    \"\"\"Approves/registers training plan file through CLI.\n\n    Args:\n        name: Training plan file name. The name should be unique. Otherwise, methods\n            throws an Exception FedbiomedTrainingPlanSecurityManagerError\n        description: Description for training plan file.\n        path: Exact path for the training plan that will be registered\n        training_plan_type: Default is `registered`. It means that training plan has been registered\n            by a user/hospital. Other value can be `default` which indicates\n            that training plan is default (training plans for tutorials/examples)\n        training_plan_id: Pre-defined id for training plan. Default is None. When it is None method\n            creates unique id for the training plan.\n        researcher_id: ID of the researcher who is owner/requester of the training plan file\n\n    Returns:\n        The ID of registered training plan\n\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: `training_plan_type` is not `registered` or `default`\n        FedbiomedTrainingPlanSecurityManagerError: training plan is already registered into database\n        FedbiomedTrainingPlanSecurityManagerError: training plan name is already used for saving another training plan\n        FedbiomedTrainingPlanSecurityManagerError: database access problem\n    \"\"\"\n\n    # Check training plan type is valid\n    if training_plan_type not in TrainingPlanStatus.list():\n        raise FedbiomedTrainingPlanSecurityManagerError(\n            f\"Unknown training plan (training_plan_type) type: {training_plan_type}\"\n        )\n\n    if not training_plan_id:\n        training_plan_id = \"training_plan_\" + str(uuid.uuid4())\n    training_plan_hash, algorithm, source = self._create_hash(path)\n\n    # Verify no such training plan is already registered\n    self._check_training_plan_not_existing(name, training_plan_hash, None)\n\n    # Training plan file creation date\n    ctime = datetime.fromtimestamp(os.path.getctime(path)).strftime(\n        \"%d-%m-%Y %H:%M:%S.%f\"\n    )\n    # Training plan file modification date\n    mtime = datetime.fromtimestamp(os.path.getmtime(path)).strftime(\n        \"%d-%m-%Y %H:%M:%S.%f\"\n    )\n    # Training plan file registration date\n    rtime = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S.%f\")\n\n    training_plan_record = dict(\n        name=name,\n        description=description,\n        hash=training_plan_hash,\n        training_plan=source,\n        training_plan_id=training_plan_id,\n        training_plan_type=training_plan_type,\n        training_plan_status=TrainingPlanApprovalStatus.APPROVED.value,\n        algorithm=algorithm,\n        researcher_id=researcher_id,\n        date_created=ctime,\n        date_modified=mtime,\n        date_registered=rtime,\n        date_last_action=rtime,\n    )\n\n    try:\n        self._db.insert(training_plan_record)\n    except Exception as err:\n        raise FedbiomedTrainingPlanSecurityManagerError(\n            ErrorNumbers.FB606.value + \" : database insertion failed with\"\n            f\" following error: {str(err)}\"\n        )\n    return training_plan_id\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.register_update_default_training_plans","title":"register_update_default_training_plans","text":"<pre><code>register_update_default_training_plans()\n</code></pre> <p>Registers or updates default training plans.</p> <p>Launched when the node is started through CLI, if <code>allow_default_training_plans</code> is enabled. Checks the files saved into <code>default_training_plans</code> directory and update/register them based on following conditions:</p> <ul> <li>Registers if there is a new training plan file which isn't saved into db.</li> <li>Updates if training plan is modified or if hashing algorithm has changed in config file.</li> </ul> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>cannot read or update training plan database</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def register_update_default_training_plans(self):\n    \"\"\"Registers or updates default training plans.\n\n    Launched when the node is started through CLI, if `allow_default_training_plans` is enabled.\n    Checks the files saved into `default_training_plans` directory and update/register them based on following\n    conditions:\n\n    - Registers if there is a new training plan file which isn't saved into db.\n    - Updates if training plan is modified or if hashing algorithm has changed in config file.\n\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: cannot read or update training plan database\n    \"\"\"\n\n    # Get training plan files saved in the directory\n    training_plans_file = os.listdir(self._default_tps)\n\n    # Get only default training plans from DB\n    try:\n        training_plans = self._db.search(\n            self._database.training_plan_type == \"default\"\n        )\n    except Exception as e:\n        raise FedbiomedTrainingPlanSecurityManagerError(\n            ErrorNumbers.FB606.value\n            + f\"database search operation failed, with following error: {str(e)}\"\n        )\n\n    # Get training plan names from list of training plans\n    training_plans_dict = {\n        training_plan.get(\"name\"): training_plan for training_plan in training_plans\n    }\n    training_plans_name_db = list(training_plans_dict.keys())\n    # Default training plans not in database\n    training_plans_not_saved = list(\n        set(training_plans_file) - set(training_plans_name_db)\n    )\n    # Default training plans that have been deleted from file system but not in DB\n    training_plans_deleted = list(\n        set(training_plans_name_db) - set(training_plans_file)\n    )\n    # Training plans have already saved and exist in the database\n    training_plans_exists = list(\n        set(training_plans_file) - set(training_plans_not_saved)\n    )\n\n    # Register new default training plans\n    for training_plan in training_plans_not_saved:\n        self.register_training_plan(\n            name=training_plan,\n            description=\"Default training plan\",\n            path=os.path.join(self._default_tps, training_plan),\n            training_plan_type=\"default\",\n        )\n\n    # Remove training plans that have been removed from file system\n    for training_plan_name in training_plans_deleted:\n        try:\n            _, training_plan_doc = self._db.get(\n                self._database.name == training_plan_name, add_docs=True\n            )\n            logger.info(\n                \"Removed default training plan file has been detected,\"\n                f\" it will be removed from DB as well: {training_plan_name}\"\n            )\n\n            self._db.remove(doc_ids=[training_plan_doc.doc_id])\n        except Exception as err:\n            raise FedbiomedTrainingPlanSecurityManagerError(\n                ErrorNumbers.FB606.value + \": failed to update database, \"\n                f\" with error {str(err)}\"\n            )\n    # Update training plans\n    for training_plan in training_plans_exists:\n        path = os.path.join(self._default_tps, training_plan)\n        mtime = datetime.fromtimestamp(os.path.getmtime(path))\n        try:\n            training_plan_info = self._db.get(self._database.name == training_plan)\n        except Exception as err:\n            raise FedbiomedTrainingPlanSecurityManagerError(\n                ErrorNumbers.FB606.value\n                + f\": failed to get training_plan info for training plan {training_plan}\"\n                f\"Details : {str(err)}\"\n            )\n\n        # Check if hashing algorithm has changed\n        try:\n            hash, algorithm, _ = self._create_hash(\n                os.path.join(self._default_tps, training_plan)\n            )\n\n            if training_plan_info[\"algorithm\"] != self._hashing:\n                # Verify no such training plan already exists in DB\n                self._check_training_plan_not_existing(None, hash, algorithm)\n                logger.info(\n                    f'Recreating hashing for : {training_plan_info[\"name\"]} \\t'\n                    '{training_plan_info[\"training_plan_id\"]}'\n                )\n\n                self._db.update(\n                    {\n                        \"hash\": hash,\n                        \"algorithm\": algorithm,\n                        \"date_last_action\": datetime.now().strftime(\n                            \"%d-%m-%Y %H:%M:%S.%f\"\n                        ),\n                    },\n                    self._database.training_plan_id\n                    == training_plan_info[\"training_plan_id\"],\n                )\n            # If default training plan file is modified update hashing\n            elif mtime &gt; datetime.strptime(\n                training_plan_info[\"date_modified\"], \"%d-%m-%Y %H:%M:%S.%f\"\n            ):\n                # only check when hash changes\n                # else we have error because this training plan exists in database with same hash\n                if hash != training_plan_info[\"hash\"]:\n                    # Verify no such training plan already exists in DB\n                    self._check_training_plan_not_existing(None, hash, algorithm)\n\n                logger.info(\n                    \"Modified default training plan file has been detected. \"\n                    f\"Hashing will be updated for: {training_plan}\"\n                )\n\n                self._db.update(\n                    {\n                        \"hash\": hash,\n                        \"algorithm\": algorithm,\n                        \"date_modified\": mtime.strftime(\"%d-%m-%Y %H:%M:%S.%f\"),\n                        \"date_last_action\": datetime.now().strftime(\n                            \"%d-%m-%Y %H:%M:%S.%f\"\n                        ),\n                    },\n                    self._database.training_plan_id\n                    == training_plan_info[\"training_plan_id\"],\n                )\n        except Exception as err:\n            # triggered if database update failed (see `update` method in tinydb code)\n            raise FedbiomedTrainingPlanSecurityManagerError(\n                ErrorNumbers.FB606.value\n                + \": Failed to update database, with error: \"\n                f\"{str(err)}\"\n            )\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.reject_training_plan","title":"reject_training_plan","text":"<pre><code>reject_training_plan(training_plan_id, extra_notes=None)\n</code></pre> <p>Approves a training plan stored into the database given its [<code>training_plan_id</code>]</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_id</code> <code>str</code> <p>id of the training plan.</p> required <code>extra_notes</code> <code>Union[str, None]</code> <p>notes detailing why training plan has been rejected. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>True</code> <p>Currently always returns True</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def reject_training_plan(\n    self, training_plan_id: str, extra_notes: Union[str, None] = None\n) -&gt; True:\n    \"\"\"Approves a training plan stored into the database given its [`training_plan_id`]\n\n    Args:\n        training_plan_id: id of the training plan.\n        extra_notes: notes detailing why training plan has been rejected. Defaults to None.\n\n    Returns:\n        Currently always returns True\n    \"\"\"\n    res = self._update_training_plan_status(\n        training_plan_id, TrainingPlanApprovalStatus.REJECTED, extra_notes\n    )\n    return res\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.reply_training_plan_approval_request","title":"reply_training_plan_approval_request","text":"<pre><code>reply_training_plan_approval_request(request)\n</code></pre> <p>Submits a training plan file (TrainingPlan) for approval. Needs an action from Node</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ApprovalRequest</code> <p>approval request message, received from Researcher</p> required Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def reply_training_plan_approval_request(self, request: ApprovalRequest):\n    \"\"\"Submits a training plan file (TrainingPlan) for approval. Needs an action from Node\n\n    Args:\n        request: approval request message, received from Researcher\n    \"\"\"\n\n    reply = {\n        \"researcher_id\": request.researcher_id,\n        \"request_id\": request.request_id,\n        \"node_id\": self._node_id,\n        \"message\": \"\",\n        \"status\": 0,  # HTTP status (set by default to 0, non-existing HTTP status code)\n    }\n\n    is_existant = False\n    training_plan_name = \"training_plan_\" + str(uuid.uuid4())\n    training_plan = request.training_plan\n    reply.update({\"training_plan_id\": training_plan_name})\n\n    try:\n        # check if training plan has already been registered into database\n        is_existant, _ = self.check_training_plan_status(training_plan, None)\n\n    except FedbiomedTrainingPlanSecurityManagerError as exp:\n        logger.error(f\"Error while training plan approval request {exp}\")\n        reply.update(\n            {\n                \"message\": \"Can not check whether training plan has already be registered or not due to error\",\n                \"success\": False,\n            }\n        )\n\n        return ApprovalReply(**reply)\n\n    if not is_existant:\n        # move training plan into corresponding directory\n        try:\n            training_plan_hash, hash_algo, _ = self._create_hash(\n                training_plan, from_string=True\n            )\n            ctime = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S.%f\")\n            training_plan_object = dict(\n                name=training_plan_name,\n                description=request.description,\n                hash=training_plan_hash,\n                training_plan=request.training_plan,\n                training_plan_id=training_plan_name,\n                training_plan_type=TrainingPlanStatus.REQUESTED.value,\n                training_plan_status=TrainingPlanApprovalStatus.PENDING.value,\n                algorithm=hash_algo,\n                date_created=ctime,\n                date_modified=ctime,\n                date_registered=ctime,\n                date_last_action=None,\n                researcher_id=request.researcher_id,\n                notes=None,\n            )\n\n            self._db.upsert(\n                training_plan_object, self._database.hash == training_plan_hash\n            )\n        except Exception as err:\n\n            logger.error(\n                f\"Cannot add training plan in database due to error : {err}\"\n            )\n            reply.update(\n                {\n                    \"message\": \"Cannot add training plan into database due to error\",\n                    \"success\": False,\n                }\n            )\n            return ApprovalReply(**reply)\n\n        else:\n            reply[\"success\"] = True\n            logger.debug(\"Training plan successfully received by Node for approval\")\n\n    else:\n        if self.check_training_plan_status(\n            training_plan, TrainingPlanApprovalStatus.PENDING\n        )[0]:\n            reply.update(\n                {\n                    \"message\": \"Training plan already sent for Approval (status Pending). \"\n                    \"Please wait for Node approval.\"\n                }\n            )\n        elif self.check_training_plan_status(\n            training_plan, TrainingPlanApprovalStatus.APPROVED\n        )[0]:\n            reply.update(\n                {\n                    \"message\": f\"Training plan '{request.description}' is already Approved. Ready \"\n                    \"to train on this training plan.\"\n                }\n            )\n        else:\n            reply.update(\n                {\"message\": \"Training plan already exists in database. Aborting\"}\n            )\n\n        reply.update({\"success\": True})\n\n    # Send training plan approval acknowledge answer to researcher\n    return ApprovalReply(**reply)\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.reply_training_plan_status_request","title":"reply_training_plan_status_request","text":"<pre><code>reply_training_plan_status_request(request)\n</code></pre> <p>Returns requested training plan file status {approved, rejected, pending} and sends TrainingPlanStatusReply to researcher.</p> <p>Called directly from Node.py when it receives TrainingPlanStatusRequest.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>TrainingPlanStatusRequest</code> <p>Message that is received from researcher. Formatted as TrainingPlanStatusRequest</p> required Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def reply_training_plan_status_request(self, request: TrainingPlanStatusRequest):\n    \"\"\"Returns requested training plan file status {approved, rejected, pending}\n    and sends TrainingPlanStatusReply to researcher.\n\n    Called directly from Node.py when it receives TrainingPlanStatusRequest.\n\n    Args:\n        request: Message that is received from researcher.\n            Formatted as TrainingPlanStatusRequest\n    \"\"\"\n\n    # Main header for the training plan status request\n    reply = {\n        \"researcher_id\": request.researcher_id,\n        \"request_id\": request.request_id,\n        \"node_id\": self._node_id,\n        \"experiment_id\": request.experiment_id,\n        \"approval_obligation\": True,\n        \"training_plan\": request.training_plan,\n        \"training_plan_id\": None,\n    }\n\n    try:\n        training_plan = self.get_training_plan_from_database(request.training_plan)\n        if training_plan is not None:\n            training_plan_status = training_plan.get(\n                \"training_plan_status\", \"Not Registered\"\n            )\n            reply.update(\n                {\"training_plan_id\": training_plan.get(\"training_plan_id\", None)}\n            )\n        else:\n            training_plan_status = \"Not Registered\"\n\n        reply.update({\"success\": True, \"status\": training_plan_status})\n        if self._tp_approval:\n            if training_plan_status == TrainingPlanApprovalStatus.APPROVED.value:\n                msg = \"Training plan has been approved by the node, training can start\"\n            elif training_plan_status == TrainingPlanApprovalStatus.PENDING.value:\n                msg = \"Training plan is pending: waiting for a review\"\n            elif training_plan_status == TrainingPlanApprovalStatus.REJECTED.value:\n                msg = \"Training plan has been rejected by the node, training is not possible\"\n            else:\n                msg = f\"Unknown training plan not in database (status {training_plan_status})\"\n            reply.update({\"msg\": msg})\n\n        else:\n            reply.update(\n                {\n                    \"approval_obligation\": False,\n                    \"msg\": \"This node does not require training plan approval (maybe for debugging purposes).\",\n                }\n            )\n\n    # Catch all exception to be able send reply back to researcher\n    except Exception as exp:\n        logger.error(exp)\n        reply.update(\n            {\n                \"success\": False,\n                \"status\": \"Error\",\n                \"msg\": f\"{ErrorNumbers.FB606.value}: Cannot check if training plan has been registered due \"\n                \"to an internal error\",\n            }\n        )\n\n    return TrainingPlanStatusReply(**reply)\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.update_training_plan_hash","title":"update_training_plan_hash","text":"<pre><code>update_training_plan_hash(training_plan_id, path)\n</code></pre> <p>Updates an existing training plan entry in training plan database.</p> <p>Training plan entry cannot be a default training plan.</p> <p>The training plan entry to update is indicated by its <code>training_plan_id</code> The new training plan file for the training plan is specified from <code>path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_id</code> <code>str</code> <p>id of the training plan to update</p> required <code>path</code> <code>str</code> <p>path where new training plan file is stored</p> required <p>Returns:</p> Type Description <code>True</code> <p>Currently always returns True.</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>try to update a default training plan</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>cannot read or update the training plan in database</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def update_training_plan_hash(self, training_plan_id: str, path: str) -&gt; True:\n    \"\"\"Updates an existing training plan entry in training plan database.\n\n    Training plan entry cannot be a default training plan.\n\n    The training plan entry to update is indicated by its `training_plan_id`\n    The new training plan file for the training plan is specified from `path`.\n\n    Args:\n        training_plan_id: id of the training plan to update\n        path: path where new training plan file is stored\n\n    Returns:\n        Currently always returns True.\n\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: try to update a default training plan\n        FedbiomedTrainingPlanSecurityManagerError: cannot read or update the training plan in database\n    \"\"\"\n\n    # Register training plan\n    try:\n        training_plan = self._db.get(\n            self._database.training_plan_id == training_plan_id\n        )\n    except Exception as err:\n        raise FedbiomedTrainingPlanSecurityManagerError(\n            ErrorNumbers.FB606.value + \": get request on database failed.\"\n            f\" Details: {str(err)}\"\n        )\n    if training_plan[\"training_plan_type\"] != TrainingPlanStatus.DEFAULT.value:\n        hash, algorithm, source = self._create_hash(path)\n        # Verify no such training plan already exists in DB\n        self._check_training_plan_not_existing(None, hash, algorithm)\n\n        # Get modification date\n        mtime = datetime.fromtimestamp(os.path.getmtime(path))\n        # Get creation date\n        ctime = datetime.fromtimestamp(os.path.getctime(path))\n\n        try:\n            self._db.update(\n                {\n                    \"hash\": hash,\n                    \"algorithm\": algorithm,\n                    \"date_modified\": mtime.strftime(\"%d-%m-%Y %H:%M:%S.%f\"),\n                    \"date_created\": ctime.strftime(\"%d-%m-%Y %H:%M:%S.%f\"),\n                    \"date_last_action\": datetime.now().strftime(\n                        \"%d-%m-%Y %H:%M:%S.%f\"\n                    ),\n                    \"training_plan\": source,\n                },\n                self._database.training_plan_id == training_plan_id,\n            )\n        except Exception as err:\n            raise FedbiomedTrainingPlanSecurityManagerError(\n                ErrorNumbers.FB606.value + \": update database failed. Details :\"\n                f\"{str(err)}\"\n            )\n    else:\n        raise FedbiomedTrainingPlanSecurityManagerError(\n            ErrorNumbers.FB606.value\n            + \"You cannot update default training plans. Please \"\n            \"update them through their files saved in `default_training_plans` directory \"\n            \"and restart your node\"\n        )\n\n    return True\n</code></pre>"},{"location":"developer/api/researcher/aggregators/","title":"Aggregators","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators-classes","title":"Classes","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Aggregator","title":"Aggregator","text":"<pre><code>Aggregator()\n</code></pre> <p>Defines methods for aggregating strategy (eg FedAvg, FedProx, SCAFFOLD, ...).</p> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def __init__(self):\n    self._aggregator_args: dict = None\n    self._fds: FederatedDataSet = None\n    self._training_plan_type: TrainingPlans = None\n    self._secagg_crypter = SecaggCrypter()\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Aggregator-functions","title":"Functions","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Aggregator.aggregate","title":"aggregate","text":"<pre><code>aggregate(model_params, weights, *args, **kwargs)\n</code></pre> <p>Strategy to aggregate models</p> <p>Parameters:</p> Name Type Description Default <code>model_params</code> <code>list</code> <p>List of model parameters received from each node</p> required <code>weights</code> <code>list</code> <p>Weight for each node-model-parameter set</p> required <p>Raises:</p> Type Description <code>FedbiomedAggregatorError</code> <p>If the method is not defined by inheritor</p> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def aggregate(self, model_params: list, weights: list, *args, **kwargs) -&gt; Dict:\n    \"\"\"\n    Strategy to aggregate models\n\n    Args:\n        model_params: List of model parameters received from each node\n        weights: Weight for each node-model-parameter set\n\n    Raises:\n        FedbiomedAggregatorError: If the method is not defined by inheritor\n    \"\"\"\n    msg = ErrorNumbers.FB401.value + \\\n        \": aggregate method should be overloaded by the choosen strategy\"\n    logger.critical(msg)\n    raise FedbiomedAggregatorError(msg)\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Aggregator.check_values","title":"check_values","text":"<pre><code>check_values(*args, **kwargs)\n</code></pre> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def check_values(self, *args, **kwargs) -&gt; True:\n    return True\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Aggregator.create_aggregator_args","title":"create_aggregator_args","text":"<pre><code>create_aggregator_args(*args, **kwargs)\n</code></pre> <p>Returns aggregator arguments that are expecting by the nodes</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>ignored</p> <code>()</code> <code>kwargs</code> <p>ignored</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict</code> <p>contains <code>Aggregator</code> parameters/argument that will be shared with the nodes</p> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def create_aggregator_args(self, *args, **kwargs) -&gt; Dict:\n    \"\"\"Returns aggregator arguments that are expecting by the nodes\n\n    Args:\n        args: ignored\n        kwargs: ignored\n\n    Returns:\n        contains `Aggregator` parameters/argument that will be shared with the nodes \n    \"\"\"\n    return self._aggregator_args or {}\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Aggregator.load_state_breakpoint","title":"load_state_breakpoint","text":"<pre><code>load_state_breakpoint(state, **kwargs)\n</code></pre> <p>use for breakpoints. load the aggregator state</p> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def load_state_breakpoint(self, state: Dict[str, Any], **kwargs) -&gt; None:\n    \"\"\"\n    use for breakpoints. load the aggregator state\n    \"\"\"\n    if not isinstance(state[\"parameters\"], Dict):\n        self._aggregator_args = Serializer.load(state['parameters'])\n    else:\n        self._aggregator_args = state['parameters']\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Aggregator.save_state_breakpoint","title":"save_state_breakpoint","text":"<pre><code>save_state_breakpoint(breakpoint_path=None, **aggregator_args_create)\n</code></pre> <p>use for breakpoints. save the aggregator state</p> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def save_state_breakpoint(\n    self,\n    breakpoint_path: Optional[str] = None,\n    **aggregator_args_create: Any,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    use for breakpoints. save the aggregator state\n    \"\"\"\n    aggregator_args = self.create_aggregator_args(**aggregator_args_create)\n    if aggregator_args:\n\n        if self._aggregator_args is None:\n            self._aggregator_args = {}\n        self._aggregator_args.update(aggregator_args)\n\n    if breakpoint_path:\n        filename = self._save_arg_to_file(breakpoint_path, 'aggregator_args', uuid.uuid4(), self._aggregator_args)\n\n    state = {\n        \"class\": type(self).__name__,\n        \"module\": self.__module__,\n        \"parameters\": filename if breakpoint_path else self._aggregator_args\n    }\n\n    return state\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Aggregator.secure_aggregation","title":"secure_aggregation","text":"<pre><code>secure_aggregation(params, encryption_factors, secagg_random, aggregation_round, total_sample_size, training_plan)\n</code></pre> <p>Apply aggregation for encrypted model parameters</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>List[List[int]]</code> <p>List containing list of encrypted parameters of each node</p> required <code>encryption_factors</code> <code>List[Dict[str, List[int]]]</code> <p>List of encrypted integers to validate encryption</p> required <code>secagg_random</code> <code>float</code> <p>Randomly generated float value to validate secure aggregation correctness</p> required <code>aggregation_round</code> <code>int</code> <p>The round of the aggregation.</p> required <code>total_sample_size</code> <code>int</code> <p>Sum of sample sizes used for training</p> required <code>training_plan</code> <code>BaseTrainingPlan</code> <p>Training plan instance used for the training.</p> required <p>Returns:</p> Type Description <p>aggregated model parameters</p> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def secure_aggregation(\n        self,\n        params: List[List[int]],\n        encryption_factors: List[Dict[str, List[int]]],\n        secagg_random: float,\n        aggregation_round: int,\n        total_sample_size: int,\n        training_plan: 'BaseTrainingPlan'\n):\n    \"\"\" Apply aggregation for encrypted model parameters\n\n    Args:\n        params: List containing list of encrypted parameters of each node\n        encryption_factors: List of encrypted integers to validate encryption\n        secagg_random: Randomly generated float value to validate secure aggregation correctness\n        aggregation_round: The round of the aggregation.\n        total_sample_size: Sum of sample sizes used for training\n        training_plan: Training plan instance used for the training.\n\n    Returns:\n        aggregated model parameters\n    \"\"\"\n\n    # TODO: verify with secagg context number of parties\n    num_nodes = len(params)\n\n    # TODO: Use server key here\n    key = -(len(params) * 10)\n\n    # IMPORTANT = Keep this key for testing purposes\n    key = -4521514305280526329525552501850970498079782904248225896786295610941010325354834129826500373412436986239012584207113747347251251180530850751209537684586944643780840182990869969844131477709433555348941386442841023261287875379985666260596635843322044109172782411303407030194453287409138194338286254652273563418119335656859169132074431378389356392955315045979603414700450628308979043208779867835835935403213000649039155952076869962677675951924910959437120608553858253906942559260892494214955907017206115207769238347962438107202114814163305602442458693305475834199715587932463252324681290310458316249381037969151400784780\n    logger.info(\"Securely aggregating model parameters...\")\n\n    aggregate = functools.partial(self._secagg_crypter.aggregate,\n                                  current_round=aggregation_round,\n                                  num_nodes=num_nodes,\n                                  key=key,\n                                  total_sample_size=total_sample_size\n                                  )\n    # Validation\n    encryption_factors = [f for k, f in encryption_factors.items()]\n    validation: List[int] = aggregate(params=encryption_factors)\n\n    if len(validation) != 1 or not math.isclose(validation[0], secagg_random, abs_tol=0.01):\n        raise FedbiomedAggregatorError(\"Aggregation is failed due to incorrect decryption.\")\n\n    aggregated_params = aggregate(params=params)\n\n    # Convert model params\n    model = training_plan.get_model_wrapper_class()\n    model_params = model.unflatten(aggregated_params)\n\n    return model_params\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Aggregator.set_fds","title":"set_fds","text":"<pre><code>set_fds(fds)\n</code></pre> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def set_fds(self, fds: FederatedDataSet) -&gt; FederatedDataSet:\n    self._fds = fds\n    return self._fds\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Aggregator.set_training_plan_type","title":"set_training_plan_type","text":"<pre><code>set_training_plan_type(training_plan_type)\n</code></pre> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def set_training_plan_type(self, training_plan_type: TrainingPlans) -&gt; TrainingPlans:\n    self._training_plan_type = training_plan_type\n    return self._training_plan_type\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.FedAverage","title":"FedAverage","text":"<pre><code>FedAverage()\n</code></pre> <p>               Bases: <code>Aggregator</code></p> <p>Defines the Federated averaging strategy</p> Source code in <code>fedbiomed/researcher/aggregators/fedavg.py</code> <pre><code>def __init__(self):\n    \"\"\"Construct `FedAverage` object as an instance of [`Aggregator`]\n    [fedbiomed.researcher.aggregators.Aggregator].\n    \"\"\"\n    super(FedAverage, self).__init__()\n    self.aggregator_name = \"FedAverage\"\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.FedAverage-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.FedAverage.aggregator_name","title":"aggregator_name  <code>instance-attribute</code>","text":"<pre><code>aggregator_name = 'FedAverage'\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.FedAverage-functions","title":"Functions","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.FedAverage.aggregate","title":"aggregate","text":"<pre><code>aggregate(model_params, weights, *args, **kwargs)\n</code></pre> <p>Aggregates  local models sent by participating nodes into a global model, following Federated Averaging strategy.</p> <p>weights is a list of single-item dictionaries, each dictionary has the node id as key, and the weight as value. model_params is a list of single-item dictionaries, each dictionary has the node is as key, and a framework-specific representation of the model parameters as value.</p> <p>Parameters:</p> Name Type Description Default <code>model_params</code> <code>Dict[str, Dict[str, Union[Tensor, ndarray]]]</code> <p>contains each model layers</p> required <code>weights</code> <code>Dict[str, float]</code> <p>contains all weights of a given layer.</p> required <p>Returns:</p> Type Description <code>Mapping[str, Union[Tensor, ndarray]]</code> <p>Aggregated parameters</p> Source code in <code>fedbiomed/researcher/aggregators/fedavg.py</code> <pre><code>def aggregate(\n        self,\n        model_params: Dict[str, Dict[str, Union['torch.Tensor', 'numpy.ndarray']]],\n        weights: Dict[str, float],\n        *args,\n        **kwargs\n) -&gt; Mapping[str, Union['torch.Tensor', 'numpy.ndarray']]:\n    \"\"\" Aggregates  local models sent by participating nodes into a global model, following Federated Averaging\n    strategy.\n\n    weights is a list of single-item dictionaries, each dictionary has the node id as key, and the weight as value.\n    model_params is a list of single-item dictionaries, each dictionary has the node is as key,\n    and a framework-specific representation of the model parameters as value.\n\n    Args:\n        model_params: contains each model layers\n        weights: contains all weights of a given layer.\n\n    Returns:\n        Aggregated parameters\n    \"\"\"\n\n    model_params_processed = []\n    weights_processed = []\n\n    for node_id, params in model_params.items():\n\n        if node_id not in weights:\n            raise FedbiomedAggregatorError(\n                f\"{ErrorNumbers.FB401.value}. Can not find corresponding calculated weight for the \"\n                f\"node {node_id}. Aggregation is aborted.\"\n            )\n\n        weight = weights[node_id]\n        model_params_processed.append(params)\n        weights_processed.append(weight)\n\n    if any([x &lt; 0. or x &gt; 1. for x in weights_processed]) or sum(weights_processed) == 0:\n        raise FedbiomedAggregatorError(\n            f\"{ErrorNumbers.FB401.value}. Aggregation aborted due to sum of the weights is equal to 0 {weights}. \"\n            f\"Sample sizes received from nodes might be corrupted.\"\n        )\n\n    agg_params = federated_averaging(model_params_processed, weights_processed)\n\n    return agg_params\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold","title":"Scaffold","text":"<pre><code>Scaffold(server_lr=1.0, fds=None)\n</code></pre> <p>               Bases: <code>Aggregator</code></p> <p>Defines the Scaffold strategy</p> <p>Despite being an algorithm of choice for federated learning, it is observed that FedAvg suffers from <code>client-drift</code> when the data is heterogeneous (non-iid), resulting in unstable and slow convergence. SCAFFOLD uses control variates (variance reduction) to correct for the <code>client-drift</code> in its local updates. Intuitively, SCAFFOLD estimates the update direction for the server model (c) and the update direction for each client (c_i). The difference (c - c_i) is then an estimate of the client-drift which is used to correct the local update.</p>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold--fed-biomed-implementation-details","title":"Fed-BioMed implementation details","text":"<p>Our implementation is heavily influenced by our design choice to prevent storing any state on the nodes between FL rounds. In particular, this means that the computation of the control variates (i.e. the correction states) needs to be performed centrally by the aggregator. Roughly, our implementation follows these steps (following the notation of the original Scaffold paper):</p> <ol> <li>let \\(\\delta_i = \\mathbf{c}_i - \\mathbf{c} \\)</li> <li>foreach(round):</li> <li>sample \\( S \\) nodes participating in this round out of \\( N \\) total</li> <li>the server communicates the global model \\( \\mathbf{x} \\) and the correction states \\( \\delta_i \\) to all clients</li> <li>parallel on each client</li> <li>initialize local model \\( \\mathbf{y}_i = \\mathbf{x} \\)</li> <li>foreach(update) until K updates have been performed</li> <li>obtain a data batch</li> <li>compute the gradients for this batch \\( g(\\mathbf{y}_i) \\)</li> <li>apply correction term to gradients \\( g(\\mathbf{y}_i) -= \\delta_i \\)</li> <li>update model with one optimizer step e.g. for SGD \\( \\mathbf{y}_i -= \\eta_i g(\\mathbf{y}_i) \\)</li> <li>end foreach(update)</li> <li>communicate updated model \\( \\mathbf{y}_i \\) and learning rate \\( \\eta_i \\)</li> <li>end parallel section on each client</li> <li>the server computes the node-wise model update \\( \\mathbf{\\Delta y}_i =  \\mathbf{x} - \\mathbf{y}_i \\)</li> <li>the server updates the node-wise states \\( \\mathbf{c}_i = \\delta_i + (\\mathbf{\\Delta y}_i) / (\\eta_i K) \\)</li> <li>the server updates the global state \\( \\mathbf{c} = (1/N) \\sum_{i \\in N} \\mathbf{c}_i \\)</li> <li>the server updates the node-wise correction state \\(\\delta_i = \\mathbf{c}_i - \\mathbf{c} \\)</li> <li>the server updates the global model by averaging \\( \\mathbf{x} = \\mathbf{x} - (\\eta/|S|) \\sum_{i \\in S} \\mathbf{\\Delta y}_i \\)</li> <li>end foreach(round)</li> </ol> <p>This diagram provides a visual representation of the algorithm.</p> <p>References:</p> <ul> <li>Scaffold: Stochastic Controlled Averaging for Federated Learning</li> <li>TCT: Convexifying Federated Learning using Bootstrapped Neural Tangent Kernels</li> </ul> <p>Attributes:</p> Name Type Description <code>aggregator_name</code> <code>str</code> <p>name of the aggregator</p> <code>server_lr</code> <code>float</code> <p>value of the server learning rate</p> <code>global_state</code> <code>Dict[str, Union[Tensor, ndarray]]</code> <p>a dictionary representing the global correction state \\( \\mathbf{c} \\) in the format   {parameter name: correction value}</p> <code>nodes_states</code> <code>Dict[str, Dict[str, Union[Tensor, ndarray]]]</code> <p>a nested dictionary   of correction parameters obtained for each client, in the format {node id: node-wise corrections}. The   node-wise corrections are a dictionary in the format {parameter name: correction value} where the   model parameters are those contained in each node's model.named_parameters().</p> <code>nodes_deltas</code> <code>Dict[str, Dict[str, Union[Tensor, ndarray]]]</code> <p>a nested dictionary of deltas for each client, in the same format as nodes_states. The deltas   are defined as \\(\\delta_i = \\mathbf{c}_i - \\mathbf{c} \\)</p> <code>nodes_lr</code> <code>Dict[str, Dict[str, float]]</code> <p>dictionary of learning rates observed at end of the latest round, in the format   {node id: learning rate}</p> <p>Parameters:</p> Name Type Description Default <code>server_lr</code> <code>float</code> <p>server's (or Researcher's) learning rate. Defaults to 1..</p> <code>1.0</code> <code>fds</code> <code>FederatedDataset</code> <p>FederatedDataset obtained after a <code>search</code> request. Defaults to None.</p> <code>None</code> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def __init__(self, server_lr: float = 1., fds: Optional[FederatedDataSet] = None):\n    \"\"\"Constructs `Scaffold` object as an instance of [`Aggregator`]\n    [fedbiomed.researcher.aggregators.Aggregator].\n\n    Args:\n        server_lr (float): server's (or Researcher's) learning rate. Defaults to 1..\n        fds (FederatedDataset, optional): FederatedDataset obtained after a `search` request. Defaults to None.\n\n    \"\"\"\n    super().__init__()\n    self.aggregator_name: str = \"Scaffold\"\n    if server_lr == 0.:\n        raise FedbiomedAggregatorError(\"SCAFFOLD Error: Server learning rate cannot be equal to 0\")\n    self.server_lr: float = server_lr\n    self.global_state: Dict[str, Union[torch.Tensor, np.ndarray]] = {}\n    self.nodes_states: Dict[str, Dict[str, Union[torch.Tensor, np.ndarray]]] = {}\n    # FIXME: `nodes_states` is mis-named, because can conflict with `node_state`s that are saved \n    # whitin 2 Rounds\n    self.nodes_deltas: Dict[str, Dict[str, Union[torch.Tensor, np.ndarray]]] = {}\n    self.nodes_lr: Dict[str, Dict[str, float]] = {}\n    if fds is not None:\n        self.set_fds(fds)\n    self._aggregator_args = {}  # we need `_aggregator_args` to be not None\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.aggregator_name","title":"aggregator_name  <code>instance-attribute</code>","text":"<pre><code>aggregator_name = 'Scaffold'\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.global_state","title":"global_state  <code>instance-attribute</code>","text":"<pre><code>global_state = {}\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.nodes_deltas","title":"nodes_deltas  <code>instance-attribute</code>","text":"<pre><code>nodes_deltas = {}\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.nodes_lr","title":"nodes_lr  <code>instance-attribute</code>","text":"<pre><code>nodes_lr = {}\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.nodes_states","title":"nodes_states  <code>instance-attribute</code>","text":"<pre><code>nodes_states = {}\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.server_lr","title":"server_lr  <code>instance-attribute</code>","text":"<pre><code>server_lr = server_lr\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold-functions","title":"Functions","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.aggregate","title":"aggregate","text":"<pre><code>aggregate(model_params, weights, global_model, training_plan, training_replies, n_updates=1, n_round=0, *args, **kwargs)\n</code></pre> <p>Aggregates local models coming from nodes into a global model, using SCAFFOLD algorithm (2nd option) Scaffold: Stochastic Controlled Averaging for Federated Learning</p>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.aggregate--performed-computations","title":"Performed computations:","text":"<ul> <li>Compute participating nodes' model update:<ul> <li>update_i = y_i - x</li> </ul> </li> <li>Compute aggregated model parameters:<ul> <li>x(+) = x - eta_g sum_S(update_i)</li> </ul> </li> <li>Update participating nodes' state:<ul> <li>c_i = delta_i + 1/(K*eta_i) * update_i</li> </ul> </li> <li>Update the global state and all nodes' correction state:<ul> <li>c = 1/N sum_{i=1}^n c_i</li> <li>delta_i = (c_i - c)</li> </ul> </li> </ul> <p>where, according to paper notations     c_i: local state variable for node <code>i</code>     c: global state variable     delta_i: (c_i - c), correction state for node <code>i</code>     eta_g: server's learning rate     eta_i: node i's learning rate     N: total number of node participating to federated learning     S: number of nodes considered during current round (S&lt;=N)     K: number of updates done during the round (ie number of data batches).     x: global model parameters     y_i: node i 's local model parameters at the end of the round</p> <p>Parameters:</p> Name Type Description Default <code>model_params</code> <code>Dict</code> <p>list of models parameters received from nodes</p> required <code>weights</code> <code>Dict[str, float]</code> <p>weights depicting sample proportions available on each node. Unused for Scaffold.</p> required <code>global_model</code> <code>Dict[str, Union[Tensor, ndarray]]</code> <p>global model, ie aggregated model</p> required <code>training_plan</code> <code>BaseTrainingPlan</code> <p>instance of TrainingPlan</p> required <code>training_replies</code> <code>Dict</code> <p>Training replies from each node that participates in the current round</p> required <code>n_updates</code> <code>int</code> <p>number of updates (number of batch performed). Defaults to 1.</p> <code>1</code> <code>n_round</code> <code>int</code> <p>current round. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Aggregated parameters, as a dict mapping weight names and values.</p> <p>Raises:</p> Type Description <code>FedbiomedAggregatorError</code> <p>If no FederatedDataset is attached to this Scaffold instance, or if <code>node_ids</code> do not belong to the dataset attached to it.</p> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def aggregate(self,\n              model_params: Dict,\n              weights: Dict[str, float],\n              global_model: Dict[str, Union[torch.Tensor, np.ndarray]],\n              training_plan: BaseTrainingPlan,\n              training_replies: Dict,\n              n_updates: int = 1,\n              n_round: int = 0,\n              *args, **kwargs) -&gt; Dict:\n    \"\"\"\n    Aggregates local models coming from nodes into a global model, using SCAFFOLD algorithm (2nd option)\n    [Scaffold: Stochastic Controlled Averaging for Federated Learning][https://arxiv.org/abs/1910.06378]\n\n    Performed computations:\n    -----------------------\n\n    - Compute participating nodes' model update:\n        * update_i = y_i - x\n    - Compute aggregated model parameters:\n        * x(+) = x - eta_g sum_S(update_i)\n    - Update participating nodes' state:\n        * c_i = delta_i + 1/(K*eta_i) * update_i\n    - Update the global state and all nodes' correction state:\n        * c = 1/N sum_{i=1}^n c_i\n        * delta_i = (c_i - c)\n\n    where, according to paper notations\n        c_i: local state variable for node `i`\n        c: global state variable\n        delta_i: (c_i - c), correction state for node `i`\n        eta_g: server's learning rate\n        eta_i: node i's learning rate\n        N: total number of node participating to federated learning\n        S: number of nodes considered during current round (S&lt;=N)\n        K: number of updates done during the round (ie number of data batches).\n        x: global model parameters\n        y_i: node i 's local model parameters at the end of the round\n\n    Args:\n        model_params: list of models parameters received from nodes\n        weights: weights depicting sample proportions available\n            on each node. Unused for Scaffold.\n        global_model: global model, ie aggregated model\n        training_plan (BaseTrainingPlan): instance of TrainingPlan\n        training_replies: Training replies from each node that participates in the current round\n        n_updates: number of updates (number of batch performed). Defaults to 1.\n        n_round: current round. Defaults to 0.\n\n    Returns:\n        Aggregated parameters, as a dict mapping weight names and values.\n\n    Raises:\n        FedbiomedAggregatorError: If no FederatedDataset is attached to this\n            Scaffold instance, or if `node_ids` do not belong to the dataset\n            attached to it.\n    \"\"\"\n    # Gather the learning rates used by nodes, updating `self.nodes_lr`.\n    self.set_nodes_learning_rate_after_training(training_plan, training_replies)\n    # At round 0, initialize zero-valued correction states.\n    if n_round == 0:\n        self.init_correction_states(global_model)\n    # Check that the input node_ids match known ones.\n    if not set(model_params).issubset(self._fds.node_ids()):\n        raise FedbiomedAggregatorError(\n            \"Received updates from nodes that are unknown to this aggregator.\"\n        )\n    # Compute the node-wise model update: (x^t - y_i^t).\n    model_updates = {\n        node_id: {\n            key: (global_model[key] - local_value)\n            for key, local_value in params.items()\n        }\n        for node_id, params in model_params.items()\n    }\n    # Update all Scaffold state variables.\n    self.update_correction_states(model_updates, n_updates)\n    # Compute and return the aggregated model parameters.\n    global_new = {}  # type: Dict[str, Union[torch.Tensor, np.ndarray]]\n    for key, val in global_model.items():\n        upd = sum(model_updates[node_id][key] for node_id in model_params)\n        global_new[key] = val - upd * (self.server_lr / len(model_params))\n    return global_new\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.check_values","title":"check_values","text":"<pre><code>check_values(n_updates, training_plan, *args, **kwargs)\n</code></pre> <p>Check if all values/parameters are correct and have been set before using aggregator.</p> <p>Raise an error otherwise.</p> <p>This can prove useful if user has set wrong hyperparameter values, so that user will have errors before performing first round of training</p> <p>Parameters:</p> Name Type Description Default <code>n_updates</code> <code>int</code> <p>number of updates. Must be non-zero and an integer.</p> required <code>training_plan</code> <code>BaseTrainingPlan</code> <p>training plan. used for checking if optimizer is SGD, otherwise, triggers warning.</p> required <p>Raises:</p> Type Description <code>FedbiomedAggregatorError</code> <p>triggered if <code>num_updates</code> entry is missing (needed for Scaffold aggregator)</p> <code>FedbiomedAggregatorError</code> <p>triggered if any of the learning rate(s) equals 0</p> <code>FedbiomedAggregatorError</code> <p>triggered if number of updates equals 0 or is not an integer</p> <code>FedbiomedAggregatorError</code> <p>triggered if FederatedDataset has not been set.</p> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def check_values(self, n_updates: int, training_plan: BaseTrainingPlan, *args, **kwargs) -&gt; True:\n    \"\"\"Check if all values/parameters are correct and have been set before using aggregator.\n\n    Raise an error otherwise.\n\n    This can prove useful if user has set wrong hyperparameter values, so that user will\n    have errors before performing first round of training\n\n    Args:\n        n_updates: number of updates. Must be non-zero and an integer.\n        training_plan: training plan. used for checking if optimizer is SGD, otherwise,\n            triggers warning.\n\n    Raises:\n        FedbiomedAggregatorError: triggered if `num_updates` entry is missing (needed for Scaffold aggregator)\n        FedbiomedAggregatorError: triggered if any of the learning rate(s) equals 0\n        FedbiomedAggregatorError: triggered if number of updates equals 0 or is not an integer\n        FedbiomedAggregatorError: triggered if [FederatedDataset][fedbiomed.researcher.datasets.FederatedDataset]\n            has not been set.\n    \"\"\"\n    if n_updates is None:\n        raise FedbiomedAggregatorError(\"Cannot perform Scaffold: missing 'num_updates' entry in the training_args\")\n    elif n_updates &lt;= 0 or int(n_updates) != float(n_updates):\n        raise FedbiomedAggregatorError(\n            \"n_updates should be a positive non zero integer, but got \"\n            f\"n_updates: {n_updates} in SCAFFOLD aggregator\"\n        )\n    if self._fds is None:\n        raise FedbiomedAggregatorError(\n            \"Federated Dataset not provided, but needed for Scaffold. Please use setter `set_fds()`.\"\n        )\n\n    # raise warnings\n    if kwargs.get('secagg'):\n        logger.warning(\"Warning: secagg setting detected. Nodes correction states involved in Scaffold algorithm\"\n                       \" are not encrypted and can be read by the `Researcher`.\\n\"\n                       \"Please consider using `declearn`'s Scaffold to encrypt both model parameters and correction\"\n                       \" terms\")\n    return True\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.create_aggregator_args","title":"create_aggregator_args","text":"<pre><code>create_aggregator_args(global_model, node_ids)\n</code></pre> <p>Return correction states that are to be sent to the nodes.</p> <p>Parameters:</p> Name Type Description Default <code>global_model</code> <code>Dict[str, Union[Tensor, ndarray]]</code> <p>parameters of the global model, formatted as a dict mapping weight tensors to their names.</p> required <code>node_ids</code> <code>Collection[str]</code> <p>identifiers of the nodes that are to receive messages.</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Aggregator arguments to share with the nodes for the next round</p> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def create_aggregator_args(\n    self,\n    global_model: Dict[str, Union[torch.Tensor, np.ndarray]],\n    node_ids: Collection[str]\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Return correction states that are to be sent to the nodes.\n\n    Args:\n        global_model: parameters of the global model, formatted as a dict\n            mapping weight tensors to their names.\n        node_ids: identifiers of the nodes that are to receive messages.\n\n    Returns:\n        Aggregator arguments to share with the nodes for the next round\n    \"\"\"\n    # Optionally initialize states, and verify that nodes are known.\n    if not self.nodes_deltas:\n        self.init_correction_states(global_model)\n    if not set(node_ids).issubset(self._fds.node_ids()):\n        raise FedbiomedAggregatorError(\n            \"Scaffold cannot create aggregator args for nodes that are not\"\n            \"covered by its attached FederatedDataset.\"\n        )\n\n    aggregator_dat = {}\n    for node_id in node_ids:\n        # If a node was late-added to the FederatedDataset, create states.\n        if node_id not in self.nodes_deltas:\n            zeros = {key: initialize(val)[1] for key, val in self.global_state.items()}\n            self.nodes_deltas[node_id] = zeros\n            self.nodes_states[node_id] = copy.deepcopy(zeros)\n        # Add information for the current node to the message dicts.\n        aggregator_dat[node_id] = {\n            'aggregator_name': self.aggregator_name,\n            'aggregator_correction': self.nodes_deltas[node_id]\n        }\n\n    return aggregator_dat\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.init_correction_states","title":"init_correction_states","text":"<pre><code>init_correction_states(global_model)\n</code></pre> <p>Initialize Scaffold state variables.</p> <p>Parameters:</p> Name Type Description Default <code>global_model</code> <code>Dict[str, Union[Tensor, ndarray]]</code> <p>parameters of the global model, formatted as a dict mapping weight tensors to their names.</p> required <p>Raises:</p> Type Description <code>FedbiomedAggregatorError</code> <p>if no FederatedDataset is attached to this aggregator.</p> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def init_correction_states(\n    self,\n    global_model: Dict[str, Union[torch.Tensor, np.ndarray]],\n) -&gt; None:\n    \"\"\"Initialize Scaffold state variables.\n\n    Args:\n        global_model: parameters of the global model, formatted as a dict\n            mapping weight tensors to their names.\n\n    Raises:\n        FedbiomedAggregatorError: if no FederatedDataset is attached to\n            this aggregator.\n    \"\"\"\n    # Gather node ids from the attached FederatedDataset.\n    if self._fds is None:\n        raise FedbiomedAggregatorError(\n            \"Cannot initialize correction states: Scaffold aggregator does \"\n            \"not have a FederatedDataset attached.\"\n        )\n    node_ids = self._fds.node_ids()\n    # Initialize nodes states with zero scalars, that will be summed into actual tensors.\n    init_params = {key: initialize(tensor)[1] for key, tensor in global_model.items()}\n    self.nodes_deltas = {node_id: copy.deepcopy(init_params) for node_id in node_ids}\n    self.nodes_states = copy.deepcopy(self.nodes_deltas)\n    self.global_state = init_params\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.load_state_breakpoint","title":"load_state_breakpoint","text":"<pre><code>load_state_breakpoint(state=None)\n</code></pre> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def load_state_breakpoint(self, state: Dict[str, Any] = None):\n    super().load_state_breakpoint(state)\n\n    self.server_lr = self._aggregator_args['server_lr']\n\n    # loading global state\n    global_state_filename = self._aggregator_args['global_state_filename']\n    self.global_state = Serializer.load(global_state_filename)\n\n    for node_id in self._aggregator_args['nodes']:\n        self.nodes_deltas[node_id] = self._aggregator_args[node_id]['aggregator_correction']\n\n    self.nodes_states = copy.deepcopy(self.nodes_deltas)\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.save_state_breakpoint","title":"save_state_breakpoint","text":"<pre><code>save_state_breakpoint(breakpoint_path, global_model)\n</code></pre> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def save_state_breakpoint(\n    self,\n    breakpoint_path: str,\n    global_model: Mapping[str, Union[torch.Tensor, np.ndarray]]\n) -&gt; Dict[str, Any]:\n    # adding aggregator parameters to the breakpoint that wont be sent to nodes\n    self._aggregator_args['server_lr'] = self.server_lr\n\n    # saving global state variable into a file\n    filename = os.path.join(breakpoint_path, f\"global_state_{uuid.uuid4()}.mpk\")\n    Serializer.dump(self.global_state, filename)\n    self._aggregator_args['global_state_filename'] = filename\n\n    self._aggregator_args[\"nodes\"] = self._fds.node_ids()\n    # adding aggregator parameters that will be sent to nodes afterwards\n    return super().save_state_breakpoint(\n        breakpoint_path, global_model=global_model, node_ids=self._fds.node_ids()\n    )\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.set_nodes_learning_rate_after_training","title":"set_nodes_learning_rate_after_training","text":"<pre><code>set_nodes_learning_rate_after_training(training_plan, training_replies)\n</code></pre> <p>Gets back learning rate of optimizer from Node (if learning rate scheduler is used)</p> <p>Parameters:</p> Name Type Description Default <code>training_plan</code> <code>BaseTrainingPlan</code> <p>training plan instance</p> required <code>training_replies</code> <code>Dict</code> <p>training replies that must contain am <code>optimizer_args</code> entry and a learning rate</p> required <p>Raises:</p> Type Description <code>FedbiomedAggregatorError</code> <p>raised when setting learning rate has been unsuccessful</p> <p>Returns:</p> Type Description <code>Dict[str, List[float]]</code> <p>Dict[str, List[float]]: dictionary mapping node_id and a list of float, as many as the number of layers contained in the model (in Pytroch, each layer can have a specific learning rate).</p> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def set_nodes_learning_rate_after_training(\n    self,\n    training_plan: BaseTrainingPlan,\n    training_replies: Dict,\n) -&gt; Dict[str, List[float]]:\n    \"\"\"Gets back learning rate of optimizer from Node (if learning rate scheduler is used)\n\n    Args:\n        training_plan: training plan instance\n        training_replies: training replies that must contain am `optimizer_args`\n            entry and a learning rate\n\n    Raises:\n        FedbiomedAggregatorError: raised when setting learning rate has been unsuccessful\n\n    Returns:\n        Dict[str, List[float]]: dictionary mapping node_id and a list of float, as many as\n            the number of layers contained in the model (in Pytroch, each layer can have a specific learning rate).\n    \"\"\"\n\n    n_model_layers = len(training_plan.get_model_params(\n        only_trainable=False,\n        exclude_buffers=True)\n    )\n    for node_id in self._fds.node_ids():\n        lrs: Dict[str, float] = {}\n\n        node = training_replies.get(node_id, None)\n        if node is not None:\n            lrs = training_replies[node_id][\"optimizer_args\"].get('lr')\n\n        if node is None or lrs is None:\n            # fall back to default value if no lr information was provided\n            lrs = training_plan.optimizer().get_learning_rate()\n\n        if len(lrs) != n_model_layers:\n            raise FedbiomedAggregatorError(\n                \"Error when setting node learning rate for SCAFFOLD: cannot extract node learning rate.\"\n            )\n\n        self.nodes_lr[node_id] = lrs\n    return self.nodes_lr\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.set_training_plan_type","title":"set_training_plan_type","text":"<pre><code>set_training_plan_type(training_plan_type)\n</code></pre> <p>Overrides <code>set_training_plan_type</code> from parent class. Checks the training plan type, and if it is SKlearnTrainingPlan, raises an error. Otherwise, calls parent method.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_type</code> <code>TrainingPlans</code> <p>training_plan type</p> required <p>Raises:</p> Type Description <code>FedbiomedAggregatorError</code> <p>raised if training_plan type has been set to SKLearn training plan</p> <p>Returns:</p> Name Type Description <code>TrainingPlans</code> <code>TrainingPlans</code> <p>training plan type</p> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def set_training_plan_type(self, training_plan_type: TrainingPlans) -&gt; TrainingPlans:\n    \"\"\"\n    Overrides `set_training_plan_type` from parent class.\n    Checks the training plan type, and if it is SKlearnTrainingPlan,\n    raises an error. Otherwise, calls parent method.\n\n    Args:\n        training_plan_type (TrainingPlans): training_plan type\n\n    Raises:\n        FedbiomedAggregatorError: raised if training_plan type has been set to SKLearn training plan\n\n    Returns:\n        TrainingPlans: training plan type\n    \"\"\"\n    if training_plan_type == TrainingPlans.SkLearnTrainingPlan:\n        raise FedbiomedAggregatorError(\"Aggregator SCAFFOLD not implemented for SKlearn\")\n    training_plan_type = super().set_training_plan_type(training_plan_type)\n\n    # TODO: trigger a warning if user is trying to use scaffold with something else than SGD\n    return training_plan_type\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.update_correction_states","title":"update_correction_states","text":"<pre><code>update_correction_states(model_updates, n_updates)\n</code></pre> <p>Update all Scaffold state variables based on node-wise model updates.</p>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold.update_correction_states--performed-computations","title":"Performed computations:","text":"<ul> <li>Update participating nodes' state:<ul> <li>c_i = delta_i + 1/(K*eta_i) * update_i</li> </ul> </li> <li>Update the global state and all nodes' correction state:<ul> <li>c = 1/N sum_{i=1}^n c_i</li> <li>delta_i = (c_i - c)</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>model_updates</code> <code>Dict[str, Dict[str, Union[ndarray, Tensor]]]</code> <p>node-wise model weight updates.</p> required <code>n_updates</code> <code>int</code> <p>number of local optimization steps.</p> required Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def update_correction_states(\n    self,\n    model_updates: Dict[str, Dict[str, Union[np.ndarray, torch.Tensor]]],\n    n_updates: int,\n) -&gt; None:\n    \"\"\"Update all Scaffold state variables based on node-wise model updates.\n\n    Performed computations:\n    ----------------------\n\n    - Update participating nodes' state:\n        * c_i = delta_i + 1/(K*eta_i) * update_i\n    - Update the global state and all nodes' correction state:\n        * c = 1/N sum_{i=1}^n c_i\n        * delta_i = (c_i - c)\n\n    Args:\n        model_updates: node-wise model weight updates.\n        n_updates: number of local optimization steps.\n    \"\"\"\n    # Update the node-wise states for participating nodes:\n    # c_i^{t+1} = delta_i^t + (x^t - y_i^t) / (M * eta)\n    for node_id, updates in model_updates.items():\n        d_i = self.nodes_deltas[node_id]\n        for (key, val) in updates.items():\n            if self.nodes_lr[node_id].get(key) is not None:\n                self.nodes_states[node_id].update(\n                    {\n                    key: d_i[key] + val / (self.nodes_lr[node_id][key] * n_updates)\n                     }\n                )\n    # Update the global state: c^{t+1} = average(c_i^{t+1})\n    for key in self.global_state:\n        self.global_state[key] = 0\n        for state in self.nodes_states.values():\n            if state.get(key) is not None:\n                self.global_state[key] = (\n                    sum(state[key] for state in self.nodes_states.values())\n                        / len(self.nodes_states)\n                    )\n\n    # Compute the new node-wise correction states:\n    # delta_i^{t+1} = c_i^{t+1} - c^{t+1}\n    self.nodes_deltas = {\n        node_id: {\n            key: val - self.global_state[key] for key, val in state.items()\n        }\n        for node_id, state in self.nodes_states.items()\n    }\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators-functions","title":"Functions","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.federated_averaging","title":"federated_averaging","text":"<pre><code>federated_averaging(model_params, weights)\n</code></pre> <p>Defines Federated Averaging (FedAvg) strategy for model aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>model_params</code> <code>List[Dict[str, Union[Tensor, ndarray]]]</code> <p>list that contains nodes' model parameters; each model is stored as an OrderedDict (maps model layer name to the model weights)</p> required <code>weights</code> <code>List[float]</code> <p>weights for performing weighted sum in FedAvg strategy (depending on the dataset size of each node). Items in the list must always sum up to 1</p> required <p>Returns:</p> Type Description <code>Mapping[str, Union[Tensor, ndarray]]</code> <p>Final model with aggregated layers, as an OrderedDict object.</p> Source code in <code>fedbiomed/researcher/aggregators/functional.py</code> <pre><code>def federated_averaging(model_params: List[Dict[str, Union[torch.Tensor, np.ndarray]]],\n                        weights: List[float]) -&gt; Mapping[str, Union[torch.Tensor, np.ndarray]]:\n    \"\"\"Defines Federated Averaging (FedAvg) strategy for model aggregation.\n\n    Args:\n        model_params: list that contains nodes' model parameters; each model is stored as an OrderedDict (maps\n            model layer name to the model weights)\n        weights: weights for performing weighted sum in FedAvg strategy (depending on the dataset size of each node).\n            Items in the list must always sum up to 1\n\n    Returns:\n        Final model with aggregated layers, as an OrderedDict object.\n    \"\"\"\n    assert len(model_params) &gt; 0, 'An empty list of models was passed.'\n    assert len(weights) == len(model_params), 'List with number of observations must have ' \\\n                                              'the same number of elements that list of models.'\n\n    # Compute proportions\n    proportions = [n_k / sum(weights) for n_k in weights]\n    return weighted_sum(model_params, proportions)\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.initialize","title":"initialize","text":"<pre><code>initialize(val)\n</code></pre> <p>Initialize tensor or array vector.</p> Source code in <code>fedbiomed/researcher/aggregators/functional.py</code> <pre><code>def initialize(val: Union[torch.Tensor, np.ndarray]) -&gt; Tuple[str, Union[torch.Tensor, np.ndarray]]:\n    \"\"\"Initialize tensor or array vector. \"\"\"\n    if isinstance(val, torch.Tensor):\n        return 'tensor', torch.zeros_like(val).float()\n\n    if isinstance(val, (list, np.ndarray)):\n        val = np.array(val)\n        return 'array', np.zeros(val.shape, dtype = float)\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.weighted_sum","title":"weighted_sum","text":"<pre><code>weighted_sum(model_params, proportions)\n</code></pre> <p>Performs weighted sum operation</p> <p>Parameters:</p> Name Type Description Default <code>model_params</code> <code>List[Dict[str, Union[Tensor, ndarray]]]</code> <p>list that contains nodes' model parameters; each model is stored as an OrderedDict (maps model layer name to the model weights)</p> required <code>proportions</code> <code>List[float]</code> <p>weights of all items whithin model_params's list</p> required <p>Returns:</p> Type Description <code>Mapping[str, Union[Tensor, ndarray]]</code> <p>Mapping[str, Union[torch.Tensor, np.ndarray]]: model resulting from the weighted sum                                             operation</p> Source code in <code>fedbiomed/researcher/aggregators/functional.py</code> <pre><code>def weighted_sum(model_params: List[Dict[str, Union[torch.Tensor, np.ndarray]]],\n                 proportions: List[float]) -&gt; Mapping[str, Union[torch.Tensor, np.ndarray]]:\n    \"\"\"Performs weighted sum operation\n\n    Args:\n        model_params (List[Dict[str, Union[torch.Tensor, np.ndarray]]]): list that contains nodes'\n            model parameters; each model is stored as an OrderedDict (maps model layer name to the model weights)\n        proportions (List[float]): weights of all items whithin model_params's list\n\n    Returns:\n        Mapping[str, Union[torch.Tensor, np.ndarray]]: model resulting from the weighted sum \n                                                       operation\n    \"\"\"\n    # Empty model parameter dictionary\n    avg_params = copy.deepcopy(model_params[0])\n\n    for key, val in avg_params.items():\n        (t, avg_params[key] ) = initialize(val)\n\n    if t == 'tensor':\n        for model, weight in zip(model_params, proportions):\n            for key in avg_params.keys():\n                avg_params[key] += weight * model[key]\n\n    if t == 'array':\n        for key in avg_params.keys():\n            matr = np.array([ d[key] for d in model_params ])\n            avg_params[key] = np.average(matr, weights=np.array(proportions), axis=0)\n\n    return avg_params\n</code></pre>"},{"location":"developer/api/researcher/cli/","title":"CLI","text":"<p>Researcher CLI</p>"},{"location":"developer/api/researcher/cli/#fedbiomed.researcher.cli-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/cli/#fedbiomed.researcher.cli-classes","title":"Classes","text":""},{"location":"developer/api/researcher/cli/#fedbiomed.researcher.cli.ResearcherCLI","title":"ResearcherCLI","text":"<pre><code>ResearcherCLI()\n</code></pre> <p>               Bases: <code>CommonCLI</code></p> <p>Researcher CLI</p> Source code in <code>fedbiomed/researcher/cli.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.description = f\"{__intro__}\\nA CLI app for fedbiomed researchers.\"\n    self.initialize()\n</code></pre>"},{"location":"developer/api/researcher/cli/#fedbiomed.researcher.cli.ResearcherCLI-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/cli/#fedbiomed.researcher.cli.ResearcherCLI.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description = f'{__intro__}\nA CLI app for fedbiomed researchers.'\n</code></pre>"},{"location":"developer/api/researcher/cli/#fedbiomed.researcher.cli.ResearcherCLI-functions","title":"Functions","text":""},{"location":"developer/api/researcher/cli/#fedbiomed.researcher.cli.ResearcherCLI.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initializes Researcher CLI</p> Source code in <code>fedbiomed/researcher/cli.py</code> <pre><code>def initialize(self):\n    \"\"\"Initializes Researcher CLI\"\"\"\n\n\n    class ComponentDirectoryActionResearcher(ComponentDirectoryAction):\n        _this = self\n        _component = ComponentType.RESEARCHER\n\n        def set_component(self, component_dir: str) -&gt; None:\n            \"\"\"Import configuration\n\n            Args:\n                config_name: Name of the config file for the component\n            \"\"\"\n            os.environ[\"FBM_RESEARCHER_COMPONENT_ROOT\"] = os.path.abspath(\n                    component_dir\n            )\n            module = importlib.import_module(\"fedbiomed.researcher.config\")\n            self._this.config = module.config\n\n    super().initialize()\n\n    self._parser.add_argument(\n        \"--path\",\n        \"-p\",\n        nargs=\"?\",\n        action=ComponentDirectoryActionResearcher,\n        default=\"fbm-researcher\",\n        help=\"Name of the config file that the CLI will be activated for. Default \"\n             \"is 'config_researcher.ini'.\"\n    )\n</code></pre>"},{"location":"developer/api/researcher/cli/#fedbiomed.researcher.cli.ResearcherControl","title":"ResearcherControl","text":"<pre><code>ResearcherControl(subparser, parser=None)\n</code></pre> <p>               Bases: <code>CLIArgumentParser</code></p> Source code in <code>fedbiomed/common/cli.py</code> <pre><code>def __init__(self, subparser: argparse.ArgumentParser, parser = None):\n\n    self._subparser = subparser\n    # Parser that is going to be add using subparser\n    self._parser = None\n\n    self._main_parser = parser\n</code></pre>"},{"location":"developer/api/researcher/cli/#fedbiomed.researcher.cli.ResearcherControl-functions","title":"Functions","text":""},{"location":"developer/api/researcher/cli/#fedbiomed.researcher.cli.ResearcherControl.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> Source code in <code>fedbiomed/researcher/cli.py</code> <pre><code>def initialize(self):\n\n    start = self._subparser.add_parser(\n        \"start\", help=\"Starts Jupyter (server) Notebook for researcher API. The default\"\n                      \"directory will be  notebook directory.\")\n\n    start.add_argument(\n        \"--directory\",\n        \"-dir\",\n        type=str,\n        nargs=\"?\",\n        required=False,\n        help=\"The directory where jupyter notebook will be started.\")\n    start.set_defaults(func=self.start)\n</code></pre>"},{"location":"developer/api/researcher/cli/#fedbiomed.researcher.cli.ResearcherControl.start","title":"start","text":"<pre><code>start(args)\n</code></pre> <p>Starts jupyter notebook</p> Source code in <code>fedbiomed/researcher/cli.py</code> <pre><code>def start(self, args):\n    \"\"\"Starts jupyter notebook\"\"\"\n\n    options = ['--NotebookApp.use_redirect_file=false']\n\n    component_path = os.path.join(os.getcwd(), args.path)\n\n    if args.directory:\n        nb_start_dir = args.directory\n    else:\n        nb_start_dir = os.path.join(component_path, NOTEBOOKS_FOLDER_NAME)\n\n    options.append(f\"--notebook-dir={nb_start_dir}\")\n\n    current_env = os.environ.copy()\n    #comp_root = os.environ.get(\"FBM_RESEARCHER_COMPONENT_ROOT\", None)\n    command = [\"jupyter\", \"notebook\"]\n    command = [*command, *options]\n    process = subprocess.Popen(command, env=current_env)\n\n    try:\n        process.wait()\n    except KeyboardInterrupt:\n        try:\n            process.terminate()\n        except Exception:\n            pass\n        process.wait()\n</code></pre>"},{"location":"developer/api/researcher/config/","title":"Config","text":""},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config.component_root","title":"component_root  <code>module-attribute</code>","text":"<pre><code>component_root = get('FBM_RESEARCHER_COMPONENT_ROOT', None)\n</code></pre>"},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config.config","title":"config  <code>module-attribute</code>","text":"<pre><code>config = initiate(root=component_root)\n</code></pre>"},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config.researcher_component","title":"researcher_component  <code>module-attribute</code>","text":"<pre><code>researcher_component = ResearcherComponent()\n</code></pre>"},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config-classes","title":"Classes","text":""},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config.ResearcherComponent","title":"ResearcherComponent","text":"<pre><code>ResearcherComponent()\n</code></pre> <p>               Bases: <code>Component</code></p> <p>Fed-BioMed Node Component Class</p> <p>This class is used for creating and validating components by given component root directory</p> Source code in <code>fedbiomed/common/config.py</code> <pre><code>def __init__(self):\n    \"\"\"Test\"\"\"\n    self._reference = '.fedbiomed'\n</code></pre>"},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config.ResearcherComponent-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config.ResearcherComponent.config_cls","title":"config_cls  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config_cls = ResearcherConfig\n</code></pre>"},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config.ResearcherComponent-functions","title":"Functions","text":""},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config.ResearcherComponent.initiate","title":"initiate","text":"<pre><code>initiate(root=None)\n</code></pre> <p>Creates or initiates existing component</p> Source code in <code>fedbiomed/researcher/config.py</code> <pre><code>def initiate(self, root: Optional[str] = None) -&gt; ResearcherConfig:\n    \"\"\"Creates or initiates existing component\"\"\"\n    config = super().initiate(root)\n\n    notebooks_path = os.path.join(config.root, NOTEBOOKS_FOLDER_NAME)\n    notebooks_share_path = os.path.join(SHARE_DIR, NOTEBOOKS_FOLDER_NAME)\n    docs_share_path = os.path.join(SHARE_DIR, DOCS_FOLDER_NAME)\n    if not os.path.isdir(notebooks_path):\n        shutil.copytree(notebooks_share_path, notebooks_path, symlinks=True)\n        shutil.copytree(\n            os.path.join(docs_share_path, TUTORIALS_FOLDER_NAME),\n            os.path.join(notebooks_path, TUTORIALS_FOLDER_NAME),\n            symlinks=True,\n            dirs_exist_ok=True,\n        )\n\n    return config\n</code></pre>"},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config.ResearcherConfig","title":"ResearcherConfig","text":"<pre><code>ResearcherConfig(root)\n</code></pre> <p>               Bases: <code>Config</code></p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory for the component</p> required Source code in <code>fedbiomed/common/config.py</code> <pre><code>def __init__(\n    self, root: str\n) -&gt; None:\n    \"\"\"Initializes configuration\n\n    Args:\n        root: Root directory for the component\n    \"\"\"\n    self._cfg = configparser.ConfigParser()\n    self.load(root)\n</code></pre>"},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config.ResearcherConfig-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config.ResearcherConfig.COMPONENT_TYPE","title":"COMPONENT_TYPE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>COMPONENT_TYPE = 'RESEARCHER'\n</code></pre>"},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config.ResearcherConfig-functions","title":"Functions","text":""},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config.ResearcherConfig.add_parameters","title":"add_parameters","text":"<pre><code>add_parameters()\n</code></pre> <p>Generate researcher config</p> Source code in <code>fedbiomed/researcher/config.py</code> <pre><code>def add_parameters(self):\n    \"\"\"Generate researcher config\"\"\"\n\n    grpc_host = os.getenv('FBM_SERVER_HOST', 'localhost')\n    grpc_port = os.getenv('FBM_SERVER_PORT', '50051')\n\n    # Generate certificate for gRPC server\n    key_file, pem_file = generate_certificate(\n        root=self.root,\n        prefix=SERVER_certificate_prefix,\n        component_id=self._cfg['default']['id'],\n        subject={'CommonName': grpc_host}\n    )\n\n    self._cfg['server'] = {\n        'host': grpc_host,\n        'port': grpc_port,\n    }\n\n    self._cfg[\"certificate\"] = {\n        \"private_key\": os.path.relpath(key_file, os.path.join(self.root, CONFIG_FOLDER_NAME)),\n        \"public_key\": os.path.relpath(pem_file, os.path.join(self.root, CONFIG_FOLDER_NAME))\n    }\n    self._cfg['security'] = {\n        'secagg_insecure_validation': os.getenv('FBM_SECURITY_SECAGG_INSECURE_VALIDATION', True)\n    }\n</code></pre>"},{"location":"developer/api/researcher/config/#fedbiomed.researcher.config-functions","title":"Functions","text":""},{"location":"developer/api/researcher/datasets/","title":"Datasets","text":"<p>Module includes the classes that allow researcher to interact with remote datasets (federated datasets).</p>"},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets-classes","title":"Classes","text":""},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets.FederatedDataSet","title":"FederatedDataSet","text":"<pre><code>FederatedDataSet(data)\n</code></pre> <p>A class that allows researcher to interact with remote datasets (federated datasets).</p> <p>It contains details about remote datasets, such as client ids, data size that can be useful for aggregating or sampling strategies on researcher's side</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict</code> <p>Dictionary of datasets. Each key is a <code>str</code> representing a node's ID. Each value is a <code>dict</code> (or a <code>list</code> containing exactly one <code>dict</code>). Each <code>dict</code> contains the description of the dataset associated to this node in the federated dataset. </p> required <p>Raises:</p> Type Description <code>FedbiomedFederatedDataSetError</code> <p>bad <code>data</code> format</p> Source code in <code>fedbiomed/researcher/datasets.py</code> <pre><code>def __init__(self, data: Dict):\n    \"\"\"Construct FederatedDataSet object.\n\n    Args:\n        data: Dictionary of datasets. Each key is a `str` representing a node's ID. Each value is\n            a `dict` (or a `list` containing exactly one `dict`). Each `dict` contains the description\n            of the dataset associated to this node in the federated dataset. \n\n    Raises:\n        FedbiomedFederatedDataSetError: bad `data` format\n    \"\"\"\n    # check structure of data\n    self._v = Validator()\n    self._v.register(\"list_or_dict\", self._dataset_type, override=True)\n    try:\n        self._v.validate(data, dict)\n        for node, ds in data.items():\n            self._v.validate(node, str)\n            self._v.validate(ds, \"list_or_dict\")\n            if isinstance(ds, list):\n                if len(ds) == 1:\n                    self._v.validate(ds[0], dict)\n                    # convert list of one dict to dict\n                    data[node] = ds[0]\n                else:\n                    errmess = f'{ErrorNumbers.FB416.value}: {node} must have one unique dataset ' \\\n                        f'but has {len(ds)} datasets.'\n                    logger.error(errmess)\n                    raise FedbiomedFederatedDataSetError(errmess)\n    except ValidatorError as e:\n        errmess = f'{ErrorNumbers.FB416.value}: bad parameter `data` must be a `dict` of ' \\\n            f'(`list` of one) `dict`: {e}'\n        logger.error(errmess)\n        raise FedbiomedFederatedDataSetError(errmess)\n\n    self._data = data\n</code></pre>"},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets.FederatedDataSet-functions","title":"Functions","text":""},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets.FederatedDataSet.data","title":"data","text":"<pre><code>data()\n</code></pre> <p>Retrieve FederatedDataset as <code>dict</code>.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Federated datasets, keys as node ids</p> Source code in <code>fedbiomed/researcher/datasets.py</code> <pre><code>def data(self) -&gt; Dict:\n    \"\"\"Retrieve FederatedDataset as [`dict`][dict].\n\n    Returns:\n       Federated datasets, keys as node ids\n    \"\"\"\n    return self._data\n</code></pre>"},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets.FederatedDataSet.node_ids","title":"node_ids","text":"<pre><code>node_ids()\n</code></pre> <p>Retrieve Node ids from <code>FederatedDataSet</code>.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of node ids</p> Source code in <code>fedbiomed/researcher/datasets.py</code> <pre><code>def node_ids(self) -&gt; List[str]:\n    \"\"\"Retrieve Node ids from `FederatedDataSet`.\n\n    Returns:\n        List of node ids\n    \"\"\"\n    return list(self._data.keys())\n</code></pre>"},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets.FederatedDataSet.sample_sizes","title":"sample_sizes","text":"<pre><code>sample_sizes()\n</code></pre> <p>Retrieve list of sample sizes of node's dataset.</p> <p>Returns:</p> Type Description <code>List[int]</code> <p>List of sample sizes in federated datasets in the same order with node_ids</p> Source code in <code>fedbiomed/researcher/datasets.py</code> <pre><code>def sample_sizes(self) -&gt; List[int]:\n    \"\"\"Retrieve list of sample sizes of node's dataset.\n\n    Returns:\n        List of sample sizes in federated datasets in the same order with\n            [node_ids][fedbiomed.researcher.datasets.FederatedDataSet.node_ids]\n    \"\"\"\n    sample_sizes = []\n    for (key, val) in self._data.items():\n        sample_sizes.append(val[\"shape\"][0])\n\n    return sample_sizes\n</code></pre>"},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets.FederatedDataSet.shapes","title":"shapes","text":"<pre><code>shapes()\n</code></pre> <p>Get shape of FederatedDatasets by node ids.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Includes <code>sample_sizes</code> by node_ids.</p> Source code in <code>fedbiomed/researcher/datasets.py</code> <pre><code>def shapes(self) -&gt; Dict[str, int]:\n    \"\"\"Get shape of FederatedDatasets by node ids.\n\n    Returns:\n        Includes [`sample_sizes`][fedbiomed.researcher.datasets.FederatedDataSet.sample_sizes] by node_ids.\n    \"\"\"\n    shapes_dict = {}\n    for node_id, node_data_size in zip(self.node_ids(),\n                                       self.sample_sizes()):\n        shapes_dict[node_id] = node_data_size\n\n    return shapes_dict\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/","title":"Federated Workflows","text":""},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows-classes","title":"Classes","text":""},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment","title":"Experiment","text":"<pre><code>Experiment(*args, aggregator=None, agg_optimizer=None, node_selection_strategy=None, round_limit=None, tensorboard=False, retain_full_history=True, **kwargs)\n</code></pre> <p>               Bases: <code>TrainingPlanWorkflow</code></p> <p>A Federated Learning Experiment based on a Training Plan.</p> <p>This class provides a comprehensive entry point for the management and orchestration of a FL experiment, including definition, execution, and interpretation of results.</p> <p>Managing model parameters</p> <p>The model parameters should be managed through the corresponding methods in the training_plan by accessing the experiment's <code>training_plan()</code> attribute and using the <code>set_model_params</code> and <code>get_model_params</code> functions, e.g. <pre><code>exp.training_plan().set_model_params(params_dict)\n</code></pre></p> <p>Do not set the training plan attribute directly</p> <p>Setting the <code>training_plan</code> attribute directly is not allowed. Instead, use the <code>set_training_plan_class</code> method to set the training plan type, and the underlying model will be correctly constructed and initialized.</p> <p>Parameters:</p> Name Type Description Default <code>aggregator</code> <code>Optional[Aggregator]</code> <p>object defining the method for aggregating local updates. Default to None (use <code>FedAverage</code> for aggregation)</p> <code>None</code> <code>agg_optimizer</code> <code>Optional[Optimizer]</code> <p><code>Optimizer</code> instance, to refine aggregated model updates prior to their application. If None, merely apply the aggregated updates.</p> <code>None</code> <code>node_selection_strategy</code> <code>Optional[Strategy]</code> <p>object defining how nodes are sampled at each round for training, and how non-responding nodes are managed. Defaults to None: - use <code>DefaultStrategy</code>     if training_data is initialized - else strategy is None (cannot be initialized), experiment cannot be launched yet</p> <code>None</code> <code>round_limit</code> <code>Union[int, None]</code> <p>the maximum number of training rounds (nodes &lt;-&gt; central server) that should be executed for the experiment. <code>None</code> means that no limit is defined. Defaults to None.</p> <code>None</code> <code>tensorboard</code> <code>bool</code> <p>whether to save scalar values  for displaying in Tensorboard during training for each node. Currently, it is only used for loss values. - If it is true, monitor instantiates a <code>Monitor</code> object     that write scalar logs into <code>./runs</code> directory. - If it is False, it stops monitoring if it was active.</p> <code>False</code> <code>retain_full_history</code> <code>bool</code> <p>whether to retain in memory the full history of node replies and aggregated params for the experiment. If False, only the last round's replies and aggregated params will be available. Defaults to True.</p> <code>True</code> <code>*args</code> <p>Extra positional arguments from parent class <code>TrainingPlanWorkflow</code></p> <code>()</code> <code>**kwargs</code> <p>Arguments of parent class <code>TrainingPlanWorkflow</code></p> <code>{}</code> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef __init__(\n    self,\n    *args,\n    aggregator: Optional[Aggregator] = None,\n    agg_optimizer: Optional[Optimizer] = None,\n    node_selection_strategy: Optional[Strategy] = None,\n    round_limit: Union[int, None] = None,\n    tensorboard: bool = False,\n    retain_full_history: bool = True,\n    **kwargs\n) -&gt; None:\n    \"\"\"Constructor of the class.\n\n    Args:\n        aggregator: object defining the method for aggregating\n            local updates. Default to None (use\n            [`FedAverage`][fedbiomed.researcher.aggregators.FedAverage] for aggregation)\n\n        agg_optimizer: [`Optimizer`][fedbiomed.common.optimizers.Optimizer] instance,\n            to refine aggregated model updates prior to their application. If None,\n            merely apply the aggregated updates.\n\n        node_selection_strategy: object defining how nodes are sampled at\n            each round for training, and how non-responding nodes are managed.\n            Defaults to None:\n            - use [`DefaultStrategy`][fedbiomed.researcher.strategies.DefaultStrategy]\n                if training_data is initialized\n            - else strategy is None (cannot be initialized), experiment cannot be launched yet\n\n        round_limit: the maximum number of training rounds (nodes &lt;-&gt; central server)\n            that should be executed for the experiment. `None` means that no limit is\n            defined. Defaults to None.\n\n        tensorboard: whether to save scalar values  for displaying in Tensorboard\n            during training for each node. Currently, it is only used for loss values.\n            - If it is true, monitor instantiates a `Monitor` object\n                that write scalar logs into `./runs` directory.\n            - If it is False, it stops monitoring if it was active.\n\n        retain_full_history: whether to retain in memory the full history\n            of node replies and aggregated params for the experiment. If False, only the\n            last round's replies and aggregated params will be available. Defaults to True.\n        *args: Extra positional arguments from parent class\n            [`TrainingPlanWorkflow`][fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow]\n        **kwargs: Arguments of parent class\n            [`TrainingPlanWorkflow`][fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow]\n    \"\"\"\n    # define new members\n    self._node_selection_strategy: Strategy = None\n    self._round_limit = None\n    self._monitor = None\n    self._aggregator = None\n    self._agg_optimizer = None\n    self.aggregator_args = {}\n    self._aggregated_params = {}\n    self._training_replies: Dict = {}\n    self._retain_full_history = None\n\n    # initialize object\n    super().__init__(*args, **kwargs)\n\n    # set self._aggregator : type Aggregator\n    self.set_aggregator(aggregator)\n\n    # set self._agg_optimizer: type Optional[Optimizer]\n    self.set_agg_optimizer(agg_optimizer)\n\n    # set self._node_selection_strategy: type Union[Strategy, None]\n    self.set_strategy(node_selection_strategy)\n\n    # \"current\" means number of rounds already trained\n    self._set_round_current(0)\n    self.set_round_limit(round_limit)\n\n    # always create a monitoring process\n    self._monitor = Monitor(self.config.vars[\"TENSORBOARD_RESULTS_DIR\"])\n    self._reqs.add_monitor_callback(self._monitor.on_message_handler)\n    self.set_tensorboard(tensorboard)\n\n    # whether to retain the full experiment history or not\n    self.set_retain_full_history(retain_full_history)\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.aggregator_args","title":"aggregator_args  <code>instance-attribute</code>","text":"<pre><code>aggregator_args = {}\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment-functions","title":"Functions","text":""},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.agg_optimizer","title":"agg_optimizer","text":"<pre><code>agg_optimizer()\n</code></pre> <p>Retrieves the optional Optimizer used to refine aggregated model updates.</p> <p>To set or update that optimizer: <code>set_agg_optimizer</code>.</p> <p>Returns:</p> Type Description <code>Optional[Optimizer]</code> <p>An Optimizer instance,</p> <code>Optional[Optimizer]</code> <p>or None.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef agg_optimizer(self) -&gt; Optional[Optimizer]:\n    \"\"\"Retrieves the optional Optimizer used to refine aggregated model updates.\n\n    To set or update that optimizer:\n    [`set_agg_optimizer`][fedbiomed.researcher.federated_workflows.Experiment.set_agg_optimizer].\n\n    Returns:\n        An [Optimizer][fedbiomed.common.optimizers.Optimizer] instance,\n        or None.\n    \"\"\"\n    return self._agg_optimizer\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.aggregated_params","title":"aggregated_params","text":"<pre><code>aggregated_params()\n</code></pre> <p>Retrieves all aggregated parameters of each round of training</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of aggregated parameters keys stand for each round of training</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef aggregated_params(self) -&gt; dict:\n    \"\"\"Retrieves all aggregated parameters of each round of training\n\n    Returns:\n        Dictionary of aggregated parameters keys stand for each round of training\n    \"\"\"\n\n    return self._aggregated_params\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.aggregator","title":"aggregator","text":"<pre><code>aggregator()\n</code></pre> <p>Retrieves aggregator class that will be used for aggregating model parameters.</p> <p>To set or update aggregator: <code>set_aggregator</code>.</p> <p>Returns:</p> Type Description <code>Aggregator</code> <p>A class or an object that is an instance of Aggregator</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef aggregator(self) -&gt; Aggregator:\n    \"\"\"Retrieves aggregator class that will be used for aggregating model parameters.\n\n    To set or update aggregator:\n    [`set_aggregator`][fedbiomed.researcher.federated_workflows.Experiment.set_aggregator].\n\n    Returns:\n        A class or an object that is an instance of [Aggregator][fedbiomed.researcher.aggregators.Aggregator]\n\n    \"\"\"\n    return self._aggregator\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.breakpoint","title":"breakpoint","text":"<pre><code>breakpoint()\n</code></pre> <p>Saves breakpoint with the state of the training at a current round.</p> <p>The following Experiment attributes will be saved:</p> <ul> <li>round_current</li> <li>round_limit</li> <li>aggregator</li> <li>agg_optimizer</li> <li>node_selection_strategy</li> <li>aggregated_params</li> </ul> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef breakpoint(self) -&gt; None:\n    \"\"\"\n    Saves breakpoint with the state of the training at a current round.\n\n    The following Experiment attributes will be saved:\n\n      - round_current\n      - round_limit\n      - aggregator\n      - agg_optimizer\n      - node_selection_strategy\n      - aggregated_params\n    \"\"\"\n    # need to have run at least 1 round to save a breakpoint\n    if self._round_current &lt; 1:\n        msg = ErrorNumbers.FB413.value + \\\n            ' - need to run at least 1 before saving a breakpoint'\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n\n    # conditions are met, save breakpoint\n    breakpoint_path, breakpoint_file_name = \\\n        choose_bkpt_file(\n            self.config.vars[\"EXPERIMENTS_DIR\"],\n            self._experimentation_folder,\n            self._round_current - 1\n        )\n\n    # predefine several breakpoint states\n    agg_bkpt = None\n    agg_optim_bkpt = None\n    strategy_bkpt = None\n    training_replies_bkpt  = None\n    if self._aggregator is not None:\n        agg_bkpt = self._aggregator.save_state_breakpoint(\n            breakpoint_path,\n            global_model=self.training_plan().after_training_params()\n        )\n    if self._agg_optimizer is not None:\n        # FIXME: harmonize naming of save_object\n        agg_optim_bkpt = self.save_optimizer(breakpoint_path)\n    if self._node_selection_strategy is not None:\n        strategy_bkpt = self._node_selection_strategy.save_state_breakpoint()\n    if self._training_replies is not None:\n        training_replies_bkpt = self.save_training_replies()\n\n    state = {\n        'round_current': self._round_current,\n        'round_limit': self._round_limit,\n        'aggregator': agg_bkpt,\n        'agg_optimizer': agg_optim_bkpt,\n        'node_selection_strategy': strategy_bkpt,\n        'aggregated_params': self.save_aggregated_params(\n            self._aggregated_params, breakpoint_path),\n        'training_replies': training_replies_bkpt,\n    }\n\n    super().breakpoint(state, self._round_current)\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.commit_experiment_history","title":"commit_experiment_history","text":"<pre><code>commit_experiment_history(training_replies, aggregated_params)\n</code></pre> <p>Commits the experiment history to memory.</p> The experiment history is defined as <ul> <li>training replies</li> <li>aggregated parameters</li> </ul> <p>This function checks the retain_full_history flag: if it is True, it simply adds (or overwrites) the current round's entry for the training_replies and aggregated_params dictionary. If the flag is set to False, we simply store the last round's values in the same dictionary format.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>def commit_experiment_history(self,\n                              training_replies: Dict[str, Dict[str, Any]],\n                              aggregated_params: Dict[str, Any]) -&gt; None:\n    \"\"\"Commits the experiment history to memory.\n\n    The experiment history is defined as:\n        - training replies\n        - aggregated parameters\n\n    This function checks the retain_full_history flag: if it is True, it simply adds\n    (or overwrites) the current round's entry for the training_replies and aggregated_params\n    dictionary. If the flag is set to False, we simply store the last round's values in the\n    same dictionary format.\n    \"\"\"\n    if self._retain_full_history:\n        # append to history\n        self._training_replies[self._round_current] = training_replies\n        self._aggregated_params[self._round_current] = {'params': aggregated_params}\n    else:\n        # only store the last round's values\n        self._training_replies = {self._round_current: training_replies}\n        self._aggregated_params = {self._round_current: {'params': aggregated_params}}\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.info","title":"info","text":"<pre><code>info()\n</code></pre> <p>Prints out the information about the current status of the experiment.</p> <p>Lists  all the parameters/arguments of the experiment and informs whether the experiment can be run.</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>Inconsistent experiment due to missing variables</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef info(self) -&gt; Tuple[Dict[str, List[str]], str]:\n    \"\"\"Prints out the information about the current status of the experiment.\n\n    Lists  all the parameters/arguments of the experiment and informs whether the experiment can be run.\n\n    Raises:\n        FedbiomedExperimentError: Inconsistent experiment due to missing variables\n    \"\"\"\n    # at this point all attributes are initialized (in constructor)\n\n    info = self._create_default_info_structure()\n\n    info['Arguments'].extend([\n        'Aggregator',\n        'Strategy',\n        'Aggregator Optimizer',\n        'Rounds already run',\n        'Rounds total',\n        'Breakpoint State',\n    ])\n    info['Values'].extend(['\\n'.join(findall('.{1,60}',\n                                     str(e))) for e in [\n        self._aggregator.aggregator_name if self._aggregator is not None else None,\n        self._node_selection_strategy,\n        self._agg_optimizer,\n        self._round_current,\n        self._round_limit,\n        self._save_breakpoints,\n    ]])\n\n    missing = self._check_missing_objects()\n    return super().info(info, missing)\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.load_breakpoint","title":"load_breakpoint  <code>classmethod</code>","text":"<pre><code>load_breakpoint(breakpoint_folder_path=None)\n</code></pre> <p>Loads breakpoint (provided a breakpoint has been saved) so experience can be resumed. Useful if training has crashed researcher side or if user wants to resume a given experiment.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Type[TExperiment]</code> <p>Experiment class</p> required <code>breakpoint_folder_path</code> <code>Union[str, None]</code> <p>path of the breakpoint folder. Path can be absolute or relative eg: \"var/experiments/Experiment_xxxx/breakpoints_xxxx\". If None, loads latest breakpoint of the latest experiment. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>TExperiment</code> <p>Reinitialized experiment object. With given object, user can then use <code>.run()</code> method to pursue model training.</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad argument type, error when reading breakpoint or bad loaded breakpoint content (corrupted)</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@classmethod\n@exp_exceptions\ndef load_breakpoint(cls: Type[TExperiment],\n                    breakpoint_folder_path: Union[str, None] = None) -&gt; TExperiment:\n    \"\"\"\n    Loads breakpoint (provided a breakpoint has been saved)\n    so experience can be resumed. Useful if training has crashed\n    researcher side or if user wants to resume a given experiment.\n\n    Args:\n      cls: Experiment class\n      breakpoint_folder_path: path of the breakpoint folder. Path can be absolute or relative eg:\n        \"var/experiments/Experiment_xxxx/breakpoints_xxxx\". If None, loads latest breakpoint of the latest\n        experiment. Defaults to None.\n\n    Returns:\n        Reinitialized experiment object. With given object, user can then use `.run()`\n            method to pursue model training.\n\n    Raises:\n        FedbiomedExperimentError: bad argument type, error when reading breakpoint or bad loaded breakpoint\n            content (corrupted)\n    \"\"\"\n    loaded_exp, saved_state = super().load_breakpoint(breakpoint_folder_path)\n    # retrieve breakpoint sampling strategy\n    bkpt_sampling_strategy_args = saved_state.get(\"node_selection_strategy\")\n    bkpt_sampling_strategy = cls._create_object(bkpt_sampling_strategy_args)\n    loaded_exp.set_strategy(bkpt_sampling_strategy)\n    # retrieve breakpoint researcher optimizer\n    bkpt_optim = Experiment._load_optimizer(saved_state.get(\"agg_optimizer\"))\n    loaded_exp.set_agg_optimizer(bkpt_optim)\n    # changing `Experiment` attributes\n    loaded_exp._set_round_current(saved_state.get('round_current'))\n    loaded_exp._aggregated_params = loaded_exp._load_aggregated_params(\n        saved_state.get('aggregated_params')\n    )\n    # retrieve and change aggregator\n    bkpt_aggregator_args = saved_state.get(\"aggregator\")\n    bkpt_aggregator = cls._create_object(bkpt_aggregator_args, training_plan=loaded_exp.training_plan())\n    loaded_exp.set_aggregator(bkpt_aggregator)\n    # load training replies\n    loaded_exp.load_training_replies(saved_state.get(\"training_replies\"))\n    logger.info(\n        f\"Experimentation reload from {breakpoint_folder_path if breakpoint_folder_path else 'last save'} successful!\"\n        )\n\n    return loaded_exp\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.load_training_replies","title":"load_training_replies","text":"<pre><code>load_training_replies(bkpt_training_replies)\n</code></pre> <p>Reads training replies from a formatted breakpoint file.</p> <p>Builds a job training replies data structure .</p> <p>Parameters:</p> Name Type Description Default <code>bkpt_training_replies</code> <code>Dict[int, Dict[str, Dict[str, Any]]]</code> <p>Extract from training replies saved in breakpoint</p> required <p>Returns:</p> Type Description <code>None</code> <p>Training replies of already executed rounds of the experiment</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>def load_training_replies(\n    self,\n    bkpt_training_replies: Dict[int, Dict[str, Dict[str, Any]]]\n) -&gt; None:\n    \"\"\"Reads training replies from a formatted breakpoint file.\n\n    Builds a job training replies data structure .\n\n    Args:\n        bkpt_training_replies: Extract from training replies saved in breakpoint\n\n    Returns:\n        Training replies of already executed rounds of the experiment\n    \"\"\"\n    if not bkpt_training_replies:\n        logger.warning(\"No Replies has been found in this breakpoint\")\n\n    rounds = set(bkpt_training_replies.keys())\n    for round_ in rounds:\n        # reload parameters from file params_path\n        for node in bkpt_training_replies[round_].values():\n            node[\"params\"] = Serializer.load(node[\"params_path\"])\n        bkpt_training_replies[int(round_)] = bkpt_training_replies.pop(round_)\n\n    self._training_replies = bkpt_training_replies\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.monitor","title":"monitor","text":"<pre><code>monitor()\n</code></pre> <p>Retrieves the monitor object</p> <p>Monitor is responsible for receiving and parsing real-time training and validation feed-back from each node participate to federated training. See <code>Monitor</code></p> <p>Returns:</p> Type Description <code>Monitor</code> <p>Monitor object that will always exist with experiment to retrieve feed-back from the nodes.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef monitor(self) -&gt; Monitor:\n    \"\"\"Retrieves the monitor object\n\n    Monitor is responsible for receiving and parsing real-time training and validation feed-back from each node\n    participate to federated training. See [`Monitor`][fedbiomed.researcher.monitor.Monitor]\n\n    Returns:\n        Monitor object that will always exist with experiment to retrieve feed-back from the nodes.\n    \"\"\"\n    return self._monitor\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.retain_full_history","title":"retain_full_history","text":"<pre><code>retain_full_history()\n</code></pre> <p>Retrieves the status of whether the full experiment history should be kept in memory.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef retain_full_history(self):\n    \"\"\"Retrieves the status of whether the full experiment history should be kept in memory.\"\"\"\n    return self._retain_full_history\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.round_current","title":"round_current","text":"<pre><code>round_current()\n</code></pre> <p>Retrieves the round where the experiment is at.</p> <p>Returns:</p> Type Description <code>int</code> <p>Indicates the round number that the experiment will perform next.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef round_current(self) -&gt; int:\n    \"\"\"Retrieves the round where the experiment is at.\n\n    Returns:\n        Indicates the round number that the experiment will perform next.\n    \"\"\"\n    return self._round_current\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.round_limit","title":"round_limit","text":"<pre><code>round_limit()\n</code></pre> <p>Retrieves the round limit from the experiment object.</p> <p>Please see  also <code>set_round_limit</code> to change or set round limit.</p> <p>Returns:</p> Type Description <code>Union[int, None]</code> <p>Round limit that shows maximum number of rounds that can be performed. <code>None</code> if it isn't declared yet.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef round_limit(self) -&gt; Union[int, None]:\n    \"\"\"Retrieves the round limit from the experiment object.\n\n    Please see  also [`set_round_limit`][fedbiomed.researcher.federated_workflows.Experiment.set_round_limit]\n    to change or set round limit.\n\n    Returns:\n        Round limit that shows maximum number of rounds that can be performed. `None` if it isn't declared yet.\n    \"\"\"\n    return self._round_limit\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.run","title":"run","text":"<pre><code>run(rounds=None, increase=False)\n</code></pre> <p>Run one or more rounds of an experiment, continuing from the point the experiment had reached.</p> <p>Parameters:</p> Name Type Description Default <code>rounds</code> <code>Optional[int]</code> <p>Number of experiment rounds to run in this call. * <code>None</code> means \"run all the rounds remaining in the experiment\" computed as     maximum rounds (<code>round_limit</code> for this experiment) minus the number of     rounds already run rounds (<code>round_current</code> for this experiment).     It does nothing and issues a warning if <code>round_limit</code> is <code>None</code> (no     round limit defined for the experiment) * <code>int</code> &gt;= 1 means \"run at most <code>rounds</code> rounds\".     If <code>round_limit</code> is <code>None</code> for the experiment, run exactly <code>rounds</code> rounds.     If a <code>round_limit</code> is set for the experiment and the number or rounds would increase beyond the <code>round_limit</code> of the experiment: - if <code>increase</code> is True, increase the <code>round_limit</code> to   (<code>round_current</code> + <code>rounds</code>) and run <code>rounds</code> rounds - if <code>increase</code> is False, run (<code>round_limit</code> - <code>round_current</code>)   rounds, don't modify the maximum <code>round_limit</code> of the experiment   and issue a warning.</p> <code>None</code> <code>increase</code> <code>bool</code> <p>automatically increase the <code>round_limit</code> of the experiment for executing the specified number of <code>rounds</code>. Does nothing if <code>round_limit</code> is <code>None</code> or <code>rounds</code> is None. Defaults to False</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of rounds have been run</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad argument type or value</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef run(self, rounds: Optional[int] = None, increase: bool = False) -&gt; int:\n    \"\"\"Run one or more rounds of an experiment, continuing from the point the\n    experiment had reached.\n\n    Args:\n        rounds: Number of experiment rounds to run in this call.\n            * `None` means \"run all the rounds remaining in the experiment\" computed as\n                maximum rounds (`round_limit` for this experiment) minus the number of\n                rounds already run rounds (`round_current` for this experiment).\n                It does nothing and issues a warning if `round_limit` is `None` (no\n                round limit defined for the experiment)\n            * `int` &gt;= 1 means \"run at most `rounds` rounds\".\n                If `round_limit` is `None` for the experiment, run exactly `rounds` rounds.\n                If a `round_limit` is set for the experiment and the number or rounds would\n            increase beyond the `round_limit` of the experiment:\n            - if `increase` is True, increase the `round_limit` to\n              (`round_current` + `rounds`) and run `rounds` rounds\n            - if `increase` is False, run (`round_limit` - `round_current`)\n              rounds, don't modify the maximum `round_limit` of the experiment\n              and issue a warning.\n        increase: automatically increase the `round_limit`\n            of the experiment for executing the specified number of `rounds`.\n            Does nothing if `round_limit` is `None` or `rounds` is None.\n            Defaults to False\n\n    Returns:\n        Number of rounds have been run\n\n    Raises:\n        FedbiomedExperimentError: bad argument type or value\n    \"\"\"\n    # check rounds is a &gt;=1 integer or None\n    if rounds is None:\n        pass\n    else:\n        msg = ErrorNumbers.FB410.value + \\\n            f', in method `run` param `rounds` : value {rounds}'\n        self._check_round_value_consistency(rounds, msg)\n\n    # check increase is a boolean\n    if not isinstance(increase, bool):\n        msg = ErrorNumbers.FB410.value + \\\n            f', in method `run` param `increase` : type {type(increase)}'\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n\n    # compute number of rounds to run + updated rounds limit\n    if rounds is None:\n        if isinstance(self._round_limit, int):\n            # run all remaining rounds in the experiment\n            rounds = self._round_limit - self._round_current\n            if rounds == 0:\n                # limit already reached\n                logger.warning(f'Round limit of {self._round_limit} already reached '\n                               'for this experiment, do nothing.')\n                return 0\n        else:\n            # cannot run if no number of rounds given and no round limit exists\n            logger.warning('Cannot run, please specify a number of `rounds` to run or '\n                           'set a `round_limit` to the experiment')\n            return 0\n\n    else:\n        # at this point, rounds is an int &gt;= 1\n        if isinstance(self._round_limit, int):\n            if (self._round_current + rounds) &gt; self._round_limit:\n                if increase:\n                    # dont change rounds, but extend self._round_limit as necessary\n                    logger.debug(f'Auto increasing total rounds for experiment from {self._round_limit} '\n                                 f'to {self._round_current + rounds}')\n                    self._round_limit = self._round_current + rounds\n                else:\n                    new_rounds = self._round_limit - self._round_current\n                    if new_rounds == 0:\n                        # limit already reached\n                        logger.warning(f'Round limit of {self._round_limit} already reached '\n                                       'for this experiment, do nothing.')\n                        return 0\n                    else:\n                        # reduce the number of rounds to run in the experiment\n                        logger.warning(f'Limit of {self._round_limit} rounds for the experiment '\n                                       f'will be reached, reducing the number of rounds for this '\n                                       f'run from {rounds} to {new_rounds}')\n                        rounds = new_rounds\n\n    # FIXME: should we print warning if both rounds and _round_limit are None?\n    # At this point `rounds` is an int &gt; 0 (not None)\n\n    # run the rounds\n    for _ in range(rounds):\n        if isinstance(self._round_limit, int) and self._round_current == (self._round_limit - 1) \\\n                and self._training_args['test_on_global_updates'] is True:\n            # Do \"validation after a round\" only if this a round limit is defined and we reached it\n            # and validation is active on global params\n            # When this condition is met, it also means we are running the last of\n            # the `rounds` rounds in this function\n            test_after = True\n        else:\n            test_after = False\n\n        increment = self.run_once(increase=False, test_after=test_after)\n\n        if increment == 0:\n            # should not happen\n            msg = ErrorNumbers.FB400.value + \\\n                f', in method `run` method `run_once` returns {increment}'\n            logger.critical(msg)\n            raise FedbiomedExperimentError(msg)\n\n    return rounds\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.run_once","title":"run_once","text":"<pre><code>run_once(increase=False, test_after=False)\n</code></pre> <p>Run at most one round of an experiment, continuing from the point the experiment had reached.</p> <p>If <code>round_limit</code> is <code>None</code> for the experiment (no round limit defined), run one round. If <code>round_limit</code> is not <code>None</code> and the <code>round_limit</code> of the experiment is already reached: * if <code>increase</code> is False, do nothing and issue a warning * if <code>increase</code> is True, increment total number of round <code>round_limit</code> and run one round</p> <p>Parameters:</p> Name Type Description Default <code>increase</code> <code>bool</code> <p>automatically increase the <code>round_limit</code> of the experiment if needed. Does nothing if <code>round_limit</code> is <code>None</code>. Defaults to False</p> <code>False</code> <code>test_after</code> <code>bool</code> <p>if True, do a second request to the nodes after the round, only for validation on aggregated params. Intended to be used after the last training round of an experiment. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of rounds really run</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad argument type or value</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef run_once(self, increase: bool = False, test_after: bool = False) -&gt; int:\n    \"\"\"Run at most one round of an experiment, continuing from the point the\n    experiment had reached.\n\n    If `round_limit` is `None` for the experiment (no round limit defined), run one round.\n    If `round_limit` is not `None` and the `round_limit` of the experiment is already reached:\n    * if `increase` is False, do nothing and issue a warning\n    * if `increase` is True, increment total number of round `round_limit` and run one round\n\n    Args:\n        increase: automatically increase the `round_limit` of the experiment if needed. Does nothing if\n            `round_limit` is `None`. Defaults to False\n        test_after: if True, do a second request to the nodes after the round, only for validation on aggregated\n            params. Intended to be used after the last training round of an experiment. Defaults to False.\n\n    Returns:\n        Number of rounds really run\n\n    Raises:\n        FedbiomedExperimentError: bad argument type or value\n    \"\"\"\n    # check increase is a boolean\n    if not isinstance(increase, bool):\n        msg = ErrorNumbers.FB410.value + \\\n            f', in method `run_once` param `increase` : type {type(increase)}'\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n\n    # nota:  we should never have self._round_current &gt; self._round_limit, only ==\n    if self._round_limit is not None and self._round_current &gt;= self._round_limit:\n        if increase is True:\n            logger.debug(f'Auto increasing total rounds for experiment from {self._round_limit} '\n                         f'to {self._round_current + 1}')\n            self._round_limit = self._round_current + 1\n        else:\n            logger.warning(f'Round limit of {self._round_limit} was reached, do nothing')\n            return 0\n\n    # check pre-requisites are met for running a round\n    # From here, node_selection_strategy is never None\n    # if self._node_selection_strategy is None:\n    #     msg = ErrorNumbers.FB411.value + ', missing `node_selection_strategy`'\n    #     logger.critical(msg)\n    #     raise FedbiomedExperimentError(msg)\n\n    missing = self._check_missing_objects()\n    if missing:\n        raise FedbiomedExperimentError(ErrorNumbers.FB411.value + ': missing one or several object needed for'\n                                       ' starting the `Experiment`. Details:\\n' + missing)\n    # Sample nodes for training\n\n    training_nodes = self._node_selection_strategy.sample_nodes(\n        from_nodes=self.filtered_federation_nodes(),\n        round_i=self._round_current\n    )\n    # Setup Secure Aggregation (it's a noop if not active)\n    secagg_arguments = self.secagg_setup(training_nodes)\n\n    # Setup aggregator\n    self._aggregator.set_training_plan_type(self.training_plan().type())\n    self._aggregator.check_values(n_updates=self._training_args.get('num_updates'),\n                                  training_plan=self.training_plan(),\n                                  secagg=self.secagg.active)\n    model_params_before_round = self.training_plan().after_training_params()\n    aggregator_args = self._aggregator.create_aggregator_args(model_params_before_round,\n                                                              training_nodes)\n\n    # Collect auxiliary variables from the aggregator optimizer, if any.\n    optim_aux_var = self._collect_optim_aux_var()\n\n    # update node states when list of nodes has changed from one round to another\n    self._update_nodes_states_agent(before_training=True)\n    # TODO check node state agent\n    nodes_state_ids = self._node_state_agent.get_last_node_states()\n\n    # if fds is updated, aggregator should be updated too\n    job = TrainingJob(\n        researcher_id= self._researcher_id,\n        requests=self._reqs,\n        nodes=training_nodes,\n        keep_files_dir=self.experimentation_path(),\n        experiment_id=self._experiment_id,\n        round_=self._round_current,\n        training_plan=self.training_plan(),\n        training_args=self._training_args,\n        model_args=self.model_args(),\n        data=self._fds,\n        nodes_state_ids=nodes_state_ids,\n        aggregator_args=aggregator_args,\n        do_training=True,\n        secagg_arguments=secagg_arguments,\n        optim_aux_var=optim_aux_var\n    )\n\n    logger.info('Sampled nodes in round ' + str(self._round_current) + ' ' + str(job.nodes))\n\n    # Collect training replies and (opt.) optimizer auxiliary variables.\n    training_replies, nodes_aux_var = job.execute()\n\n    # Update node states with node answers + when used node list has changed during the round.\n    self._update_nodes_states_agent(before_training=False, training_replies=training_replies)\n\n    # If no Optimizer is used but auxiliary variables were received, raise.\n    if (self._agg_optimizer is None) and nodes_aux_var:\n        raise FedbiomedExperimentError(\n            \"Received auxiliary variables from 1+ node Optimizer, but \"\n            \"no `agg_optimizer` was set for this Experiment to process \"\n            \"them.\\nThese variables come from the following plug-in \"\n            f\"modules: {set(key for aux in nodes_aux_var for key in aux)}.\"\n        )\n\n    # Collect and refine/normalize model weights received from nodes.\n    model_params, weights, total_sample_size, encryption_factors = (\n        self._node_selection_strategy.refine(\n            training_replies, self._round_current\n        )\n    )\n\n    # (Secure-)Aggregate model parameters and optimizer auxiliary variables.\n    if self._secagg.active:\n        aggregated_params, aggregated_auxvar = (\n            self._aggregate_encrypted_model_params_and_optim_auxvar(\n                model_params, nodes_aux_var, encryption_factors, total_sample_size\n            )\n        )\n    else:\n        aggregated_params = self._aggregator.aggregate(\n            model_params,\n            weights,\n            global_model=model_params_before_round,\n            training_plan=self.training_plan(),\n            training_replies=training_replies,\n            node_ids=job.nodes,\n            n_updates=self._training_args.get('num_updates'),\n            n_round=self._round_current,\n        )\n        aggregated_auxvar = (\n            self._aggregate_cleartext_optim_auxvar(nodes_aux_var)\n            if nodes_aux_var else None\n        )\n\n    # Process optimizer auxiliary variables if any.\n\n    if aggregated_auxvar:\n        self._agg_optimizer.set_aux(aggregated_auxvar)\n\n    # Optionally refine the aggregated updates using an Optimizer.\n    aggregated_params = self._run_agg_optimizer(\n        self.training_plan(), aggregated_params\n    )\n\n    # Update the training plan with the aggregated parameters\n    self.training_plan().set_model_params(aggregated_params)\n\n    # Update experiment's in-memory history\n    self.commit_experiment_history(training_replies, aggregated_params)\n\n    # Increase round number (should be incremented before call to `breakpoint`)\n    self._set_round_current(self._round_current + 1)\n    if self._save_breakpoints:\n        self.breakpoint()\n\n    # do final validation after saving breakpoint :\n    # not saved in breakpoint for current round, but more simple\n    if test_after:\n        # FIXME: should we sample nodes here too?\n        aggr_args = self._aggregator.create_aggregator_args(self.training_plan().after_training_params(),\n                                                            training_nodes)\n\n        job = TrainingJob(\n            researcher_id=self._researcher_id,\n            requests=self._reqs,\n            nodes=training_nodes,\n            keep_files_dir=self.experimentation_path(),\n            experiment_id=self._experiment_id,\n            round_=self._round_current,\n            training_plan=self.training_plan(),\n            training_args=self._training_args,\n            model_args=self.model_args(),\n            data=self._fds,\n            nodes_state_ids=nodes_state_ids,\n            aggregator_args=aggr_args,\n            do_training=False\n        )\n        job.execute()\n\n\n    return 1\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.save_aggregated_params","title":"save_aggregated_params  <code>staticmethod</code>","text":"<pre><code>save_aggregated_params(aggregated_params_init, breakpoint_path)\n</code></pre> <p>Extract and format fields from aggregated_params that need to be saved in breakpoint.</p> <p>Creates link to the params file from the <code>breakpoint_path</code> and use them to reference the params files.</p> <p>Parameters:</p> Name Type Description Default <code>aggregated_params_init</code> <code>dict</code> <p>aggregated parameters</p> required <code>breakpoint_path</code> <code>str</code> <p>path to the directory where breakpoints files and links will be saved</p> required <p>Returns:</p> Type Description <code>Dict[int, dict]</code> <p>Extract from <code>aggregated_params</code></p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad arguments type</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@staticmethod\n@exp_exceptions\ndef save_aggregated_params(aggregated_params_init: dict, breakpoint_path: str) -&gt; Dict[int, dict]:\n    \"\"\"Extract and format fields from aggregated_params that need to be saved in breakpoint.\n\n    Creates link to the params file from the `breakpoint_path` and use them to reference the params files.\n\n    Args:\n        aggregated_params_init (dict): aggregated parameters\n        breakpoint_path: path to the directory where breakpoints files and links will be saved\n\n    Returns:\n        Extract from `aggregated_params`\n\n    Raises:\n        FedbiomedExperimentError: bad arguments type\n    \"\"\"\n    # check arguments type, though is should have been done before\n    if not isinstance(aggregated_params_init, dict):\n        msg = ErrorNumbers.FB413.value + ' - save failed. ' + \\\n            f'Bad type for aggregated params, should be `dict` not {type(aggregated_params_init)}'\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n    if not isinstance(breakpoint_path, str):\n        msg = ErrorNumbers.FB413.value + ' - save failed. ' + \\\n            f'Bad type for breakpoint path, should be `str` not {type(breakpoint_path)}'\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n\n    aggregated_params = {}\n    for round_, params_dict in aggregated_params_init.items():\n        if not isinstance(params_dict, dict):\n            msg = ErrorNumbers.FB413.value + ' - save failed. ' + \\\n                f'Bad type for aggregated params item {str(round_)}, ' + \\\n                f'should be `dict` not {type(params_dict)}'\n            logger.critical(msg)\n            raise FedbiomedExperimentError(msg)\n\n        params_path = os.path.join(breakpoint_path, f\"aggregated_params_{uuid.uuid4()}.mpk\")\n        Serializer.dump(params_dict['params'], params_path)\n        aggregated_params[round_] = {'params_path': params_path}\n\n    return aggregated_params\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.save_optimizer","title":"save_optimizer","text":"<pre><code>save_optimizer(breakpoint_path)\n</code></pre> <p>Save the researcher-side Optimizer attached to this Experiment.</p> <p>Parameters:</p> Name Type Description Default <code>breakpoint_path</code> <code>str</code> <p>Path to the breakpoint folder.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Path to the optimizer's save file, or None if no Optimizer is used.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef save_optimizer(self, breakpoint_path: str) -&gt; Optional[str]:\n    \"\"\"Save the researcher-side Optimizer attached to this Experiment.\n\n    Args:\n        breakpoint_path: Path to the breakpoint folder.\n\n    Returns:\n        Path to the optimizer's save file, or None if no Optimizer is used.\n    \"\"\"\n    # Case when no researcher optimizer is used.\n    if self._agg_optimizer is None:\n        return None\n    # Case when an Optimizer is used: save its state and return the path.\n    state = self._agg_optimizer.get_state()\n    path = os.path.join(breakpoint_path, f\"optimizer_{uuid.uuid4()}.mpk\")\n    Serializer.dump(state, path)\n    return path\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.save_training_replies","title":"save_training_replies","text":"<pre><code>save_training_replies()\n</code></pre> <p>Extracts a copy of <code>training_replies</code> and prepares it for saving in breakpoint</p> <ul> <li>strip unwanted fields</li> <li>structure as list/dict, so it can be saved with JSON</li> </ul> <p>Returns:</p> Type Description <code>Dict[int, Dict[str, Dict[str, Any]]]</code> <p>Extract from <code>training_replies</code> formatted for breakpoint</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>def save_training_replies(self) -&gt; Dict[int, Dict[str, Dict[str, Any]]]:\n    \"\"\"Extracts a copy of `training_replies` and prepares it for saving in breakpoint\n\n    - strip unwanted fields\n    - structure as list/dict, so it can be saved with JSON\n\n    Returns:\n        Extract from `training_replies` formatted for breakpoint\n    \"\"\"\n    converted_training_replies = copy.deepcopy(self.training_replies())\n    for training_reply in converted_training_replies.values():\n        # we want to strip some fields for the breakpoint\n        for reply in training_reply.values():\n            reply.pop('params', None)\n            reply.pop('optim_aux_var', None)\n    return converted_training_replies\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.set_agg_optimizer","title":"set_agg_optimizer","text":"<pre><code>set_agg_optimizer(agg_optimizer)\n</code></pre> <p>Sets the optional researcher optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>agg_optimizer</code> <code>Optional[Optimizer]</code> <p>Optional fedbiomed Optimizer instance to be used so as to refine aggregated updates prior to applying them. If None, equivalent to using vanilla SGD with 1.0 learning rate.</p> required <p>Returns:</p> Type Description <code>Optional[Optimizer]</code> <p>The optional researcher optimizer attached to this Experiment.</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>if <code>optimizer</code> is of unproper type.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef set_agg_optimizer(\n    self,\n    agg_optimizer: Optional[Optimizer],\n) -&gt; Optional[Optimizer]:\n    \"\"\"Sets the optional researcher optimizer.\n\n    Args:\n        agg_optimizer: Optional fedbiomed Optimizer instance to be\n            used so as to refine aggregated updates prior to applying them.\n            If None, equivalent to using vanilla SGD with 1.0 learning rate.\n\n    Returns:\n        The optional researcher optimizer attached to this Experiment.\n\n    Raises:\n        FedbiomedExperimentError: if `optimizer` is of unproper type.\n    \"\"\"\n    if not (\n        agg_optimizer is None or\n        isinstance(agg_optimizer, Optimizer)\n    ):\n        raise FedbiomedExperimentError(\n            f\"{ErrorNumbers.FB410.value}: 'agg_optimizer' must be an \"\n            f\"Optimizer instance or None, not {type(agg_optimizer)}.\"\n        )\n    self._agg_optimizer = agg_optimizer\n    return self._agg_optimizer\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.set_aggregator","title":"set_aggregator","text":"<pre><code>set_aggregator(aggregator=None)\n</code></pre> <p>Sets aggregator + verification on arguments type</p> <p>Ensures consistency with the training data.</p> <p>Parameters:</p> Name Type Description Default <code>aggregator</code> <code>Optional[Aggregator]</code> <p>Object defining the method for aggregating local updates. Default to None (use <code>FedAverage</code> for aggregation)</p> <code>None</code> <p>Returns:</p> Type Description <code>Aggregator</code> <p>aggregator (Aggregator)</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad aggregator type</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef set_aggregator(self, aggregator: Optional[Aggregator] = None) -&gt; Aggregator:\n    \"\"\"Sets aggregator + verification on arguments type\n\n    Ensures consistency with the training data.\n\n    Args:\n        aggregator: Object defining the method for aggregating local updates. Default to None\n            (use `FedAverage` for aggregation)\n\n    Returns:\n        aggregator (Aggregator)\n\n    Raises:\n        FedbiomedExperimentError : bad aggregator type\n    \"\"\"\n\n    if aggregator is None:\n        # default aggregator\n        self._aggregator = FedAverage()\n\n    elif not isinstance(aggregator, Aggregator):\n\n        msg = f\"{ErrorNumbers.FB410.value}: aggregator is not an instance of Aggregator.\"\n        logger.critical(msg)\n        raise FedbiomedTypeError(msg)\n    else:\n        # at this point, `agregator` is an instance / inheriting of `Aggregator`\n        self._aggregator = aggregator\n    self.aggregator_args[\"aggregator_name\"] = self._aggregator.aggregator_name\n    # ensure consistency with federated dataset\n    self._aggregator.set_fds(self._fds)\n\n    return self._aggregator\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.set_retain_full_history","title":"set_retain_full_history","text":"<pre><code>set_retain_full_history(retain_full_history_=True)\n</code></pre> <p>Sets the status of whether the full experiment history should be kept in memory.</p> <p>Parameters:</p> Name Type Description Default <code>retain_full_history_</code> <code>bool</code> <p>whether to retain in memory the full history of node replies and aggregated params for the experiment. If False, only the last round's replies and aggregated params will be available. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <p>The status of whether the full experiment history should be kept in memory.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef set_retain_full_history(self, retain_full_history_: bool = True):\n    \"\"\"Sets the status of whether the full experiment history should be kept in memory.\n\n    Args:\n        retain_full_history_: whether to retain in memory the full history of node replies and aggregated params\n            for the experiment. If False, only the last round's replies and aggregated params will be available.\n            Defaults to True.\n\n    Returns:\n        The status of whether the full experiment history should be kept in memory.\n    \"\"\"\n    if not isinstance(retain_full_history_, bool):\n        msg = ErrorNumbers.FB410.value + f': retain_full_history should be a bool, instead got ' \\\n                                         f'{type(retain_full_history_)} '\n        logger.critical(msg)\n        raise FedbiomedTypeError(msg)\n    self._retain_full_history = retain_full_history_\n    return self._retain_full_history\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.set_round_limit","title":"set_round_limit","text":"<pre><code>set_round_limit(round_limit)\n</code></pre> <p>Sets <code>round_limit</code> + verification on arguments type</p> <p>Parameters:</p> Name Type Description Default <code>round_limit</code> <code>Union[int, None]</code> <p>the maximum number of training rounds (nodes &lt;-&gt; central server) that should be executed for the experiment. <code>None</code> means that no limit is defined.</p> required <p>Returns:</p> Type Description <code>Union[int, None]</code> <p>Round limit for experiment of federated learning</p> <p>Raises:</p> Type Description <code>FedbiomedValueError</code> <p>bad rounds type or value</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef set_round_limit(self, round_limit: Union[int, None]) -&gt; Union[int, None]:\n    \"\"\"Sets `round_limit` + verification on arguments type\n\n    Args:\n        round_limit: the maximum number of training rounds (nodes &lt;-&gt; central server) that should be executed\n            for the experiment. `None` means that no limit is defined.\n\n    Returns:\n        Round limit for experiment of federated learning\n\n    Raises:\n        FedbiomedValueError : bad rounds type or value\n    \"\"\"\n    # at this point round_current exists and is an int &gt;= 0\n\n    if round_limit is None:\n        # no limit for training rounds\n        self._round_limit = None\n    else:\n        self._check_round_value_consistency(round_limit, \"round_limit\")\n        if round_limit &lt; self._round_current:\n            # self._round_limit can't be less than current round\n            msg = f'cannot set `round_limit` to less than the number of already run rounds ' \\\n                f'({self._round_current})'\n            logger.critical(msg)\n            raise FedbiomedValueError(msg)\n\n        else:\n            self._round_limit = round_limit\n\n    # at this point self._round_limit is a Union[int, None]\n    return self._round_limit\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.set_strategy","title":"set_strategy","text":"<pre><code>set_strategy(node_selection_strategy=None)\n</code></pre> <p>Sets for <code>node_selection_strategy</code> + verification on arguments type</p> <p>Parameters:</p> Name Type Description Default <code>node_selection_strategy</code> <code>Optional[Strategy]</code> <p>object defining how nodes are sampled at each round for training, and how non-responding nodes are managed. Defaults to None: - use <code>DefaultStrategy</code> if training_data is initialized - else strategy is None (cannot be initialized), experiment cannot   be launched yet</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Strategy, None]</code> <p>node selection strategy class</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad strategy type</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef set_strategy(\n    self,\n    node_selection_strategy: Optional[Strategy] = None\n) -&gt; Union[Strategy, None]:\n    \"\"\"Sets for `node_selection_strategy` + verification on arguments type\n\n    Args:\n        node_selection_strategy: object defining how nodes are sampled at each round for training, and\n            how non-responding nodes are managed. Defaults to None:\n            - use `DefaultStrategy` if training_data is initialized\n            - else strategy is None (cannot be initialized), experiment cannot\n              be launched yet\n\n    Returns:\n        node selection strategy class\n\n    Raises:\n        FedbiomedExperimentError : bad strategy type\n    \"\"\"\n    if node_selection_strategy is None:\n        # default node_selection_strategy\n        self._node_selection_strategy = DefaultStrategy()\n    elif not isinstance(node_selection_strategy, Strategy):\n\n        msg = f\"{ErrorNumbers.FB410.value}: wrong type for \" \\\n              \"node_selection_strategy {type(node_selection_strategy)} \" \\\n              \"it should be an instance of Strategy\"\n        logger.critical(msg)\n        raise FedbiomedTypeError(msg)\n    else:\n        self._node_selection_strategy = node_selection_strategy\n    # at this point self._node_selection_strategy is a Union[Strategy, None]\n    return self._node_selection_strategy\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.set_tensorboard","title":"set_tensorboard","text":"<pre><code>set_tensorboard(tensorboard)\n</code></pre> <p>Sets the tensorboard flag</p> <p>Parameters:</p> Name Type Description Default <code>tensorboard</code> <code>bool</code> <p>If <code>True</code> tensorboard log files will be writen after receiving training feedbacks</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Status of tensorboard</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef set_tensorboard(self, tensorboard: bool) -&gt; bool:\n    \"\"\"\n    Sets the tensorboard flag\n\n    Args:\n        tensorboard: If `True` tensorboard log files will be writen after receiving training feedbacks\n\n    Returns:\n        Status of tensorboard\n    \"\"\"\n\n    if isinstance(tensorboard, bool):\n        self._tensorboard = tensorboard\n        self._monitor.set_tensorboard(tensorboard)\n    else:\n        msg = ErrorNumbers.FB410.value + f' `tensorboard` : {type(tensorboard)}'\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n\n    return self._tensorboard\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.set_test_metric","title":"set_test_metric","text":"<pre><code>set_test_metric(metric, **metric_args)\n</code></pre> <p>Sets a metric for federated model validation</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Union[MetricTypes, str, None]</code> <p>A class as an instance of <code>MetricTypes</code>. <code>str</code> for referring one of  metric which provided as attributes in <code>MetricTypes</code>. None, if it isn't declared yet.</p> required <code>**metric_args</code> <code>dict</code> <p>A dictionary that contains arguments for metric function. Arguments should be compatible with corresponding metrics in <code>sklearn.metrics</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Union[str, None], Dict[str, Any]]</code> <p>Metric and  metric args as tuple</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>Invalid type for <code>metric</code> argument</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef set_test_metric(self, metric: Union[MetricTypes, str, None], **metric_args: dict) -&gt; \\\n        Tuple[Union[str, None], Dict[str, Any]]:\n    \"\"\" Sets a metric for federated model validation\n\n    Args:\n        metric: A class as an instance of [`MetricTypes`][fedbiomed.common.metrics.MetricTypes]. [`str`][str] for\n            referring one of  metric which provided as attributes in [`MetricTypes`]\n            [fedbiomed.common.metrics.MetricTypes]. None, if it isn't declared yet.\n        **metric_args: A dictionary that contains arguments for metric function. Arguments\n            should be compatible with corresponding metrics in [`sklearn.metrics`][sklearn.metrics].\n\n    Returns:\n        Metric and  metric args as tuple\n\n    Raises:\n        FedbiomedExperimentError: Invalid type for `metric` argument\n    \"\"\"\n    self._training_args['test_metric'] = metric\n\n    # using **metric_args, we know `test_metric_args` is a Dict[str, Any]\n    self._training_args['test_metric_args'] = metric_args\n    return metric, metric_args\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.set_test_on_global_updates","title":"set_test_on_global_updates","text":"<pre><code>set_test_on_global_updates(flag=True)\n</code></pre> <p>Setter for test_on_global_updates, that indicates whether to  perform a validation on the federated model updates on the node side before training model locally where aggregated model parameters are received.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>bool</code> <p>whether to perform model validation on global updates. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>Value of the flag <code>test_on_global_updates</code>.</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad flag type</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef set_test_on_global_updates(self, flag: bool = True) -&gt; bool:\n    \"\"\"\n    Setter for test_on_global_updates, that indicates whether to  perform a validation on the federated model\n    updates on the node side before training model locally where aggregated model parameters are received.\n\n    Args:\n        flag (bool, optional): whether to perform model validation on global updates. Defaults to True.\n\n    Returns:\n        Value of the flag `test_on_global_updates`.\n\n    Raises:\n        FedbiomedExperimentError : bad flag type\n    \"\"\"\n    self._training_args['test_on_global_updates'] = flag\n    return self._training_args['test_on_global_updates']\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.set_test_on_local_updates","title":"set_test_on_local_updates","text":"<pre><code>set_test_on_local_updates(flag=True)\n</code></pre> <p>Setter for <code>test_on_local_updates</code>, that indicates whether to perform a validation on the federated model on the node side where model parameters are updated locally after training in each node.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>bool</code> <p>whether to perform model validation on local updates. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>value of the flag <code>test_on_local_updates</code></p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad flag type</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef set_test_on_local_updates(self, flag: bool = True) -&gt; bool:\n    \"\"\"\n    Setter for `test_on_local_updates`, that indicates whether to perform a validation on the federated model on the\n    node side where model parameters are updated locally after training in each node.\n\n    Args:\n        flag (bool, optional): whether to perform model validation on local updates. Defaults to True.\n\n    Returns:\n        value of the flag `test_on_local_updates`\n\n    Raises:\n        FedbiomedExperimentError: bad flag type\n    \"\"\"\n    self._training_args['test_on_local_updates'] = flag\n\n    return self._training_args['test_on_local_updates'] #, self._training_args['shuffle_data_local_updates']\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.set_test_ratio","title":"set_test_ratio","text":"<pre><code>set_test_ratio(ratio, shuffle_testing_dataset=False)\n</code></pre> <p>Sets validation ratio for model validation.</p> <p>When setting test_ratio, nodes will allocate (1 - <code>test_ratio</code>) fraction of data for training and the remaining for validating model. This could be useful for validating the model, once every round, as well as controlling overfitting, doing early stopping, ....</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> <code>float</code> <p>validation ratio. Must be within interval [0,1].</p> required <code>shuffle_testing_dataset</code> <code>bool</code> <p>Whether testing dataset should                      be shuffled from one <code>Round</code> to another.                      Defaults to False</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>Tuple of Validation ratio that is set and shuffle_testing_dataset</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad data type</p> <code>FedbiomedExperimentError</code> <p>ratio is not within interval [0, 1]</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef set_test_ratio(self, ratio: float, shuffle_testing_dataset: bool = False) -&gt; float:\n    \"\"\" Sets validation ratio for model validation.\n\n    When setting test_ratio, nodes will allocate (1 - `test_ratio`) fraction of data for training and the\n    remaining for validating model. This could be useful for validating the model, once every round, as well as\n    controlling overfitting, doing early stopping, ....\n\n    Args:\n        ratio: validation ratio. Must be within interval [0,1].\n        shuffle_testing_dataset: Whether testing dataset should\n                                 be shuffled from one `Round` to another.\n                                 Defaults to False\n\n    Returns:\n        Tuple of Validation ratio that is set and shuffle_testing_dataset\n\n    Raises:\n        FedbiomedExperimentError: bad data type\n        FedbiomedExperimentError: ratio is not within interval [0, 1]\n    \"\"\"\n    self._training_args['shuffle_testing_dataset'] = shuffle_testing_dataset\n    self._training_args['test_ratio'] = ratio\n    return ratio, shuffle_testing_dataset\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.set_training_data","title":"set_training_data","text":"<pre><code>set_training_data(training_data, from_tags=False)\n</code></pre> <p>Sets training data for federated training + verification on arguments type</p> <p>See <code>FederatedWorkflow.set_training_data</code> for more information.</p> <p>Ensures consistency also with the Experiment's aggregator and node state agent</p> <p>Setting to None forfeits consistency checks</p> <p>Setting training_data to None does not trigger consistency checks, and may therefore leave the class in an inconsistent state.</p> <p>Returns:</p> Type Description <code>Union[FederatedDataSet, None]</code> <p>Dataset metadata</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef set_training_data(\n        self,\n        training_data: Union[FederatedDataSet, dict, None],\n        from_tags: bool = False) -&gt; \\\n        Union[FederatedDataSet, None]:\n    \"\"\"Sets training data for federated training + verification on arguments type\n\n    See\n    [`FederatedWorkflow.set_training_data`][fedbiomed.researcher.federated_workflows.FederatedWorkflow.set_training_data]\n    for more information.\n\n    Ensures consistency also with the Experiment's aggregator and node state agent\n\n    !!! warning \"Setting to None forfeits consistency checks\"\n        Setting training_data to None does not trigger consistency checks, and may therefore leave the class in an\n        inconsistent state.\n\n    Returns:\n        Dataset metadata\n    \"\"\"\n    super().set_training_data(training_data, from_tags)\n    # Below: Experiment-specific operations for consistency\n    if self._aggregator is not None and self._fds is not None:\n        # update the aggregator's training data\n        self._aggregator.set_fds(self._fds)\n    if self._node_state_agent is not None and self._fds is not None:\n        # update the node state agent (member of FederatedWorkflow)\n        self._node_state_agent.update_node_states(self.all_federation_nodes())\n    return self._fds\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.strategy","title":"strategy","text":"<pre><code>strategy()\n</code></pre> <p>Retrieves the class that represents the node selection strategy.</p> <p>Please see also <code>set_strategy</code> to set or update node selection strategy.</p> <p>Returns:</p> Type Description <code>Union[Strategy, None]</code> <p>A class or object as an instance of <code>Strategy</code>. <code>None</code> if it is not declared yet. It means that node selection strategy will be <code>DefaultStrategy</code>.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef strategy(self) -&gt; Union[Strategy, None]:\n    \"\"\"Retrieves the class that represents the node selection strategy.\n\n    Please see also [`set_strategy`][fedbiomed.researcher.federated_workflows.Experiment.set_strategy]\n    to set or update node selection strategy.\n\n    Returns:\n        A class or object as an instance of [`Strategy`][fedbiomed.researcher.strategies.Strategy]. `None` if\n            it is not declared yet. It means that node selection strategy will be\n            [`DefaultStrategy`][fedbiomed.researcher.strategies.DefaultStrategy].\n    \"\"\"\n    return self._node_selection_strategy\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.test_metric","title":"test_metric","text":"<pre><code>test_metric()\n</code></pre> <p>Retrieves the metric for validation routine.</p> <p>Please see also <code>set_test_metric</code>     to change/set <code>test_metric</code></p> <p>Returns:</p> Type Description <code>Union[MetricTypes, str, None]</code> <p>A class as an instance of <code>MetricTypes</code>. <code>str</code> for referring one of  metric which provided as attributes in <code>MetricTypes</code>. None, if it isn't declared yet.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef test_metric(self) -&gt; Union[MetricTypes, str, None]:\n    \"\"\"Retrieves the metric for validation routine.\n\n    Please see also [`set_test_metric`][fedbiomed.researcher.federated_workflows.Experiment.set_test_metric]\n        to change/set `test_metric`\n\n    Returns:\n        A class as an instance of [`MetricTypes`][fedbiomed.common.metrics.MetricTypes]. [`str`][str] for referring\n            one of  metric which provided as attributes in [`MetricTypes`][fedbiomed.common.metrics.MetricTypes].\n            None, if it isn't declared yet.\n    \"\"\"\n\n    return self._training_args['test_metric']\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.test_metric_args","title":"test_metric_args","text":"<pre><code>test_metric_args()\n</code></pre> <p>Retrieves the metric argument for the metric function that is going to be used.</p> <p>Please see also <code>set_test_metric</code> to change/set <code>test_metric</code> and get more information on the arguments can be used.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary that contains arguments for metric function. See <code>set_test_metric</code></p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef test_metric_args(self) -&gt; Dict[str, Any]:\n    \"\"\"Retrieves the metric argument for the metric function that is going to be used.\n\n    Please see also [`set_test_metric`][fedbiomed.researcher.federated_workflows.Experiment.set_test_metric]\n    to change/set `test_metric` and get more information on the arguments can be used.\n\n    Returns:\n        A dictionary that contains arguments for metric function. See [`set_test_metric`]\n            [fedbiomed.researcher.federated_workflows.Experiment.set_test_metric]\n    \"\"\"\n    return self._training_args['test_metric_args']\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.test_on_global_updates","title":"test_on_global_updates","text":"<pre><code>test_on_global_updates()\n</code></pre> <p>Retrieves the status of whether validation will be performed on globally updated (aggregated) parameters by the nodes at the beginning of each round.</p> <p>Please see also <code>set_test_on_global_updates</code>.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True, if validation is active on globally updated (aggregated) parameters. False for vice versa.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef test_on_global_updates(self) -&gt; bool:\n    \"\"\" Retrieves the status of whether validation will be performed on globally updated (aggregated)\n    parameters by the nodes at the beginning of each round.\n\n    Please see also [`set_test_on_global_updates`]\n    [fedbiomed.researcher.federated_workflows.Experiment.set_test_on_global_updates].\n\n    Returns:\n        True, if validation is active on globally updated (aggregated) parameters. False for vice versa.\n    \"\"\"\n    return self._training_args['test_on_global_updates']\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.test_on_local_updates","title":"test_on_local_updates","text":"<pre><code>test_on_local_updates()\n</code></pre> <p>Retrieves the status of whether validation will be performed on locally updated parameters by the nodes at the end of each round.</p> <p>Please see also     <code>set_test_on_local_updates</code>.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True, if validation is active on locally updated parameters. False for vice versa.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef test_on_local_updates(self) -&gt; bool:\n    \"\"\"Retrieves the status of whether validation will be performed on locally updated parameters by\n    the nodes at the end of each round.\n\n    Please see also\n        [`set_test_on_local_updates`][fedbiomed.researcher.federated_workflows.Experiment.set_test_on_local_updates].\n\n    Returns:\n        True, if validation is active on locally updated parameters. False for vice versa.\n    \"\"\"\n\n    return self._training_args['test_on_local_updates']\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.test_ratio","title":"test_ratio","text":"<pre><code>test_ratio()\n</code></pre> <p>Retrieves the ratio for validation partition of entire dataset.</p> <p>Please see also <code>set_test_ratio</code> to     change/set <code>test_ratio</code></p> <p>Returns:</p> Type Description <code>Tuple[float, bool]</code> <p>The ratio for validation part, <code>1 - test_ratio</code> is ratio for training set.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef test_ratio(self) -&gt; Tuple[float, bool]:\n    \"\"\"Retrieves the ratio for validation partition of entire dataset.\n\n    Please see also [`set_test_ratio`][fedbiomed.researcher.federated_workflows.Experiment.set_test_ratio] to\n        change/set `test_ratio`\n\n    Returns:\n        The ratio for validation part, `1 - test_ratio` is ratio for training set.\n    \"\"\"\n\n    return self._training_args['test_ratio'], self._training_args['shuffle_testing_dataset']\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.Experiment.training_replies","title":"training_replies","text":"<pre><code>training_replies()\n</code></pre> <p>Retrieves training replies of each round of training.</p> <p>Training replies contains timing statistics and the files parth/URLs that has been received after each round.</p> <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>Dictionary of training replies with format {round (int) : replies (dict)}</p> Source code in <code>fedbiomed/researcher/federated_workflows/_experiment.py</code> <pre><code>@exp_exceptions\ndef training_replies(self) -&gt; Union[dict, None]:\n    \"\"\"Retrieves training replies of each round of training.\n\n    Training replies contains timing statistics and the files parth/URLs that has been received after each round.\n\n    Returns:\n        Dictionary of training replies with format {round (int) : replies (dict)}\n    \"\"\"\n\n    return self._training_replies\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow","title":"FederatedWorkflow","text":"<pre><code>FederatedWorkflow(tags=None, nodes=None, training_data=None, experimentation_folder=None, secagg=False, save_breakpoints=False, config_path=None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>A FederatedWorkflow is the abstract entry point for the researcher to orchestrate both local and remote operations.</p> <p>The FederatedWorkflow is an abstract base class from which the actual classes used by the researcher must inherit. It manages the life-cycle of:</p> <ul> <li>the training arguments</li> <li>secure aggregation</li> <li>the node state agent</li> </ul> <p>Additionally, it provides the basis for the breakpoint functionality, and manages some backend functionalities such as the temporary directory, the experiment ID, etc...</p> <p>The attributes <code>training_data</code> and <code>tags</code> are co-dependent. Attempting to modify one of those may result in side effects modifying the other, according to the following rules: - modifying tags if training data is not None will reset the training data based on the new tags - modifying the training data using a FederatedDataset object or a dict will set tags to None</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Optional[List[str] | str]</code> <p>list of string with data tags or string with one data tag. Empty list of tags ([]) means any dataset is accepted, it is different from None (tags not set, cannot search for training_data yet).</p> <code>None</code> <code>nodes</code> <code>Optional[List[str]]</code> <p>list of node_ids to filter the nodes to be involved in the experiment. Defaults to None (no filtering).</p> <code>None</code> <code>training_data</code> <code>Union[FederatedDataSet, dict, None]</code> <ul> <li>If it is a FederatedDataSet object, use this value as training_data.</li> <li>else if it is a dict, create and use a FederatedDataSet object     from the dict and use this value as training_data. The dict should use     node ids as keys, values being list of dicts (each dict representing a     dataset on a node).</li> <li>else if it is None (no training data provided)</li> <li>if <code>tags</code> is not None, set training_data by     searching for datasets with a query to the nodes using <code>tags</code> and <code>nodes</code></li> <li>if <code>tags</code> is None, set training_data to None (no training_data set yet,     experiment is not fully initialized and cannot be launched) Defaults to None (query nodes for dataset if <code>tags</code> is not None, set training_data to None else)</li> </ul> <code>None</code> <code>save_breakpoints</code> <code>bool</code> <p>whether to save breakpoints or not after each training round. Breakpoints can be used for resuming a crashed experiment.</p> <code>False</code> <code>experimentation_folder</code> <code>Union[str, None]</code> <p>choose a specific name for the folder where experimentation result files and breakpoints are stored. This should just contain the name for the folder not a path. The name is used as a subdirectory of <code>config.vars[EXPERIMENTS_DIR])</code>. Defaults to None (auto-choose a folder name) - Caveat : if using a specific name this experimentation will not be     automatically detected as the last experimentation by <code>load_breakpoint</code> - Caveat : do not use a <code>experimentation_folder</code> name finishing     with numbers ([0-9]+) as this would confuse the last experimentation     detection heuristic by <code>load_breakpoint</code>.</p> <code>None</code> <code>secagg</code> <code>Union[bool, SecureAggregation]</code> <p>whether to setup a secure aggregation context for this experiment, and use it to send encrypted updates from nodes to researcher. Defaults to <code>False</code>,</p> <code>False</code> <code>config_name</code> <p>Allows to use specific configuration for reseracher instead of default one. Confiuration file are kept in <code>{FEDBIOMED_DIR}/etc</code>, and a new configuration file will be generated if it is not existing.</p> required Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef __init__(\n    self,\n    tags: Optional[List[str] | str] = None,\n    nodes: Optional[List[str]] = None,\n    training_data: Union[FederatedDataSet, dict, None] = None,\n    experimentation_folder: Union[str, None] = None,\n    secagg: Union[bool, SecureAggregation] = False,\n    save_breakpoints: bool = False,\n    config_path: str | None = None\n) -&gt; None:\n    \"\"\"Constructor of the class.\n\n    Args:\n        tags: list of string with data tags or string with one data tag. Empty list of\n            tags ([]) means any dataset is accepted, it is different from None\n            (tags not set, cannot search for training_data yet).\n\n        nodes: list of node_ids to filter the nodes to be involved in the experiment.\n            Defaults to None (no filtering).\n\n        training_data:\n            * If it is a FederatedDataSet object, use this value as training_data.\n            * else if it is a dict, create and use a FederatedDataSet object\n                from the dict and use this value as training_data. The dict should use\n                node ids as keys, values being list of dicts (each dict representing a\n                dataset on a node).\n            * else if it is None (no training data provided)\n              - if `tags` is not None, set training_data by\n                searching for datasets with a query to the nodes using `tags` and `nodes`\n              - if `tags` is None, set training_data to None (no training_data set yet,\n                experiment is not fully initialized and cannot be launched)\n            Defaults to None (query nodes for dataset if `tags` is not None, set training_data\n            to None else)\n        save_breakpoints: whether to save breakpoints or not after each training\n            round. Breakpoints can be used for resuming a crashed experiment.\n\n        experimentation_folder: choose a specific name for the folder\n            where experimentation result files and breakpoints are stored. This\n            should just contain the name for the folder not a path. The name is used\n            as a subdirectory of `config.vars[EXPERIMENTS_DIR])`. Defaults to None\n            (auto-choose a folder name)\n            - Caveat : if using a specific name this experimentation will not be\n                automatically detected as the last experimentation by `load_breakpoint`\n            - Caveat : do not use a `experimentation_folder` name finishing\n                with numbers ([0-9]+) as this would confuse the last experimentation\n                detection heuristic by `load_breakpoint`.\n        secagg: whether to setup a secure aggregation context for this experiment, and\n            use it to send encrypted updates from nodes to researcher.\n            Defaults to `False`,\n        config_name: Allows to use specific configuration for reseracher instead of default\n            one. Confiuration file are kept in `{FEDBIOMED_DIR}/etc`, and a new configuration\n            file will be generated if it is not existing.\n    \"\"\"\n\n    if config_path:\n        config.load(root=config_path)\n\n    self.config = config\n    # predefine all class variables, so no need to write try/except\n    # block each time we use it\n    self._fds: Optional[FederatedDataSet] = None  # dataset metadata from the full federation\n    self._reqs: Requests = Requests(config=self.config)\n    self._nodes_filter: Optional[List[str]] = None  # researcher-defined nodes filter\n    self._tags: Optional[List[str]] = None\n    self._experimentation_folder: Optional[str] = None\n    self._secagg: Union[SecureAggregation, bool] = False\n    self._save_breakpoints: Optional[bool] = None\n    self._node_state_agent: Optional[NodeStateAgent] = None\n    self._researcher_id: str = config.get('default', 'id')\n    self._experiment_id: str = EXPERIMENT_PREFIX + str(uuid.uuid4())  # creating a unique experiment id\n\n    # set internal members from constructor arguments\n    self.set_secagg(secagg)\n\n    # TODO: Manage tags within the FederatedDataset to avoid conflicts\n    if training_data is not None and tags is not None:\n        msg = f\"{ErrorNumbers.FB410.value}: Can not set `training_data` and `tags` at the \" \\\n            \"same time. Please provide only `training_data`, or tags to search for \" \\\n            \"training data.\"\n        logger.critical(msg)\n        raise FedbiomedValueError(msg)\n\n    # Set tags if it tags is not None\n    if tags:\n        self.set_tags(tags)\n\n    if training_data:\n        self.set_training_data(training_data)\n\n    self.set_nodes(nodes)\n    self.set_save_breakpoints(save_breakpoints)\n\n    self.set_experimentation_folder(experimentation_folder)\n    self._node_state_agent = NodeStateAgent(list(self._fds.data().keys())\n                                            if self._fds and self._fds.data() else [])\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Retrieves the unique experiment identifier.</p>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.requests","title":"requests  <code>property</code>","text":"<pre><code>requests\n</code></pre> <p>Returns requests object</p>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.researcher_id","title":"researcher_id  <code>property</code>","text":"<pre><code>researcher_id\n</code></pre> <p>Returns researcher id</p>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.secagg","title":"secagg  <code>property</code>","text":"<pre><code>secagg\n</code></pre> <p>Gets secagg object <code>SecureAggregation</code></p> <p>Returns:</p> Type Description <code>SecureAggregation</code> <p>Secure aggregation object.</p>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow-functions","title":"Functions","text":""},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.all_federation_nodes","title":"all_federation_nodes","text":"<pre><code>all_federation_nodes()\n</code></pre> <p>Returns all the node ids in the federation</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef all_federation_nodes(self) -&gt; List[str]:\n    \"\"\"Returns all the node ids in the federation\"\"\"\n    return list(self._fds.data().keys()) if self._fds is not None else []\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.breakpoint","title":"breakpoint","text":"<pre><code>breakpoint(state, bkpt_number)\n</code></pre> <p>Saves breakpoint with the state of the workflow.</p> <p>The following attributes will be saved:</p> <ul> <li>tags</li> <li>experimentation_folder</li> <li>training_data</li> <li>training_args</li> <li>secagg</li> <li>node_state</li> </ul> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>experiment not fully defined, experiment did not run any round yet, or error when saving breakpoint</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef breakpoint(self,\n               state: Dict,\n               bkpt_number: int) -&gt; None:\n    \"\"\"\n    Saves breakpoint with the state of the workflow.\n\n    The following attributes will be saved:\n\n      - tags\n      - experimentation_folder\n      - training_data\n      - training_args\n      - secagg\n      - node_state\n\n    Raises:\n        FedbiomedExperimentError: experiment not fully defined, experiment did not run any round\n            yet, or error when saving breakpoint\n    \"\"\"\n    state.update({\n        'id': self._experiment_id,\n        'breakpoint_version': str(__breakpoints_version__),\n        'training_data': self._fds.data(),\n        'experimentation_folder': self._experimentation_folder,\n        'tags': self._tags,\n        'nodes': self._nodes_filter,\n        'secagg': self._secagg.save_state_breakpoint(),\n        'node_state': self._node_state_agent.save_state_breakpoint()\n    })\n\n    # save state into a json file\n    breakpoint_path, breakpoint_file_name = choose_bkpt_file(\n        self.config.vars[\"EXPERIMENTS_DIR\"],\n        self._experimentation_folder,\n        bkpt_number - 1\n    )\n    breakpoint_file_path = os.path.join(breakpoint_path, breakpoint_file_name)\n    try:\n        with open(breakpoint_file_path, 'w', encoding=\"UTF-8\") as bkpt:\n            json.dump(state, bkpt)\n        logger.info(f\"breakpoint number {bkpt_number - 1} saved at \" +\n                    os.path.dirname(breakpoint_file_path))\n    except (OSError, PermissionError, ValueError, TypeError, RecursionError) as e:\n        # - OSError: heuristic for catching open() and write() errors\n        # - see json.dump() documentation for documented errors for this call\n        msg = ErrorNumbers.FB413.value + f' - save failed with message {str(e)}'\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg) from e\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.experimentation_folder","title":"experimentation_folder","text":"<pre><code>experimentation_folder()\n</code></pre> <p>Retrieves the folder name where experiment data/result are saved.</p> <p>Please see also <code>set_experimentation_folder</code></p> <p>Returns:</p> Type Description <code>str</code> <p>File name where experiment related files are saved</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef experimentation_folder(self) -&gt; str:\n    \"\"\"Retrieves the folder name where experiment data/result are saved.\n\n    Please see also [`set_experimentation_folder`]\n    [fedbiomed.researcher.federated_workflows.FederatedWorkflow.set_experimentation_folder]\n\n    Returns:\n        File name where experiment related files are saved\n    \"\"\"\n\n    return self._experimentation_folder\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.experimentation_path","title":"experimentation_path","text":"<pre><code>experimentation_path()\n</code></pre> <p>Retrieves the file path where experimentation folder is located and experiment related files are saved.</p> <p>Returns:</p> Type Description <code>str</code> <p>Experiment directory where all experiment related files are saved</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef experimentation_path(self) -&gt; str:\n    \"\"\"Retrieves the file path where experimentation folder is located and experiment related files are saved.\n\n    Returns:\n        Experiment directory where all experiment related files are saved\n    \"\"\"\n\n    return os.path.join(config.vars['EXPERIMENTS_DIR'], self._experimentation_folder)\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.filtered_federation_nodes","title":"filtered_federation_nodes","text":"<pre><code>filtered_federation_nodes()\n</code></pre> <p>Returns the node ids in the federation after filtering with the nodes filter</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef filtered_federation_nodes(self) -&gt; List[str]:\n    \"\"\"Returns the node ids in the federation after filtering with the nodes filter\"\"\"\n    if self._nodes_filter is not None:\n        return [node for node in self.all_federation_nodes() if node in self._nodes_filter]\n    else:\n        return self.all_federation_nodes()\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.info","title":"info","text":"<pre><code>info(info=None, missing='')\n</code></pre> <p>Prints out the information about the current status of the experiment.</p> <p>Lists  all the parameters/arguments of the experiment and informs whether the experiment can be run.</p> <p>Parameters:</p> Name Type Description Default <code>info</code> <code>Dict[str, List[str]]</code> <p>Dictionary of sub-classes relevant attributes status that will be completed with some additional attributes status defined in this class. Defaults to None (no entries of sub-classes available or of importance).</p> <code>None</code> <code>missing_object_to_check</code> <p>dictionary mapping sub-classes attributes to attribute names, that may be needed to fully run the object. Defaults to None (no check will be performed).</p> required <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>dictionary containing all pieces of information, with 2 entries: <code>Arguments</code> mapping a list</p> <code>str</code> <p>of all argument, and <code>Values</code> mapping a list containing all the values.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef info(self,\n         info: Dict[str, List[str]] = None,\n         missing: str = '') -&gt; Tuple[Dict[str, List[str]], str]:\n    \"\"\"Prints out the information about the current status of the experiment.\n\n    Lists  all the parameters/arguments of the experiment and informs whether the experiment can be run.\n\n    Args:\n        info: Dictionary of sub-classes relevant attributes status that will be completed with some additional\n            attributes status defined in this class. Defaults to None (no entries of sub-classes available or\n            of importance).\n        missing_object_to_check: dictionary mapping sub-classes attributes to attribute names, that may be\n            needed to fully run the object. Defaults to None (no check will be performed).\n\n    Returns:\n        dictionary containing all pieces of information, with 2 entries: `Arguments` mapping a list\n        of all argument, and `Values` mapping a list containing all the values.\n    \"\"\"\n    if info is None:\n        info = self._create_default_info_structure()\n    info['Arguments'].extend([\n        'Tags',\n        'Nodes filter',\n        'Training Data',\n        'Experiment folder',\n        'Experiment Path',\n        'Secure Aggregation'\n    ])\n\n    info['Values'].extend(['\\n'.join(findall('.{1,60}', str(e))) for e in [\n        self._tags,\n        self._nodes_filter,\n        self._fds,\n        self._experimentation_folder,\n        self.experimentation_path(),\n        f'- Using: {self._secagg}\\n- Active: {self._secagg.active}'\n    ]])\n\n    # printing list of items set / not set yet\n    print(tabulate.tabulate(info, headers='keys'))\n\n    if missing:\n        print(\"\\nWarning: Object not fully defined, missing\"\n              f\": \\n{missing}\")\n    else:\n        print(f\"{self.__class__.__name__} can be run now (fully defined)\")\n    return info, missing\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.load_breakpoint","title":"load_breakpoint  <code>classmethod</code>","text":"<pre><code>load_breakpoint(breakpoint_folder_path=None)\n</code></pre> <p>Loads breakpoint (provided a breakpoint has been saved) so the workflow can be resumed.</p> <p>Parameters:</p> Name Type Description Default <code>breakpoint_folder_path</code> <code>Optional[str]</code> <p>path of the breakpoint folder. Path can be absolute or relative eg: \"var/experiments/Experiment_xxxx/breakpoints_xxxx\". If None, loads the latest breakpoint of the latest workflow. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[TFederatedWorkflow, dict]</code> <p>Tuple contaning reinitialized workflow object and the saved state as a dictionary</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad argument type, error when reading breakpoint or bad loaded breakpoint content (corrupted)</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@classmethod\n@exp_exceptions\ndef load_breakpoint(\n    cls,\n    breakpoint_folder_path: Optional[str] = None\n) -&gt; Tuple[TFederatedWorkflow, dict]:\n    \"\"\"\n    Loads breakpoint (provided a breakpoint has been saved)\n    so the workflow can be resumed.\n\n    Args:\n      breakpoint_folder_path: path of the breakpoint folder. Path can be absolute\n        or relative eg: \"var/experiments/Experiment_xxxx/breakpoints_xxxx\".\n        If None, loads the latest breakpoint of the latest workflow. Defaults to None.\n\n    Returns:\n        Tuple contaning reinitialized workflow object and the saved state as a dictionary\n\n    Raises:\n        FedbiomedExperimentError: bad argument type, error when reading breakpoint or\n            bad loaded breakpoint content (corrupted)\n    \"\"\"\n    # check parameters type\n    if not isinstance(breakpoint_folder_path, str) and breakpoint_folder_path is not None:\n        msg = (\n            f\"{ErrorNumbers.FB413.value}: load failed, `breakpoint_folder_path`\"\n            f\" has bad type {type(breakpoint_folder_path)}\"\n        )\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n\n    # get breakpoint folder path (if it is None) and state file\n    breakpoint_folder_path, state_file = find_breakpoint_path(\n        config.vars['EXPERIMENTS_DIR'],\n        breakpoint_folder_path\n    )\n    breakpoint_folder_path = os.path.abspath(breakpoint_folder_path)\n\n    try:\n        path = os.path.join(breakpoint_folder_path, state_file)\n        with open(path, \"r\", encoding=\"utf-8\") as file:\n            saved_state = json.load(file)\n    except (json.JSONDecodeError, OSError) as exc:\n        # OSError: heuristic for catching file access issues\n        msg = (\n            f\"{ErrorNumbers.FB413.value}: load failed,\"\n            f\" reading breakpoint file failed with message {exc}\"\n        )\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg) from exc\n    if not isinstance(saved_state, dict):\n        msg = (\n            f\"{ErrorNumbers.FB413.value}: load failed, breakpoint file seems\"\n            f\" corrupted. Type should be `dict` not {type(saved_state)}\"\n        )\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n\n    # First, check version of breakpoints\n    bkpt_version = saved_state.get('breakpoint_version', __default_version__)\n    raise_for_version_compatibility(bkpt_version, __breakpoints_version__,\n                                    f\"{ErrorNumbers.FB413.value}: Breakpoint \"\n                                    \"file was generated with version %s \"\n                                    f\"which is incompatible with the current version %s.\")\n\n    # retrieve breakpoint training data\n    bkpt_fds = saved_state.get('training_data')\n    bkpt_fds = FederatedDataSet(bkpt_fds)\n\n    # initializing experiment\n    loaded_exp = cls()\n    loaded_exp._experiment_id = saved_state.get('id')\n    loaded_exp.set_training_data(bkpt_fds)\n    loaded_exp._tags = saved_state.get('tags')\n    loaded_exp.set_nodes(saved_state.get('nodes'))\n    loaded_exp.set_experimentation_folder(saved_state.get('experimentation_folder'))\n\n    secagg = SecureAggregation.load_state_breakpoint(saved_state.get('secagg'))\n    loaded_exp.set_secagg(secagg)\n    loaded_exp._node_state_agent.load_state_breakpoint(saved_state.get('node_state'))\n    loaded_exp.set_save_breakpoints(True)\n\n    return loaded_exp, saved_state\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.nodes","title":"nodes","text":"<pre><code>nodes()\n</code></pre> <p>Retrieves the nodes filter for the execution of the workflow.</p> <p>If nodes is None, then no filtering is applied, and all the nodes in the federation participate in the execution of the workflow. If nodes is not None, then the semantics of the nodes filter are as follows:</p> node_id in nodes filter node_id in training data outcome yes yes this node is part of the federation, and will take part in the execution the workflow yes no ignored no yes this node is part of the federation but will not be considered for executing the workflow no no ignored <p>Please see <code>set_nodes</code> to set <code>nodes</code>.</p> <p>Returns:</p> Type Description <code>Union[List[str], None]</code> <p>The list of nodes to keep for workflow execution, or None if no filtering is applied</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef nodes(self) -&gt; Union[List[str], None]:\n    \"\"\"Retrieves the nodes filter for the execution of the workflow.\n\n    If nodes is None, then no filtering is applied, and all the nodes in the federation participate in the\n    execution of the workflow.\n    If nodes is not None, then the semantics of the nodes filter are as follows:\n\n    | node_id in nodes filter | node_id in training data | outcome |\n    | --- | --- | --- |\n    | yes | yes | this node is part of the federation, and will take part in the execution the workflow |\n    | yes | no | ignored |\n    | no | yes | this node is part of the federation but will not be considered for executing the workflow |\n    | no | no | ignored |\n\n    Please see [`set_nodes`][fedbiomed.researcher.federated_workflows.FederatedWorkflow.set_nodes] to set `nodes`.\n\n    Returns:\n        The list of nodes to keep for workflow execution, or None if no filtering is applied\n    \"\"\"\n    return self._nodes_filter\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run()\n</code></pre> <p>Run the experiment</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@abstractmethod\ndef run(self) -&gt; int:\n    \"\"\"Run the experiment\"\"\"\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.save_breakpoints","title":"save_breakpoints","text":"<pre><code>save_breakpoints()\n</code></pre> <p>Retrieves the status of saving breakpoint after each round of training.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code>, If saving breakpoint is active. <code>False</code>, vice versa.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef save_breakpoints(self) -&gt; bool:\n    \"\"\"Retrieves the status of saving breakpoint after each round of training.\n\n    Returns:\n        `True`, If saving breakpoint is active. `False`, vice versa.\n    \"\"\"\n\n    return self._save_breakpoints\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.secagg_setup","title":"secagg_setup","text":"<pre><code>secagg_setup(sampled_nodes)\n</code></pre> <p>Retrieves the secagg arguments for setup.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>def secagg_setup(self, sampled_nodes: List[str]) -&gt; Dict:\n    \"\"\"Retrieves the secagg arguments for setup.\"\"\"\n    secagg_arguments = {}\n    if self._secagg.active:  # type: ignore\n        if not self._secagg.setup(  # type: ignore\n            parties=sampled_nodes,\n            experiment_id=self._experiment_id,\n            researcher_id=self._researcher_id,\n            insecure_validation=self.config.getbool('security', 'secagg_insecure_validation')\n        ):\n            raise FedbiomedSecureAggregationError(\n                f\"{ErrorNumbers.FB417.value}: Could not setup secure aggregation crypto \"\n                \"context.\"\n            )\n        secagg_arguments = self._secagg.train_arguments()  # type: ignore\n    return secagg_arguments\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.set_experimentation_folder","title":"set_experimentation_folder","text":"<pre><code>set_experimentation_folder(experimentation_folder=None)\n</code></pre> <p>Sets <code>experimentation_folder</code>, the folder name where experiment data/result are saved.</p> <p>Parameters:</p> Name Type Description Default <code>experimentation_folder</code> <code>Optional[str]</code> <p>File name where experiment related files are saved</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to experimentation folder.</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad <code>experimentation_folder</code> type</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef set_experimentation_folder(self, experimentation_folder: Optional[str] = None) -&gt; str:\n    \"\"\"Sets `experimentation_folder`, the folder name where experiment data/result are saved.\n\n    Args:\n        experimentation_folder: File name where experiment related files are saved\n\n    Returns:\n        The path to experimentation folder.\n\n    Raises:\n        FedbiomedExperimentError : bad `experimentation_folder` type\n    \"\"\"\n    if experimentation_folder is None:\n        self._experimentation_folder = create_exp_folder(\n            self.config.vars[\"EXPERIMENTS_DIR\"]\n        )\n    elif isinstance(experimentation_folder, str):\n        sanitized_folder = sanitize_filename(experimentation_folder, platform='auto')\n        self._experimentation_folder = create_exp_folder(\n            self.config.vars[\"EXPERIMENTS_DIR\"], sanitized_folder\n        )\n        if sanitized_folder != experimentation_folder:\n            logger.warning(f'`experimentation_folder` was sanitized from '\n                           f'{experimentation_folder} to {sanitized_folder}')\n    else:\n        msg = ErrorNumbers.FB410.value + \\\n            f' `experimentation_folder` : {type(experimentation_folder)}'\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n\n        # at this point self._experimentation_folder is a str valid for a foldername\n\n    return self._experimentation_folder\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.set_nodes","title":"set_nodes","text":"<pre><code>set_nodes(nodes)\n</code></pre> <p>Sets the nodes filter + verifications on argument type</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Union[List[str], None]</code> <p>List of node_ids to filter the nodes to be involved in the experiment.</p> required <p>Returns:</p> Type Description <code>Union[List[str], None]</code> <p>List of nodes that are set. None, if the argument <code>nodes</code> is None.</p> <p>Raises:</p> Type Description <code>FedbiomedTypeError</code> <p>Bad nodes type</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef set_nodes(self, nodes: Union[List[str], None]) -&gt; Union[List[str], None]:\n    \"\"\"Sets the nodes filter + verifications on argument type\n\n    Args:\n        nodes: List of node_ids to filter the nodes to be involved in the experiment.\n\n    Returns:\n        List of nodes that are set. None, if the argument `nodes` is None.\n\n    Raises:\n        FedbiomedTypeError : Bad nodes type\n    \"\"\"\n    # immediately exit if setting nodes to None\n    if nodes is None:\n        self._nodes_filter = None\n    # set nodes\n    elif isinstance(nodes, list):\n        if not all(map(lambda node: isinstance(node, str), nodes)):\n            msg = ErrorNumbers.FB410.value + ' `nodes` argument must be a list of strings or None.'\n            logger.critical(msg)\n            raise FedbiomedTypeError(msg)\n        self._nodes_filter = nodes\n    else:\n        msg = ErrorNumbers.FB410.value + ' `nodes` argument must be a list of strings or None.'\n        logger.critical(msg)\n        raise FedbiomedTypeError(msg)\n    return self._nodes_filter\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.set_save_breakpoints","title":"set_save_breakpoints","text":"<pre><code>set_save_breakpoints(save_breakpoints)\n</code></pre> <p>Setter for save_breakpoints + verification on arguments type</p> <p>Parameters:</p> Name Type Description Default <code>save_breakpoints</code> <code>bool</code> <p>whether to save breakpoints or not after each training round. Breakpoints can be used for resuming a crashed experiment.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Status of saving breakpoints</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad save_breakpoints type</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef set_save_breakpoints(self, save_breakpoints: bool) -&gt; bool:\n    \"\"\" Setter for save_breakpoints + verification on arguments type\n\n    Args:\n        save_breakpoints (bool): whether to save breakpoints or\n            not after each training round. Breakpoints can be used for resuming\n            a crashed experiment.\n\n    Returns:\n        Status of saving breakpoints\n\n    Raises:\n        FedbiomedExperimentError: bad save_breakpoints type\n    \"\"\"\n    if isinstance(save_breakpoints, bool):\n        self._save_breakpoints = save_breakpoints\n        # no warning if done during experiment, we may change breakpoint policy at any time\n    else:\n        msg = ErrorNumbers.FB410.value + f' `save_breakpoints` : {type(save_breakpoints)}'\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n\n    return self._save_breakpoints\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.set_secagg","title":"set_secagg","text":"<pre><code>set_secagg(secagg, scheme=SecureAggregationSchemes.LOM)\n</code></pre> <p>Sets secure aggregation status and scheme</p> <p>Build secure aggregation controller/instance or sets given secure aggregation class</p> <p>Parameters:</p> Name Type Description Default <code>secagg</code> <code>Union[bool, SecureAggregation]</code> <p>If True activates training request with secure aggregation by building <code>SecureAggregation</code> class with default arguments. Or if argument is an instance of <code>SecureAggregation</code> it does only assignment. Secure aggregation activation and configuration depends on the instance provided.</p> required <code>scheme</code> <code>SecureAggregationSchemes</code> <p>Secure aggregation scheme to use. Ig a <code>SecureAggregation</code> object is provided, the argument is not used, as the scheme comes from the object. Defaults is SecureAggregationSchemes.LOM.</p> <code>LOM</code> <p>Returns:</p> Type Description <p>Secure aggregation controller instance.</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad argument type or value</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef set_secagg(\n        self,\n        secagg: Union[bool, SecureAggregation],\n        scheme: SecureAggregationSchemes = SecureAggregationSchemes.LOM):\n    \"\"\"Sets secure aggregation status and scheme\n\n    Build secure aggregation controller/instance or sets given\n    secure aggregation class\n\n    Args:\n        secagg: If True activates training request with secure aggregation by building\n            [`SecureAggregation`][fedbiomed.researcher.secagg.SecureAggregation] class\n            with default arguments. Or if argument is an instance of `SecureAggregation`\n            it does only assignment. Secure aggregation activation and configuration\n            depends on the instance provided.\n        scheme: Secure aggregation scheme to use. Ig a `SecureAggregation` object is provided,\n            the argument is not used, as the scheme comes from the object. Defaults is\n            SecureAggregationSchemes.LOM.\n\n    Returns:\n        Secure aggregation controller instance.\n\n    Raises:\n        FedbiomedExperimentError: bad argument type or value\n    \"\"\"\n    if not isinstance(scheme, SecureAggregationSchemes):\n        raise FedbiomedExperimentError(\n            f\"{ErrorNumbers.FB410.value}: Expected `scheme` argument \"\n            \"`SecureAggregationSchemes`, but got {type(scheme)}\")\n\n    if isinstance(secagg, bool):\n        self._secagg = SecureAggregation(\n            scheme=scheme, active=secagg\n        )\n    elif isinstance(secagg, SecureAggregation):\n        self._secagg = secagg\n    else:\n        raise FedbiomedExperimentError(\n            f\"{ErrorNumbers.FB410.value}: Expected `secagg` argument bool or \"\n            f\"`SecureAggregation` but got {type(secagg)}\")\n\n    return self._secagg\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.set_tags","title":"set_tags","text":"<pre><code>set_tags(tags)\n</code></pre> <p>Sets tags and verification on argument type</p> <p>Setting tags updates also training data by executing [<code>set_training_data</code>].[fedbiomed.researcher.federated_workflows.FederatedWorkflow.set_training_data] method.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Union[List[str], str]</code> <p>List of string with data tags or string with one data tag. Empty list of tags ([]) means any dataset is accepted, it is different from None (tags not set, cannot search for training_data yet).</p> required <p>Returns:     List of tags that are set.</p> <p>Raises:</p> Type Description <code>FedbiomedTypeError</code> <p>Bad tags type</p> <code>FedbiomedValueError</code> <p>Some issue prevented resetting the training data after an inconsistency was detected</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef set_tags(\n    self,\n    tags: Union[List[str], str],\n) -&gt; List[str]:\n    \"\"\"Sets tags and verification on argument type\n\n    Setting tags updates also training data by executing\n    [`set_training_data`].[fedbiomed.researcher.federated_workflows.FederatedWorkflow.set_training_data]\n    method.\n\n    Args:\n        tags: List of string with data tags or string with one data tag. Empty list\n            of tags ([]) means any dataset is accepted, it is different from None\n            (tags not set, cannot search for training_data yet).\n    Returns:\n        List of tags that are set.\n\n    Raises:\n        FedbiomedTypeError: Bad tags type\n        FedbiomedValueError: Some issue prevented resetting the training\n            data after an inconsistency was detected\n    \"\"\"\n    # preprocess the tags argument to correct typing\n    if not tags:\n        msg = f\"{ErrorNumbers.FB410.value}: Invalid value for tags argument {tags}, tags \" \\\n            \"should be non-empty list of str or non-empty str.\"\n        logger.critical(msg)\n        raise FedbiomedValueError(msg)\n\n    if isinstance(tags, list):\n        if not all(map(lambda tag: isinstance(tag, str), tags)):\n            msg = f\"{ErrorNumbers.FB410.value}: `tags` must be a non-empty str or \" \\\n                \"a non-empty list of str.\"\n            logger.critical(msg)\n            raise FedbiomedTypeError(msg)\n\n        # If it is empty list\n        tags_to_set = tags\n\n    elif isinstance(tags, str):\n        tags_to_set = [tags]\n    else:\n        msg = f\"{ErrorNumbers.FB410.value} `tags` must be a non-empty str, \" \\\n            \"a non-empty list of str\"\n        logger.critical(msg)\n        raise FedbiomedTypeError(msg)\n\n    self._tags = tags_to_set\n\n    # Set training data\n    logger.info(\n        \"Updating training data. This action will update FederatedDataset, \"\n        \"and the nodes that will participate to the experiment.\")\n\n    self.set_training_data(None, from_tags=True)\n\n    return self._tags\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.set_training_data","title":"set_training_data","text":"<pre><code>set_training_data(training_data, from_tags=False)\n</code></pre> <p>Sets training data for federated training + verification on arguments type</p> <p>The full expected behaviour when changing training data is given in the table below:</p> New value of <code>training_data</code> <code>from_tags</code> Outcome dict or FederatedDataset True fail because user is attempting to set from tags but also providing a training_data argument dict or FederatedDataset False set fds attribute, set tags to None None True fail if tags are not set, else set fds attribute based tags None False set tags to None and keep same value and tags <p>Setting to None forfeits consistency checks</p> <p>Setting training_data to None does not trigger consistency checks, and may therefore leave the class in an inconsistent state.</p> <p>Parameters:</p> Name Type Description Default <code>training_data</code> <code>Union[FederatedDataSet, dict, None]</code> <ul> <li>If it is a FederatedDataSet object, use this value as training_data.</li> <li>else if it is a dict, create and use a FederatedDataSet object from the dict   and use this value as training_data. The dict should use node ids as keys,   values being list of dicts (each dict representing a dataset on a node).</li> <li>else if it is None (no training data provided)</li> <li>if <code>from_tags</code> is True and <code>tags</code> is not None, set training_data by     searching for datasets with a query to the nodes using <code>tags</code> and <code>nodes</code></li> <li>if <code>from_tags</code> is False or <code>tags</code> is None, set training_data to None (no training_data set yet,     experiment is not fully initialized and cannot be launched)</li> </ul> required <code>from_tags</code> <code>bool</code> <p>If True, query nodes for datasets when no <code>training_data</code> is provided. Not used when <code>training_data</code> is provided.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[FederatedDataSet, None]</code> <p>FederatedDataSet metadata</p> <p>Raises:</p> Type Description <code>FedbiomedTypeError</code> <p>bad training_data or from_tags type.</p> <code>FedbiomedValueError</code> <p>Invalid value for the arguments  <code>training_data</code> or <code>from_tags</code>.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef set_training_data(\n        self,\n        training_data: Union[FederatedDataSet, dict, None],\n        from_tags: bool = False) -&gt; \\\n        Union[FederatedDataSet, None]:\n    \"\"\"Sets training data for federated training + verification on arguments type\n\n\n    The full expected behaviour when changing training data is given in the table below:\n\n    | New value of `training_data` | `from_tags` | Outcome |\n    | --- | --- | --- |\n    | dict or FederatedDataset | True  | fail because user is attempting to set from tags but also providing a training_data argument|\n    | dict or FederatedDataset | False | set fds attribute, set tags to None |\n    | None | True | fail if tags are not set, else set fds attribute based tags |\n    | None | False | set tags to None and keep same value and tags |\n\n    !!! warning \"Setting to None forfeits consistency checks\"\n        Setting training_data to None does not trigger consistency checks, and may therefore leave the class in an\n        inconsistent state.\n\n    Args:\n        training_data:\n            * If it is a FederatedDataSet object, use this value as training_data.\n            * else if it is a dict, create and use a FederatedDataSet object from the dict\n              and use this value as training_data. The dict should use node ids as keys,\n              values being list of dicts (each dict representing a dataset on a node).\n            * else if it is None (no training data provided)\n              - if `from_tags` is True and `tags` is not None, set training_data by\n                searching for datasets with a query to the nodes using `tags` and `nodes`\n              - if `from_tags` is False or `tags` is None, set training_data to None (no training_data set yet,\n                experiment is not fully initialized and cannot be launched)\n        from_tags: If True, query nodes for datasets when no `training_data` is provided.\n            Not used when `training_data` is provided.\n\n    Returns:\n        FederatedDataSet metadata\n\n    Raises:\n        FedbiomedTypeError: bad training_data or from_tags type.\n        FedbiomedValueError: Invalid value for the arguments  `training_data` or `from_tags`.\n    \"\"\"\n\n    if not isinstance(from_tags, bool):\n        msg = ErrorNumbers.FB410.value + \\\n            f' `from_tags` : got {type(from_tags)} but expected a boolean'\n        logger.critical(msg)\n        raise FedbiomedTypeError(msg)\n    if from_tags and training_data is not None:\n        msg = ErrorNumbers.FB410.value + \\\n            ' set_training_data: cannot specify a training_data argument if ' \\\n            'from_tags is True'\n        logger.critical(msg)\n        raise FedbiomedValueError(msg)\n\n    # case where no training data are passed\n    if training_data is None:\n        if from_tags is True:\n            if not self._tags:\n                msg = f\"{ErrorNumbers.FB410.value}: attempting to \" \\\n                    \"set training data from undefined tags. Please consider set tags before \" \\\n                    \"using set_tags method of the experiment.\"\n                logger.critical(msg)\n                raise FedbiomedValueError(msg)\n            training_data = self._reqs.search(self._tags, self._nodes_filter)\n        else:\n            msg = f\"{ErrorNumbers.FB410.value}: Can not set training data to `None`. \" \\\n                \"Please set from_tags=True or provide a valid training data\"\n            logger.critical(msg)\n            raise FedbiomedValueError(msg)\n\n    if isinstance(training_data, FederatedDataSet):\n        self._fds = training_data\n    elif isinstance(training_data, dict):\n        self._fds = FederatedDataSet(training_data)\n    else:\n        msg = ErrorNumbers.FB410.value + \\\n            f' `training_data` has incorrect type: {type(training_data)}'\n        logger.critical(msg)\n        raise FedbiomedTypeError(msg)\n\n    # check and ensure consistency\n    self._tags = self._tags if from_tags else None\n\n    # return the new value\n    return self._fds\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.tags","title":"tags","text":"<pre><code>tags()\n</code></pre> <p>Retrieves the tags from the experiment object.</p> <p>Please see <code>set_tags</code> to set tags.</p> <p>Returns:</p> Type Description <code>Union[List[str], None]</code> <p>List of tags that has been set. <code>None</code> if it isn't declare yet.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef tags(self) -&gt; Union[List[str], None]:\n    \"\"\"Retrieves the tags from the experiment object.\n\n    Please see [`set_tags`][fedbiomed.researcher.federated_workflows.FederatedWorkflow.set_tags] to set tags.\n\n    Returns:\n        List of tags that has been set. `None` if it isn't declare yet.\n    \"\"\"\n    return self._tags\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.FederatedWorkflow.training_data","title":"training_data","text":"<pre><code>training_data()\n</code></pre> <p>Retrieves the training data which is an instance of <code>FederatedDataset</code></p> <p>This represents the dataset metadata available for the full federation.</p> <p>Please see <code>set_training_data</code> to set or update training data.</p> <p>Returns:</p> Type Description <code>Union[FederatedDataSet, None]</code> <p>Object that contains metadata for the datasets of each node. <code>None</code> if it isn't set yet.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_federated_workflow.py</code> <pre><code>@exp_exceptions\ndef training_data(self) -&gt; Union[FederatedDataSet, None]:\n    \"\"\"Retrieves the training data which is an instance of\n    [`FederatedDataset`][fedbiomed.researcher.datasets.FederatedDataSet]\n\n    This represents the dataset metadata available for the full federation.\n\n    Please see [`set_training_data`][fedbiomed.researcher.federated_workflows.FederatedWorkflow.set_training_data]\n    to set or update training data.\n\n    Returns:\n        Object that contains metadata for the datasets of each node. `None` if it isn't set yet.\n    \"\"\"\n    return self._fds\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow","title":"TrainingPlanWorkflow","text":"<pre><code>TrainingPlanWorkflow(*args, training_plan_class=None, training_args=None, model_args=None, **kwargs)\n</code></pre> <p>               Bases: <code>FederatedWorkflow</code>, <code>ABC</code></p> <p>A <code>TrainingPlanWorkflow</code> is an abstract entry point to orchestrate an experiment which uses a training plan.</p> <p>In addition to the functionalities provided by <code>FederatedWorkflow</code>, the <code>TrainingPlanWorkflow</code> also manages the life-cycle of the training plan.</p> <p>Use <code>set_training_plan_class</code> to manage the training plan</p> <p>Please only ever use the <code>set_training_plan_class</code> function to manage the training plan. Do not set the training plan or training plan class directly!</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_class</code> <code>Optional[TrainingPlanT]</code> <p>training plan class to be used for training. For experiment to be properly and fully defined <code>training_plan_class</code> needs to be a <code>TrainingPlanT</code> Defaults to None (no training plan class defined yet.</p> <code>None</code> <code>model_args</code> <code>Optional[Dict]</code> <p>contains model arguments passed to the constructor of the training plan when instantiating it : output and input feature dimension, etc.</p> <code>None</code> <code>training_args</code> <code>Optional[Union[TrainingArgs, dict]]</code> <p>contains training arguments passed to the <code>training_routine</code> of the training plan when launching it: lr, epochs, batch_size...</p> <code>None</code> <code>*args</code> <p>Extra positional arguments from parent class <code>FederatedWorkflow</code></p> <code>()</code> <code>**kwargs</code> <p>Arguments of parent class <code>FederatedWorkflow</code></p> <code>{}</code> Source code in <code>fedbiomed/researcher/federated_workflows/_training_plan_workflow.py</code> <pre><code>@exp_exceptions\ndef __init__(\n    self,\n    *args,\n    training_plan_class: Optional[TrainingPlanT] = None,\n    training_args: Optional[Union[TrainingArgs, dict]] = None,\n    model_args: Optional[Dict] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Constructor of the class.\n\n    Args:\n        training_plan_class: training plan class to be used for training.\n            For experiment to be properly and fully defined `training_plan_class`\n            needs to be a `TrainingPlanT` Defaults to None (no training plan class\n            defined yet.\n        model_args: contains model arguments passed to the constructor\n            of the training plan when instantiating it :\n            output and input feature dimension, etc.\n        training_args: contains training arguments passed to the `training_routine`\n            of the training plan when launching it: lr, epochs, batch_size...\n        *args: Extra positional arguments from parent class\n            [`FederatedWorkflow`][fedbiomed.researcher.federated_workflows.FederatedWorkflow]\n        **kwargs: Arguments of parent class\n            [`FederatedWorkflow`][fedbiomed.researcher.federated_workflows.FederatedWorkflow]\n    \"\"\"\n    # Check arguments\n    if training_plan_class is not None and not inspect.isclass(training_plan_class):\n        raise FedbiomedTypeError(\n            f\"{ErrorNumbers.FB410.value}: bad type for argument \"\n            f\"`training_plan_class` {type(training_plan_class)}\")\n\n    if training_plan_class is not None and \\\n            not issubclass(training_plan_class, TRAINING_PLAN_TYPES):\n\n        raise FedbiomedTypeError(\n            f\"{ErrorNumbers.FB410.value}: bad type for argument `training_plan_class`.\"\n            f\" It is not subclass of supported training plans {TRAINING_PLAN_TYPES}\")\n\n    # _training_plan_class determines the life-cycle of the training plan:\n    # if training_plass_class changes, then the training plan must be reinitialized\n    self._training_plan_class = None\n    # model args is also tied to the life-cycle of training plan:\n    # if model_args changes, the training plan must be reinitialized\n    self._model_args = None\n    # The _training_plan attribute represents the *actual instance*\n    # of a _training_plan_class that is currently\n    # being used in the workflow. The training plan cannot be modified by the user.\n    self._training_plan = None\n    self._training_args: Optional[TrainingArgs] = None  # FIXME: is it ok to have this here?\n    # The _training_plan_file attribute represents the path of the file where the training plan is saved.\n    # It cannot be modified by the user\n    self._training_plan_file = None\n\n    # initialize object\n    super().__init__(*args, **kwargs)\n\n    self.set_training_args(training_args)\n    self.set_model_args(model_args)\n    self.set_training_plan_class(training_plan_class)\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow-functions","title":"Functions","text":""},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.breakpoint","title":"breakpoint","text":"<pre><code>breakpoint(state, bkpt_number)\n</code></pre> <p>Saves breakpoint with the state of the workflow.</p> <p>The following attributes will be saved:</p> <ul> <li>training_args</li> <li>training_plan_class</li> <li>model_args</li> </ul> Source code in <code>fedbiomed/researcher/federated_workflows/_training_plan_workflow.py</code> <pre><code>@exp_exceptions\ndef breakpoint(self,\n               state,\n               bkpt_number) -&gt; None:\n    \"\"\"\n    Saves breakpoint with the state of the workflow.\n\n    The following attributes will be saved:\n\n      - training_args\n      - training_plan_class\n      - model_args\n    \"\"\"\n    # save training plan to file\n    training_plan_module = 'model_' + str(uuid.uuid4())\n    training_plan_file = os.path.join(self.experimentation_path(), training_plan_module + '.py')\n    self.training_plan().save_code(training_plan_file)\n\n    state.update({\n        'model_args': self._model_args,\n        'training_plan_class_name': self._training_plan_class.__name__,\n        'training_args': self._training_args.get_state_breakpoint(),\n    })\n\n    breakpoint_path, breakpoint_file_name = \\\n        choose_bkpt_file(\n            self.config.vars[\"EXPERIMENTS_DIR\"],\n            self._experimentation_folder,\n            bkpt_number - 1\n        )\n\n    # rewrite paths in breakpoint : use the links in breakpoint directory\n    state['training_plan_path'] = create_unique_link(\n        breakpoint_path,\n        # - Need a file with a restricted characters set in name to be able to import as module\n        'model_' + str(\"{:04d}\".format(bkpt_number - 1)), '.py',\n        # - Prefer relative path, eg for using experiment result after\n        # experiment in a different tree\n        os.path.join('..', os.path.basename(training_plan_file))\n    )\n    params_path = os.path.join(breakpoint_path, f\"model_params_{uuid.uuid4()}.mpk\")\n    Serializer.dump(self.training_plan().get_model_wrapper_class().get_weights(\n        only_trainable = False, exclude_buffers = False), params_path)\n    state['model_weights_path'] = params_path\n\n    super().breakpoint(state, bkpt_number)\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.check_training_plan_status","title":"check_training_plan_status","text":"<pre><code>check_training_plan_status()\n</code></pre> <p>Method for checking training plan status, ie whether it is approved or not by the nodes</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>if the training data is not defined.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Training plan status for answering nodes</p> Source code in <code>fedbiomed/researcher/federated_workflows/_training_plan_workflow.py</code> <pre><code>@exp_exceptions\ndef check_training_plan_status(self) -&gt; Dict:\n    \"\"\" Method for checking training plan status, ie whether it is approved or not by the nodes\n\n    Raises:\n        FedbiomedExperimentError: if the training data is not defined.\n\n    Returns:\n        Training plan status for answering nodes\n    \"\"\"\n    if self.training_data() is None:\n        msg = f\"{ErrorNumbers.FB410.value}. Cannot check training plan status: training data is not defined.\" \\\n              f\"Please either use the `set_tags` or `set_training_data` method to fix this.\"\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n\n    job = TrainingPlanCheckJob(\n        researcher_id=self._researcher_id,\n        requests=self._reqs,\n        nodes=self.training_data().node_ids(),\n        keep_files_dir=self.experimentation_path(),\n        experiment_id=self._experiment_id,\n        training_plan=self.training_plan()\n    )\n    responses = job.execute()\n    return responses\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.info","title":"info","text":"<pre><code>info(info=None, missing='')\n</code></pre> <p>Prints out the information about the current status of the experiment.</p> <p>Lists  all the parameters/arguments of the experiment and informs whether the experiment can be run.</p> <p>Parameters:</p> Name Type Description Default <code>info</code> <code>Optional[Dict]</code> <p>Dictionary of sub-classes relevant attributes status that will be completed with some additional attributes status defined in this class. Defaults to None (no entries of sub-classes available or of importance).</p> <code>None</code> <code>missing_object_to_check</code> <p>dictionary mapping sub-classes attributes to attribute names, that may be needed to fully run the object. Defaults to None (no check will be performed).</p> required <p>Returns:</p> Type Description <code>Tuple[Dict[str, List[str]], str]</code> <p>dictionary containing all pieces of information, with 2 entries: <code>Arguments</code> mapping a list of all argument, and <code>Values</code> mapping a list copntaining all the values.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>if <code>Arguments</code> or <code>Values</code> entry is missing in passing argument <code>info</code></p> Source code in <code>fedbiomed/researcher/federated_workflows/_training_plan_workflow.py</code> <pre><code>@exp_exceptions\ndef info(\n    self,\n    info: Optional[Dict] = None,\n    missing: str = ''\n) -&gt; Tuple[Dict[str, List[str]], str]:\n    \"\"\"Prints out the information about the current status of the experiment.\n\n    Lists  all the parameters/arguments of the experiment and informs whether\n    the experiment can be run.\n\n    Args:\n        info: Dictionary of sub-classes relevant attributes status that will be\n            completed with some additional attributes status defined in this class.\n            Defaults to None (no entries of sub-classes available or of importance).\n        missing_object_to_check: dictionary mapping sub-classes attributes to\n            attribute names, that may be needed to fully run the object. Defaults\n            to None (no check will be performed).\n\n    Returns:\n        dictionary containing all pieces of information, with 2 entries:\n            `Arguments` mapping a list of all argument, and `Values` mapping\n            a list copntaining all the values.\n\n    Raises:\n        KeyError: if `Arguments` or `Values` entry is missing in passing argument `info`\n    \"\"\"\n    # at this point all attributes are initialized (in constructor)\n    if info is None:\n        info = self._create_default_info_structure()\n    info['Arguments'].extend([\n        'Training Plan Class',\n        'Model Arguments',\n        'Training Arguments'\n    ])\n    info['Values'].extend(['\\n'.join(findall('.{1,60}',\n                                     str(e))) for e in [\n        self._training_plan_class,\n        self._model_args,\n        self._training_args\n    ]])\n\n    return super().info(info, missing)\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.load_breakpoint","title":"load_breakpoint  <code>classmethod</code>","text":"<pre><code>load_breakpoint(breakpoint_folder_path=None)\n</code></pre> <p>Loads breakpoint (provided a breakpoint has been saved) so the workflow can be resumed.</p> <p>Parameters:</p> Name Type Description Default <code>breakpoint_folder_path</code> <code>Optional[str]</code> <p>path of the breakpoint folder. Path can be absolute or relative eg: \"var/experiments/Experiment_xxxx/breakpoints_xxxx\". If None, loads the latest breakpoint of the latest workflow. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[TrainingPlanWorkflowT, dict]</code> <p>Reinitialized workflow object.</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad argument type, error when reading breakpoint or bad loaded breakpoint content (corrupted)</p> Source code in <code>fedbiomed/researcher/federated_workflows/_training_plan_workflow.py</code> <pre><code>@classmethod\n@exp_exceptions\ndef load_breakpoint(cls,\n                    breakpoint_folder_path: Optional[str] = None) -&gt; Tuple[TrainingPlanWorkflowT, dict]:\n    \"\"\"\n    Loads breakpoint (provided a breakpoint has been saved)\n    so the workflow can be resumed.\n\n    Args:\n      breakpoint_folder_path: path of the breakpoint folder. Path can be absolute or relative eg:\n        \"var/experiments/Experiment_xxxx/breakpoints_xxxx\". If None, loads the latest breakpoint of the latest\n        workflow. Defaults to None.\n\n    Returns:\n        Reinitialized workflow object.\n\n    Raises:\n        FedbiomedExperimentError: bad argument type, error when reading breakpoint or bad loaded breakpoint\n            content (corrupted)\n    \"\"\"\n    loaded_exp, saved_state = super().load_breakpoint(breakpoint_folder_path)\n\n    # Define type for pylint\n    loaded_exp: TrainingPlanWorkflow\n\n    # Import TP class\n    _, tp_class = import_class_from_file(\n        module_path=saved_state.get(\"training_plan_path\"),\n        class_name=saved_state.get(\"training_plan_class_name\")\n    )\n\n    loaded_exp.set_model_args(saved_state[\"model_args\"])\n    loaded_exp.set_training_plan_class(tp_class)\n    loaded_exp.set_training_args(\n        TrainingArgs.load_state_breakpoint(\n            saved_state.get('training_args')))\n    training_plan = loaded_exp.training_plan()\n    if training_plan is None:\n        msg = ErrorNumbers.FB413.value + ' - load failed, ' + \\\n            'breakpoint file seems corrupted, `training_plan` is None'\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n    param_path = saved_state['model_weights_path']\n    params = Serializer.load(param_path)\n    loaded_exp.training_plan().get_model_wrapper_class().set_weights(params)\n\n    return loaded_exp, saved_state\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.model_args","title":"model_args","text":"<pre><code>model_args()\n</code></pre> <p>Retrieves model arguments.</p> <p>Please see also <code>set_model_args</code></p> <p>Returns:</p> Type Description <code>dict</code> <p>The arguments that are going to be passed to the <code>init_model</code> function of the training plan during</p> <code>dict</code> <p>initialization of the model instance</p> Source code in <code>fedbiomed/researcher/federated_workflows/_training_plan_workflow.py</code> <pre><code>@exp_exceptions\ndef model_args(self) -&gt; dict:\n    \"\"\"Retrieves model arguments.\n\n    Please see also [`set_model_args`][fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.set_model_args]\n\n    Returns:\n        The arguments that are going to be passed to the `init_model` function of the training plan during\n        initialization of the model instance\n    \"\"\"\n    return self._model_args\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.set_model_args","title":"set_model_args","text":"<pre><code>set_model_args(model_args, keep_weights=True)\n</code></pre> <p>Sets <code>model_args</code> + verification on arguments type</p> <p>Resets the training plan</p> <p>This function has an important (and intended!) side-effect: it resets the <code>training_plan</code> attribute. By default, it tries to keep the same weights as the current training plan, if available.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>dict</code> <p>contains model arguments passed to the constructor of the training plan when instantiating it : output and input feature dimension, etc.</p> required <code>keep_weights</code> <code>bool</code> <p>try to keep the same weights as the current training plan</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Model arguments that have been set.</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad model_args type</p> Source code in <code>fedbiomed/researcher/federated_workflows/_training_plan_workflow.py</code> <pre><code>@exp_exceptions\ndef set_model_args(self,\n                   model_args: dict,\n                   keep_weights: bool = True) -&gt; dict:\n    \"\"\"Sets `model_args` + verification on arguments type\n\n    !!! warning \"Resets the training plan\"\n        This function has an important (and intended!) side-effect: it resets the `training_plan` attribute.\n        By default, it tries to keep the same weights as the current training plan, if available.\n\n    Args:\n        model_args (dict): contains model arguments passed to the constructor\n            of the training plan when instantiating it : output and input feature\n            dimension, etc.\n        keep_weights: try to keep the same weights as the current training plan\n\n    Returns:\n        Model arguments that have been set.\n\n    Raises:\n        FedbiomedExperimentError : bad model_args type\n    \"\"\"\n    if model_args is None or isinstance(model_args, dict):\n        self._model_args = model_args\n    else:\n        # bad type\n        msg = ErrorNumbers.FB410.value + f' `model_args` : {type(model_args)}'\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n    # self._model_args always exist at this point\n\n    self._update_training_plan(keep_weights)  # resets the training plan attribute\n\n    return self._model_args\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.set_training_args","title":"set_training_args","text":"<pre><code>set_training_args(training_args)\n</code></pre> <p>Sets <code>training_args</code> + verification on arguments type</p> <p>Parameters:</p> Name Type Description Default <code>training_args</code> <code>Union[dict, TrainingArgs, None]</code> <p>contains training arguments passed to the training plan's <code>training_routine</code> such as lr, epochs, batch_size...</p> required <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>Training arguments</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad training_args type</p> Source code in <code>fedbiomed/researcher/federated_workflows/_training_plan_workflow.py</code> <pre><code>@exp_exceptions\ndef set_training_args(\n    self,\n    training_args: Union[dict, TrainingArgs, None]\n) -&gt; Union[dict, None]:\n    \"\"\" Sets `training_args` + verification on arguments type\n\n    Args:\n        training_args: contains training arguments passed to the\n            training plan's `training_routine` such as lr, epochs, batch_size...\n\n    Returns:\n        Training arguments\n\n    Raises:\n        FedbiomedExperimentError : bad training_args type\n    \"\"\"\n\n    if isinstance(training_args, TrainingArgs):\n        self._training_args = deepcopy(training_args)\n    elif isinstance(training_args, dict) or training_args is None:\n        self._training_args = TrainingArgs(training_args, only_required=False)\n    else:\n        msg = f\"{ErrorNumbers.FB410.value} in function `set_training_args`. \" \\\n              \"Expected type TrainingArgs, dict, or \" \\\n              f\"None, got {type(training_args)} instead.\"\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n\n    return self._training_args.dict()\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.set_training_plan_class","title":"set_training_plan_class","text":"<pre><code>set_training_plan_class(training_plan_class, keep_weights=True)\n</code></pre> <p>Sets  the training plan type + verification on arguments type</p> <p>Resets the training plan</p> <p>This function has an important (and intended!) side-effect: it resets the <code>training_plan</code> attribute. By default, it tries to keep the same weights as the current training plan, if available.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_class</code> <code>Union[TrainingPlanT, None]</code> <p>training plan class to be used for training. For experiment to be properly and fully defined <code>training_plan_class</code> needs to be a <code>TrainingPlanT</code> Defaults to None (no training plan class defined yet)</p> required <code>keep_weights</code> <code>bool</code> <p>try to keep the same weights as the current training plan</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[TrainingPlanT, None]</code> <p><code>training_plan_class</code> that is set for experiment</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad training_plan_class type</p> Source code in <code>fedbiomed/researcher/federated_workflows/_training_plan_workflow.py</code> <pre><code>@exp_exceptions\ndef set_training_plan_class(self,\n                            training_plan_class: Union[TrainingPlanT, None],\n                            keep_weights: bool = True\n                            ) -&gt; Union[TrainingPlanT, None]:\n    \"\"\"Sets  the training plan type + verification on arguments type\n\n    !!! warning \"Resets the training plan\"\n        This function has an important (and intended!) side-effect: it resets the `training_plan` attribute.\n        By default, it tries to keep the same weights as the current training plan, if available.\n\n    Args:\n        training_plan_class: training plan class to be used for training.\n            For experiment to be properly and fully defined `training_plan_class` needs to be a `TrainingPlanT`\n            Defaults to None (no training plan class defined yet)\n        keep_weights: try to keep the same weights as the current training plan\n\n    Returns:\n        `training_plan_class` that is set for experiment\n\n    Raises:\n        FedbiomedExperimentError : bad training_plan_class type\n    \"\"\"\n    if training_plan_class is None:\n        self._training_plan_class = None\n    elif inspect.isclass(training_plan_class):\n        # training_plan_class must be a subclass of a valid training plan\n        if issubclass(training_plan_class, TRAINING_PLAN_TYPES):\n            # valid class\n            self._training_plan_class = training_plan_class\n        else:\n            # bad class\n            msg = ErrorNumbers.FB410.value + f' `training_plan_class` : {training_plan_class} class'\n            logger.critical(msg)\n            raise FedbiomedExperimentError(msg)\n    else:\n        # bad type\n        msg = ErrorNumbers.FB410.value + f' `training_plan_class` of type: {type(training_plan_class)}'\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n\n    self._update_training_plan(keep_weights)  # resets the training plan attribute\n\n    return self._training_plan_class\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.training_args","title":"training_args","text":"<pre><code>training_args()\n</code></pre> <p>Retrieves training arguments.</p> <p>Please see also <code>set_training_args</code></p> <p>Returns:</p> Type Description <code>dict</code> <p>The arguments that are going to be passed to the training plan's <code>training_routine</code> to perfom training on the node side. An example training routine: <code>TorchTrainingPlan.training_routine</code></p> Source code in <code>fedbiomed/researcher/federated_workflows/_training_plan_workflow.py</code> <pre><code>@exp_exceptions\ndef training_args(self) -&gt; dict:\n    \"\"\"Retrieves training arguments.\n\n    Please see also [`set_training_args`][fedbiomed.researcher.\\\n    federated_workflows.FederatedWorkflow.set_training_args]\n\n    Returns:\n        The arguments that are going to be passed to the training plan's\n            `training_routine` to perfom training on the node side. An example\n            training routine: [`TorchTrainingPlan.training_routine`]\n            [fedbiomed.common.training_plans.TorchTrainingPlan.training_routine]\n    \"\"\"\n\n    return self._training_args.dict()\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.training_plan","title":"training_plan","text":"<pre><code>training_plan()\n</code></pre> <p>Retrieves the training plan instance currently being used in the federated workflow.</p> <p>Returns:</p> Type Description <code>Optional[TrainingPlan]</code> <p>training plan: the training plan instance</p> Source code in <code>fedbiomed/researcher/federated_workflows/_training_plan_workflow.py</code> <pre><code>@exp_exceptions\ndef training_plan(self) -&gt; Optional[TrainingPlan]:\n    \"\"\"Retrieves the training plan instance currently being used in the federated workflow.\n\n    Returns:\n        training plan: the training plan instance\n    \"\"\"\n    return self._training_plan\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.training_plan_approve","title":"training_plan_approve","text":"<pre><code>training_plan_approve(description='no description provided')\n</code></pre> <p>Send a training plan and a ApprovalRequest message to node(s).</p> <p>This is a simple redirect to the Requests.training_plan_approve() method.</p> <p>If a list of node id(s) is provided, the message will be individually sent to all nodes of the list. If the node id(s) list is None (default), the message is broadcast to all nodes.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>Description for training plan approve request</p> <code>'no description provided'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>a dictionary of pairs (node_id: status), where status indicates to the researcher</p> <code>dict</code> <p>that the training plan has been correctly downloaded on the node side.</p> <code>Warning</code> <code>dict</code> <p>status does not mean that the training plan is approved, only that it has been added</p> <code>dict</code> <p>to the \"approval queue\" on the node side.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_training_plan_workflow.py</code> <pre><code>@exp_exceptions\ndef training_plan_approve(self,\n                          description: str = \"no description provided\") -&gt; dict:\n    \"\"\"Send a training plan and a ApprovalRequest message to node(s).\n\n    This is a simple redirect to the Requests.training_plan_approve() method.\n\n    If a list of node id(s) is provided, the message will be individually sent\n    to all nodes of the list.\n    If the node id(s) list is None (default), the message is broadcast to all nodes.\n\n    Args:\n        description: Description for training plan approve request\n\n    Returns:\n        a dictionary of pairs (node_id: status), where status indicates to the researcher\n        that the training plan has been correctly downloaded on the node side.\n        Warning: status does not mean that the training plan is approved, only that it has been added\n        to the \"approval queue\" on the node side.\n    \"\"\"\n    job = TrainingPlanApproveJob(\n        researcher_id=self._researcher_id,\n        requests=self._reqs,\n        nodes=self.training_data().node_ids(),\n        keep_files_dir=self.experimentation_path(),\n        training_plan=self.training_plan(),\n        description=description,\n    )\n    responses = job.execute()\n    return responses\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.training_plan_class","title":"training_plan_class","text":"<pre><code>training_plan_class()\n</code></pre> <p>Retrieves the type of the training plan that is created for training.</p> <p>Please see also <code>set_training_plan_class</code>.</p> <p>Returns:</p> Name Type Description <code>training_plan_class</code> <code>Optional[TrainingPlanT]</code> <p>the class type of the training plan.</p> Source code in <code>fedbiomed/researcher/federated_workflows/_training_plan_workflow.py</code> <pre><code>@exp_exceptions\ndef training_plan_class(self) -&gt; Optional[TrainingPlanT]:\n    \"\"\"Retrieves the type of the training plan that is created for training.\n\n    Please see also\n    [`set_training_plan_class`][fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.set_training_plan_class].\n\n    Returns:\n        training_plan_class: the class type of the training plan.\n    \"\"\"\n\n    return self._training_plan_class\n</code></pre>"},{"location":"developer/api/researcher/federated_workflows/#fedbiomed.researcher.federated_workflows.TrainingPlanWorkflow.training_plan_file","title":"training_plan_file","text":"<pre><code>training_plan_file(display=True)\n</code></pre> <p>Retrieves the path of the file where the training plan is saved, and optionally displays it.</p> <p>Parameters:</p> Name Type Description Default <code>display</code> <code>bool</code> <p>If <code>True</code>, prints the content of the training plan file. Default is <code>True</code></p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the training plan file</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad argument type, or cannot read training plan file content</p> Source code in <code>fedbiomed/researcher/federated_workflows/_training_plan_workflow.py</code> <pre><code>@exp_exceptions\ndef training_plan_file(self, display: bool = True) -&gt; str:\n    \"\"\"Retrieves the path of the file where the training plan is saved, and optionally displays it.\n\n    Args:\n        display: If `True`, prints the content of the training plan file. Default is `True`\n\n    Returns:\n        Path to the training plan file\n\n    Raises:\n        FedbiomedExperimentError: bad argument type, or cannot read training plan file content\n    \"\"\"\n    if not isinstance(display, bool):\n        # bad type\n        msg = ErrorNumbers.FB410.value + \\\n            f', in method `training_plan_file` param `display` : type {type(display)}'\n        logger.critical(msg)\n        raise FedbiomedExperimentError(msg)\n\n    if display and self._training_plan_file is not None:\n        try:\n            with open(self._training_plan_file) as file:\n                content = file.read()\n                file.close()\n                print(content)\n        except OSError as e:\n            # cannot read training plan file content\n            msg = ErrorNumbers.FB412.value + \\\n                f', in method `training_plan_file` : error when reading training plan file - {e}'\n            logger.critical(msg)\n            raise FedbiomedExperimentError(msg)\n\n    return self._training_plan_file\n</code></pre>"},{"location":"developer/api/researcher/filetools/","title":"Filetools","text":"<p>Functions for managing Job/Experiment files.</p>"},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools-functions","title":"Functions","text":""},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools.choose_bkpt_file","title":"choose_bkpt_file","text":"<pre><code>choose_bkpt_file(experiments_dir, experimentation_folder, round_=0)\n</code></pre> <p>It creates a breakpoint folder and chooses a breakpoint file name for each round.</p> <p>Parameters:</p> Name Type Description Default <code>experiments_dir</code> <p>Base directory for storing experiments files</p> required <code>experimentation_folder</code> <code>str</code> <p>indicates the experimentation folder name. This should just contain the name of the folder not a full path.</p> required <code>round_</code> <code>int</code> <p>the current number of already run rounds minus one. Starts from 0. Defaults to 0.</p> <code>0</code> <p>Raises:</p> Type Description <code>PermissionError</code> <p>cannot create experimentation folder</p> <code>OSError</code> <p>cannot create experimentation folder</p> <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>A tuple that contains following instacens breakpoint_folder_path: name of the created folder that will contain all data for the current round breakpoint_file: name of the file that will contain the state of an experiment.</p> Source code in <code>fedbiomed/researcher/filetools.py</code> <pre><code>def choose_bkpt_file(\n    experiments_dir,\n    experimentation_folder: str,\n    round_: int = 0\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    It creates a breakpoint folder and chooses a breakpoint file name for each round.\n\n    Args:\n        experiments_dir: Base directory for storing experiments files\n        experimentation_folder (str): indicates the experimentation folder name.\n            This should just contain the name of the folder not a full path.\n        round_: the current number of already run rounds minus one.\n            Starts from 0. Defaults to 0.\n\n    Raises:\n        PermissionError: cannot create experimentation folder\n        OSError: cannot create experimentation folder\n\n    Returns:\n        A tuple that contains following instacens\n            breakpoint_folder_path: name of the created folder that will contain all data\n            for the current round\n            breakpoint_file: name of the file that will contain the state of an experiment.\n    \"\"\"\n\n    breakpoint_folder = \"breakpoint_\" + str(\"{:04d}\".format(round_))\n    breakpoint_folder_path = os.path.join(experiments_dir,\n                                          experimentation_folder,\n                                          breakpoint_folder)\n\n    try:\n        os.makedirs(breakpoint_folder_path, exist_ok=True)\n    except (PermissionError, OSError) as err:\n        logger.error(\"Can not save breakpoint folder at \" +\n                     f\"{breakpoint_folder_path} due to some error {err}\")\n        raise\n    breakpoint_file = breakpoint_folder + \".json\"\n\n    return breakpoint_folder_path, breakpoint_file\n</code></pre>"},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools.copy_file","title":"copy_file","text":"<pre><code>copy_file(filepath, breakpoint_path)\n</code></pre> Source code in <code>fedbiomed/researcher/filetools.py</code> <pre><code>def copy_file(filepath: str, breakpoint_path: str) -&gt; str:\n    filename = os.path.dirname(filepath)\n    file_copy_path = os.path.join( breakpoint_path, filename)\n    shutil.copy2(filepath, file_copy_path )\n    return file_copy_path\n</code></pre>"},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools.create_exp_folder","title":"create_exp_folder","text":"<pre><code>create_exp_folder(experiments_dir, experimentation_folder=None)\n</code></pre> <p>Creates a folder for the current experiment (ie the current run of the model).</p> <p>Experiment files to keep are stored here: model file, all versions of node parameters, all versions of aggregated parameters, breakpoints. The created folder is a subdirectory of config.vars[EXPERIMENTS_DIR]</p> <p>Parameters:</p> Name Type Description Default <code>experiments_dir</code> <p>Base directory for storing experiments files</p> required <code>experimentation_folder</code> <code>str</code> <p>optionaly provide an experimentation folder name. This should just contain the name of the folder not a path. default; if no folder name is given, generate a <code>Experiment_x</code> name where <code>x-1</code> is the number of experiments already run (<code>x</code>=0 for the first experiment)</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Experimentation folder</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>cannot create experimentation folder</p> <code>OSError</code> <p>cannot create experimentation folder</p> <code>ValueError</code> <p>bad <code>experimentation_folder</code> argument</p> Source code in <code>fedbiomed/researcher/filetools.py</code> <pre><code>def create_exp_folder(experiments_dir, experimentation_folder: str = None) -&gt; str:\n    \"\"\"Creates a folder for the current experiment (ie the current run of the model).\n\n    Experiment files to keep are stored here: model file, all versions of node parameters,\n    all versions of aggregated parameters, breakpoints. The created folder is a\n    subdirectory of config.vars[EXPERIMENTS_DIR]\n\n    Args:\n        experiments_dir: Base directory for storing experiments files\n        experimentation_folder (str, optional): optionaly provide an experimentation\n            folder name. This should just contain the name of the folder not a path.\n            default; if no folder name is given, generate a `Experiment_x` name where `x-1`\n            is the number of experiments already run (`x`=0 for the first experiment)\n\n    Returns:\n        Experimentation folder\n\n    Raises:\n        PermissionError: cannot create experimentation folder\n        OSError: cannot create experimentation folder\n        ValueError: bad `experimentation_folder` argument\n    \"\"\"\n    if not experimentation_folder:\n        # FIXME: improve method robustness (here nb of exp equals nb of files\n        # in directory)\n        all_files = os.listdir(experiments_dir)\n        experimentation_folder = \"Experiment_\" + str(\"{:04d}\".format(len(all_files)))\n    else:\n        if os.path.basename(experimentation_folder) != experimentation_folder:\n            # experimentation folder cannot be a path\n            raise ValueError(f\"Bad experimentation folder {experimentation_folder} - \" +\n                             \"it cannot be a path\")\n    try:\n        os.makedirs(os.path.join(experiments_dir, experimentation_folder),\n                    exist_ok=True)\n    except (PermissionError, OSError) as err:\n        logger.error(\"Can not save experiment files because \" +\n                     f\"{experiments_dir}/{experimentation_folder} \" +\n                     f\"folder could not be created due to {err}\")\n        raise\n\n    return experimentation_folder\n</code></pre>"},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools.create_unique_file_link","title":"create_unique_file_link","text":"<pre><code>create_unique_file_link(breakpoint_folder_path, file_path)\n</code></pre> <p>Create a symbolic link in <code>breakpoint_folder_path</code> with a non-existing name derived from basename of <code>file_path</code>. The symbolic link points to the real file targeted by <code>file_path</code></p> <p>Parameters:</p> Name Type Description Default <code>breakpoint_folder_path</code> <code>str</code> <p>directory for the source link</p> required <code>file_path</code> <code>str</code> <p>path to the target of the link</p> required <p>Returns:</p> Type Description <code>str</code> <p>Path of the created link</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>bad name for link source or destination</p> Source code in <code>fedbiomed/researcher/filetools.py</code> <pre><code>def create_unique_file_link(breakpoint_folder_path: str, file_path: str) -&gt; str:\n    \"\"\"\n    Create a symbolic link in `breakpoint_folder_path` with a non-existing name derived from basename of\n    `file_path`. The symbolic link points to the real file targeted by `file_path`\n\n    Args:\n        breakpoint_folder_path: directory for the source link\n        file_path: path to the target of the link\n\n    Returns:\n        Path of the created link\n\n    Raises:\n        ValueError: bad name for link source or destination\n    \"\"\"\n\n    try:\n        real_file_path = os.path.realpath(file_path)\n        real_bkpt_folder_path = os.path.realpath(breakpoint_folder_path)\n        if not os.path.isdir(real_bkpt_folder_path) \\\n                or not os.path.isdir(os.path.dirname(real_file_path)):\n            raise ValueError\n\n        # - use relative path for link target for portability\n        # - link to the real file, not to a link-to-the-file\n        link_target = os.path.relpath(real_file_path, start=real_bkpt_folder_path)\n    except ValueError as err:\n        mess = 'Saving breakpoint error, ' + \\\n            f'cannot get relative path to {file_path} from {breakpoint_folder_path}, ' + \\\n            f'due to error {err}'\n        logger.error(mess)\n        raise\n\n    # heuristic : assume limited set of characters in filename postfix\n    re_src_prefix = re.search(\"(.+)\\.[a-zA-Z]+$\",\n                              os.path.basename(file_path))\n    re_src_postfix = re.search(\".+(\\.[a-zA-Z]+)$\",\n                               os.path.basename(file_path))\n    if not re_src_prefix or not re_src_postfix:\n        error_message = f'Saving breakpoint error, bad filename {file_path} gives ' + \\\n            f'prefix {re_src_prefix} and postfix {re_src_postfix}'\n        logger.error(error_message)\n        raise ValueError(error_message)\n\n    link_src_prefix = re_src_prefix.group(1)\n    link_src_postfix = re_src_postfix.group(1)\n\n    return create_unique_link(breakpoint_folder_path,\n                              link_src_prefix, link_src_postfix,\n                              link_target)\n</code></pre>"},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools.create_unique_link","title":"create_unique_link","text":"<pre><code>create_unique_link(breakpoint_folder_path, link_src_prefix, link_src_postfix, link_target_path)\n</code></pre> <p>Find a non-existing name in <code>breakpoint_folder_path</code> and create a symbolic link to a given target name.</p> <p>Parameters:</p> Name Type Description Default <code>breakpoint_folder_path</code> <code>str</code> <p>directory for the source link</p> required <code>link_src_prefix</code> <code>str</code> <p>beginning of the name for the source link (before unique id)</p> required <code>link_src_postfix</code> <code>str</code> <p>end of the name for the source link (after unique id)</p> required <code>link_target_path</code> <code>str</code> <p>target for the symbolic link</p> required <p>Returns:</p> Type Description <code>str</code> <p>Path of the created link</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>cannot create symlink</p> <code>OSError</code> <p>cannot create symlink</p> <code>FileExistsError</code> <p>cannot create symlink</p> <code>FileNotFoundError</code> <p>non-existent directory</p> Source code in <code>fedbiomed/researcher/filetools.py</code> <pre><code>def create_unique_link(breakpoint_folder_path: str,\n                       link_src_prefix: str,\n                       link_src_postfix: str,\n                       link_target_path: str) -&gt; str:\n    \"\"\" Find a non-existing name in `breakpoint_folder_path` and create a symbolic link to a given target name.\n\n    Args:\n        breakpoint_folder_path: directory for the source link\n        link_src_prefix: beginning of the name for the source link (before unique id)\n        link_src_postfix: end of the name for the source link (after unique id)\n        link_target_path: target for the symbolic link\n\n    Returns:\n        Path of the created link\n\n    Raises:\n        PermissionError: cannot create symlink\n        OSError: cannot create symlink\n        FileExistsError: cannot create symlink\n        FileNotFoundError : non-existent directory\n    \"\"\"\n    stub = 0\n    link_src_path = os.path.join(breakpoint_folder_path,\n                                 link_src_prefix + link_src_postfix)\n\n    # Need to ensure unique name for link (e.g. when replaying from non-last breakpoint)\n    while os.path.exists(link_src_path) or os.path.islink(link_src_path):\n        stub += 1\n        link_src_path = os.path.join(breakpoint_folder_path,\n                                     link_src_prefix + '_' + str(\"{:02}\".format(stub)) + link_src_postfix)\n    try:\n        os.symlink(link_target_path, link_src_path)\n    except(FileExistsError, PermissionError, OSError, FileNotFoundError) as err:\n        logger.error(f\"Can not create link to experiment file {link_target_path} \" +\n                     f\"from {link_src_path} due to error {err}\")\n        raise\n\n    return link_src_path\n</code></pre>"},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools.find_breakpoint_path","title":"find_breakpoint_path","text":"<pre><code>find_breakpoint_path(experiment_dir, breakpoint_folder_path=None)\n</code></pre> <p>Finds breakpoint folder path and file, depending on if user specifies a specific breakpoint path (unchanged in this case) or not (try to guess the latest breakpoint).</p> <p>Parameters:</p> Name Type Description Default <code>experiments_dir</code> <p>Base directory for storing experiments files</p> required <code>breakpoint_folder_path</code> <code>str | None</code> <p>path towards breakpoint. If None, (default), consider the latest breakpoint saved on default path (provided at least one breakpoint exists). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>With length of two that represents respectively:</p> <ul> <li>path to breakpoint folder (unchanged if specified by user)</li> <li>breakpoint file.</li> </ul> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>triggered either if breakpoint cannot be found, folder is empty or file cannot be parsed</p> Source code in <code>fedbiomed/researcher/filetools.py</code> <pre><code>def find_breakpoint_path(experiment_dir: str, breakpoint_folder_path: str | None = None) -&gt; Tuple[str, str]:\n    \"\"\" Finds breakpoint folder path and file, depending on if user specifies a\n    specific breakpoint path (unchanged in this case) or not (try to guess the\n    latest breakpoint).\n\n    Args:\n        experiments_dir: Base directory for storing experiments files\n        breakpoint_folder_path: path towards breakpoint. If None, (default), consider the\n            latest breakpoint saved on default path (provided at least one breakpoint\n            exists). Defaults to None.\n\n    Returns:\n        With length of two that represents respectively:\n\n            - path to breakpoint folder (unchanged if specified by user)\n            - breakpoint file.\n\n    Raises:\n        FileNotFoundError: triggered either if breakpoint cannot be found, folder is empty or file cannot be parsed\n    \"\"\"\n\n    # First, let's test if folder is a real folder path\n    if breakpoint_folder_path is None:\n        try:\n            # retrieve latest experiment\n\n            # for error message\n            latest_exp_folder = experiment_dir + \"/NO_FOLDER_FOUND\"\n\n            # content of breakpoint folder\n            experiment_folders = os.listdir(experiment_dir)\n\n            latest_exp_folder = _get_latest_file(\n                experiment_dir,\n                experiment_folders,\n                only_folder=True)\n\n            latest_exp_folder = os.path.join(experiment_dir,\n                                             latest_exp_folder)\n\n            bkpt_folders = os.listdir(latest_exp_folder)\n\n            breakpoint_folder_path = _get_latest_file(\n                latest_exp_folder,\n                bkpt_folders,\n                only_folder=True)\n            breakpoint_folder_path = os.path.join(latest_exp_folder,\n                                                  breakpoint_folder_path)\n        except FileNotFoundError as err:\n            logger.error(\"Cannot find latest breakpoint in \" + latest_exp_folder +\n                         \" Are you sure at least one breakpoint is saved there ? \" +\n                         \" - Error: \" + str(err))\n            raise\n    else:\n        if not os.path.isdir(breakpoint_folder_path):\n            raise FileNotFoundError(\n                f\"Breakpoint folder {breakpoint_folder_path} is not a directory\")\n\n    # check if folder is a valid breakpoint\n\n    #\n    # verify the validity of the breakpoint content\n    # TODO: be more robust\n    all_breakpoint_materials = os.listdir(breakpoint_folder_path)\n    if len(all_breakpoint_materials) == 0:\n        raise FileNotFoundError(f'Breakpoint folder {breakpoint_folder_path} is empty !')\n\n    state_file = None\n    for breakpoint_material in all_breakpoint_materials:\n        # look for the json file containing experiment state\n        # (it should be named `breakpoint_xx.json`)\n        json_match = re.fullmatch(r'breakpoint_\\d*\\.json',\n                                  breakpoint_material)\n        # there should be at most one - TODO: verify\n        if json_match is not None:\n            logger.debug(f\"found json file containing states at\\\n                {breakpoint_material}\")\n            state_file = breakpoint_material\n\n    if state_file is None:\n        message = \"Cannot find JSON file containing \" + \\\n            f\"model state at {breakpoint_folder_path}. Aborting\"\n        logger.error(message)\n        raise FileNotFoundError(message)\n\n    return breakpoint_folder_path, state_file\n</code></pre>"},{"location":"developer/api/researcher/jobs/","title":"Jobs","text":""},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs-classes","title":"Classes","text":""},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.Job","title":"Job","text":"<pre><code>Job(*, researcher_id, requests, nodes, keep_files_dir)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Job represents a task to be executed on the node.</p> <p>This is a base class that provides the basic functionality necessary to establish communication with the remote nodes. Actual tasks should inherit from <code>Job</code> to implement their own domain logic.</p> <p>Functional life-cycle</p> <p>Jobs must follow a \"functional\" life-cycle, meaning that they should be created just before the execution of the task, and destroyed shortly after. Jobs should not persist outside the scope of the function that requested the execution of the task.</p> <p>Attributes:</p> Name Type Description <code>requests</code> <code>List[RequestPolicy] | None</code> <p>read-only <code>Requests</code> object handling communication with      remote nodes</p> <code>nodes</code> <code>List[str]</code> <p>node IDs participating in the task</p> <p>Parameters:</p> Name Type Description Default <code>researcher_id</code> <code>str</code> <p>Unique ID of the researcher</p> required <code>requests</code> <code>Requests</code> <p>Object for handling communications</p> required <code>nodes</code> <code>List[str] | None</code> <p>A dict of node_id containing the nodes used for training</p> required <code>keep_files_dir</code> <code>str</code> <p>Directory for storing files created by the job that we want to keep beyond the execution of the job.</p> required Source code in <code>fedbiomed/researcher/federated_workflows/jobs/_job.py</code> <pre><code>def __init__(\n    self,\n    *,\n    researcher_id: str,\n    requests: Requests,\n    nodes: List[str] | None,\n    keep_files_dir: str\n):\n    \"\"\"Constructor of the class\n\n    Args:\n        researcher_id: Unique ID of the researcher\n        requests: Object for handling communications\n        nodes: A dict of node_id containing the nodes used for training\n        keep_files_dir: Directory for storing files created by the job that we want to keep beyond the execution\n            of the job.\n\n    \"\"\"\n\n    self._researcher_id = researcher_id\n    self._reqs = requests\n    self._nodes: List[str] = nodes or []  # List of node ids participating in this task\n    self._keep_files_dir = keep_files_dir\n    self._policies: List[RequestPolicy] | None = None\n</code></pre>"},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.Job-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.Job.nodes","title":"nodes  <code>property</code>","text":"<pre><code>nodes\n</code></pre>"},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.Job.requests","title":"requests  <code>property</code>","text":"<pre><code>requests\n</code></pre>"},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.Job-classes","title":"Classes","text":""},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.Job.RequestTimer","title":"RequestTimer","text":"<pre><code>RequestTimer(nodes)\n</code></pre> <p>Context manager that computes the processing time elapsed for the request and the reply</p> <p>Usage: <pre><code>nodes = ['node_1', 'node_2']\njob = Job(nodes, file)\nwith job._timer() as my_timer:\n    # ... send some request\n\nmy_timer\n# {node_1: 2.22, node_2: 2.21} # request time for each Node in second\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>List[str]</code> <p>existing nodes that will be requested for the Job</p> required Source code in <code>fedbiomed/researcher/federated_workflows/jobs/_job.py</code> <pre><code>def __init__(self, nodes: List[str]):\n    \"\"\"\n    Constructor of NodeTimer\n\n    Args:\n        nodes: existing nodes that will be requested for the Job\n    \"\"\"\n    self._timer = {node_id: 0.  for node_id in nodes}\n</code></pre>"},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.Job-functions","title":"Functions","text":""},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.Job.execute","title":"execute  <code>abstractmethod</code>","text":"<pre><code>execute()\n</code></pre> <p>Payload of the job.</p> <p>Completes a request to the job's nodes and collects replies.</p> <p>Returns:</p> Type Description <code>Any</code> <p>values specific to the type of job</p> Source code in <code>fedbiomed/researcher/federated_workflows/jobs/_job.py</code> <pre><code>@abstractmethod\ndef execute(self) -&gt; Any:\n    \"\"\"Payload of the job.\n\n    Completes a request to the job's nodes and collects replies.\n\n    Returns:\n        values specific to the type of job\n    \"\"\"\n</code></pre>"},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.TrainingJob","title":"TrainingJob","text":"<pre><code>TrainingJob(experiment_id, round_, training_plan, training_args, model_args, data, nodes_state_ids, aggregator_args, secagg_arguments=None, do_training=True, optim_aux_var=None, **kwargs)\n</code></pre> <p>               Bases: <code>Job</code></p> <p>TrainingJob is a task for training an ML model on the nodes by executing a TrainingPlan.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_id</code> <code>str</code> <p>unique ID of this experiment</p> required <code>round_</code> <code>int</code> <p>current number of round the algorithm is performing (a round is considered to be all the training steps of a federated model between 2 aggregations).</p> required <code>training_plan</code> <code>BaseTrainingPlan</code> <p>TrainingPlan with properly initialized model and optimizer</p> required <code>training_args</code> <code>TrainingArgs</code> <p>arguments for training</p> required <code>model_args</code> <code>Optional[dict]</code> <p>arguments for the model</p> required <code>data</code> <code>FederatedDataSet</code> <p>metadata of the federated data set</p> required <code>nodes_state_ids</code> <code>Dict[str, str]</code> <p>unique IDs of the node states saved remotely</p> required <code>aggregator_args</code> <code>Dict[str, Dict[str, Any]]</code> <p>aggregator arguments required for remote execution</p> required <code>secagg_arguments</code> <code>Union[Dict, None]</code> <p>Secure aggregation arguments, some depending on scheme used</p> <code>None</code> <code>do_training</code> <code>bool</code> <p>if False, skip training in this round (do only validation). Defaults to True.</p> <code>True</code> <code>optim_aux_var</code> <code>Optional[Dict[str, AuxVar]]</code> <p>Auxiliary variables of the researcher-side Optimizer, if any. Note that such variables may only be used if both the Experiment and node-side training plan hold a declearn-based Optimizer, and their plug-ins are coherent with each other as to expected information exchange.</p> <code>None</code> <code>*args</code> <p>Positional argument of parent class <code>Job</code></p> required <code>**kwargs</code> <p>Named arguments of parent class. Please see <code>Job</code></p> <code>{}</code> Source code in <code>fedbiomed/researcher/federated_workflows/jobs/_training_job.py</code> <pre><code>def __init__(\n    self,\n    experiment_id: str,\n    round_: int,\n    training_plan: BaseTrainingPlan,\n    training_args: TrainingArgs,\n    model_args: Optional[dict],\n    data: FederatedDataSet,\n    nodes_state_ids: Dict[str, str],\n    aggregator_args: Dict[str, Dict[str, Any]],\n    secagg_arguments: Union[Dict, None] = None,\n    do_training: bool = True,\n    optim_aux_var: Optional[Dict[str, AuxVar]] = None,\n    **kwargs\n):\n\n    \"\"\" Constructor of the class\n\n    Args:\n        experiment_id: unique ID of this experiment\n        round_: current number of round the algorithm is performing (a round is considered to be all the\n            training steps of a federated model between 2 aggregations).\n        training_plan: TrainingPlan with properly initialized model and optimizer\n        training_args: arguments for training\n        model_args: arguments for the model\n        data: metadata of the federated data set\n        nodes_state_ids: unique IDs of the node states saved remotely\n        aggregator_args: aggregator arguments required for remote execution\n        secagg_arguments: Secure aggregation arguments, some depending on scheme used\n        do_training: if False, skip training in this round (do only validation). Defaults to True.\n        optim_aux_var: Auxiliary variables of the researcher-side Optimizer, if any.\n            Note that such variables may only be used if both the Experiment and node-side training plan\n            hold a declearn-based [Optimizer][fedbiomed.common.optimizers.Optimizer], and their plug-ins\n            are coherent with each other as to expected information exchange.\n        *args: Positional argument of parent class\n            [`Job`][fedbiomed.researcher.federated_workflows.jobs.Job]\n        **kwargs: Named arguments of parent class. Please see\n            [`Job`][fedbiomed.researcher.federated_workflows.jobs.Job]\n    \"\"\"\n    super().__init__(**kwargs)\n    # to be used for `execute()`\n    self._experiment_id = experiment_id\n    self._round_ = round_\n    self._training_plan = training_plan\n    self._training_args = training_args\n    self._model_args = model_args\n    self._data = data\n    self._nodes_state_ids = nodes_state_ids\n    self._aggregator_args = aggregator_args\n    self._secagg_arguments = secagg_arguments or {}  # Assign empty dict to secagg arguments if it is None\n    self._do_training = do_training\n    self._optim_aux_var = optim_aux_var\n</code></pre>"},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.TrainingJob-functions","title":"Functions","text":""},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.TrainingJob.execute","title":"execute","text":"<pre><code>execute()\n</code></pre> <p>Sends training request to nodes and waits for the responses</p> <p>Returns:</p> Type Description <code>Tuple[Dict[str, Dict[str, Any]], Union[Dict[str, Dict[str, AuxVar]], Dict[str, EncryptedAuxVar]]]</code> <p>A tuple of * training replies for this round * node-wise optimizer auxiliary variables, as a dict with format <code>{node_name: encrypted_aux_var}</code> is secagg is used, and <code>{node_name: {module_name: module_aux_var}}</code> otherwise.</p> Source code in <code>fedbiomed/researcher/federated_workflows/jobs/_training_job.py</code> <pre><code>def execute(self) -&gt; Tuple[\n    Dict[str, Dict[str, Any]],  # inner dicts are TrainReply dumps\n    Union[Dict[str, Dict[str, AuxVar]], Dict[str, EncryptedAuxVar]],\n]:\n    \"\"\" Sends training request to nodes and waits for the responses\n\n    Returns:\n        A tuple of\n          * training replies for this round\n          * node-wise optimizer auxiliary variables, as a dict with format\n            `{node_name: encrypted_aux_var}` is secagg is used, and\n            `{node_name: {module_name: module_aux_var}}` otherwise.\n    \"\"\"\n\n    # Populate request message\n    msg = {\n        'researcher_id': self._researcher_id,\n        'experiment_id': self._experiment_id,\n        'training_args': self._training_args.dict(),\n        'training': self._do_training,\n        'model_args': self._model_args if self._model_args is not None else {},\n        'round': self._round_,\n        'training_plan': self._training_plan.source(),\n        'training_plan_class': self._training_plan.__class__.__name__,\n        'params': self._training_plan.get_model_params(\n            exclude_buffers=not self._training_args.dict()['share_persistent_buffers']),\n        'secagg_arguments': self._secagg_arguments,\n        'aggregator_args': {},\n        'optim_aux_var': self._optim_aux_var\n    }\n\n    # Loop over nodes, add node specific data and send train request\n    messages = MessagesByNode()\n\n    for node in self._nodes:\n        msg['dataset_id'] = self._data.data()[node]['dataset_id']\n        msg['state_id'] = self._nodes_state_ids.get(node)\n\n        # add aggregator parameters to message header\n        msg['aggregator_args'] = self._aggregator_args.get(node, {}) if self._aggregator_args else {}\n\n        self._log_round_info(node=node, training=self._do_training)\n\n        messages.update({node: TrainRequest(**msg)})  # send request to node\n\n    with self.RequestTimer(self._nodes) as timer:  # compute request time\n        # Send training request\n        with self._reqs.send(messages, self._nodes, self._policies) as federated_req:\n            errors = federated_req.errors()\n            replies = federated_req.replies()\n\n    training_replies = self._get_training_results(replies=replies,\n                                                  errors=errors)\n\n    timing_results = self._get_timing_results(replies, timer)\n    # `training_replies` can be empty if there wasnot any replies\n    for node_id in replies:\n        if training_replies.get(node_id):\n            training_replies[node_id].update({'timing': timing_results[node_id]})\n\n    # Extract aux variables from training replies.\n    if self._do_training:\n        aux_vars = self._extract_received_optimizer_aux_var_from_round(replies)\n    else:\n        aux_vars = {}\n    return training_replies, aux_vars\n</code></pre>"},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.TrainingPlanApproveJob","title":"TrainingPlanApproveJob","text":"<pre><code>TrainingPlanApproveJob(training_plan, description, **kwargs)\n</code></pre> <p>               Bases: <code>Job</code></p> <p>Task for requesting nodes approval for running a given TrainingPlan on these nodes.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan</code> <code>BaseTrainingPlan</code> <p>an instance of a TrainingPlan object</p> required <code>description</code> <code>str</code> <p>human-readable description of the TrainingPlan for the reviewer on the node</p> required <code>*args</code> <p>Positonal argument of parent class <code>Job</code></p> required <code>**kwargs</code> <p>Named arguments of parent class. Please see <code>Job</code></p> <code>{}</code> Source code in <code>fedbiomed/researcher/federated_workflows/jobs/_training_plan_approval_job.py</code> <pre><code>def __init__(self,\n             training_plan: BaseTrainingPlan,\n             description: str,\n             **kwargs\n             ):\n    \"\"\"Constructor of the class.\n\n    Args:\n        training_plan: an instance of a TrainingPlan object\n        description: human-readable description of the TrainingPlan for the reviewer on the node\n        *args: Positonal argument of parent class\n            [`Job`][fedbiomed.researcher.federated_workflows.jobs.Job]\n        **kwargs: Named arguments of parent class. Please see\n            [`Job`][fedbiomed.researcher.federated_workflows.jobs.Job]\n    \"\"\"\n    super().__init__(**kwargs)\n    self._policies = [DiscardOnTimeout(5)]  # specific policy for TrainingApproval\n    self._training_plan = training_plan\n    self._description = description\n</code></pre>"},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.TrainingPlanApproveJob-functions","title":"Functions","text":""},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.TrainingPlanApproveJob.execute","title":"execute","text":"<pre><code>execute()\n</code></pre> <p>Requests the approval of the provided TrainingPlan.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <p>a dictionary of pairs (node_id: status), where status indicates to the researcher</p> <code>Dict</code> <p>that the training plan has been correctly downloaded on the node side.</p> <code>Warning</code> <code>Dict</code> <p>status does not mean that the training plan is approved, only that it has been added</p> <code>Dict</code> <p>to the \"approval queue\" on the node side.</p> Source code in <code>fedbiomed/researcher/federated_workflows/jobs/_training_plan_approval_job.py</code> <pre><code>def execute(self) -&gt; Dict:\n    \"\"\"Requests the approval of the provided TrainingPlan.\n\n    Returns:\n        a dictionary of pairs (node_id: status), where status indicates to the researcher\n        that the training plan has been correctly downloaded on the node side.\n        Warning: status does not mean that the training plan is approved, only that it has been added\n        to the \"approval queue\" on the node side.\n    \"\"\"\n    return self._reqs.training_plan_approve(self._training_plan,\n                                            self._description,\n                                            self._nodes,\n                                            self._policies)\n</code></pre>"},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.TrainingPlanCheckJob","title":"TrainingPlanCheckJob","text":"<pre><code>TrainingPlanCheckJob(experiment_id, training_plan, **kwargs)\n</code></pre> <p>               Bases: <code>Job</code></p> <p>Task for checking if nodes accept running a given TrainingPlan.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_id</code> <code>str</code> <p>unique ID of this experiment</p> required <code>training_plan</code> <code>BaseTrainingPlan</code> <p>an instance of a TrainingPlan object</p> required <code>**kwargs</code> <p>Named arguments of parent class. Please see <code>Job</code></p> <code>{}</code> Source code in <code>fedbiomed/researcher/federated_workflows/jobs/_training_plan_approval_job.py</code> <pre><code>def __init__(\n    self,\n    experiment_id: str,\n    training_plan: BaseTrainingPlan,\n    **kwargs\n):\n    \"\"\"Constructor of the class.\n\n    Args:\n        experiment_id: unique ID of this experiment\n        training_plan: an instance of a TrainingPlan object\n        **kwargs: Named arguments of parent class. Please see\n            [`Job`][fedbiomed.researcher.federated_workflows.jobs.Job]\n\n    \"\"\"\n    super().__init__(**kwargs)\n    self._policies = [DiscardOnTimeout(5)]  # specific policy for TrainingApproval\n    self._experiment_id = experiment_id\n    self._training_plan = training_plan\n</code></pre>"},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.TrainingPlanCheckJob-functions","title":"Functions","text":""},{"location":"developer/api/researcher/jobs/#fedbiomed.researcher.federated_workflows.jobs.TrainingPlanCheckJob.execute","title":"execute","text":"<pre><code>execute()\n</code></pre> <p>Checks whether model is approved or not.</p> <p>This method sends <code>training-plan-status</code> request to the nodes. It should be run before running experiment. So, researchers can find out if their model has been approved</p> <p>Returns:</p> Type Description <code>Dict</code> <p>A dict of <code>Message</code> objects indexed by node ID, one for each job's nodes</p> Source code in <code>fedbiomed/researcher/federated_workflows/jobs/_training_plan_approval_job.py</code> <pre><code>def execute(self) -&gt; Dict:\n    \"\"\"Checks whether model is approved or not.\n\n    This method sends `training-plan-status` request to the nodes. It should be run before running experiment.\n    So, researchers can find out if their model has been approved\n\n    Returns:\n        A dict of `Message` objects indexed by node ID, one for each job's nodes\n    \"\"\"\n\n    message = TrainingPlanStatusRequest(**{\n        'researcher_id': self._researcher_id,\n        'experiment_id': self._experiment_id,\n        'training_plan': self._training_plan.source(),\n    })\n\n    # Send message to each node that has been found after dataset search request\n    # TODO: add timer to compute request time\n    with self._reqs.send(message, self._nodes, policies=self._policies) as federated_req:\n        replies = federated_req.replies()\n\n        for node_id, reply in replies.items():\n            if reply.success is True:\n                if reply.approval_obligation is True:\n                    if reply.status == TrainingPlanApprovalStatus.APPROVED.value:\n                        logger.info(f'Training plan has been approved by the node: {node_id}')\n                    else:\n                        logger.warning(f'Training plan has NOT been approved by the node: {node_id}.' +\n                                       f'Training plan status : {reply.status}')\n                else:\n                    logger.info(f'Training plan approval is not required by the node: {node_id}')\n            else:\n                logger.warning(f\"Node : {node_id} : {reply.msg}\")\n\n    # Get the nodes that haven't replied training-plan-status request\n    non_replied_nodes = list(set(self._nodes) - set(replies.keys()))\n    if non_replied_nodes:\n        logger.warning(f\"Request for checking training plan status hasn't been replied \\\n                         by the nodes: {non_replied_nodes}. You might get error \\\n                             while running your experiment. \")\n\n    return replies\n</code></pre>"},{"location":"developer/api/researcher/monitor/","title":"Monitor","text":"<p>monitor class to trap information sent during training and sned it to tensordboard</p>"},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor-classes","title":"Classes","text":""},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.MetricStore","title":"MetricStore","text":"<p>               Bases: <code>dict</code></p> <p>Storage facility, used for storing training loss and testing metric values, in order to display them on Tensorboard. Inheriting from a dictionary, providing methods to simplify queries and saving metric values.</p> <p>Storage architecture: <pre><code>{&lt;node&gt;:\n    {&lt;for_&gt;:\n        {&lt;metric_name&gt;:\n            {&lt;round_&gt;: { &lt;iterations/values&gt;: List[float] }\n            }\n        }\n    }\n}\n</code></pre> Where: - <code>node</code>: node id - <code>for_</code>: either testing_global_updates, testing_local_updates, or training - <code>metric_name</code>: metric 's name. Custom or Custom_xxx if testing_step has been defined in TrainingPlan (custom metric) - <code>round_</code>: round number - <code>iterations</code>: index of iterations stored - <code>values</code>: metric value</p>"},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.MetricStore-functions","title":"Functions","text":""},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.MetricStore.add_iteration","title":"add_iteration","text":"<pre><code>add_iteration(node, train, test_on_global_updates, round_, metric, iter_)\n</code></pre> <p>Method adding iteration to MetricStore based on node, training/validation, round and metric.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>The node id that metric value received from</p> required <code>train</code> <code>bool</code> <p>Training status, If true metric value is for training, Otherwise for validation</p> required <code>test_on_global_updates</code> <code>bool</code> <p>If True metric value is for validation on global updates. Otherwise, for validation on local updates</p> required <code>round_</code> <code>int</code> <p>The round that metric value has received at</p> required <code>metric</code> <code>dict</code> <p>Dictionary that contains metric names and their values e.g {'':} required <code>iter_</code> <code>int</code> <p>Iteration number for validation/training.</p> required <p>Returns      List of cumulative iteration for each metric/validation result</p> Source code in <code>fedbiomed/researcher/monitor.py</code> <pre><code>def add_iteration(self,\n                  node: str,\n                  train: bool,\n                  test_on_global_updates: bool,\n                  round_: int,\n                  metric: dict,\n                  iter_: int) -&gt; list[int]:\n    \"\"\"\n    Method adding iteration to MetricStore based on node, training/validation, round and metric.\n\n    Args:\n        node: The node id that metric value received from\n        train: Training status, If true metric value is for training, Otherwise for validation\n        test_on_global_updates: If True metric value is for validation on global updates. Otherwise,\n            for validation on local updates\n        round_: The round that metric value has received at\n        metric: Dictionary that contains metric names and their values e.g {'&lt;metric-name&gt;':&lt;value&gt;}\n        iter_: Iteration number for validation/training.\n\n    Returns\n         List of cumulative iteration for each metric/validation result\n    \"\"\"\n\n    if node not in self:\n        self._register_node(node=node)\n\n    cum_iter = []\n    for metric_name, metric_value in metric.items():\n\n        for_ = 'training' if train is True else 'testing_global_updates' \\\n            if test_on_global_updates is True else 'testing_local_updates'\n\n        if metric_name not in self[node][for_]:\n            self._register_metric(node=node, for_=for_, metric_name=metric_name)\n\n        # FIXME: for now, if validation is done on global updates (before model local update)\n        # last testing metric value computed on global updates at last round is overwritten\n        # by the first one computed at first round\n        if round_ in self[node][for_][metric_name]:\n\n            # Each duplication means a new epoch for training, and it is not expected for\n            # validation part. Especially for `testing_on_global_updates`. If there is a duplication\n            # last value should overwrite\n            duplicate = self._iter_duplication_status(round_=self[node][for_][metric_name][round_],\n                                                      next_iter=iter_)\n            if duplicate and test_on_global_updates:\n                self._add_new_iteration(node, for_, metric_name, round_, iter_, metric_value, True)\n            else:\n                self._add_new_iteration(node, for_, metric_name, round_, iter_, metric_value)\n        else:\n            self._add_new_iteration(node, for_, metric_name, round_, iter_, metric_value, True)\n\n        cum_iter.append(self._cumulative_iteration(self[node][for_][metric_name]))\n    return cum_iter\n</code></pre>"},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.Monitor","title":"Monitor","text":"<pre><code>Monitor(results_dir)\n</code></pre> <p>Monitors nodes scalar feed-backs during training</p> <p>Parameters:</p> Name Type Description Default <code>results_dir</code> <code>str</code> <p>Directory for storing monitoring information.</p> required Source code in <code>fedbiomed/researcher/monitor.py</code> <pre><code>def __init__(self, results_dir: str):\n    \"\"\"Constructor of the class.\n\n    Args:\n        results_dir: Directory for storing monitoring information.\n    \"\"\"\n\n    self._log_dir = results_dir\n    self._round = 1\n    self._metric_store = MetricStore()\n    self._event_writers = {}\n    self._round_state = 0\n    self._tensorboard = False\n\n    if os.listdir(self._log_dir):\n        logger.info('Removing tensorboard logs from previous experiment')\n        # Clear logs' directory from the files from other experiments.\n        self._remove_logs()\n</code></pre>"},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.Monitor-functions","title":"Functions","text":""},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.Monitor.close_writer","title":"close_writer","text":"<pre><code>close_writer()\n</code></pre> <p>Closes <code>SummaryWriter</code> for each node</p> Source code in <code>fedbiomed/researcher/monitor.py</code> <pre><code>def close_writer(self):\n    \"\"\" Closes `SummaryWriter` for each node \"\"\"\n    # Close each open SummaryWriter\n    for node in self._event_writers:\n        self._event_writers[node].close()\n</code></pre>"},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.Monitor.on_message_handler","title":"on_message_handler","text":"<pre><code>on_message_handler(msg)\n</code></pre> <p>Handler for messages received through general/monitoring channel. This method is used as callback function in Requests class</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Dict[str, Any]</code> <p>incoming message from Node. Must contain key named <code>command</code>, describing the nature of the command (currently the command is only add_scalar).</p> required Source code in <code>fedbiomed/researcher/monitor.py</code> <pre><code>def on_message_handler(self, msg: Dict[str, Any]):\n    \"\"\" Handler for messages received through general/monitoring channel. This method is used as callback function\n    in Requests class\n\n    Args:\n        msg: incoming message from Node. Must contain key named `command`, describing the nature\n            of the command (currently the command is only add_scalar).\n    \"\"\"\n\n    # For now monitor can only handle add_scalar messages\n\n        # Save iteration value\n    cumulative_iter, *_ = self._metric_store.add_iteration(\n        node=msg['node_id'],\n        train=msg['train'],\n        test_on_global_updates=msg['test_on_global_updates'],\n        metric=msg['metric'],\n        round_=self._round,\n        iter_=msg['iteration'])\n\n    # Log metric result\n    self._log_metric_result(message=msg, cum_iter=cumulative_iter)\n</code></pre>"},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.Monitor.set_round","title":"set_round","text":"<pre><code>set_round(round_)\n</code></pre> <p>Setts round number that metric results will be received for.</p> <p>By default, at the beginning round is equal to 1 which stands for the first round. T his method should be called by experiment <code>run_once</code> after each round completed, and round should be set to current round + 1. This will inform monitor about the current round where the metric values are getting received.</p> <p>Parameters:</p> Name Type Description Default <code>round_</code> <p>The round that metric value will be saved at they are received</p> required Source code in <code>fedbiomed/researcher/monitor.py</code> <pre><code>def set_round(self, round_: int) -&gt; int:\n    \"\"\" Setts round number that metric results will be received for.\n\n    By default, at the beginning round is equal to 1 which stands for the first round. T\n    his method should be called by experiment `run_once` after each round completed, and round should be set\n    to current round + 1. This will inform monitor about the current round where the metric values are getting\n    received.\n\n    Args:\n        round_ : The round that metric value will be saved at they are received\n    \"\"\"\n    self._round = round_\n\n    return self._round\n</code></pre>"},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.Monitor.set_tensorboard","title":"set_tensorboard","text":"<pre><code>set_tensorboard(tensorboard)\n</code></pre> <p>Sets tensorboard flag, which is used to decide the behavior of the writing scalar values into  tensorboard log files.</p> <p>Parameters:</p> Name Type Description Default <code>tensorboard</code> <code>bool</code> <p>if True, data contained in AddScalarReply message will be passed to tensorboard          if False, fata will only be logged on the console</p> required Source code in <code>fedbiomed/researcher/monitor.py</code> <pre><code>def set_tensorboard(self, tensorboard: bool):\n    \"\"\" Sets tensorboard flag, which is used to decide the behavior of the writing scalar values into\n     tensorboard log files.\n\n    Args:\n        tensorboard: if True, data contained in AddScalarReply message will be passed to tensorboard\n                     if False, fata will only be logged on the console\n    \"\"\"\n    if isinstance(tensorboard, bool):\n        self._tensorboard = tensorboard\n    else:\n        logger.error(\"tensorboard should be a boolean\")\n        self._tensorboard = False\n</code></pre>"},{"location":"developer/api/researcher/node_state_agent/","title":"NodeStateAgent","text":""},{"location":"developer/api/researcher/node_state_agent/#fedbiomed.researcher.node_state_agent-classes","title":"Classes","text":""},{"location":"developer/api/researcher/node_state_agent/#fedbiomed.researcher.node_state_agent.NodeStateAgent","title":"NodeStateAgent","text":"<pre><code>NodeStateAgent(node_ids)\n</code></pre> <p>Manages Node States collection, gathered from <code>Nodes</code> replies.</p> <p>Initializes state ID of each node provided in <code>node_ids</code> to None (will maintain state, no previous node state yet)</p> <p>Parameters:</p> Name Type Description Default <code>node_ids</code> <code>List[str]</code> <p>list of node IDs of the nodes for which to maintain state ID</p> required Source code in <code>fedbiomed/researcher/node_state_agent.py</code> <pre><code>def __init__(self, node_ids: List[str]) -&gt; None:\n    \"\"\"Constructor for NodeStateAgent.\n\n    Initializes state ID of each node provided in `node_ids` to None (will maintain state,\n    no previous node state yet)\n\n    Args:\n        node_ids: list of node IDs of the nodes for which to maintain state ID\n    \"\"\"\n    self._collection_state_ids: Dict[str, str] = {  # Mapping &lt;node_id, state_id&gt;\n        node_id: None for node_id in node_ids\n    }\n</code></pre>"},{"location":"developer/api/researcher/node_state_agent/#fedbiomed.researcher.node_state_agent.NodeStateAgent-functions","title":"Functions","text":""},{"location":"developer/api/researcher/node_state_agent/#fedbiomed.researcher.node_state_agent.NodeStateAgent.get_last_node_states","title":"get_last_node_states","text":"<pre><code>get_last_node_states()\n</code></pre> <p>Returns a dictionary mapping  from latest <code>Nodes</code> replies. <p>If used before the end of the first Round, each state_id is set to None</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Mapping of <code>&lt;node_id, state_id&gt;</code></p> Source code in <code>fedbiomed/researcher/node_state_agent.py</code> <pre><code>def get_last_node_states(self) -&gt; Dict[str, str]:\n    \"\"\"Returns a dictionary mapping &lt;node_id, state_id&gt; from latest `Nodes` replies.\n\n    If used before the end of the first Round, each state_id is set to None\n\n    Returns:\n        Mapping of `&lt;node_id, state_id&gt;`\n    \"\"\"\n    return self._collection_state_ids\n</code></pre>"},{"location":"developer/api/researcher/node_state_agent/#fedbiomed.researcher.node_state_agent.NodeStateAgent.load_state_breakpoint","title":"load_state_breakpoint","text":"<pre><code>load_state_breakpoint(node_state)\n</code></pre> <p>Loads NodeStateAgent's state from saved state.</p> <p>Parameters:</p> Name Type Description Default <code>node_state</code> <code>Dict</code> <p>state of <code>NodeStateAgent</code> to be loaded.</p> required Source code in <code>fedbiomed/researcher/node_state_agent.py</code> <pre><code>def load_state_breakpoint(self, node_state: Dict):\n    \"\"\"Loads NodeStateAgent's state from saved state.\n\n    Args:\n        node_state: state of `NodeStateAgent` to be loaded.\n    \"\"\"\n    self._collection_state_ids = node_state.get('collection_state_ids')\n</code></pre>"},{"location":"developer/api/researcher/node_state_agent/#fedbiomed.researcher.node_state_agent.NodeStateAgent.save_state_breakpoint","title":"save_state_breakpoint","text":"<pre><code>save_state_breakpoint()\n</code></pre> <p>NodeStateAgent's state, to be saved in a breakpoint.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Node state for breakpoint</p> Source code in <code>fedbiomed/researcher/node_state_agent.py</code> <pre><code>def save_state_breakpoint(self) -&gt; Dict:\n    \"\"\"NodeStateAgent's state, to be saved in a breakpoint.\n\n    Returns:\n        Node state for breakpoint\n    \"\"\"\n    return {\n        'collection_state_ids': self._collection_state_ids\n    }\n</code></pre>"},{"location":"developer/api/researcher/node_state_agent/#fedbiomed.researcher.node_state_agent.NodeStateAgent.update_node_states","title":"update_node_states","text":"<pre><code>update_node_states(all_node_ids, resp=None)\n</code></pre> <p>Updates the state_id collection with respect to current nodes and latest Nodes replies.</p> <p>Adds node IDs contained in node_ids argument that was not part of the previous Round, and discards node_ids that do not belong to the current Round anymore.</p> <p>Parameters:</p> Name Type Description Default <code>all_node_ids</code> <code>List[str]</code> <p>all possible nodes that can participate to the training.</p> required <code>resp</code> <code>optional</code> <p>latest Nodes replies. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedNodeStateAgentError</code> <p>raised if Nodes replies have a missing entry that needs to be collected.</p> Source code in <code>fedbiomed/researcher/node_state_agent.py</code> <pre><code>def update_node_states(self, all_node_ids: List[str], resp: Optional[Dict] = None):\n    \"\"\"Updates the state_id collection with respect to current nodes and latest Nodes replies.\n\n    Adds node IDs contained in node_ids argument that was not part of the previous Round, and discards node_ids that\n    do not belong to the current Round anymore.\n\n    Args:\n        all_node_ids: all possible nodes that can participate to the training.\n        resp (optional): latest Nodes replies. Defaults to None.\n\n    Raises:\n        FedbiomedNodeStateAgentError: raised if Nodes replies have a missing entry that needs to be collected.\n    \"\"\"\n    # first, we update _collection_state_id wrt new FederatedDataset (if it has been modified)\n    self._update_collection_state_ids(all_node_ids)\n    if resp is not None:\n        for node_reply in resp.values():\n            # adds Node replies\n            try:\n                node_id, state_id = node_reply['node_id'], node_reply['state_id']\n            except KeyError as ke:\n                raise FedbiomedNodeStateAgentError(f\"{ErrorNumbers.FB419.value}: Missing entry in Response\") from ke\n            if node_id in self._collection_state_ids:\n                self._collection_state_ids[node_id] = state_id\n</code></pre>"},{"location":"developer/api/researcher/requests/","title":"Requests","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests-classes","title":"Classes","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.DiscardOnTimeout","title":"DiscardOnTimeout","text":"<pre><code>DiscardOnTimeout(timeout=5, nodes=None)\n</code></pre> <p>               Bases: <code>_ReplyTimeoutPolicy</code></p> <p>Discards request that do not answer in given timeout</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>maximum time for policy</p> <code>5</code> <code>nodes</code> <code>Optional[List[str]]</code> <p>optional list of nodes to apply the policy. By default applies to all known nodes of request.</p> <code>None</code> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def __init__(self, timeout: int = 5, nodes: Optional[List[str]] = None) -&gt; None:\n    \"\"\"Implements timeout attributes\n\n    Args:\n        timeout: maximum time for policy\n        nodes: optional list of nodes to apply the policy. By default applies to all known nodes of request.\n    \"\"\"\n    super().__init__(nodes)\n    self.timeout = timeout\n    self._time = None\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.DiscardOnTimeout-functions","title":"Functions","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.DiscardOnTimeout.continue_","title":"continue_","text":"<pre><code>continue_(requests)\n</code></pre> <p>Discards requests that reach timeout, always continue</p> <p>Returns:</p> Type Description <code>PolicyStatus</code> <p>CONTINUE</p> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def continue_(self, requests: TRequest) -&gt; PolicyStatus:\n    \"\"\"Discards requests that reach timeout, always continue\n\n    Returns:\n        CONTINUE\n    \"\"\"\n    return self.apply(requests, False)\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.FederatedRequest","title":"FederatedRequest","text":"<pre><code>FederatedRequest(message, nodes, policy=None)\n</code></pre> <p>Dispatches federated requests</p> <p>This class has been design to be send a request and wait until a response is received</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Union[Message, MessagesByNode]</code> <p>either a common <code>Message</code> to send to nodes or a dict of distinct message per node indexed by the node ID</p> required <code>nodes</code> <code>List[NodeAgent]</code> <p>list of nodes that are sent the message</p> required <code>policy</code> <code>Optional[List[RequestPolicy]]</code> <p>list of policies for controlling the handling of the request</p> <code>None</code> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def __init__(\n    self,\n    message: Union[Message, MessagesByNode],\n    nodes: List[NodeAgent],\n    policy: Optional[List[RequestPolicy]] = None\n):\n    \"\"\"Constructor of the class.\n\n    Args:\n        message: either a common `Message` to send to nodes or a dict of distinct message per node\n            indexed by the node ID\n        nodes: list of nodes that are sent the message\n        policy: list of policies for controlling the handling of the request\n    \"\"\"\n\n    self._message = message\n    self._nodes = nodes\n    self._requests = []\n    self._request_id = REQUEST_PREFIX + str(uuid.uuid4())\n    self._nodes_status = {}\n\n    self._pending_replies = threading.Semaphore(value=0)\n\n    # Set-up policies\n    self._policy = PolicyController(policy)\n\n    # Set up single requests\n    if isinstance(self._message, Message):\n        for node in self._nodes:\n            self._requests.append(\n                Request(self._message, node, self._pending_replies, self._request_id)\n            )\n\n    # Different message for each node\n    elif isinstance(self._message, MessagesByNode):\n        for node in self._nodes:\n            if m := self._message.get(node.id):\n                self._requests.append(\n                    Request(m, node, self._pending_replies, self._request_id)\n                )\n            else:\n                logger.warning(f\"Node {node.id} is unknown. Send message to others, ignore this one.\")\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.FederatedRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.FederatedRequest.policy","title":"policy  <code>property</code>","text":"<pre><code>policy\n</code></pre> <p>Returns policy controller</p>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.FederatedRequest.requests","title":"requests  <code>property</code>","text":"<pre><code>requests\n</code></pre> <p>Returns requests</p>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.FederatedRequest-functions","title":"Functions","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.FederatedRequest.disconnected_requests","title":"disconnected_requests","text":"<pre><code>disconnected_requests()\n</code></pre> <p>Returns the requests to disconnected nodes</p> <p>Returns:</p> Type Description <code>List[Message]</code> <p>A list of request <code>Message</code> sent to disconnected nodes</p> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def disconnected_requests(self) -&gt; List[Message]:\n    \"\"\"Returns the requests to disconnected nodes\n\n    Returns:\n        A list of request `Message` sent to disconnected nodes\n    \"\"\"\n    return [req for req in self._requests if req.status == RequestStatus.DISCONNECT]\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.FederatedRequest.errors","title":"errors","text":"<pre><code>errors()\n</code></pre> <p>Returns errors of each request</p> <p>Returns:</p> Type Description <code>Dict[str, ErrorMessage]</code> <p>A dict of error <code>Message</code> received for this request, indexed by node ID</p> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def errors(self) -&gt; Dict[str, ErrorMessage]:\n    \"\"\"Returns errors of each request\n\n    Returns:\n        A dict of error `Message` received for this request, indexed by node ID\n    \"\"\"\n\n    return {req.node.id: req.error for req in self._requests if req.error}\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.FederatedRequest.replies","title":"replies","text":"<pre><code>replies()\n</code></pre> <p>Returns replies of each request</p> <p>Returns:</p> Type Description <code>Dict[str, Message]</code> <p>A dict of replies <code>Message</code> received for this request, indexed by node ID</p> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def replies(self) -&gt; Dict[str, Message]:\n    \"\"\"Returns replies of each request\n\n    Returns:\n        A dict of replies `Message` received for this request, indexed by node ID\n    \"\"\"\n\n    return {req.node.id: req.reply for req in self._requests if req.reply}\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.FederatedRequest.send","title":"send","text":"<pre><code>send()\n</code></pre> <p>Sends federated request</p> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def send(self) -&gt; None:\n    \"\"\"Sends federated request\"\"\"\n    for req in self._requests:\n        req.send()\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.FederatedRequest.wait","title":"wait","text":"<pre><code>wait()\n</code></pre> <p>Waits for the replies of the messages that are sent</p> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def wait(self) -&gt; None:\n    \"\"\"Waits for the replies of the messages that are sent\"\"\"\n\n    while self._policy.continue_all(self._requests) == PolicyStatus.CONTINUE:\n        self._pending_replies.acquire(timeout=REQUEST_STATUS_CHECK_TIMEOUT)\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.MessagesByNode","title":"MessagesByNode","text":"<p>               Bases: <code>dict</code></p> <p>Type to defined messages by node</p>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.PolicyController","title":"PolicyController","text":"<pre><code>PolicyController(policies=None)\n</code></pre> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def __init__(\n    self,\n    policies: Optional[List[RequestPolicy]] = None,\n):\n\n    policies = policies or []\n    policies.insert(0, RequestPolicy())\n    self._policies = policies\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.PolicyController-functions","title":"Functions","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.PolicyController.continue_all","title":"continue_all","text":"<pre><code>continue_all(requests)\n</code></pre> <p>Checks if all policies indicate to continue.</p> <p>Parameters:</p> Name Type Description Default <code>requests</code> <code>List[TRequest]</code> <p>List of Request objects to check against policies</p> required <p>Returns:</p> Type Description <code>PolicyStatus</code> <p>CONTINUE if all policies indicates to continue</p> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def continue_all(self, requests: List[TRequest]) -&gt; PolicyStatus:\n    \"\"\"Checks if all policies indicate to continue.\n\n    Args:\n        requests: List of [Request][fedbiomed.researcher.requests.Request] objects to\n            check against policies\n\n    Returns:\n        CONTINUE if all policies indicates to continue\n    \"\"\"\n\n    if not requests:\n        return False\n\n    status = all(\n        [policy.continue_(requests=requests) == PolicyStatus.CONTINUE\n            for policy in self._policies]\n    )\n\n    return PolicyStatus.CONTINUE if status else PolicyStatus.COMPLETED\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.PolicyController.has_stopped_any","title":"has_stopped_any","text":"<pre><code>has_stopped_any()\n</code></pre> <p>Checks if any of the policies indicates to stop</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if request has stopped due to given strategy</p> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def has_stopped_any(self) -&gt; bool:\n    \"\"\"Checks if any of the policies indicates to stop\n\n    Returns:\n        True if request has stopped due to given strategy\n    \"\"\"\n\n    is_stopped = any(\n        [policy.status == PolicyStatus.STOPPED for policy in self._policies]\n    )\n\n    return is_stopped\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.PolicyController.report","title":"report","text":"<pre><code>report()\n</code></pre> <p>Reports strategy stop status</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict of policies stopped, indexed by the node ID that caused the stop</p> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def report(self) -&gt; Dict[str, str]:\n    \"\"\"Reports strategy stop status\n\n    Returns:\n        Dict of policies stopped, indexed by the node ID that caused the stop\n    \"\"\"\n    report = {}\n    for st in self._policies:\n        if st.status == PolicyStatus.STOPPED:\n            report.update({st.stop_caused_by.node.id : st.__class__.__name__})\n\n    return report\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.PolicyStatus","title":"PolicyStatus","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.PolicyStatus-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.PolicyStatus.COMPLETED","title":"COMPLETED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>COMPLETED = 'COMPLETED'\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.PolicyStatus.CONTINUE","title":"CONTINUE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CONTINUE = 'CONTINUE'\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.PolicyStatus.STOPPED","title":"STOPPED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STOPPED = 'STOPPED'\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Request","title":"Request","text":"<pre><code>Request(message, node, sem_pending, request_id=None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to send to the node</p> required <code>node</code> <code>NodeAgent</code> <p>Node agent</p> required <code>request_id</code> <code>Optional[str]</code> <p>unique ID of request</p> <code>None</code> <code>sem_pending</code> <code>Semaphore</code> <p>semaphore for signaling new pending reply</p> required Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def __init__(\n    self,\n    message: Message,\n    node: NodeAgent,\n    sem_pending: threading.Semaphore,\n    request_id: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Single request for node\n\n    Args:\n        message: Message to send to the node\n        node: Node agent\n        request_id: unique ID of request\n        sem_pending: semaphore for signaling new pending reply\n    \"\"\"\n    self._request_id = request_id if request_id else str(uuid.uuid4())\n    self._node = node\n    self._message = message\n\n    self._sem_pending = sem_pending\n\n    self.reply = None\n    self.error = None\n    self.status = None\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Request-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Request.error","title":"error  <code>instance-attribute</code>","text":"<pre><code>error = None\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Request.node","title":"node  <code>property</code>","text":"<pre><code>node\n</code></pre> <p>Returns node agent</p>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Request.reply","title":"reply  <code>instance-attribute</code>","text":"<pre><code>reply = None\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Request.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status = None\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Request-functions","title":"Functions","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Request.flush","title":"flush","text":"<pre><code>flush(stopped)\n</code></pre> <p>Flushes the reply that has been processed</p> <p>Parameters:</p> Name Type Description Default <code>stopped</code> <code>bool</code> <p>True if the request was stopped before completion</p> required Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def flush(self, stopped: bool) -&gt; None:\n    \"\"\"Flushes the reply that has been processed\n\n    Args:\n        stopped: True if the request was stopped before completion\n    \"\"\"\n    self._node.flush(self._request_id, stopped)\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Request.has_finished","title":"has_finished","text":"<pre><code>has_finished()\n</code></pre> <p>Queries if the request has finished.</p> <p>Also tracks if node has disconnected.</p> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def has_finished(self) -&gt; bool:\n    \"\"\"Queries if the request has finished.\n\n    Also tracks if node has disconnected.\n\n    Args:\n        True if a reply was received from node\n    \"\"\"\n\n    if self._node.status == NodeActiveStatus.DISCONNECTED:\n        self.status = RequestStatus.DISCONNECT\n        return True  # Don't expect any reply\n\n    return True if self.reply or self.error else False\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Request.on_reply","title":"on_reply","text":"<pre><code>on_reply(reply)\n</code></pre> <p>Callback for node agent to execute once it replies.</p> <p>Parameters:</p> Name Type Description Default <code>reply</code> <code>Message</code> <p>reply message received from node</p> required Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def on_reply(self, reply: Message) -&gt; None:\n    \"\"\"Callback for node agent to execute once it replies.\n\n    Args:\n        reply: reply message received from node\n    \"\"\"\n\n    if isinstance(reply, ErrorMessage):\n        self.error = reply\n        self.status = RequestStatus.ERROR\n    else:\n        self.reply = reply\n        self.status = RequestStatus.SUCCESS\n\n    self._sem_pending.release()\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Request.send","title":"send","text":"<pre><code>send()\n</code></pre> <p>Sends the request</p> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def send(self) -&gt; None:\n    \"\"\"Sends the request\"\"\"\n    self._message.request_id = self._request_id\n    self._node.send(self._message, self.on_reply)\n    self.status = RequestStatus.NO_REPLY_YET\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestPolicy","title":"RequestPolicy","text":"<pre><code>RequestPolicy(nodes=None)\n</code></pre> <p>Base strategy to collect replies from remote agents</p> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def __init__(self, nodes: Optional[List[str]] = None):\n    self.status = None\n    self._nodes = nodes\n    self.stop_caused_by = None\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestPolicy-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestPolicy.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status = None\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestPolicy.stop_caused_by","title":"stop_caused_by  <code>instance-attribute</code>","text":"<pre><code>stop_caused_by = None\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestPolicy-functions","title":"Functions","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestPolicy.completed","title":"completed","text":"<pre><code>completed()\n</code></pre> <p>Updates status of strategy as completed without any issue</p> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def completed(self) -&gt; PolicyStatus:\n    \"\"\"Updates status of strategy as completed without any issue\"\"\"\n    self.status = PolicyStatus.COMPLETED\n\n    return PolicyStatus.COMPLETED\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestPolicy.continue_","title":"continue_","text":"<pre><code>continue_(requests)\n</code></pre> <p>Default strategy stops collecting result once all nodes has answered</p> <p>Returns:</p> Type Description <code>PolicyStatus</code> <p>False stops the iteration</p> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def continue_(self, requests) -&gt; PolicyStatus:\n    \"\"\"Default strategy stops collecting result once all nodes has answered\n\n    Returns:\n        False stops the iteration\n    \"\"\"\n\n    has_finished = all([req.has_finished() for req in requests])\n    return self.keep() if not has_finished else self.completed()\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestPolicy.keep","title":"keep","text":"<pre><code>keep()\n</code></pre> <p>Keeps continue collecting replies from nodes</p> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def keep(self) -&gt; PolicyStatus:\n    \"\"\"Keeps continue collecting replies from nodes\"\"\"\n    self.status = PolicyStatus.CONTINUE\n\n    return PolicyStatus.CONTINUE\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestPolicy.stop","title":"stop","text":"<pre><code>stop(req)\n</code></pre> <p>Stop sign for strategy</p> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def stop(self, req) -&gt; PolicyStatus:\n    \"\"\"Stop sign for strategy\"\"\"\n    self.status = PolicyStatus.STOPPED\n    self.stop_caused_by = req\n\n    return PolicyStatus.STOPPED\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestStatus","title":"RequestStatus","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestStatus-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestStatus.DISCONNECT","title":"DISCONNECT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DISCONNECT = 'DISCONNECT'\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestStatus.ERROR","title":"ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR = 'ERROR'\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestStatus.NO_REPLY_YET","title":"NO_REPLY_YET  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NO_REPLY_YET = 'NO_REPLY_YET'\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestStatus.SUCCESS","title":"SUCCESS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SUCCESS = 'SUCCESS'\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.RequestStatus.TIMEOUT","title":"TIMEOUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TIMEOUT = 'TIMEOUT'\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests","title":"Requests","text":"<pre><code>Requests(config)\n</code></pre> <p>Manages communication between researcher and nodes.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ResearcherConfig</code> <p>Object for handling the component configuration</p> required Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def __init__(\n    self,\n    config: ResearcherConfig,\n):\n    \"\"\"Constructor of the class\n\n    Args:\n        config: Object for handling the component configuration\n    \"\"\"\n    self._monitor_message_callback = None\n\n    server_host=config.get('server', 'host')\n    server_port=config.get('server', 'port')\n    cert_priv=os.path.join(\n            config.root, CONFIG_FOLDER_NAME, config.get('certificate', 'private_key')\n    )\n    cert_pub=os.path.join(\n            config.root, CONFIG_FOLDER_NAME, config.get('certificate', 'public_key')\n    )\n\n    # Creates grpc server and starts it\n    self._researcher_id = config.get(\"default\", \"id\")\n    self._grpc_server = GrpcServer(\n        host=server_host,\n        port=server_port,\n        on_message=self.on_message,\n        ssl=SSLCredentials(\n            key=cert_priv,\n            cert=cert_pub)\n\n    )\n    self.start_messaging()\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests-functions","title":"Functions","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.add_monitor_callback","title":"add_monitor_callback","text":"<pre><code>add_monitor_callback(callback)\n</code></pre> <p>Adds callback function for monitor messages</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[Dict], None]</code> <p>Callback function for handling monitor messages that come due 'general/monitoring' channel</p> required Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def add_monitor_callback(self, callback: Callable[[Dict], None]):\n    \"\"\" Adds callback function for monitor messages\n\n    Args:\n        callback: Callback function for handling monitor messages that come due 'general/monitoring' channel\n    \"\"\"\n\n    self._monitor_message_callback = callback\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.list","title":"list","text":"<pre><code>list(nodes=None, verbose=False)\n</code></pre> <p>Lists available data in each node</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Optional[list]</code> <p>optionally filter nodes with this list. Default is None, no filtering, consider all nodes</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If it is true it prints datasets in readable format</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dict with node_id as keys, and list of dicts describing available data as values</p> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def list(self, nodes: Optional[list] = None, verbose: bool = False) -&gt; dict:\n    \"\"\"Lists available data in each node\n\n    Args:\n        nodes: optionally filter nodes with this list. Default is None, no filtering, consider all nodes\n        verbose: If it is true it prints datasets in readable format\n\n    Returns:\n        A dict with node_id as keys, and list of dicts describing available data as values\n    \"\"\"\n\n    message = ListRequest(researcher_id=self._researcher_id)\n\n    data_found = {}\n    with self.send(message, nodes, policies=[DiscardOnTimeout(5)]) as federated_req:\n        for node_id, reply in federated_req.replies().items():\n            data_found[node_id] = reply.databases\n\n    if verbose:\n        for node in data_found:\n            if len(data_found[node]) &gt; 0:\n                rows = [row.values() for row in data_found[node]]\n                headers = data_found[node][0].keys()\n                info = '\\n Node: {} | Number of Datasets: {} \\n'.format(node, len(data_found[node]))\n                logger.info(info + tabulate.tabulate(rows, headers, tablefmt=\"grid\") + '\\n')\n            else:\n                logger.info('\\n Node: {} | Number of Datasets: {}'.format(node, len(data_found[node])) +\n                            \" No data has been set up for this node.\")\n\n    return data_found\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.on_message","title":"on_message","text":"<pre><code>on_message(msg, type_)\n</code></pre> <p>Handles arbitrary messages received from the remote agents</p> <p>This callback is only used for feedback messages from nodes (logs, experiment monitor), not for node replies to requests.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Union[Dict[str, Any], Message]</code> <p>de-serialized msg</p> required <code>type_</code> <code>MessageType</code> <p>Reply type one of reply, log, scalar</p> required Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def on_message(self, msg: Union[Dict[str, Any], Message], type_: MessageType) -&gt; None:\n    \"\"\"Handles arbitrary messages received from the remote agents\n\n    This callback is only used for feedback messages from nodes (logs, experiment\n    monitor), not for node replies to requests.\n\n    Args:\n        msg: de-serialized msg\n        type_: Reply type one of reply, log, scalar\n    \"\"\"\n\n    if type_ == MessageType.LOG:\n        # forward the treatment to node_log_handling() (same thread)\n        self.print_node_log_message(msg.get_dict())\n\n    elif type_ == MessageType.SCALAR:\n        if self._monitor_message_callback is not None:\n            # Pass message to Monitor's on message handler\n            self._monitor_message_callback(msg.get_dict())\n    else:\n        logger.error(f\"Undefined message type received  {type_} - IGNORING\")\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.ping_nodes","title":"ping_nodes","text":"<pre><code>ping_nodes()\n</code></pre> <p>Pings online nodes</p> <p>Returns:</p> Type Description <code>list</code> <p>List of ID of up and running nodes</p> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def ping_nodes(self) -&gt; list:\n    \"\"\" Pings online nodes\n\n    Returns:\n        List of ID of up and running nodes\n    \"\"\"\n    ping = PingRequest(researcher_id=self._researcher_id)\n    with self.send(ping, policies=[DiscardOnTimeout(5)]) as federated_req:\n        nodes_online = [node_id for node_id, reply in federated_req.replies().items()]\n\n    return nodes_online\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.print_node_log_message","title":"print_node_log_message  <code>staticmethod</code>","text":"<pre><code>print_node_log_message(log)\n</code></pre> <p>Prints logger messages coming from the node</p> <p>It is run on the communication process and must be as quick as possible: - all logs (coming from the nodes) are forwarded to the researcher logger (immediate display on console/file/whatever)</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>Dict[str, Any]</code> <p>log message and its metadata</p> required Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>@staticmethod\ndef print_node_log_message(log: Dict[str, Any]) -&gt; None:\n    \"\"\"Prints logger messages coming from the node\n\n    It is run on the communication process and must be as quick as possible:\n    - all logs (coming from the nodes) are forwarded to the researcher logger\n    (immediate display on console/file/whatever)\n\n    Args:\n        log: log message and its metadata\n    \"\"\"\n\n    # log contains the original message sent by the node\n    # FIXME: we should use `fedbiomed.common.json.deserialize` method\n    # instead of the json method when extracting json message\n    original_msg = json.loads(log[\"msg\"])\n\n    # Loging fancy feedback for training\n    logger.info(\"\\033[1m{}\\033[0m\\n\"\n                \"\\t\\t\\t\\t\\t\\033[1m NODE\\033[0m {}\\n\"\n                \"\\t\\t\\t\\t\\t\\033[1m MESSAGE:\\033[0m {}\\033[0m\\n\"\n                \"{}\".format(log[\"level\"],\n                            log[\"node_id\"],\n                            original_msg[\"message\"],\n                            5 * \"-------------\"))\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.remove_monitor_callback","title":"remove_monitor_callback","text":"<pre><code>remove_monitor_callback()\n</code></pre> <p>Removes callback function for Monitor class.</p> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def remove_monitor_callback(self):\n    \"\"\" Removes callback function for Monitor class. \"\"\"\n\n    self._monitor_message_callback = None\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.search","title":"search","text":"<pre><code>search(tags, nodes=None)\n</code></pre> <p>Searches available data by tags</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>List[str]</code> <p>List containing tags associated to the data researcher is looking for.</p> required <code>nodes</code> <code>Optional[list]</code> <p>optionally filter nodes with this list. Default is None, no filtering, consider all nodes</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dict with node_id as keys, and list of dicts describing available data as values</p> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def search(self, tags: List[str], nodes: Optional[list] = None) -&gt; dict:\n    \"\"\"Searches available data by tags\n\n    Args:\n        tags: List containing tags associated to the data researcher is looking for.\n        nodes: optionally filter nodes with this list. Default is None, no filtering, consider all nodes\n\n    Returns:\n        A dict with node_id as keys, and list of dicts describing available data as values\n    \"\"\"\n    message = SearchRequest(\n        researcher_id=self._researcher_id,\n        tags=tags,\n    )\n\n    data_found = {}\n    with self.send(message, nodes, policies=[DiscardOnTimeout(5)]) as federated_req:\n\n        for node_id, reply in federated_req.replies().items():\n            if reply.databases:\n                data_found[node_id] = reply.databases\n                logger.info('Node selected for training -&gt; {}'.format(reply.node_id))\n\n        for node_id, error in federated_req.errors().items():\n            logger.warning(f\"Node {node_id} has returned error from search request {error.extra_msg}\")\n\n\n        if not data_found:\n            logger.info(\"No available dataset has found in nodes with tags: {}\".format(tags))\n\n    return data_found\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.send","title":"send","text":"<pre><code>send(message, nodes=None, policies=None)\n</code></pre> <p>Sends federated request to given nodes with given message</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Union[Message, MessagesByNode]</code> <p>either a common <code>Message</code> to send to nodes or a dict of distinct message per node indexed by the node ID</p> required <code>nodes</code> <code>Optional[List[str]]</code> <p>list of nodes that are sent the message. If None, send the message to all known active nodes.</p> <code>None</code> <code>policy</code> <p>list of policies for controlling the handling of the request, or None</p> required <p>Returns:</p> Type Description <code>FederatedRequest</code> <p>The object for handling the communications for this request</p> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def send(\n        self,\n        message: Union[Message, MessagesByNode],\n        nodes: Optional[List[str]] = None,\n        policies: List[RequestPolicy] = None\n) -&gt; FederatedRequest:\n    \"\"\"Sends federated request to given nodes with given message\n\n    Args:\n        message: either a common `Message` to send to nodes or a dict of distinct message per node\n            indexed by the node ID\n        nodes: list of nodes that are sent the message. If None, send the message to all known active nodes.\n        policy: list of policies for controlling the handling of the request, or None\n\n    Returns:\n        The object for handling the communications for this request\n    \"\"\"\n\n    if nodes is not None:\n        nodes = [self._grpc_server.get_node(node) for node in nodes]\n    else:\n        nodes = self._grpc_server.get_all_nodes()\n\n    return FederatedRequest(message, nodes, policies)\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.start_messaging","title":"start_messaging","text":"<pre><code>start_messaging()\n</code></pre> <p>Start communications endpoint</p> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def start_messaging(self) -&gt; None:\n    \"\"\"Start communications endpoint\n    \"\"\"\n    self._grpc_server.start()\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.training_plan_approve","title":"training_plan_approve","text":"<pre><code>training_plan_approve(training_plan, description='no description provided', nodes=None, policies=None)\n</code></pre> <p>Send a training plan and a ApprovalRequest message to node(s).</p> <p>If a list of node id(s) is provided, the message will be individually sent to all nodes of the list. If the node id(s) list is None (default), the message is broadcast to all nodes.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan</code> <code>BaseTrainingPlan</code> <p>the training plan class to send to the nodes for approval.</p> required <code>description</code> <code>str</code> <p>Description of training plan approval request</p> <code>'no description provided'</code> <code>nodes</code> <code>Optional[List[str]]</code> <p>list of nodes (specified by their UUID)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>a dictionary of pairs (node_id: status), where status indicates to the researcher</p> <code>dict</code> <p>that the training plan has been correctly downloaded on the node side.</p> <code>Warning</code> <code>dict</code> <p>status does not mean that the training plan is approved, only that it has been added</p> <code>dict</code> <p>to the \"approval queue\" on the node side.</p> Source code in <code>fedbiomed/researcher/requests/_requests.py</code> <pre><code>def training_plan_approve(\n        self,\n        training_plan: BaseTrainingPlan,\n        description: str = \"no description provided\",\n        nodes: Optional[List[str]] = None,\n        policies: Optional[List] = None\n) -&gt; dict:\n    \"\"\"Send a training plan and a ApprovalRequest message to node(s).\n\n    If a list of node id(s) is provided, the message will be individually sent\n    to all nodes of the list.\n    If the node id(s) list is None (default), the message is broadcast to all nodes.\n\n    Args:\n        training_plan: the training plan class to send to the nodes for approval.\n        description: Description of training plan approval request\n        nodes: list of nodes (specified by their UUID)\n\n    Returns:\n        a dictionary of pairs (node_id: status), where status indicates to the researcher\n        that the training plan has been correctly downloaded on the node side.\n        Warning: status does not mean that the training plan is approved, only that it has been added\n        to the \"approval queue\" on the node side.\n    \"\"\"\n\n    training_plan_instance = training_plan\n    training_plan_module = 'model_' + str(uuid.uuid4())\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        training_plan_file = os.path.join(tmp_dir, training_plan_module + '.py')\n        try:\n            training_plan_instance.save_code(training_plan_file)\n        except Exception as e:\n            logger.error(f\"Cannot save the training plan to a local tmp dir : {e}\")\n            return {}\n\n        try:\n            _, training_plan_instance = import_class_object_from_file(\n                training_plan_file, training_plan.__class__.__name__)\n            tp_source = training_plan_instance.source()\n        except Exception as e:\n            logger.error(f\"Cannot instantiate the training plan: {e}\")\n            return {}\n\n    try:\n        minify(tp_source,\n               remove_annotations=False,\n               combine_imports=False,\n               remove_pass=False,\n               hoist_literals=False,\n               remove_object_base=True,\n               rename_locals=False)\n    except Exception as e:\n        # minify does not provide any specific exception\n        logger.error(f\"This file is not a python file ({e})\")\n        return {}\n    print(tp_source)\n    # send message to node(s)\n    message = ApprovalRequest(\n        researcher_id=self._researcher_id,\n        description=str(description),\n        training_plan=tp_source)\n\n    with self.send(message, nodes, policies=policies) as federated_req:\n        errors = federated_req.errors()\n        replies = federated_req.replies()\n        results = {req.node.id: False for req in federated_req.requests}\n\n        # TODO: Loop over errors and replies\n        for node_id, error in errors.items():\n            logger.info(f\"Node ({node_id}) has returned error {error.errnum}, {error.extra_msg}\")\n\n    return {id: rep.get_dict() for id, rep in replies.items()}\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.StopOnDisconnect","title":"StopOnDisconnect","text":"<pre><code>StopOnDisconnect(timeout=5, nodes=None)\n</code></pre> <p>               Bases: <code>_ReplyTimeoutPolicy</code></p> <p>Stops collecting results if a node disconnects</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>maximum time for policy</p> <code>5</code> <code>nodes</code> <code>Optional[List[str]]</code> <p>optional list of nodes to apply the policy. By default applies to all known nodes of request.</p> <code>None</code> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def __init__(self, timeout: int = 5, nodes: Optional[List[str]] = None) -&gt; None:\n    \"\"\"Implements timeout attributes\n\n    Args:\n        timeout: maximum time for policy\n        nodes: optional list of nodes to apply the policy. By default applies to all known nodes of request.\n    \"\"\"\n    super().__init__(nodes)\n    self.timeout = timeout\n    self._time = None\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.StopOnDisconnect-functions","title":"Functions","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.StopOnDisconnect.continue_","title":"continue_","text":"<pre><code>continue_(requests)\n</code></pre> <p>Continues federated request if nodes are not disconnect</p> <p>Returns:</p> Type Description <code>PolicyStatus</code> <p>CONTINUE if no node disconnect found, STOPPED if some node disconnect found and timeout is reached</p> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def continue_(self, requests: TRequest) -&gt; PolicyStatus:\n    \"\"\"Continues federated request if nodes are not disconnect\n\n    Returns:\n        CONTINUE if no node disconnect found, STOPPED if some node disconnect found\n            and timeout is reached\n    \"\"\"\n\n    if self._nodes:\n        requests = [req for req in requests if req.node.id in self._nodes]\n\n    for req in requests:\n        is_timeout = self.is_timeout()\n        if req.status == RequestStatus.DISCONNECT and is_timeout:\n            return self.stop(req)\n\n    return PolicyStatus.CONTINUE\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.StopOnError","title":"StopOnError","text":"<pre><code>StopOnError(nodes=None)\n</code></pre> <p>               Bases: <code>RequestPolicy</code></p> <p>Stops collecting results if a node returns an error</p> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def __init__(self, nodes: Optional[List[str]] = None):\n    self.status = None\n    self._nodes = nodes\n    self.stop_caused_by = None\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.StopOnError-functions","title":"Functions","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.StopOnError.continue_","title":"continue_","text":"<pre><code>continue_(requests)\n</code></pre> <p>Continues federated request if nodes does not return error</p> <p>Returns:</p> Type Description <code>PolicyStatus</code> <p>CONTINUE if no error found, STOPPED if some error found</p> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def continue_(self, requests: TRequest) -&gt; PolicyStatus:\n    \"\"\"Continues federated request if nodes does not return error\n\n    Returns:\n        CONTINUE if no error found, STOPPED if some error found\n    \"\"\"\n\n    if self._nodes:\n        requests = [req for req in requests if req.node.id in self._nodes]\n\n    for req in requests:\n        if req.error:\n            return self.stop(req)\n\n    return PolicyStatus.CONTINUE\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.StopOnTimeout","title":"StopOnTimeout","text":"<pre><code>StopOnTimeout(timeout=5, nodes=None)\n</code></pre> <p>               Bases: <code>_ReplyTimeoutPolicy</code></p> <p>Stops the request if nodes do not answer in given timeout</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>maximum time for policy</p> <code>5</code> <code>nodes</code> <code>Optional[List[str]]</code> <p>optional list of nodes to apply the policy. By default applies to all known nodes of request.</p> <code>None</code> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def __init__(self, timeout: int = 5, nodes: Optional[List[str]] = None) -&gt; None:\n    \"\"\"Implements timeout attributes\n\n    Args:\n        timeout: maximum time for policy\n        nodes: optional list of nodes to apply the policy. By default applies to all known nodes of request.\n    \"\"\"\n    super().__init__(nodes)\n    self.timeout = timeout\n    self._time = None\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.StopOnTimeout-functions","title":"Functions","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.StopOnTimeout.continue_","title":"continue_","text":"<pre><code>continue_(requests)\n</code></pre> <p>Continues federated request if nodes dont reach timeout</p> <p>Returns:</p> Type Description <code>PolicyStatus</code> <p>CONTINUE if no node reached timeout, STOPPED if some node reached timeout and timeout is reached</p> Source code in <code>fedbiomed/researcher/requests/_policies.py</code> <pre><code>def continue_(self, requests) -&gt; PolicyStatus:\n    \"\"\"Continues federated request if nodes dont reach timeout\n\n    Returns:\n        CONTINUE if no node reached timeout, STOPPED if some node reached timeout\n            and timeout is reached\n    \"\"\"\n    return self.apply(requests, True)\n</code></pre>"},{"location":"developer/api/researcher/secagg/","title":"Secagg","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg-classes","title":"Classes","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.JoyeLibertSecureAggregation","title":"JoyeLibertSecureAggregation","text":"<pre><code>JoyeLibertSecureAggregation(active=True, clipping_range=None)\n</code></pre> <p>               Bases: <code>_SecureAggregation</code></p> <p>Secure aggregation controller of researcher component for Joye-Libert.</p> <p>This class is responsible for;</p> <ul> <li>setting up the context for Joye-Libert secure aggregation</li> <li>Applying secure aggregation after receiving encrypted model parameters from nodes</li> </ul> <p>Attributes:</p> Name Type Description <code>_servkey</code> <code>SecaggServkeyContext | None</code> <p>Server-key context setup instance.</p> <p>Assigns default values for attributes</p> <p>Parameters:</p> Name Type Description Default <code>active</code> <code>bool</code> <p>True if secure aggregation is activated for the experiment</p> <code>True</code> <code>clipping_range</code> <code>Union[None, int]</code> <p>Clipping range that will be used for quantization of model parameters on the node side. The default will be <code>VEParameters.CLIPPING_RANGE</code>. The default value will be automatically set on the node side.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedSecureAggregationError</code> <p>bad argument type</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def __init__(\n    self,\n    active: bool = True,\n    clipping_range: Union[None, int] = None,\n) -&gt; None:\n    \"\"\"Class constructor\n\n    Assigns default values for attributes\n\n    Args:\n        active: True if secure aggregation is activated for the experiment\n        clipping_range: Clipping range that will be used for quantization of model\n            parameters on the node side. The default will be\n            [`VEParameters.CLIPPING_RANGE`][fedbiomed.common.constants.VEParameters].\n            The default value will be automatically set on the node side.\n\n    Raises:\n        FedbiomedSecureAggregationError: bad argument type\n    \"\"\"\n    super().__init__(active, clipping_range)\n\n    self._servkey: SecaggServkeyContext | None = None\n    self._secagg_crypter: SecaggCrypter = SecaggCrypter()\n    self._scheme = SecureAggregationSchemes.JOYE_LIBERT\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.JoyeLibertSecureAggregation-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.JoyeLibertSecureAggregation.servkey","title":"servkey  <code>property</code>","text":"<pre><code>servkey\n</code></pre> <p>Gets servkey object</p> <p>Returns:</p> Type Description <code>Union[None, SecaggServkeyContext]</code> <p>Servkey object, None if servkey is not setup</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.JoyeLibertSecureAggregation-functions","title":"Functions","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.JoyeLibertSecureAggregation.aggregate","title":"aggregate","text":"<pre><code>aggregate(*args, round_, total_sample_size, model_params, encryption_factors={}, num_expected_params=1, **kwargs)\n</code></pre> <p>Aggregates given model parameters</p> <p>Parameters:</p> Name Type Description Default <code>round_</code> <code>int</code> <p>current training round number</p> required <code>total_sample_size</code> <code>int</code> <p>sum of number of samples used by all nodes</p> required <code>model_params</code> <code>Dict[str, List[int]]</code> <p>model parameters from the participating nodes</p> required <code>encryption_factors</code> <code>Dict[str, Union[List[int], None]]</code> <p>encryption factors from the participating nodes</p> <code>{}</code> <code>num_expected_params</code> <code>int</code> <p>number of decrypted parameters to decode from the model parameters</p> <code>1</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>Aggregated parameters</p> <p>Raises:</p> Type Description <code>FedbiomedSecureAggregationError</code> <p>secure aggregation context not properly configured</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def aggregate(\n    self,\n    *args,\n    round_: int,\n    total_sample_size: int,\n    model_params: Dict[str, List[int]],\n    encryption_factors: Dict[str, Union[List[int], None]] = {},\n    num_expected_params: int = 1,\n    **kwargs,\n) -&gt; List[float]:\n    \"\"\"Aggregates given model parameters\n\n    Args:\n        round_: current training round number\n        total_sample_size: sum of number of samples used by all nodes\n        model_params: model parameters from the participating nodes\n        encryption_factors: encryption factors from the participating nodes\n        num_expected_params: number of decrypted parameters to decode from the model parameters\n\n    Returns:\n        Aggregated parameters\n\n    Raises:\n        FedbiomedSecureAggregationError: secure aggregation context not properly configured\n    \"\"\"\n\n    if self._servkey is None:\n        raise FedbiomedSecureAggregationError(\n            f\"{ErrorNumbers.FB417.value}: Can not aggregate parameters, Servkey context is \"\n            f\"not configured. Please setup secure aggregation before the aggregation.\"\n        )\n\n    if not self._servkey.status:\n        raise FedbiomedSecureAggregationError(\n            f\"{ErrorNumbers.FB417.value}: Can not aggregate parameters, one of Biprime or Servkey context is \"\n            f\"not set properly\"\n        )\n\n    key = self._servkey.context[\"server_key\"]\n    biprime = self._servkey.context[\"biprime\"]\n    num_nodes = len(model_params)\n\n    aggregate = functools.partial(\n        self._secagg_crypter.aggregate,\n        current_round=round_,\n        num_nodes=num_nodes,\n        key=key,\n        total_sample_size=total_sample_size,\n        biprime=biprime,\n        clipping_range=self.clipping_range,\n    )\n\n    return self._aggregate(\n        model_params,\n        encryption_factors,\n        aggregate,\n        num_expected_params,\n    )\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.JoyeLibertSecureAggregation.load_state_breakpoint","title":"load_state_breakpoint  <code>classmethod</code>","text":"<pre><code>load_state_breakpoint(state)\n</code></pre> <p>Create a <code>SecureAggregation</code> object from a saved state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Dict</code> <p>saved state to restore in the created object</p> required <p>Returns:</p> Type Description <code>SecureAggregation</code> <p>The created <code>SecureAggregation</code> object</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>@classmethod\ndef load_state_breakpoint(cls, state: Dict) -&gt; \"SecureAggregation\":\n    \"\"\"Create a `SecureAggregation` object from a saved state\n\n    Args:\n        state: saved state to restore in the created object\n\n    Returns:\n        The created `SecureAggregation` object\n    \"\"\"\n    if state[\"attributes\"][\"_servkey\"] is not None:\n        state[\"attributes\"][\"_servkey\"] = (\n            SecaggServkeyContext.load_state_breakpoint(\n                state=state[\"attributes\"][\"_servkey\"]\n            )\n        )\n\n    return super().load_state_breakpoint(state)\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.JoyeLibertSecureAggregation.save_state_breakpoint","title":"save_state_breakpoint","text":"<pre><code>save_state_breakpoint()\n</code></pre> <p>Saves state of the secagg</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The secagg state to be saved</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def save_state_breakpoint(self) -&gt; Dict[str, Any]:\n    \"\"\"Saves state of the secagg\n\n    Returns:\n        The secagg state to be saved\n    \"\"\"\n    state = super().save_state_breakpoint()\n\n    state[\"attributes\"].update(\n        {\n            \"_servkey\": (\n                self._servkey.save_state_breakpoint()\n                if self._servkey is not None\n                else None\n            ),\n        }\n    )\n\n    return state\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.JoyeLibertSecureAggregation.setup","title":"setup","text":"<pre><code>setup(parties, experiment_id, researcher_id, force=False, insecure_validation=True)\n</code></pre> <p>Setup secure aggregation instruments.</p> <p>Requires setting <code>parties</code> and <code>experiment_id</code> if they are not set in previous secagg setups. It is possible to execute without any argument if SecureAggregation has already <code>parties</code> and <code>experiment_id</code> defined. This feature provides researcher execute <code>secagg.setup()</code> if any connection issu#e</p> <p>Parameters:</p> Name Type Description Default <code>parties</code> <code>List[str]</code> <p>Parties that participates secure aggregation</p> required <code>experiment_id</code> <code>str</code> <p>The id of the experiment</p> required <code>researcher_id</code> <code>str</code> <p>ID of the researcher that context will be created for.</p> required <code>force</code> <code>bool</code> <p>Forces secagg setup even context is already existing</p> <code>False</code> <code>insecure_validation</code> <code>bool</code> <p>True if the insecure mechanism for validation secagg data coherence is enabled</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>Status of setup</p> <p>Raises     FedbiomedSecureAggregationError: Invalid argument type</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def setup(\n    self,\n    parties: List[str],\n    experiment_id: str,\n    researcher_id: str,\n    force: bool = False,\n    insecure_validation: bool = True\n) -&gt; bool:\n    \"\"\"Setup secure aggregation instruments.\n\n    Requires setting `parties` and `experiment_id` if they are not set in previous secagg\n    setups. It is possible to execute without any argument if SecureAggregation\n    has already `parties` and `experiment_id` defined. This feature provides researcher\n    execute `secagg.setup()` if any connection issu#e\n\n    Args:\n        parties: Parties that participates secure aggregation\n        experiment_id: The id of the experiment\n        researcher_id: ID of the researcher that context will be created for.\n        force: Forces secagg setup even context is already existing\n        insecure_validation: True if the insecure mechanism for validation secagg data\n            coherence is enabled\n\n    Returns:\n        Status of setup\n\n    Raises\n        FedbiomedSecureAggregationError: Invalid argument type\n    \"\"\"\n    super().setup(parties, experiment_id, researcher_id, force, insecure_validation)\n\n    self._servkey = cast(SecaggServkeyContext, self._servkey)\n    if not self._servkey.status or force:\n        self._servkey.setup()\n\n    return True\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.JoyeLibertSecureAggregation.train_arguments","title":"train_arguments","text":"<pre><code>train_arguments()\n</code></pre> <p>Gets train arguments for secagg train request</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Arguments that are going to be attached to the experiment.</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def train_arguments(self) -&gt; Dict:\n    \"\"\"Gets train arguments for secagg train request\n\n    Returns:\n        Arguments that are going to be attached to the experiment.\n    \"\"\"\n    arguments = super().train_arguments()\n    arguments.update(\n        {\n            \"secagg_servkey_id\": (\n                self._servkey.secagg_id if self._servkey is not None else None\n            ),\n        }\n    )\n    return arguments\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.LomSecureAggregation","title":"LomSecureAggregation","text":"<pre><code>LomSecureAggregation(active=True, clipping_range=None)\n</code></pre> <p>               Bases: <code>_SecureAggregation</code></p> <p>Secure aggregation controller of researcher component for Low Overhead Masking.</p> <p>This class is responsible for;</p> <ul> <li>setting up the context for LOM secure aggregation</li> <li>Applying secure aggregation after receiving encrypted model parameters from nodes</li> </ul> <p>Attributes:</p> Name Type Description <code>_dh</code> <code>SecaggDHContext | None</code> <p>Diffie Hellman keypairs per node context setup instance.</p> <p>Assigns default values for attributes</p> <p>Parameters:</p> Name Type Description Default <code>active</code> <code>bool</code> <p>True if secure aggregation is activated for the experiment</p> <code>True</code> <code>clipping_range</code> <code>Union[None, int]</code> <p>Clipping range that will be used for quantization of model parameters on the node side. The default will be <code>VEParameters.CLIPPING_RANGE</code>. The default value will be automatically set on the node side.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedSecureAggregationError</code> <p>bad argument type</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def __init__(\n    self,\n    active: bool = True,\n    clipping_range: Union[None, int] = None,\n) -&gt; None:\n    \"\"\"Class constructor\n\n    Assigns default values for attributes\n\n    Args:\n        active: True if secure aggregation is activated for the experiment\n        clipping_range: Clipping range that will be used for quantization of model\n            parameters on the node side. The default will be\n            [`VEParameters.CLIPPING_RANGE`][fedbiomed.common.constants.VEParameters].\n            The default value will be automatically set on the node side.\n\n    Raises:\n        FedbiomedSecureAggregationError: bad argument type\n    \"\"\"\n    super().__init__(active, clipping_range)\n\n    self._dh: SecaggDHContext | None = None\n    self._secagg_crypter = SecaggLomCrypter()\n    self._scheme = SecureAggregationSchemes.LOM\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.LomSecureAggregation-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.LomSecureAggregation.dh","title":"dh  <code>property</code>","text":"<pre><code>dh\n</code></pre> <p>Gets Diffie Hellman keypairs object</p> <p>Returns:</p> Type Description <code>Union[None, SecaggDHContext]</code> <p>DH object, None if DH is not setup</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.LomSecureAggregation-functions","title":"Functions","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.LomSecureAggregation.aggregate","title":"aggregate","text":"<pre><code>aggregate(*args, model_params, total_sample_size, encryption_factors={}, **kwargs)\n</code></pre> <p>Aggregates given model parameters</p> <p>Parameters:</p> Name Type Description Default <code>total_sample_size</code> <code>int</code> <p>sum of number of samples used by all nodes</p> required <code>model_params</code> <code>Dict[str, List[int]]</code> <p>model parameters from the participating nodes</p> required <code>encryption_factors</code> <code>Dict[str, Union[List[int], None]]</code> <p>encryption factors from the participating nodes</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>Aggregated parameters</p> <p>Raises:</p> Type Description <code>FedbiomedSecureAggregationError</code> <p>secure aggregation context not properly configured</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def aggregate(\n    self,\n    *args,\n    model_params: Dict[str, List[int]],\n    total_sample_size: int,\n    encryption_factors: Dict[str, Union[List[int], None]] = {},\n    **kwargs,\n) -&gt; List[float]:\n    \"\"\"Aggregates given model parameters\n\n    Args:\n        total_sample_size: sum of number of samples used by all nodes\n        model_params: model parameters from the participating nodes\n        encryption_factors: encryption factors from the participating nodes\n\n    Returns:\n        Aggregated parameters\n\n    Raises:\n        FedbiomedSecureAggregationError: secure aggregation context not properly configured\n    \"\"\"\n\n    if self._dh is None:\n        raise FedbiomedSecureAggregationError(\n            f\"{ErrorNumbers.FB417.value}: Can not aggregate parameters, Diffie \"\n            \"Hellman context is not configured. Please setup secure aggregation \"\n            \"before the aggregation.\"\n        )\n\n    if not self._dh.status:\n        raise FedbiomedSecureAggregationError(\n            f\"{ErrorNumbers.FB417.value}: Can not aggregate parameters, Diffie \"\n            \"Hellman context is not set properly\"\n        )\n\n    aggregate = functools.partial(\n        self._secagg_crypter.aggregate,\n        total_sample_size=total_sample_size,\n        clipping_range=self.clipping_range,\n    )\n\n    return self._aggregate(\n        model_params,\n        encryption_factors,\n        aggregate,\n    )\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.LomSecureAggregation.load_state_breakpoint","title":"load_state_breakpoint  <code>classmethod</code>","text":"<pre><code>load_state_breakpoint(state)\n</code></pre> <p>Create a <code>SecureAggregation</code> object from a saved state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Dict</code> <p>saved state to restore in the created object</p> required <p>Returns:</p> Type Description <code>SecureAggregation</code> <p>The created <code>SecureAggregation</code> object</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>@classmethod\ndef load_state_breakpoint(cls, state: Dict) -&gt; \"SecureAggregation\":\n    \"\"\"Create a `SecureAggregation` object from a saved state\n\n    Args:\n        state: saved state to restore in the created object\n\n    Returns:\n        The created `SecureAggregation` object\n    \"\"\"\n    if state[\"attributes\"][\"_dh\"] is not None:\n        state[\"attributes\"][\"_dh\"] = SecaggDHContext.load_state_breakpoint(\n            state=state[\"attributes\"][\"_dh\"]\n        )\n\n    return super().load_state_breakpoint(state)\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.LomSecureAggregation.save_state_breakpoint","title":"save_state_breakpoint","text":"<pre><code>save_state_breakpoint()\n</code></pre> <p>Saves state of the secagg</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The secagg state to be saved</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def save_state_breakpoint(self) -&gt; Dict[str, Any]:\n    \"\"\"Saves state of the secagg\n\n    Returns:\n        The secagg state to be saved\n    \"\"\"\n    state = super().save_state_breakpoint()\n\n    state[\"attributes\"].update(\n        {\n            \"_dh\": (\n                self._dh.save_state_breakpoint() if self._dh is not None else None\n            ),\n        }\n    )\n\n    return state\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.LomSecureAggregation.setup","title":"setup","text":"<pre><code>setup(parties, experiment_id, researcher_id, force=False, insecure_validation=True)\n</code></pre> <p>Setup secure aggregation instruments.</p> <p>Requires setting <code>parties</code> and <code>experiment_id</code> if they are not set in previous secagg setups. It is possible to execute without any argument if SecureAggregation has already <code>parties</code> and <code>experiment_id</code> defined. This feature provides researcher execute <code>secagg.setup()</code> if any connection issue</p> <p>Parameters:</p> Name Type Description Default <code>parties</code> <code>List[str]</code> <p>Parties that participates secure aggregation</p> required <code>experiment_id</code> <code>str</code> <p>The id of the experiment</p> required <code>researcher_id</code> <code>str</code> <p>ID of the researcher that executes secagg setup.</p> required <code>force</code> <code>bool</code> <p>Forces secagg setup even if context is already existing</p> <code>False</code> <code>insecure_validation</code> <code>bool</code> <p>True if the insecure mechanism for validation secagg data coherence is enabled</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>Status of setup</p> <p>Raises     FedbiomedSecureAggregationError: Invalid argument type</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def setup(\n    self,\n    parties: List[str],\n    experiment_id: str,\n    researcher_id: str,\n    force: bool = False,\n    insecure_validation: bool = True\n) -&gt; bool:\n    \"\"\"Setup secure aggregation instruments.\n\n    Requires setting `parties` and `experiment_id` if they are not set in previous secagg\n    setups. It is possible to execute without any argument if SecureAggregation\n    has already `parties` and `experiment_id` defined. This feature provides researcher\n    execute `secagg.setup()` if any connection issue\n\n    Args:\n        parties: Parties that participates secure aggregation\n        experiment_id: The id of the experiment\n        researcher_id: ID of the researcher that executes secagg setup.\n        force: Forces secagg setup even if context is already existing\n        insecure_validation: True if the insecure mechanism for validation secagg data\n            coherence is enabled\n\n    Returns:\n        Status of setup\n\n    Raises\n        FedbiomedSecureAggregationError: Invalid argument type\n    \"\"\"\n\n    parties = list(filter(lambda x: x != researcher_id, parties))\n\n    super().setup(parties, experiment_id, researcher_id, force, insecure_validation)\n\n    self._dh = cast(SecaggDHContext, self._dh)\n\n    if not self._dh.status or force:\n        self._dh.setup()\n\n    return self._dh.status\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.LomSecureAggregation.train_arguments","title":"train_arguments","text":"<pre><code>train_arguments()\n</code></pre> <p>Gets train arguments for secagg train request</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Arguments that are going to be attached to the experiment.</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def train_arguments(self) -&gt; Dict:\n    \"\"\"Gets train arguments for secagg train request\n\n    Returns:\n        Arguments that are going to be attached to the experiment.\n    \"\"\"\n    arguments = super().train_arguments()\n    arguments.update(\n        {\n            \"secagg_dh_id\": self._dh.secagg_id if self._dh is not None else None,\n        }\n    )\n    return arguments\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext","title":"SecaggContext","text":"<pre><code>SecaggContext(researcher_id, parties, experiment_id, secagg_id=None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Handles a Secure Aggregation context element on the researcher side.</p> <p>Parameters:</p> Name Type Description Default <code>researcher_id</code> <code>str</code> <p>ID of the researcher that context will be created for.</p> required <code>parties</code> <code>List[str]</code> <p>list of parties participating in the secagg context element setup, named by their unique id (<code>node_id</code>, <code>researcher_id</code>). There must be at least 3 parties, and the first party is this researcher</p> required <code>experiment_id</code> <code>str</code> <p>ID of the experiment to which this secagg context element is attached.</p> required <code>secagg_id</code> <code>Union[str, None]</code> <p>optional secagg context element ID to use for this element. Default is None, which means a unique element ID will be generated.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedSecaggError</code> <p>bad argument type or value</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    researcher_id: str,\n    parties: List[str],\n    experiment_id: str,\n    secagg_id: Union[str, None] = None\n):\n    \"\"\"Constructor of the class.\n\n    Args:\n        researcher_id: ID of the researcher that context will be created for.\n        parties: list of parties participating in the secagg context element setup, named\n            by their unique id (`node_id`, `researcher_id`).\n            There must be at least 3 parties, and the first party is this researcher\n        experiment_id: ID of the experiment to which this secagg context element is attached.\n        secagg_id: optional secagg context element ID to use for this element.\n            Default is None, which means a unique element ID will be generated.\n\n    Raises:\n        FedbiomedSecaggError: bad argument type or value\n    \"\"\"\n    self._v = Validator()\n\n    self._v.register(\n        \"nonempty_str_or_none\", self._check_secagg_id_type, override=True\n    )\n    try:\n        self._v.validate(secagg_id, \"nonempty_str_or_none\")\n    except ValidatorError as e:\n        raise FedbiomedSecaggError(\n            f\"{ErrorNumbers.FB415.value}: bad parameter \"\n            \"`secagg_id` must be a None or non-empty string: {e}\"\n        ) from e\n\n    try:\n        self._v.validate(parties, list)\n        for p in parties:\n            self._v.validate(p, str)\n    except ValidatorError as e:\n        raise FedbiomedSecaggError(\n            f\"{ErrorNumbers.FB415.value}: bad parameter \"\n            f\"`parties` must be a list of strings: {e}\"\n        ) from e\n\n    self._researcher_id = researcher_id\n    self._db = config.vars[\"DB\"]\n    self._secagg_id = (\n        secagg_id if secagg_id is not None else \"secagg_\" + str(uuid.uuid4())\n    )\n    self._parties = parties\n    self._requests = Requests(config)\n    self._status = False\n    self._context = None\n    self._experiment_id = experiment_id\n    self._element: SecaggElementTypes\n\n    # to be set in subclasses\n    self._secagg_manager: Optional[BaseSecaggManager] = None\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext.context","title":"context  <code>property</code>","text":"<pre><code>context\n</code></pre> <p>Getter for secagg context element content</p> <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>secagg context element, or <code>None</code> if it doesn't exist</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext.experiment_id","title":"experiment_id  <code>property</code>","text":"<pre><code>experiment_id\n</code></pre> <p>Getter for secagg context element experiment_id</p> <p>Returns:</p> Type Description <code>str</code> <p>secagg context element experiment_id</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext.parties","title":"parties  <code>property</code>","text":"<pre><code>parties\n</code></pre> <p>Getter for secagg parties</p> <p>Returns:</p> Type Description <code>str</code> <p>Parties that participates secure aggregation</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext.secagg_id","title":"secagg_id  <code>property</code>","text":"<pre><code>secagg_id\n</code></pre> <p>Getter for secagg context element ID</p> <p>Returns:</p> Type Description <code>str</code> <p>secagg context element unique ID</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext.status","title":"status  <code>property</code>","text":"<pre><code>status\n</code></pre> <p>Getter for secagg context element status</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if secagg context element exists, <code>False</code> otherwise</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext-functions","title":"Functions","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext.delete","title":"delete","text":"<pre><code>delete()\n</code></pre> <p>Delete secagg context element on defined parties.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if secagg context element could be deleted for all parties, False if at least one of the parties could not delete context element.</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>def delete(self) -&gt; bool:\n    \"\"\"Delete secagg context element on defined parties.\n\n    Returns:\n        True if secagg context element could be deleted for all parties, False if at least\n            one of the parties could not delete context element.\n    \"\"\"\n    self._status = False\n    self._context = None\n    request = self._REQUEST_DELETE(\n        researcher_id=self._researcher_id,\n        secagg_id=self._secagg_id,\n        element=self._element.value,\n        experiment_id=self._experiment_id,\n    )\n\n    _ = self._launch_request(request)\n\n    status = self._secagg_manager.remove(self._secagg_id, self.experiment_id)\n    if status:\n        logger.debug(\n            f\"Context element successfully deleted for researcher_id='{self._researcher_id}' \"\n            f\"secagg_id='{self._secagg_id}'\"\n        )\n    else:\n        logger.error(\n            f\"{ErrorNumbers.FB415.value}: No such context element secagg_id={self._secagg_id} \"\n            f\"on researcher researcher_id='{self._researcher_id}'\"\n        )\n\n    return True\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext.load_state_breakpoint","title":"load_state_breakpoint  <code>staticmethod</code>","text":"<pre><code>load_state_breakpoint(state)\n</code></pre> <p>Method for loading secagg state from breakpoint state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Dict[str, Any]</code> <p>The state that will be loaded</p> required Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>@staticmethod\ndef load_state_breakpoint(state: Dict[str, Any]) -&gt; \"SecaggContext\":\n    \"\"\"\n    Method for loading secagg state from breakpoint state\n\n    Args:\n        state: The state that will be loaded\n    \"\"\"\n\n    # Get class\n    cls = getattr(importlib.import_module(state[\"module\"]), state[\"class\"])\n\n    secagg = cls(**state[\"arguments\"])\n    for key, value in state[\"attributes\"].items():\n        setattr(secagg, key, value)\n\n    return secagg\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext.save_state_breakpoint","title":"save_state_breakpoint","text":"<pre><code>save_state_breakpoint()\n</code></pre> <p>Method for saving secagg state for saving breakpoints</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The state of the secagg</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>def save_state_breakpoint(self) -&gt; Dict[str, Any]:\n    \"\"\"Method for saving secagg state for saving breakpoints\n\n    Returns:\n        The state of the secagg\n    \"\"\"\n    # `_v` and `_requests` dont need to be savec (properly initiated in constructor)\n    state = {\n        \"class\": type(self).__name__,\n        \"module\": self.__module__,\n        \"arguments\": {\n            \"secagg_id\": self._secagg_id,\n            \"parties\": self._parties,\n            \"experiment_id\": self._experiment_id,\n            \"researcher_id\": self._researcher_id,\n        },\n        \"attributes\": {\n            \"_status\": self._status,\n            \"_context\": self._context,\n        },\n    }\n    return state\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext.secagg_round","title":"secagg_round  <code>abstractmethod</code>","text":"<pre><code>secagg_round(request)\n</code></pre> <p>Negotiate secagg context element action with defined parties.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <p>message sent to the parties during the round</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if secagg context element action could be done for all parties, False if at least one of the parties could not do the context element action.</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>@abstractmethod\ndef secagg_round(\n    self,\n    request: Message,\n) -&gt; bool:\n    \"\"\"Negotiate secagg context element action with defined parties.\n\n    Args:\n        msg: message sent to the parties during the round\n\n    Returns:\n        True if secagg context element action could be done for all parties, False if at least\n            one of the parties could not do the context element action.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext.setup","title":"setup","text":"<pre><code>setup()\n</code></pre> <p>Setup secagg context element on defined parties.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>True if secagg context element could be setup for all parties, False if at least one of the parties could not setup context element.</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>def setup(self) -&gt; Dict:\n    \"\"\"Setup secagg context element on defined parties.\n\n    Returns:\n        True if secagg context element could be setup for all parties, False if at least\n            one of the parties could not setup context element.\n    \"\"\"\n    context = self._secagg_manager.get(self._secagg_id, self._experiment_id)\n    if context and matching_parties_servkey(context, self._parties):\n        logger.info(\n            f\"{ErrorNumbers.FB415.value}: secagg context for {self._secagg_id} exists\"\n        )\n        self._context = context['context']\n    else:\n        request = self._REQUEST_SETUP(\n            researcher_id=self._researcher_id,\n            secagg_id=self._secagg_id,\n            element=self._element.value,\n            experiment_id=self._experiment_id,\n            parties=self._parties,\n        )\n\n        self._context = self.secagg_round(request)\n\n    self._status = True\n\n    return self._status\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggDHContext","title":"SecaggDHContext","text":"<pre><code>SecaggDHContext(researcher_id, parties, experiment_id, secagg_id=None)\n</code></pre> <p>               Bases: <code>SecaggContext</code></p> <p>Handles a Secure Aggregation Diffie Hellman context element on the researcher side.</p> <p>Parameters:</p> Name Type Description Default <code>researcher_id</code> <code>str</code> <p>ID of the researcher that context will be created for.</p> required <code>parties</code> <code>List[str]</code> <p>list of parties participating in the secagg context element setup, named by their unique id (<code>node_id</code>, <code>researcher_id</code>). There must be at least 3 parties, and the first party is this researcher</p> required <code>experiment_id</code> <code>str</code> <p>ID of the experiment to which this secagg context element is attached.</p> required <code>secagg_id</code> <code>Union[str, None]</code> <p>optional secagg context element ID to use for this element. Default is None, which means a unique element ID will be generated.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedSecaggError</code> <p>bad argument type or value</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>def __init__(\n    self,\n    researcher_id: str,\n    parties: List[str],\n    experiment_id: str,\n    secagg_id: Union[str, None] = None\n):\n    \"\"\"Constructor of the class.\n\n    Args:\n        researcher_id: ID of the researcher that context will be created for.\n        parties: list of parties participating in the secagg context element setup, named\n            by their unique id (`node_id`, `researcher_id`).\n            There must be at least 3 parties, and the first party is this researcher\n        experiment_id: ID of the experiment to which this secagg context element is attached.\n        secagg_id: optional secagg context element ID to use for this element.\n            Default is None, which means a unique element ID will be generated.\n\n    Raises:\n        FedbiomedSecaggError: bad argument type or value\n    \"\"\"\n    super().__init__(researcher_id, parties, experiment_id, secagg_id)\n    self._element = SecaggElementTypes.DIFFIE_HELLMAN\n    self._raise_if_missing_parties(parties)\n    self._secagg_manager = SecaggDhManager(self._db)\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggDHContext-functions","title":"Functions","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggDHContext.secagg_round","title":"secagg_round","text":"<pre><code>secagg_round(request)\n</code></pre> <p>Negotiate secagg context element action with defined parties for DH key exchange.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Message</code> <p>message sent to the parties during the round</p> required <p>Returns:</p> Type Description <code>Tuple[dict, dict[str, bool]]</code> <p>A tuple of - a dict containing the context describing this secagg context element - a dict where key/values are node ids/boolean with success status for     secagg on each party</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>def secagg_round(\n    self,\n    request: Message,\n) -&gt; Tuple[dict, dict[str, bool]]:\n    \"\"\"Negotiate secagg context element action with defined parties for DH key exchange.\n\n    Args:\n        request: message sent to the parties during the round\n\n    Returns:\n        A tuple of\n            - a dict containing the context describing this secagg context element\n            - a dict where key/values are node ids/boolean with success status for\n                secagg on each party\n\n    \"\"\"\n\n    _ = self._launch_request(request)\n    context = {}\n    return self._register(context)\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggServkeyContext","title":"SecaggServkeyContext","text":"<pre><code>SecaggServkeyContext(researcher_id, parties, experiment_id, secagg_id=None)\n</code></pre> <p>               Bases: <code>SecaggContext</code></p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>def __init__(\n    self,\n    researcher_id,\n    parties: List[str],\n    experiment_id: str,\n    secagg_id: str | None = None,\n):\n    \"\"\"Constructs key context class\"\"\"\n    super().__init__(researcher_id, parties, experiment_id, secagg_id)\n\n    self._element = SecaggElementTypes.SERVER_KEY\n    self._raise_if_missing_parties(parties)\n    self._secagg_manager = SecaggServkeyManager(self._db)\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggServkeyContext-functions","title":"Functions","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggServkeyContext.secagg_round","title":"secagg_round","text":"<pre><code>secagg_round(request)\n</code></pre> <p>Negotiate secagg context element action with defined parties for secret sharing.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Message</code> <p>message sent to the parties during the round</p> required <p>Returns:</p> Type Description <code>Tuple[dict, dict[str, bool]]</code> <p>A tuple of - a dict containing the context describing this secagg context element - a dict where key/values are node ids/boolean with success status     for secagg on each party</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>def secagg_round(\n    self,\n    request: Message,\n) -&gt; Tuple[dict, dict[str, bool]]:\n    \"\"\"Negotiate secagg context element action with defined parties for secret sharing.\n\n    Args:\n        request: message sent to the parties during the round\n\n    Returns:\n        A tuple of\n            - a dict containing the context describing this secagg context element\n            - a dict where key/values are node ids/boolean with success status\n                for secagg on each party\n    \"\"\"\n\n    replies = self._launch_request(request)\n    servkey: int = AdditiveShares([AdditiveShare(rep.share) for\n        rep in replies.values()]).reconstruct()\n\n    biprime = get_default_biprime()\n    context = {'server_key': -servkey, 'biprime': biprime}\n    return self._register(context)\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecureAggregation","title":"SecureAggregation","text":"<pre><code>SecureAggregation(*args, scheme=SecureAggregationSchemes.LOM, **kwargs)\n</code></pre> <p>Interface for different secure aggregation classes</p> <p>Builds corresponding secure aggregation object/scheme based on given scheme argument</p> <p>Parameters:</p> Name Type Description Default <code>scheme</code> <code>SecureAggregationSchemes</code> <p>Secure aggregation scheme</p> <code>LOM</code> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    scheme: SecureAggregationSchemes = SecureAggregationSchemes.LOM,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Constructs secure aggregation class\n\n    Builds corresponding secure aggregation object/scheme based\n    on given scheme argument\n\n    Args:\n        scheme: Secure aggregation scheme\n    \"\"\"\n\n    self.__scheme = scheme\n\n    match self.__scheme:\n        case SecureAggregationSchemes.LOM:\n            self.__secagg = LomSecureAggregation(*args, **kwargs)\n        case SecureAggregationSchemes.JOYE_LIBERT:\n            self.__secagg = JoyeLibertSecureAggregation(*args, **kwargs)\n        case _:\n            self.__secagg = LomSecureAggregation(*args, **kwargs)\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecureAggregation-functions","title":"Functions","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecureAggregation.load_state_breakpoint","title":"load_state_breakpoint  <code>classmethod</code>","text":"<pre><code>load_state_breakpoint(state)\n</code></pre> <p>Create a <code>SecureAggregation</code> object from a saved state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Dict</code> <p>saved state to restore in the created object</p> required <p>Returns:</p> Type Description <code>SecureAggregation</code> <p>The created <code>SecureAggregation</code> object</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>@classmethod\ndef load_state_breakpoint(cls, state: Dict) -&gt; \"SecureAggregation\":\n    \"\"\"Create a `SecureAggregation` object from a saved state\n\n    Args:\n        state: saved state to restore in the created object\n\n    Returns:\n        The created `SecureAggregation` object\n    \"\"\"\n    secagg = cls(scheme=SecureAggregationSchemes(state[\"arguments\"][\"scheme\"]))\n\n    for name, val in state[\"attributes_states\"].items():\n\n        _sub_cls = getattr(importlib.import_module(val[\"module\"]), val[\"class\"])\n        instance = _sub_cls.load_state_breakpoint(val)\n        setattr(secagg, name, instance)\n\n    return secagg\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecureAggregation.save_state_breakpoint","title":"save_state_breakpoint  <code>abstractmethod</code>","text":"<pre><code>save_state_breakpoint()\n</code></pre> <p>Saves state of the secagg</p> <p>This methods also save states of <code>__secagg</code> which provides a single entry point for secagg schemes</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The secagg state to be saved</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>@abstractmethod\ndef save_state_breakpoint(self) -&gt; Dict[str, Any]:\n    \"\"\"Saves state of the secagg\n\n    This methods also save states of `__secagg` which provides\n    a single entry point for secagg schemes\n\n    Returns:\n        The secagg state to be saved\n    \"\"\"\n\n    state = {\n        \"class\": type(self).__name__,\n        \"module\": self.__module__,\n        \"arguments\": {\n            \"scheme\": self.__scheme.value,\n        },\n        \"attributes\": {},\n        \"attributes_states\": {\n            \"_SecureAggregation__secagg\": self.__secagg.save_state_breakpoint()\n        },\n    }\n\n    return state\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecureAggregationSchemes","title":"SecureAggregationSchemes","text":"<p>               Bases: <code>_BaseEnum</code></p> <p>Enumeration class for secure aggregation schemes</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecureAggregationSchemes-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecureAggregationSchemes.JOYE_LIBERT","title":"JOYE_LIBERT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>JOYE_LIBERT = 1\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecureAggregationSchemes.LOM","title":"LOM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LOM = 2\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecureAggregationSchemes.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = 0\n</code></pre>"},{"location":"developer/api/researcher/strategies/","title":"Strategies","text":""},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies-classes","title":"Classes","text":""},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.DefaultStrategy","title":"DefaultStrategy","text":"<pre><code>DefaultStrategy()\n</code></pre> <p>               Bases: <code>Strategy</code></p> <p>Default strategy to be used when sampling/selecting nodes and checking whether nodes have responded or not</p> <p>Strategy is: - select all node for each round - raise an error if one node does not answer - raise an error is one node returns an error</p> <pre><code>    used for federated training.\n</code></pre> Source code in <code>fedbiomed/researcher/strategies/strategy.py</code> <pre><code>def __init__(self):\n    \"\"\"\n\n    Args:\n        data: Object that includes all active nodes and the meta-data of the dataset that is going to be\n            used for federated training.\n    \"\"\"\n    self._sampling_node_history = {}\n    self._success_node_history = {}\n    self._parameters = None\n</code></pre>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.DefaultStrategy-functions","title":"Functions","text":""},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.DefaultStrategy.refine","title":"refine","text":"<pre><code>refine(training_replies, round_i)\n</code></pre> <p>The method where node selection is completed by extracting parameters and length from the training replies</p> <p>Parameters:</p> Name Type Description Default <code>training_replies</code> <code>Dict</code> <p>is a list of elements of type  Response( { 'success': m['success'],              'msg': m['msg'],              'dataset_id': m['dataset_id'],              'node_id': m['node_id'],              'params_path': params_path,              'params': params,              'sample_size': sample_size              } )</p> required <code>round_i</code> <code>int</code> <p>Current round of experiment</p> required <p>Returns:</p> Name Type Description <code>weights</code> <code>Dict[str, Dict[str, Union[Tensor, ndarray]]]</code> <p>Proportions list, each element of this list represents a dictionary with its only key as the node_id and its value the proportion of lines the node has with respect to the whole,</p> <code>model_params</code> <code>Dict[str, float]</code> <p>list with each element representing a dictionary. Its only key represents the node_id and the corresponding value is a dictionary containing list of weight matrices of every node : [{\"n1\":{\"layer1\":m1,\"layer2\":m2},{\"layer3\":\"m3\"}},{\"n2\": ...}] Including the node_id is useful for the proper functioning of some strategies like Scaffold: At each round, local model params are linked to a certain correction. The correction is updated every round. The computation of correction states at round i is dependant to client states and correction states of round i-1. Since training_replies    can potentially order the node replies differently from round to round, the bridge between all these parameters is represented by the node_id.</p> <code>total_rows</code> <code>int</code> <p>sum of number of samples used by all nodes</p> <code>encryption_factors</code> <code>Dict[str, List[int]]</code> <p>encryption factors from the participating nodes</p> <p>Raises:</p> Type Description <code>FedbiomedStrategyError</code> <ul> <li>Miss-matched in answered nodes and existing nodes</li> <li>If not all nodes successfully completes training</li> <li>if a Node has not sent <code>sample_size</code> value in the TrainingReply, making it impossible to compute aggregation weights.</li> </ul> Source code in <code>fedbiomed/researcher/strategies/default_strategy.py</code> <pre><code>def refine(\n        self,\n        training_replies: Dict,\n        round_i: int\n) -&gt; Tuple[Dict[str, Dict[str, Union['torch.Tensor', 'numpy.ndarray']]],\n           Dict[str, float],\n           int,\n           Dict[str, List[int]]]:\n    \"\"\"\n    The method where node selection is completed by extracting parameters and length from the training replies\n\n    Args:\n        training_replies: is a list of elements of type\n             Response( { 'success': m['success'],\n                         'msg': m['msg'],\n                         'dataset_id': m['dataset_id'],\n                         'node_id': m['node_id'],\n                         'params_path': params_path,\n                         'params': params,\n                         'sample_size': sample_size\n                         } )\n        round_i: Current round of experiment\n\n    Returns:\n        weights: Proportions list, each element of this list represents a dictionary with\n            its only key as the node_id and its value the proportion of lines the node has\n            with respect to the whole,\n        model_params: list with each element representing a dictionary. Its only key represents\n            the node_id and the corresponding value is a dictionary containing list of weight\n            matrices of every node : [{\"n1\":{\"layer1\":m1,\"layer2\":m2},{\"layer3\":\"m3\"}},{\"n2\":\n            ...}] Including the node_id is useful for the proper functioning of some strategies\n            like Scaffold: At each round, local model params are linked to a certain correction.\n            The correction is updated every round. The computation of correction states at round\n            i is dependant to client states and correction states of round i-1. Since\n            training_replies    can potentially order the node replies differently from round to\n            round, the bridge between all these parameters is represented by the node_id.\n        total_rows: sum of number of samples used by all nodes\n        encryption_factors: encryption factors from the participating nodes\n\n    Raises:\n        FedbiomedStrategyError: - Miss-matched in answered nodes and existing nodes\n            - If not all nodes successfully completes training\n            - if a Node has not sent `sample_size` value in the TrainingReply, making it\n            impossible to compute aggregation weights.\n    \"\"\"\n    # check that all nodes answered\n    answers_count = 0\n\n    if self._sampling_node_history.get(round_i) is None:\n        raise FedbiomedStrategyError(\n            ErrorNumbers.FB408.value\n            + f\": Missing Nodes replies for round: {round_i}\"\n        )\n\n    for cl in self._sampling_node_history[round_i]:\n        if cl in training_replies:\n            answers_count += 1\n        else:\n            # this node did not answer\n            logger.error(ErrorNumbers.FB408.value + \" (node = \" + cl + \")\")\n\n    if len(self._sampling_node_history[round_i]) != answers_count:\n        msg = ErrorNumbers.FB408.value\n        if answers_count == 0:\n            # none of the nodes answered\n            msg = ErrorNumbers.FB407.value\n        raise FedbiomedStrategyError(msg)\n\n    # check that all nodes that answer could successfully train\n    self._success_node_history[round_i] = []\n    all_success = True\n    model_params = {}\n    sample_sizes = {}\n    encryption_factors = {}\n    total_rows = 0\n    for tr in training_replies.values():\n        if tr[\"success\"] is True:\n            model_params[tr[\"node_id\"]] = tr[\"params\"]\n            encryption_factors[tr[\"node_id\"]] = tr.get(\"encryption_factor\", None)\n\n            if tr[\"sample_size\"] is None:\n                # if a Node `sample_size` is None, we cannot compute the weights: in this case\n                # return an error\n                raise FedbiomedStrategyError(\n                    ErrorNumbers.FB402.value\n                    + f\" : Node {tr['node_id']} did not return \"\n                    \"any `sample_size` value (number of samples seen during one Round),\"\n                    \" can not compute weights for the aggregation. Aborting\"\n                )\n            sample_sizes[tr[\"node_id\"]] = tr[\"sample_size\"]\n\n            total_rows += tr[\"sample_size\"]\n            self._success_node_history[round_i].append(tr[\"node_id\"])\n        else:\n            all_success = False\n            logger.error(f\"{ErrorNumbers.FB409.value} (node = {tr['node_id']} )\")\n\n    if not all_success:\n        raise FedbiomedStrategyError(ErrorNumbers.FB402.value)\n\n    weights = {\n        node_id: (\n            sample_size / total_rows if total_rows != 0 else 1 / len(sample_sizes)\n        )\n        for node_id, sample_size in sample_sizes.items()\n    }\n\n    logger.info(\n        f\"Nodes that successfully reply in round {round_i} \"\n        f\"{self._success_node_history[round_i]}\"\n    )\n\n    return model_params, weights, total_rows, encryption_factors\n</code></pre>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.DefaultStrategy.sample_nodes","title":"sample_nodes","text":"<pre><code>sample_nodes(from_nodes, round_i)\n</code></pre> <p>Samples and selects nodes on which to train local model. In this strategy we will consider all existing nodes</p> <p>Parameters:</p> Name Type Description Default <code>from_nodes</code> <code>List[str]</code> <p>the node ids which may be sampled</p> required <code>round_i</code> <code>int</code> <p>number of round.</p> required <p>Returns:</p> Name Type Description <code>node_ids</code> <code>List[str]</code> <p>list of all node ids considered for training during this round <code>round_i</code>.</p> Source code in <code>fedbiomed/researcher/strategies/default_strategy.py</code> <pre><code>def sample_nodes(self, from_nodes: List[str], round_i: int) -&gt; List[str]:\n    \"\"\"Samples and selects nodes on which to train local model. In this strategy we will consider all existing\n    nodes\n\n    Args:\n        from_nodes: the node ids which may be sampled\n        round_i: number of round.\n\n    Returns:\n        node_ids: list of all node ids considered for training during\n            this round `round_i`.\n    \"\"\"\n    self._sampling_node_history[round_i] = copy.copy(from_nodes)\n\n    return from_nodes\n</code></pre>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.Strategy","title":"Strategy","text":"<pre><code>Strategy()\n</code></pre> <p>Default Strategy as Parent class. Custom strategy classes must inherit from this parent class.</p> <p>Inconsistent history</p> <p>The Strategy class keeps a history of sampled and successful nodes. No attempt is made to keep this history consistent when the <code>_fds</code> member is modified.</p> <pre><code>    used for federated training.\n</code></pre> Source code in <code>fedbiomed/researcher/strategies/strategy.py</code> <pre><code>def __init__(self):\n    \"\"\"\n\n    Args:\n        data: Object that includes all active nodes and the meta-data of the dataset that is going to be\n            used for federated training.\n    \"\"\"\n    self._sampling_node_history = {}\n    self._success_node_history = {}\n    self._parameters = None\n</code></pre>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.Strategy-functions","title":"Functions","text":""},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.Strategy.load_state_breakpoint","title":"load_state_breakpoint","text":"<pre><code>load_state_breakpoint(state=None)\n</code></pre> <p>Method for loading strategy state from breakpoint state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Dict[str, Any]</code> <p>The state that will be loaded</p> <code>None</code> Source code in <code>fedbiomed/researcher/strategies/strategy.py</code> <pre><code>def load_state_breakpoint(self, state: Dict[str, Any] = None):\n    \"\"\"\n    Method for loading strategy state from breakpoint state\n\n    Args:\n        state: The state that will be loaded\n    \"\"\"\n    # fds may be modified and diverge from Experiment\n    self._parameters = state['parameters']\n</code></pre>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.Strategy.refine","title":"refine","text":"<pre><code>refine(training_replies, round_i)\n</code></pre> <p>Abstract method that must be implemented by child class</p> <p>Parameters:</p> Name Type Description Default <code>training_replies</code> <code>Dict</code> <p>is a list of elements of type  Response( { 'success': m['success'],              'msg': m['msg'],              'dataset_id': m['dataset_id'],              'node_id': m['node_id'],              'params_path': params_path,              'params': params } )</p> required <code>round_i</code> <code>int</code> <p>Current round of experiment</p> required <p>Raises:</p> Type Description <code>FedbiomedStrategyError</code> <p>If method is not implemented by child class</p> Source code in <code>fedbiomed/researcher/strategies/strategy.py</code> <pre><code>def refine(\n        self,\n        training_replies: Dict,\n        round_i: int\n           ) -&gt; Tuple[Dict[str, Dict[str, Union['torch.Tensor', 'numpy.ndarray']]],\n                      Dict[str, float],\n                      int,\n                      Dict[str, List[int]]]:\n    \"\"\"\n    Abstract method that must be implemented by child class\n\n    Args:\n        training_replies: is a list of elements of type\n             Response( { 'success': m['success'],\n                         'msg': m['msg'],\n                         'dataset_id': m['dataset_id'],\n                         'node_id': m['node_id'],\n                         'params_path': params_path,\n                         'params': params } )\n        round_i: Current round of experiment\n\n    Raises:\n        FedbiomedStrategyError: If method is not implemented by child class\n    \"\"\"\n    msg = ErrorNumbers.FB402.value + \\\n        \": refine method should be overloaded by the provided strategy\"\n    logger.critical(msg)\n    raise FedbiomedStrategyError(msg)\n</code></pre>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.Strategy.sample_nodes","title":"sample_nodes","text":"<pre><code>sample_nodes(from_nodes, round_i)\n</code></pre> <p>Abstract method that must be implemented by child class</p> <p>Parameters:</p> Name Type Description Default <code>from_nodes</code> <code>List[str]</code> <p>the node ids which may be sampled</p> required <code>round_i</code> <code>int</code> <p>Current round of experiment</p> required Source code in <code>fedbiomed/researcher/strategies/strategy.py</code> <pre><code>def sample_nodes(self, from_nodes: List[str], round_i: int):\n    \"\"\"\n    Abstract method that must be implemented by child class\n\n    Args:\n        from_nodes: the node ids which may be sampled\n        round_i: Current round of experiment\n    \"\"\"\n    msg = ErrorNumbers.FB402.value + \\\n        \": sample nodes method should be overloaded by the provided strategy\"\n    logger.critical(msg)\n    raise FedbiomedStrategyError(msg)\n</code></pre>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.Strategy.save_state_breakpoint","title":"save_state_breakpoint","text":"<pre><code>save_state_breakpoint()\n</code></pre> <p>Method for saving strategy state for saving breakpoints</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The state of the strategy</p> Source code in <code>fedbiomed/researcher/strategies/strategy.py</code> <pre><code>def save_state_breakpoint(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Method for saving strategy state for saving breakpoints\n\n    Returns:\n        The state of the strategy\n    \"\"\"\n\n    state = {\n        \"class\": type(self).__name__,\n        \"module\": self.__module__,\n        \"parameters\": self._parameters,\n    }\n    return state\n</code></pre>"},{"location":"developer/api/transport/client/","title":"Client","text":""},{"location":"developer/api/transport/client/#fedbiomed.transport.client-attributes","title":"Attributes","text":""},{"location":"developer/api/transport/client/#fedbiomed.transport.client.GRPC_CLIENT_CONN_RETRY_TIMEOUT","title":"GRPC_CLIENT_CONN_RETRY_TIMEOUT  <code>module-attribute</code>","text":"<pre><code>GRPC_CLIENT_CONN_RETRY_TIMEOUT = 2\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.GRPC_CLIENT_TASK_REQUEST_TIMEOUT","title":"GRPC_CLIENT_TASK_REQUEST_TIMEOUT  <code>module-attribute</code>","text":"<pre><code>GRPC_CLIENT_TASK_REQUEST_TIMEOUT = 3600\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client-classes","title":"Classes","text":""},{"location":"developer/api/transport/client/#fedbiomed.transport.client.Channels","title":"Channels","text":"<pre><code>Channels(researcher)\n</code></pre> <p>Keeps gRPC server channels</p> <p>Parameters:</p> Name Type Description Default <code>researcher</code> <code>ResearcherCredentials</code> <p>An instance of ResearcherCredentials</p> required Source code in <code>fedbiomed/transport/client.py</code> <pre><code>def __init__(self, researcher: ResearcherCredentials):\n    \"\"\"Create channels and stubs\n\n    Args:\n        researcher: An instance of ResearcherCredentials\n    \"\"\"\n    self._researcher = researcher\n\n    self._channels = {}\n    self._stubs = {}\n    self._stub_types = [\n        _StubType.LISTENER_TASK_STUB,\n        _StubType.SENDER_TASK_STUB,\n        _StubType.SENDER_FEEDBACK_STUB,\n    ]\n    for st in self._stub_types:\n        self._channels[st]: grpc.aio.Channel = None\n        self._stubs[st]: ResearcherServiceStub = None\n\n    # lock for accessing channels and stubs\n    self._channels_stubs_lock = asyncio.Lock()\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.Channels-functions","title":"Functions","text":""},{"location":"developer/api/transport/client/#fedbiomed.transport.client.Channels.connect","title":"connect  <code>async</code>","text":"<pre><code>connect(stub_type=_StubType.ANY_STUB)\n</code></pre> <p>Connects gRPC server and instatiates stubs.</p> <p>Parameters:</p> Name Type Description Default <code>stub_type</code> <code>_StubType</code> <p>only (re)connect for matching stub type(s)</p> <code>ANY_STUB</code> Source code in <code>fedbiomed/transport/client.py</code> <pre><code>async def connect(self, stub_type: _StubType = _StubType.ANY_STUB):\n    \"\"\"Connects gRPC server and instatiates stubs.\n\n    Args:\n        stub_type: only (re)connect for matching stub type(s)\n    \"\"\"\n\n    async with self._channels_stubs_lock:\n        # Closes if channels are open\n        for st, channel in self._channels.items():\n            if channel and (stub_type == _StubType.ANY_STUB or stub_type == st):\n                await channel.close()\n\n        # Creates channels\n        for st in self._channels.keys():\n            if stub_type == _StubType.ANY_STUB or stub_type == st:\n                self._channels[st] = self._create()\n                self._stubs[st] = ResearcherServiceStub(channel=self._channels[st])\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.Channels.stub","title":"stub  <code>async</code>","text":"<pre><code>stub(stub_type)\n</code></pre> <p>Get stub for a given stub type.</p> <p>Parameters:</p> Name Type Description Default <code>stub_type</code> <code>_StubType</code> <p>the stub type to get</p> required <p>Returns:</p> Type Description <code>ResearcherServiceStub</code> <p>the stub if it exists or None</p> Source code in <code>fedbiomed/transport/client.py</code> <pre><code>async def stub(self, stub_type: _StubType) -&gt; ResearcherServiceStub:\n    \"\"\"Get stub for a given stub type.\n\n    Args:\n        stub_type: the stub type to get\n\n    Returns:\n        the stub if it exists or None\n    \"\"\"\n    if stub_type in self._stub_types:\n        async with self._channels_stubs_lock:\n            return self._stubs[stub_type]\n    else:\n        return None\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.ClientStatus","title":"ClientStatus","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.ClientStatus-attributes","title":"Attributes","text":""},{"location":"developer/api/transport/client/#fedbiomed.transport.client.ClientStatus.CONNECTED","title":"CONNECTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CONNECTED = 1\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.ClientStatus.DISCONNECTED","title":"DISCONNECTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DISCONNECTED = 0\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.ClientStatus.FAILED","title":"FAILED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FAILED = 2\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.GrpcClient","title":"GrpcClient","text":"<pre><code>GrpcClient(node_id, researcher, update_id_map)\n</code></pre> <p>An agent of remote researcher gRPC server.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>unique ID of this node (connection client)</p> required <code>researcher</code> <code>ResearcherCredentials</code> <p>the researcher to which the node connects (connection server)</p> required <code>update_id_map</code> <code>Awaitable</code> <p>awaitable to call when updating the researcher ID, needs proper prototype</p> required Source code in <code>fedbiomed/transport/client.py</code> <pre><code>def __init__(\n    self, node_id: str, researcher: ResearcherCredentials, update_id_map: Awaitable\n) -&gt; None:\n    \"\"\"Class constructor\n\n    Args:\n        node_id: unique ID of this node (connection client)\n        researcher: the researcher to which the node connects (connection server)\n        update_id_map: awaitable to call when updating the researcher ID, needs proper prototype\n    \"\"\"\n    self._id = None\n    self._researcher = researcher\n    self._channels = Channels(researcher)\n\n    self._task_listener = TaskListener(\n        channels=self._channels,\n        node_id=node_id,\n        on_status_change=self._on_status_change,\n        update_id=self._update_id,\n    )\n\n    self._sender = Sender(\n        channels=self._channels, on_status_change=self._on_status_change\n    )\n\n    # TODO: use `self._status` for finer gRPC agent handling.\n    # Currently, the (tentative) status is maintained but not used\n    self._status = ClientStatus.DISCONNECTED\n    # lock for accessing self._status\n    self._status_lock = asyncio.Lock()\n\n    self._update_id_map = update_id_map\n    self._tasks = []\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.GrpcClient-functions","title":"Functions","text":""},{"location":"developer/api/transport/client/#fedbiomed.transport.client.GrpcClient.send","title":"send  <code>async</code>","text":"<pre><code>send(message)\n</code></pre> <p>Sends messages from node to researcher server.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>message to send from node to server</p> required Source code in <code>fedbiomed/transport/client.py</code> <pre><code>async def send(self, message: Message) -&gt; None:\n    \"\"\"Sends messages from node to researcher server.\n\n    Args:\n        message: message to send from node to server\n    \"\"\"\n\n    await self._sender.send(message)\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.GrpcClient.start","title":"start","text":"<pre><code>start(on_task)\n</code></pre> <p>Start researcher gRPC agent.</p> <p>Starts long-lived tasks, one waiting for server requests, one waiting on the async queue for the replies from the node that are going to be sent back to researcher.</p> <p>Parameters:</p> Name Type Description Default <code>on_task</code> <p>Callback function to execute once a payload received from researcher.</p> required <p>Returns:</p> Type Description <code>List[Awaitable[Optional[Callable]]]</code> <p>A list of task objects of the agent</p> Source code in <code>fedbiomed/transport/client.py</code> <pre><code>def start(self, on_task) -&gt; List[Awaitable[Optional[Callable]]]:\n    \"\"\"Start researcher gRPC agent.\n\n    Starts long-lived tasks, one waiting for server requests, one waiting on the async queue\n    for the replies from the node that are going to be sent back to researcher.\n\n    Args:\n        on_task: Callback function to execute once a payload received from researcher.\n\n    Returns:\n        A list of task objects of the agent\n    \"\"\"\n\n    async def run():\n        \"\"\"Connects and dispatches the tasks\"\"\"\n\n        # First connects to channel\n        await self._connect()\n\n        # Launch listeners\n        await asyncio.gather(\n            self._task_listener.listen(on_task), self._sender.listen()\n        )\n\n    # Returns client task\n    return asyncio.create_task(run())\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.Listener","title":"Listener","text":"<pre><code>Listener(channels)\n</code></pre> <p>Abstract generic listener method for a node's communications.</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>Channels</code> <p>Keeps channels and stubs.</p> required Source code in <code>fedbiomed/transport/client.py</code> <pre><code>def __init__(self, channels: Channels) -&gt; None:\n    \"\"\"Constructs task listener channels\n\n    Args:\n        channels: Keeps channels and stubs.\n    \"\"\"\n    self._channels = channels\n    self._retry_on_error = False\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.Listener-functions","title":"Functions","text":""},{"location":"developer/api/transport/client/#fedbiomed.transport.client.Listener.listen","title":"listen","text":"<pre><code>listen(callback=None)\n</code></pre> <p>Listens for tasks from given channels</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Optional[Callable]</code> <p>Callback function to execute once a task is processed</p> <code>None</code> <p>Returns:</p> Type Description <code>Awaitable[Optional[Callable]]</code> <p>Asyncio task to run task listener</p> Source code in <code>fedbiomed/transport/client.py</code> <pre><code>def listen(\n    self, callback: Optional[Callable] = None\n) -&gt; Awaitable[Optional[Callable]]:\n    \"\"\"Listens for tasks from given channels\n\n    Args:\n        callback: Callback function to execute once a task is processed\n\n    Returns:\n        Asyncio task to run task listener\n    \"\"\"\n    return asyncio.create_task(self._listen(callback))\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.ResearcherCredentials","title":"ResearcherCredentials  <code>dataclass</code>","text":"<pre><code>ResearcherCredentials(port, host, certificate=None)\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.ResearcherCredentials-attributes","title":"Attributes","text":""},{"location":"developer/api/transport/client/#fedbiomed.transport.client.ResearcherCredentials.certificate","title":"certificate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>certificate = None\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.ResearcherCredentials.host","title":"host  <code>instance-attribute</code>","text":"<pre><code>host\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.ResearcherCredentials.port","title":"port  <code>instance-attribute</code>","text":"<pre><code>port\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.Sender","title":"Sender","text":"<pre><code>Sender(channels, on_status_change)\n</code></pre> <p>               Bases: <code>Listener</code></p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>Channels</code> <p>RPC channels and stubs to be used for polling tasks from researcher</p> required <code>on_status_change</code> <code>Awaitable</code> <p>Callback awaitable to run for changing node agent status</p> required Source code in <code>fedbiomed/transport/client.py</code> <pre><code>def __init__(\n    self,\n    channels: Channels,\n    on_status_change: Awaitable,\n) -&gt; None:\n    \"\"\"Class constructor.\n\n    Args:\n        channels: RPC channels and stubs to be used for polling tasks from researcher\n        on_status_change: Callback awaitable to run for changing node agent status\n    \"\"\"\n    super().__init__(channels)\n\n    self._queue = asyncio.Queue()\n    self._on_status_change = on_status_change\n    self._retry_count = 0\n    self._retry_item = None\n    self._stub_type = _StubType.NO_STUB\n    self._retry_on_error = True\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.Sender-functions","title":"Functions","text":""},{"location":"developer/api/transport/client/#fedbiomed.transport.client.Sender.send","title":"send  <code>async</code>","text":"<pre><code>send(message)\n</code></pre> <p>Send a message to peer researcher.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to send</p> required Source code in <code>fedbiomed/transport/client.py</code> <pre><code>async def send(self, message: Message) -&gt; None:\n    \"\"\"Send a message to peer researcher.\n\n    Args:\n        message: Message to send\n    \"\"\"\n    # Switch-case for message type and gRPC calls\n    match message.__class__.__name__:\n        case FeedbackMessage.__name__:\n            # Note: FeedbackMessage is designed as proto serializable message.\n            await self._queue.put(\n                {\"stub\": _StubType.SENDER_FEEDBACK_STUB, \"message\": message}\n            )\n\n        case _:\n            await self._queue.put(\n                {\"stub\": _StubType.SENDER_TASK_STUB, \"message\": message}\n            )\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client.TaskListener","title":"TaskListener","text":"<pre><code>TaskListener(channels, node_id, on_status_change, update_id)\n</code></pre> <p>               Bases: <code>Listener</code></p> <p>Listener for the task assigned by the researcher component</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>Channels</code> <p>RPC channels and stubs to be used for polling tasks from researcher</p> required <code>node_id</code> <code>str</code> <p>unique ID for this node</p> required <code>on_status_change</code> <code>Awaitable</code> <p>Callback awaitable to run for changing node agent status</p> required <code>update_id</code> <code>Awaitable</code> <p>Callback function to run updating peer researcher ID</p> required Source code in <code>fedbiomed/transport/client.py</code> <pre><code>def __init__(\n    self,\n    channels: Channels,\n    node_id: str,\n    on_status_change: Awaitable,\n    update_id: Awaitable,\n) -&gt; None:\n    \"\"\"Class constructor.\n\n    Args:\n        channels: RPC channels and stubs to be used for polling tasks from researcher\n        node_id: unique ID for this node\n        on_status_change: Callback awaitable to run for changing node agent status\n        update_id: Callback function to run updating peer researcher ID\n    \"\"\"\n    super().__init__(channels)\n\n    self._node_id = node_id\n    self._on_status_change = on_status_change\n    self._update_id = update_id\n    self._retry_count = 0\n</code></pre>"},{"location":"developer/api/transport/client/#fedbiomed.transport.client-functions","title":"Functions","text":""},{"location":"developer/api/transport/client/#fedbiomed.transport.client.is_server_alive","title":"is_server_alive","text":"<pre><code>is_server_alive(host, port)\n</code></pre> <p>Checks if the server is alive</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The host/ip of researcher/server component</p> required <code>port</code> <code>str</code> <p>Port number of researcher/server component</p> required Source code in <code>fedbiomed/transport/client.py</code> <pre><code>def is_server_alive(host: str, port: str):\n    \"\"\"Checks if the server is alive\n\n    Args:\n        host: The host/ip of researcher/server component\n        port: Port number of researcher/server component\n    \"\"\"\n\n    port = int(port)\n    address_info = socket.getaddrinfo(host, port, socket.AF_INET, socket.SOCK_STREAM)\n    for family, socktype, protocol, _, address in address_info:\n        s = socket.socket(family, socktype, protocol)\n        # Need this timeout for the case where the server does not answer\n        # If not present, socket timeout increases and this function takes more\n        # than GRPC_CLIENT_CONN_RETRY_TIMEOUT to execute\n        s.settimeout(GRPC_CLIENT_CONN_RETRY_TIMEOUT)\n        try:\n            s.connect(address)\n        except socket.error:\n            return False\n        else:\n            s.close()\n            return True\n</code></pre>"},{"location":"developer/api/transport/controller/","title":"Controller","text":""},{"location":"developer/api/transport/controller/#fedbiomed.transport.controller-attributes","title":"Attributes","text":""},{"location":"developer/api/transport/controller/#fedbiomed.transport.controller-classes","title":"Classes","text":""},{"location":"developer/api/transport/controller/#fedbiomed.transport.controller.GrpcAsyncTaskController","title":"GrpcAsyncTaskController","text":"<pre><code>GrpcAsyncTaskController(node_id, researchers, on_message, debug=False)\n</code></pre> <p>RPC asynchronous task controller</p> <p>Launches async tasks for listening the requests/tasks coming from researcher as well as listener to send the replies that are created by the node. All the methods of this class are awaitable, except the constructor.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node component that runs RPC client</p> required <code>researchers</code> <code>List[ResearcherCredentials]</code> <p>List of researchers that the RPC client will connect to.</p> required <code>on_message</code> <code>Callable</code> <p>Callback function to be executed once a task received from the researcher</p> required <code>debug</code> <code>bool</code> <p>Activates debug mode for <code>asyncio</code></p> <code>False</code> <p>Raises:</p> Type Description <code>FedbiomedCommunicationError</code> <p>bad argument type</p> Source code in <code>fedbiomed/transport/controller.py</code> <pre><code>def __init__(\n        self,\n        node_id: str,\n        researchers: List[ResearcherCredentials],\n        on_message: Callable,\n        debug: bool = False\n) -&gt; None:\n    \"\"\"Constructs GrpcAsyncTaskController\n\n    Args:\n        node_id: The ID of the node component that runs RPC client\n        researchers: List of researchers that the RPC client will connect to.\n        on_message: Callback function to be executed once a task received from the researcher\n        debug: Activates debug mode for `asyncio`\n\n    Raises:\n        FedbiomedCommunicationError: bad argument type\n    \"\"\"\n\n    # inform all threads whether communication client is started\n    self._is_started = threading.Event()\n\n    self._node_id = node_id\n    self._researchers = researchers\n\n    self._loop = None\n\n    # Maps researcher ip to corresponding ids\n    self._ip_id_map_lock = None\n    self._ip_id_map = {}\n\n    # Clients lock not needed for now (client list not modified after initialization)\n    # but guarantees to be future safe for dynamic researcher clients' list\n    self._clients_lock = None\n    self._clients: Dict[str, GrpcClient] = {}\n\n    self._debug = debug\n    self._on_message = on_message\n</code></pre>"},{"location":"developer/api/transport/controller/#fedbiomed.transport.controller.GrpcAsyncTaskController-functions","title":"Functions","text":""},{"location":"developer/api/transport/controller/#fedbiomed.transport.controller.GrpcAsyncTaskController.is_connected","title":"is_connected  <code>async</code>","text":"<pre><code>is_connected()\n</code></pre> <p>Checks if there is running tasks</p> Source code in <code>fedbiomed/transport/controller.py</code> <pre><code>async def is_connected(self) -&gt; bool:\n    \"\"\"Checks if there is running tasks\"\"\"\n\n    async with self._clients_lock:\n        tasks = [not task.done() for client in self._clients.values() for task in client.tasks]\n        return all(tasks)\n</code></pre>"},{"location":"developer/api/transport/controller/#fedbiomed.transport.controller.GrpcAsyncTaskController.send","title":"send  <code>async</code>","text":"<pre><code>send(message, broadcast=False)\n</code></pre> <p>Sends message to researcher.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to send</p> required <code>broadcast</code> <code>bool</code> <p>Broadcast the message to all available researcher. This option should be used for general node state messages (e.g. general Error)</p> <code>False</code> Source code in <code>fedbiomed/transport/controller.py</code> <pre><code>async def send(self, message: Message, broadcast: bool = False) -&gt; None:\n    \"\"\"Sends message to researcher.\n\n    Args:\n        message: Message to send\n        broadcast: Broadcast the message to all available researcher. This option should be used for general\n            node state messages (e.g. general Error)\n    \"\"\"\n    if broadcast:\n        return await self._broadcast(message)\n\n    async with self._clients_lock:\n        async with self._ip_id_map_lock:\n            researcher = message.researcher_id\n            await self._clients[self._ip_id_map[researcher]].send(message)\n</code></pre>"},{"location":"developer/api/transport/controller/#fedbiomed.transport.controller.GrpcAsyncTaskController.start","title":"start  <code>async</code>","text":"<pre><code>start()\n</code></pre> <p>\"Starts the tasks for each GrpcClient</p> Source code in <code>fedbiomed/transport/controller.py</code> <pre><code>async def start(self) -&gt; None:\n    \"\"\"\"Starts the tasks for each GrpcClient\"\"\"\n\n    tasks = []\n    for researcher in self._researchers:\n        client = GrpcClient(self._node_id, researcher, self._update_id_ip_map)\n        tasks.append(client.start(on_task=self._on_message))\n        self._clients[f\"{researcher.host}:{researcher.port}\"] = client\n\n    self._loop = asyncio.get_running_loop()\n\n    # Create asyncio locks\n    self._ip_id_map_lock = asyncio.Lock()\n    self._clients_lock = asyncio.Lock()\n\n    self._is_started.set()\n\n    logger.info(\"Starting task listeners\")\n\n    # Run GrpcClient asyncio tasks\n    await asyncio.gather(*tasks)\n</code></pre>"},{"location":"developer/api/transport/controller/#fedbiomed.transport.controller.GrpcController","title":"GrpcController","text":"<pre><code>GrpcController(node_id, researchers, on_message, debug=False)\n</code></pre> <p>               Bases: <code>GrpcAsyncTaskController</code></p> <p>\"gRPC Controller class</p> <p>This class is responsible of managing GrpcConnections with researcher components. It is wrapper class of GrpcClients. It has been designed to be called main or different threads than the one grpc client runs.</p> <p>Attributes:</p> Name Type Description <code>_thread</code> <code>Optional[Thread]</code> <p>background thread of gRPC controller</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node component that runs RPC client</p> required <code>researchers</code> <code>List[ResearcherCredentials]</code> <p>List of researchers that the RPC client will connect to.</p> required <code>on_message</code> <code>Callable</code> <p>Callback function to be executed once a task received from the researcher</p> required <code>debug</code> <code>bool</code> <p>Activates debug mode for <code>asyncio</code></p> <code>False</code> <p>Raises:</p> Type Description <code>FedbiomedCommunicationError</code> <p>bad argument type</p> Source code in <code>fedbiomed/transport/controller.py</code> <pre><code>def __init__(\n        self,\n        node_id: str,\n        researchers: List[ResearcherCredentials],\n        on_message: Callable,\n        debug: bool = False\n) -&gt; None:\n    \"\"\"Constructs GrpcAsyncTaskController\n\n    Args:\n        node_id: The ID of the node component that runs RPC client\n        researchers: List of researchers that the RPC client will connect to.\n        on_message: Callback function to be executed once a task received from the researcher\n        debug: Activates debug mode for `asyncio`\n\n    Raises:\n        FedbiomedCommunicationError: bad argument type\n    \"\"\"\n\n    # inform all threads whether communication client is started\n    self._is_started = threading.Event()\n\n    self._node_id = node_id\n    self._researchers = researchers\n\n    self._loop = None\n\n    # Maps researcher ip to corresponding ids\n    self._ip_id_map_lock = None\n    self._ip_id_map = {}\n\n    # Clients lock not needed for now (client list not modified after initialization)\n    # but guarantees to be future safe for dynamic researcher clients' list\n    self._clients_lock = None\n    self._clients: Dict[str, GrpcClient] = {}\n\n    self._debug = debug\n    self._on_message = on_message\n</code></pre>"},{"location":"developer/api/transport/controller/#fedbiomed.transport.controller.GrpcController-functions","title":"Functions","text":""},{"location":"developer/api/transport/controller/#fedbiomed.transport.controller.GrpcController.is_connected","title":"is_connected","text":"<pre><code>is_connected()\n</code></pre> <p>\"Checks GrpcController is connected to any RPC client.</p> <p>This method should only be called from different thread than the one that asyncio loop running in.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Connection status</p> <p>Raises:</p> Type Description <code>FedbiomedCommunicationError</code> <p>node is not started</p> Source code in <code>fedbiomed/transport/controller.py</code> <pre><code>def is_connected(self) -&gt; bool:\n    \"\"\"\"Checks GrpcController is connected to any RPC client.\n\n    This method should only be called from different thread than the one that asyncio loop running in.\n\n    Returns:\n        Connection status\n\n    Raises:\n        FedbiomedCommunicationError: node is not started\n    \"\"\"\n    if self._thread is None or not self._is_started.is_set():\n        raise FedbiomedCommunicationError(f\"{ErrorNumbers.FB628}: Communication client is not initialized.\")\n\n    if not self._thread.is_alive():\n        return False\n\n    future = asyncio.run_coroutine_threadsafe(\n        super().is_connected(), self._loop\n    )\n    return future.result()\n</code></pre>"},{"location":"developer/api/transport/controller/#fedbiomed.transport.controller.GrpcController.send","title":"send","text":"<pre><code>send(message, broadcast=False)\n</code></pre> <p>Sends given message to researcher</p> <p>Researcher id must exist in the message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to send to researcher</p> required <code>broadcast</code> <code>bool</code> <p>If True, broadcasts the given message to all available. This does not prevent adding <code>researcher_id</code> to the message. The attribute <code>researcher_id</code> in the message should be <code>&lt;unknown&gt;</code></p> <code>False</code> <p>Raises:</p> Type Description <code>FedbiomedCommunicationError</code> <p>bad argument type</p> <code>FedbiomedCommunicationError</code> <p>node is not started</p> Source code in <code>fedbiomed/transport/controller.py</code> <pre><code>def send(self, message: Message, broadcast: bool = False) -&gt; None:\n    \"\"\"Sends given message to researcher\n\n    Researcher id must exist in the message.\n\n    Args:\n        message: Message to send to researcher\n        broadcast: If True, broadcasts the given message to all available.\n            This does not prevent adding `researcher_id` to the message.\n            The attribute `researcher_id` in the message should be `&lt;unknown&gt;`\n\n    Raises:\n        FedbiomedCommunicationError: bad argument type\n        FedbiomedCommunicationError: node is not started\n    \"\"\"\n    if not isinstance(message, Message):\n        raise FedbiomedCommunicationError(\n            f\"{ErrorNumbers.FB628}: bad argument type for message, expected `Message`, got `{type(message)}`\")\n\n    if not self._is_started.is_set():\n        raise FedbiomedCommunicationError(f\"{ErrorNumbers.FB628}: Communication client is not initialized.\")\n\n    asyncio.run_coroutine_threadsafe(\n        super().send(message, broadcast), self._loop\n    )\n</code></pre>"},{"location":"developer/api/transport/controller/#fedbiomed.transport.controller.GrpcController.start","title":"start","text":"<pre><code>start(on_finish=None)\n</code></pre> <p>Start GRPCClients in a thread.</p> <p>Parameters:</p> Name Type Description Default <code>on_finish</code> <code>Optional[Callable]</code> <p>Called when the tasks for handling all known researchers have finished.  Callable has no argument. If None, then no action is taken.</p> <code>None</code> Source code in <code>fedbiomed/transport/controller.py</code> <pre><code>def start(self, on_finish: Optional[Callable] = None) -&gt; None:\n    \"\"\"Start GRPCClients in a thread.\n\n    Args:\n        on_finish: Called when the tasks for handling all known researchers have finished. \n            Callable has no argument. If None, then no action is taken.\n    \"\"\"\n    # Adds grpc handler to send node logs to researchers\n    logger.add_grpc_handler(on_log=self.send, node_id=self._node_id)\n\n    self._thread = threading.Thread(target=self._run, args=(on_finish,), daemon=True)\n    self._thread.start()\n</code></pre>"},{"location":"developer/api/transport/node_agent/","title":"NodeAgent","text":""},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent-attributes","title":"Attributes","text":""},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.GRPC_SERVER_TASK_WAIT_TIMEOUT","title":"GRPC_SERVER_TASK_WAIT_TIMEOUT  <code>module-attribute</code>","text":"<pre><code>GRPC_SERVER_TASK_WAIT_TIMEOUT = 10\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent-classes","title":"Classes","text":""},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.AgentStore","title":"AgentStore","text":"<pre><code>AgentStore(loop, on_forward)\n</code></pre> <p>Stores node agents</p> <p>Parameters:</p> Name Type Description Default <code>loop</code> <code>AbstractEventLoop</code> <p>asyncio event loop that research server runs. Agent store should use same event loop for async operations</p> required <code>on_forward</code> <code>Optional[Awaitable[None]]</code> <p>Coroutine for handling overlay messages to forward unchanged to a node</p> required Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>def __init__(\n    self, loop: asyncio.AbstractEventLoop, on_forward: Optional[Awaitable[None]]\n) -&gt; None:\n    \"\"\"Constructor of the agent store\n\n    Args:\n        loop: asyncio event loop that research server runs. Agent store should use\n            same event loop for async operations\n        on_forward: Coroutine for handling overlay messages to forward unchanged to a node\n    \"\"\"\n    self._node_agents: NodeAgent = {}\n\n    self._loop = loop\n    self._on_forward = on_forward\n\n    # protect read/write operations on self._node_agents\n    self._store_lock = asyncio.Lock()\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.AgentStore-functions","title":"Functions","text":""},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.AgentStore.get","title":"get  <code>async</code>","text":"<pre><code>get(node_id)\n</code></pre> <p>Gets node agent by given node id</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>Id of the node, or None if no agent exists for this node ID</p> required Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>async def get(self, node_id: str) -&gt; Optional[NodeAgent]:\n    \"\"\"Gets node agent by given node id\n\n    Args:\n        node_id: Id of the node, or None if no agent exists for this node ID\n    \"\"\"\n    async with self._store_lock:\n        return self._node_agents.get(node_id)\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.AgentStore.get_all","title":"get_all  <code>async</code>","text":"<pre><code>get_all()\n</code></pre> <p>Returns all node agents regardless of their status (ACTIVE, DISCONNECTED, ...).</p> <p>Returns:</p> Type Description <code>Dict[str, NodeAgent]</code> <p>Dictionary of node agent objects, by node ID</p> Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>async def get_all(self) -&gt; Dict[str, NodeAgent]:\n    \"\"\"Returns all node agents regardless of their status (ACTIVE, DISCONNECTED, ...).\n\n    Returns:\n        Dictionary of node agent objects, by node ID\n    \"\"\"\n\n    async with self._store_lock:\n        # a shallow copy is wanted so that\n        # - we have a distinct (stable) list of NodeAgents that can be processed in calling func\n        # - we use same NodeAgents objects (not a copy)\n        return copy.copy(self._node_agents)\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.AgentStore.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(node_id)\n</code></pre> <p>Retrieves a node agent for a given node ID.</p> <p>Depending if this node is already known to the store this method gets existing agent or. registers a new agent.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>ID of receiving node</p> required Return <p>The node agent to manage tasks that are assigned to it.</p> Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>async def retrieve(self, node_id: str) -&gt; NodeAgent:\n    \"\"\"Retrieves a node agent for a given node ID.\n\n    Depending if this node is already known to the store this method gets existing agent or.\n    registers a new agent.\n\n    Args:\n        node_id: ID of receiving node\n\n    Return:\n        The node agent to manage tasks that are assigned to it.\n    \"\"\"\n    # Lock during all sequence to ensure atomicity\n    async with self._store_lock:\n        node = self._node_agents.get(node_id)\n        if not node:\n            node = NodeAgent(\n                id=node_id, loop=self._loop, on_forward=self._on_forward\n            )\n            self._node_agents.update({node_id: node})\n\n    return node\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeActiveStatus","title":"NodeActiveStatus","text":"<p>               Bases: <code>Enum</code></p> <p>Node active status types</p> <p>Attributes:</p> Name Type Description <code>WAITING</code> <p>Corresponds status where researcher server waits another GetTask request after the previous one is completed.</p> <code>ACTIVE</code> <p>Listening for the task with open RPC call</p> <code>DISCONNECTED</code> <p>No GetTask RPC call running from the node</p>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeActiveStatus-attributes","title":"Attributes","text":""},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeActiveStatus.ACTIVE","title":"ACTIVE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ACTIVE = 2\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeActiveStatus.DISCONNECTED","title":"DISCONNECTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DISCONNECTED = 3\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeActiveStatus.WAITING","title":"WAITING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WAITING = 1\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgent","title":"NodeAgent","text":"<pre><code>NodeAgent(id, loop, on_forward)\n</code></pre> <p>               Bases: <code>NodeAgentAsync</code></p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>node unique ID</p> required <code>loop</code> <code>AbstractEventLoop</code> <p>event loop</p> required <code>on_forward</code> <code>Optional[Awaitable[None]]</code> <p>Coroutine for handling overlay messages to forward unchanged to a node</p> required Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>def __init__(\n    self,\n    id: str,\n    loop: asyncio.AbstractEventLoop,\n    on_forward: Optional[Awaitable[None]],\n) -&gt; None:\n    \"\"\"Represent the client that connects to gRPC server\n\n    Args:\n        id: node unique ID\n        loop: event loop\n        on_forward: Coroutine for handling overlay messages to forward unchanged to a node\n    \"\"\"\n    self._id: str = id\n    self._on_forward = on_forward\n    self._last_request: Optional[datetime] = None\n    self._replies = Replies()\n    self._stopped_request_ids = []\n    # Node should be active when it is first instantiated\n    self._status: NodeActiveStatus = NodeActiveStatus.ACTIVE\n\n    self._queue = asyncio.Queue()\n    self._loop = loop\n    self._status_task: Optional[asyncio.Task] = None\n\n    # protect read/write operations on self._status + self._status_task)\n    self._status_lock = asyncio.Lock()\n    self._replies_lock = asyncio.Lock()\n    self._stopped_request_ids_lock = asyncio.Lock()\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgent-attributes","title":"Attributes","text":""},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgent.status","title":"status  <code>property</code>","text":"<pre><code>status\n</code></pre> <p>Getter for node status.</p> <p>Returns:</p> Type Description <code>NodeActiveStatus</code> <p>node status</p>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgent-functions","title":"Functions","text":""},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgent.flush","title":"flush","text":"<pre><code>flush(request_id, stopped=False)\n</code></pre> <p>Flush processed replies</p> <p>Parameters:</p> Name Type Description Default <code>request_id</code> <code>str</code> <p>request ID for which the replies should be flushed</p> required <code>stopped</code> <code>bool</code> <p>the request was stopped during processing</p> <code>False</code> Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>def flush(self, request_id: str, stopped: bool = False) -&gt; None:\n    \"\"\"Flush processed replies\n\n    Args:\n        request_id: request ID for which the replies should be flushed\n        stopped: the request was stopped during processing\n    \"\"\"\n    asyncio.run_coroutine_threadsafe(super().flush(request_id, stopped), self._loop)\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgent.send","title":"send","text":"<pre><code>send(message, on_reply=None)\n</code></pre> <p>Send message to researcher.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to send to the researcher</p> required Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>def send(self, message: Message, on_reply: Optional[Callable] = None) -&gt; None:\n    \"\"\"Send message to researcher.\n\n    Args:\n        message: Message to send to the researcher\n    \"\"\"\n    asyncio.run_coroutine_threadsafe(\n        self.send_async(message=message, on_reply=on_reply), self._loop\n    )\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgentAsync","title":"NodeAgentAsync","text":"<pre><code>NodeAgentAsync(id, loop, on_forward)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>node unique ID</p> required <code>loop</code> <code>AbstractEventLoop</code> <p>event loop</p> required <code>on_forward</code> <code>Optional[Awaitable[None]]</code> <p>Coroutine for handling overlay messages to forward unchanged to a node</p> required Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>def __init__(\n    self,\n    id: str,\n    loop: asyncio.AbstractEventLoop,\n    on_forward: Optional[Awaitable[None]],\n) -&gt; None:\n    \"\"\"Represent the client that connects to gRPC server\n\n    Args:\n        id: node unique ID\n        loop: event loop\n        on_forward: Coroutine for handling overlay messages to forward unchanged to a node\n    \"\"\"\n    self._id: str = id\n    self._on_forward = on_forward\n    self._last_request: Optional[datetime] = None\n    self._replies = Replies()\n    self._stopped_request_ids = []\n    # Node should be active when it is first instantiated\n    self._status: NodeActiveStatus = NodeActiveStatus.ACTIVE\n\n    self._queue = asyncio.Queue()\n    self._loop = loop\n    self._status_task: Optional[asyncio.Task] = None\n\n    # protect read/write operations on self._status + self._status_task)\n    self._status_lock = asyncio.Lock()\n    self._replies_lock = asyncio.Lock()\n    self._stopped_request_ids_lock = asyncio.Lock()\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgentAsync-attributes","title":"Attributes","text":""},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgentAsync.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Getter for node id.</p> <p>Returns:</p> Type Description <code>str</code> <p>node id</p>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgentAsync-functions","title":"Functions","text":""},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgentAsync.change_node_status_after_task","title":"change_node_status_after_task  <code>async</code>","text":"<pre><code>change_node_status_after_task()\n</code></pre> <p>Coroutine to execute each time RPC call is completed</p> Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>async def change_node_status_after_task(self) -&gt; None:\n    \"\"\"Coroutine to execute each time RPC call is completed\"\"\"\n    async with self._status_lock:\n        self._status = NodeActiveStatus.WAITING\n\n        if self._status_task is None:\n            self._status_task = asyncio.create_task(\n                self._change_node_status_disconnected()\n            )\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgentAsync.flush","title":"flush  <code>async</code>","text":"<pre><code>flush(request_id, stopped=False)\n</code></pre> <p>Flushes processed reply</p> <p>Parameters:</p> Name Type Description Default <code>request_id</code> <code>str</code> <p>request ID for which the replies should be flushed</p> required <code>stopped</code> <code>bool</code> <p>the request was stopped during processing</p> <code>False</code> Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>async def flush(self, request_id: str, stopped: bool = False) -&gt; None:\n    \"\"\"Flushes processed reply\n\n    Args:\n        request_id: request ID for which the replies should be flushed\n        stopped: the request was stopped during processing\n    \"\"\"\n    async with self._replies_lock:\n        if stopped and self._replies[request_id][\"reply\"] is None:\n            async with self._stopped_request_ids_lock:\n                self._stopped_request_ids.append(request_id)\n\n        self._replies.pop(request_id, None)\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgentAsync.get_task","title":"get_task","text":"<pre><code>get_task()\n</code></pre> <p>Get tasks assigned by the main thread</p> <p>Returns coroutine</p> <p>This function return an asyncio coroutine. Please use <code>await</code> while calling.</p> <p>Returns:</p> Type Description <code>Awaitable[Message]</code> <p>A coroutine to await for retrieving a list of: a task ; a number of send retries already done ; the time of first sending attempt in seconds since epoch</p> Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>def get_task(self) -&gt; Awaitable[Message]:\n    \"\"\"Get tasks assigned by the main thread\n\n    !!! note \"Returns coroutine\"\n        This function return an asyncio coroutine. Please use `await` while calling.\n\n    Returns:\n        A coroutine to await for retrieving a list of: a task ; a number of send retries already done ;\n            the time of first sending attempt in seconds since epoch\n    \"\"\"\n    return self._queue.get()\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgentAsync.on_reply","title":"on_reply  <code>async</code>","text":"<pre><code>on_reply(message)\n</code></pre> <p>Callback to execute each time new reply received from the node</p> Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>async def on_reply(self, message: Dict) -&gt; None:\n    \"\"\"Callback to execute each time new reply received from the node\"\"\"\n\n    message = Message.from_dict(message)\n\n    # Handle overlay messages to relay to a node\n    if isinstance(message, OverlayMessage):\n        await self._on_forward(message)\n        return\n\n    # Handle RequestReply messages for the researcher\n    if not message.request_id:\n        logger.error(\n            f\"Server received a reply from the client {self._id} that does \"\n            \"not contains request id.\"\n        )\n\n    async with self._replies_lock:\n        if message.request_id in self._replies:\n            if self._replies[message.request_id][\"reply\"] is None:\n                self._replies[message.request_id][\"reply\"] = message\n                self._replies[message.request_id][\"callback\"](message)\n            else:\n                # Handle case of multiple replies\n                # Avoid conflict with consumption of reply.\n                logger.warning(\n                    f\"Received multiple replies for request {message.request_id}. \"\n                    \"Keep first reply, ignore subsequent replies\"\n                )\n        else:\n            async with self._stopped_request_ids_lock:\n                if message.request_id in self._stopped_request_ids:\n                    logger.warning(\n                        \"Received a reply from a federated request that has been \"\n                        f\"stopped: {message.request_id}.\"\n                    )\n                    self._stopped_request_ids.remove(message.request_id)\n                else:\n                    logger.warning(\n                        f\"Received a reply from an unexpected request: {message.request_id}\"\n                    )\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgentAsync.send_async","title":"send_async  <code>async</code>","text":"<pre><code>send_async(message, on_reply=None, retry_count=0, first_send_time=None)\n</code></pre> <p>Async function send message to researcher.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to send to the researcher</p> required <code>on_reply</code> <code>Optional[Callable]</code> <p>optional callback to execute when receiving message reply</p> <code>None</code> <code>retry_count</code> <code>int</code> <p>number of retries already done for this message</p> <code>0</code> <code>first_send_time</code> <code>Optional[float]</code> <p>time of first send attempt for this message</p> <code>None</code> Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>async def send_async(\n    self,\n    message: Message,\n    on_reply: Optional[Callable] = None,\n    retry_count: int = 0,\n    first_send_time: Optional[float] = None,\n) -&gt; None:\n    \"\"\"Async function send message to researcher.\n\n    Args:\n        message: Message to send to the researcher\n        on_reply: optional callback to execute when receiving message reply\n        retry_count: number of retries already done for this message\n        first_send_time: time of first send attempt for this message\n    \"\"\"\n\n    async with self._status_lock:\n        if self._status == NodeActiveStatus.DISCONNECTED:\n            logger.info(f\"Node {self._id} is disconnected. Discard message.\")\n            return\n\n        if self._status == NodeActiveStatus.WAITING:\n            logger.info(\n                f\"Node {self._id} is in WAITING status. Server is \"\n                \"waiting for receiving a request from \"\n                \"this node to convert it as ACTIVE. Node will be updated \"\n                \"as DISCONNECTED soon if no request received.\"\n            )\n\n    # Updates replies\n    #\n    # Note: as forwarded messages don't have a `request_id` field we don't have to test\n    # if this is an OverlayMessage but check whether the field exists\n    async with self._replies_lock:\n        # update replies only for (1) request-response messages\n        # (2) that are not yet registered as pending request\n        if (\n            hasattr(message, \"request_id\")\n            and message.request_id\n            and message.request_id not in self._replies\n        ):\n            self._replies.update(\n                {message.request_id: {\"callback\": on_reply, \"reply\": None}}\n            )\n\n    if first_send_time is None:\n        first_send_time = time.time()\n    await self._queue.put([message, retry_count, first_send_time])\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgentAsync.set_active","title":"set_active  <code>async</code>","text":"<pre><code>set_active()\n</code></pre> <p>Updates node status as active</p> Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>async def set_active(self) -&gt; None:\n    \"\"\"Updates node status as active\"\"\"\n\n    async with self._status_lock:\n\n        # Inform user that node is online again\n        if self._status == NodeActiveStatus.DISCONNECTED:\n            logger.info(f\"Node {self._id} is back online!\")\n\n        self._status = NodeActiveStatus.ACTIVE\n\n        # Cancel status task if there is any running\n        if self._status_task:\n            self._status_task.cancel()\n            self._status_task = None\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgentAsync.status_async","title":"status_async  <code>async</code>","text":"<pre><code>status_async()\n</code></pre> <p>Getter for node status.</p> <p>Returns:</p> Type Description <code>NodeActiveStatus</code> <p>node status</p> Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>async def status_async(self) -&gt; NodeActiveStatus:\n    \"\"\"Getter for node status.\n\n    Returns:\n        node status\n    \"\"\"\n    async with self._status_lock:\n        # (deep)copy is not needed as long as this remains a simple value ...\n        return self._status\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.NodeAgentAsync.task_done","title":"task_done","text":"<pre><code>task_done()\n</code></pre> <p>Acknowledge completion of a de-queued task</p> Source code in <code>fedbiomed/transport/node_agent.py</code> <pre><code>def task_done(self) -&gt; None:\n    \"\"\"Acknowledge completion of a de-queued task\"\"\"\n    self._queue.task_done()\n</code></pre>"},{"location":"developer/api/transport/node_agent/#fedbiomed.transport.node_agent.Replies","title":"Replies","text":"<p>               Bases: <code>dict</code></p>"},{"location":"developer/api/transport/server/","title":"Server","text":""},{"location":"developer/api/transport/server/#fedbiomed.transport.server-attributes","title":"Attributes","text":""},{"location":"developer/api/transport/server/#fedbiomed.transport.server.GRPC_SERVER_SETUP_TIMEOUT","title":"GRPC_SERVER_SETUP_TIMEOUT  <code>module-attribute</code>","text":"<pre><code>GRPC_SERVER_SETUP_TIMEOUT = GRPC_CLIENT_CONN_RETRY_TIMEOUT + server_setup_timeout\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.MAX_GRPC_SERVER_SETUP_TIMEOUT","title":"MAX_GRPC_SERVER_SETUP_TIMEOUT  <code>module-attribute</code>","text":"<pre><code>MAX_GRPC_SERVER_SETUP_TIMEOUT = 20 * server_setup_timeout\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.MAX_SEND_DURATION","title":"MAX_SEND_DURATION  <code>module-attribute</code>","text":"<pre><code>MAX_SEND_DURATION = 300\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.server_setup_timeout","title":"server_setup_timeout  <code>module-attribute</code>","text":"<pre><code>server_setup_timeout = int(getenv('GRPC_SERVER_SETUP_TIMEOUT', 1))\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server-classes","title":"Classes","text":""},{"location":"developer/api/transport/server/#fedbiomed.transport.server.GrpcServer","title":"GrpcServer","text":"<pre><code>GrpcServer(host, port, on_message, ssl, debug=False)\n</code></pre> <p>               Bases: <code>_GrpcAsyncServer</code></p> <p>Grpc server implementation to be used by threads</p> <p>This class extends async implementation of gRPC server to be able to call async methods from different thread. Currently, it is used by [fedbiomed.researcher.requests.Requests][<code>Requests</code>] class that is instantiated in the main thread</p> <p>Attributes:</p> Name Type Description <code>_thread</code> <code>Optional[Thread]</code> <p>background thread of gRPC server</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>server DNS name or IP address</p> required <code>port</code> <code>str</code> <p>server TCP port</p> required <code>on_message</code> <code>Callable</code> <p>Callback function to execute once a message received from the nodes</p> required <code>ssl</code> <code>SSLCredentials</code> <p>Ssl credentials.</p> required <code>debug</code> <code>bool</code> <p>Activate debug mode for gRPC asyncio</p> <code>False</code> Source code in <code>fedbiomed/transport/server.py</code> <pre><code>def __init__(\n        self,\n        host: str,\n        port: str,\n        on_message: Callable,\n        ssl: SSLCredentials,\n        debug: bool = False,\n) -&gt; None:\n    \"\"\"Class constructor\n\n    Args:\n        host: server DNS name or IP address\n        port: server TCP port\n        on_message: Callback function to execute once a message received from the nodes\n        ssl: Ssl credentials.\n        debug: Activate debug mode for gRPC asyncio\n    \"\"\"\n\n    # inform all threads whether server is started\n    self._is_started = threading.Event()\n    self._ssl = ssl\n    self._host = host\n    self._port = port\n\n    self._server = None\n    self._debug = debug\n    self._on_message = on_message\n    self._loop = None\n    self._agent_store : Optional[AgentStore] = None\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.GrpcServer-functions","title":"Functions","text":""},{"location":"developer/api/transport/server/#fedbiomed.transport.server.GrpcServer.broadcast","title":"broadcast","text":"<pre><code>broadcast(message)\n</code></pre> <p>Broadcast message to all known and reachable nodes</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to broadcast</p> required <p>Raises:</p> Type Description <code>FedbiomedCommunicationError</code> <p>bad argument type</p> <code>FedbiomedCommunicationError</code> <p>server is not started</p> Source code in <code>fedbiomed/transport/server.py</code> <pre><code>def broadcast(self, message: Message) -&gt; None:\n    \"\"\"Broadcast message to all known and reachable nodes\n\n    Args:\n        message: Message to broadcast\n\n    Raises:\n        FedbiomedCommunicationError: bad argument type\n        FedbiomedCommunicationError: server is not started\n    \"\"\"\n    if not isinstance(message, Message):\n        raise FedbiomedCommunicationError(\n            f\"{ErrorNumbers.FB628}: bad argument type for message, expected `Message`, got `{type(message)}`\")\n\n    if not self._is_started.is_set():\n        raise FedbiomedCommunicationError(\n            f\"{ErrorNumbers.FB628}: Can not broadcast given message. \"\n            \"Communication client is not initialized.\")\n\n    self._run_threadsafe(super().broadcast(message))\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.GrpcServer.get_all_nodes","title":"get_all_nodes","text":"<pre><code>get_all_nodes()\n</code></pre> <p>Returns all known nodes</p> <p>Returns:</p> Type Description <code>List[NodeAgent]</code> <p>A list of node agents</p> <p>Raises:</p> Type Description <code>FedbiomedCommunicationError</code> <p>server is not started</p> Source code in <code>fedbiomed/transport/server.py</code> <pre><code>def get_all_nodes(self) -&gt; List[NodeAgent]:\n    \"\"\"Returns all known nodes\n\n    Returns:\n        A list of node agents\n\n    Raises:\n        FedbiomedCommunicationError: server is not started\n    \"\"\"\n    if not self._is_started.is_set():\n        raise FedbiomedCommunicationError(\n            f\"{ErrorNumbers.FB628}: Error while getting all nodes \"\n            \"connected:  Communication client is not initialized.\")\n\n    return self._run_threadsafe(super().get_all_nodes())\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.GrpcServer.get_node","title":"get_node","text":"<pre><code>get_node(node_id)\n</code></pre> <p>Returns given node</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <p>ID of node to retrieve</p> required <p>Returns:</p> Type Description <code>Optional[NodeAgent]</code> <p>A node agent</p> <p>Raises:</p> Type Description <code>FedbiomedCommunicationError</code> <p>server is not started</p> Source code in <code>fedbiomed/transport/server.py</code> <pre><code>def get_node(self, node_id) -&gt; Optional[NodeAgent]:\n    \"\"\"Returns given node\n\n    Args:\n        node_id: ID of node to retrieve\n\n    Returns:\n        A node agent\n\n    Raises:\n        FedbiomedCommunicationError: server is not started\n    \"\"\"\n    if not self._is_started.is_set():\n        raise FedbiomedCommunicationError(\n            f\"{ErrorNumbers.FB628}: Error while getting node '{node_id}':\"\n            \"Communication client is not initialized.\")\n\n    return self._run_threadsafe(super().get_node(node_id))\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.GrpcServer.is_alive","title":"is_alive","text":"<pre><code>is_alive()\n</code></pre> <p>Checks if the thread running gRPC server still alive</p> <p>Returns:</p> Type Description <code>bool</code> <p>gRPC server running status</p> <p>Raises:</p> Type Description <code>FedbiomedCommunicationError</code> <p>server is not started</p> Source code in <code>fedbiomed/transport/server.py</code> <pre><code>def is_alive(self) -&gt; bool:\n    \"\"\"Checks if the thread running gRPC server still alive\n\n    Returns:\n        gRPC server running status\n\n    Raises:\n        FedbiomedCommunicationError: server is not started\n    \"\"\"\n    if not self._is_started.is_set():\n        raise FedbiomedCommunicationError(\n            f\"{ErrorNumbers.FB628}: Can not check if thread is alive.\"\n            \"Communication client is not initialized.\")\n\n    # TODO: more tests about gRPC server and task status ?\n    return False if not isinstance(self._thread, threading.Thread) else self._thread.is_alive()\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.GrpcServer.send","title":"send","text":"<pre><code>send(message, node_id)\n</code></pre> <p>Send message to a specific node.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to send</p> required <code>node_id</code> <code>str</code> <p>Destination node unique ID</p> required <p>Raises:</p> Type Description <code>FedbiomedCommunicationError</code> <p>bad argument type</p> <code>FedbiomedCommunicationError</code> <p>server is not started</p> Source code in <code>fedbiomed/transport/server.py</code> <pre><code>def send(self, message: Message, node_id: str) -&gt; None:\n    \"\"\"Send message to a specific node.\n\n    Args:\n        message: Message to send\n        node_id: Destination node unique ID\n\n    Raises:\n        FedbiomedCommunicationError: bad argument type\n        FedbiomedCommunicationError: server is not started\n    \"\"\"\n    if not isinstance(message, Message):\n        raise FedbiomedCommunicationError(\n            f\"{ErrorNumbers.FB628}: bad argument type for message, expected `Message`, got `{type(message)}`\")\n\n    if not self._is_started.is_set():\n        raise FedbiomedCommunicationError(\n            f\"{ErrorNumbers.FB628.value}: Can not send message. \"\n            \"Communication client is not initialized.\")\n\n    self._run_threadsafe(super().send(message, node_id))\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.GrpcServer.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starts async GrpcServer</p> Source code in <code>fedbiomed/transport/server.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Starts async GrpcServer \"\"\"\n\n    self._thread = threading.Thread(target=self._run, daemon=True)\n    self._thread.start()\n\n    # FIXME: This implementation assumes that nodes will be able connect and server complete setup with this delay\n    logger.info(\"Starting researcher service...\")\n\n\n    logger.info(f'Waiting {GRPC_SERVER_SETUP_TIMEOUT}s for nodes to connect...')\n    time.sleep(GRPC_SERVER_SETUP_TIMEOUT)\n\n    sleep_ = 0\n    while len(self.get_all_nodes()) == 0:\n\n        if sleep_ == 0:\n            logger.info(f\"No nodes found, server will wait \"\n                        f\"{MAX_GRPC_SERVER_SETUP_TIMEOUT - GRPC_SERVER_SETUP_TIMEOUT} \"\n                        \"more seconds until a node creates connection.\")\n\n        if sleep_ &gt; MAX_GRPC_SERVER_SETUP_TIMEOUT - GRPC_SERVER_SETUP_TIMEOUT:\n            if len(self.get_all_nodes()) == 0:\n                logger.warning(\"Server has not received connection from any remote nodes in \"\n                               f\"MAX_GRPC_SERVER_SETUP_TIMEOUT: {MAX_GRPC_SERVER_SETUP_TIMEOUT} \"\n                               \"This may effect the request created right after the server initialization. \"\n                               \"However, server will keep running in the background so you can retry the \"\n                               \"operations for sending requests to remote nodes until one receives.\")\n            break\n\n        time.sleep(1)\n        sleep_ += 1\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.ResearcherServicer","title":"ResearcherServicer","text":"<pre><code>ResearcherServicer(agent_store, on_message)\n</code></pre> <p>               Bases: <code>ResearcherServiceServicer</code></p> <p>RPC Servicer</p> <p>Parameters:</p> Name Type Description Default <code>agent_store</code> <code>AgentStore</code> <p>The class that stores node agents</p> required <code>on_message</code> <code>Callable</code> <p>Callback function to execute once a message received from the nodes</p> required Source code in <code>fedbiomed/transport/server.py</code> <pre><code>def __init__(\n        self,\n        agent_store: AgentStore,\n        on_message: Callable\n) -&gt; None:\n    \"\"\"Constructor of gRPC researcher servicer\n\n    Args:\n        agent_store: The class that stores node agents\n        on_message: Callback function to execute once a message received from the nodes\n    \"\"\"\n    super().__init__()\n    self._agent_store = agent_store\n    self._on_message = on_message\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.ResearcherServicer-functions","title":"Functions","text":""},{"location":"developer/api/transport/server/#fedbiomed.transport.server.ResearcherServicer.Feedback","title":"Feedback  <code>async</code>","text":"<pre><code>Feedback(request, unused_context)\n</code></pre> <p>Executed for Feedback request received from the nodes</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Message</code> <p>Feedback message</p> required <code>unused_context</code> <code>ServicerContext</code> <p>Request service context</p> required Source code in <code>fedbiomed/transport/server.py</code> <pre><code>async def Feedback(\n        self,\n        request: ProtoBufMessage,\n        unused_context: grpc.aio.ServicerContext\n) -&gt; None:\n    \"\"\"Executed for Feedback request received from the nodes\n\n    Args:\n        request: Feedback message\n        unused_context: Request service context\n    \"\"\"\n\n    # Get the type of Feedback | log or scalar\n    one_of = request.WhichOneof(\"feedback_type\")\n    feedback = FeedbackMessage.from_proto(request)\n\n    # Execute on_message assigned by the researcher.requests modules\n    self._on_message(feedback.get_param(one_of), MessageType.convert(one_of))\n\n    return Empty()\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.ResearcherServicer.GetTaskUnary","title":"GetTaskUnary  <code>async</code>","text":"<pre><code>GetTaskUnary(request, context)\n</code></pre> <p>Gets unary RPC request and return stream of response</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Message</code> <p>RPC request</p> required <code>context</code> <code>ServicerContext</code> <p>RPC peer context</p> required Source code in <code>fedbiomed/transport/server.py</code> <pre><code>async def GetTaskUnary(\n        self,\n        request: ProtoBufMessage,\n        context: grpc.aio.ServicerContext\n) -&gt; None:\n    \"\"\"Gets unary RPC request and return stream of response\n\n    Args:\n        request: RPC request\n        context: RPC peer context\n    \"\"\"\n    task_request = TaskRequest.from_proto(request).get_dict()\n    logger.debug(f\"Node: {task_request.get('node')} polling for the tasks\")\n\n    node_agent = await self._agent_store.retrieve(node_id=task_request[\"node\"])\n\n    # Update node active status as active\n    await node_agent.set_active()\n\n    task = None\n    try:\n        while True:\n            task, retry_count, first_send_time = await node_agent.get_task()\n\n            # Choice: mark task as de-queued as soon only if really sent\n            node_agent.task_done()\n\n            # discard if message too old\n            if first_send_time + MAX_SEND_DURATION &gt; time.time():\n                break\n            else:\n                task = None\n                logger.warning(f\"Message to send is older than {MAX_SEND_DURATION} seconds. Discard message.\")\n\n        task_bytes = Serializer.dumps(task.to_dict())\n\n        chunk_range = range(0, len(task_bytes), MAX_MESSAGE_BYTES_LENGTH)\n        for start, iter_ in zip(chunk_range, range(1, len(chunk_range) + 1)):\n            stop = start + MAX_MESSAGE_BYTES_LENGTH\n\n            try:\n                yield TaskResponse(\n                    size=len(chunk_range),\n                    iteration=iter_,\n                    bytes_=task_bytes[start:stop]\n                ).to_proto()\n            except GeneratorExit:\n                # schedule resend if task sending could not be completed\n                # =&gt; retry send as long as (1) send not successful\n                # (2) max retries not reached\n                # =&gt; else discard message\n                #\n                # Note: if node is disconnected then back online, message is retried after reconnection.\n                # This is not fully coherent with upper layers (Requests) that may trigger an application\n                # level failure in the while, but it is mitigated by the MAX_SEND_DURATION\n                if retry_count &lt; MAX_SEND_RETRIES:\n                    await node_agent.send_async(\n                        message=task, on_reply=None, retry_count=retry_count + 1, first_send_time=first_send_time\n                    )\n                else:\n                    logger.warning(f\"Message cannot be sent after {MAX_SEND_RETRIES} retries. Discard message.\")\n                await node_agent.change_node_status_after_task()\n                # need return here to avoid RuntimeError\n                return\n\n    except asyncio.CancelledError:\n        if task is not None and retry_count is not None and first_send_time is not None:\n            # schedule resend if task was pulled from queue\n            if retry_count &lt; MAX_SEND_RETRIES:\n                await node_agent.send_async(\n                    message=task, on_reply=None, retry_count=retry_count + 1, first_send_time=first_send_time\n                )\n            else:\n                logger.warning(f\"Message cannot be sent after {MAX_SEND_RETRIES} retries. Discard message.\")\n    finally:\n        await node_agent.change_node_status_after_task()\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.ResearcherServicer.ReplyTask","title":"ReplyTask  <code>async</code>","text":"<pre><code>ReplyTask(request_iterator, unused_context)\n</code></pre> <p>Gets stream replies from the nodes</p> <p>Parameters:</p> Name Type Description Default <code>request_iterator</code> <code>Iterable[Message]</code> <p>Iterator for streaming</p> required <code>unused_context</code> <code>ServicerContext</code> <p>Request service context</p> required Source code in <code>fedbiomed/transport/server.py</code> <pre><code>async def ReplyTask(\n        self,\n        request_iterator: Iterable[ProtoBufMessage],\n        unused_context: grpc.aio.ServicerContext\n) -&gt; None:\n    \"\"\"Gets stream replies from the nodes\n\n    Args:\n        request_iterator: Iterator for streaming\n        unused_context: Request service context\n    \"\"\"\n\n    reply = bytes()\n    async for answer in request_iterator:\n        reply += answer.bytes_\n        if answer.size != answer.iteration:\n            continue\n\n        # Deserialize message\n        message = Serializer.loads(reply)\n\n        # Replies are handled by node agent callbacks\n        node = await self._agent_store.get(message[\"node_id\"])\n        await node.on_reply(message)\n\n        reply = bytes()\n\n    return Empty()\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.SSLCredentials","title":"SSLCredentials","text":"<pre><code>SSLCredentials(key, cert)\n</code></pre> <p>Contains credentials for SSL certifcate of the gRPC server</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>path to private key</p> required <code>cert</code> <code>str</code> <p>path to certificate</p> required Source code in <code>fedbiomed/transport/server.py</code> <pre><code>def __init__(self, key: str, cert: str):\n    \"\"\"Reads private key and cert file\n\n    Args:\n        key: path to private key\n        cert: path to certificate\n    \"\"\"\n    with open(key, 'rb') as f:\n        self.private_key = f.read()\n    with open(cert, 'rb') as f:\n        self.certificate = f.read()\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.SSLCredentials-attributes","title":"Attributes","text":""},{"location":"developer/api/transport/server/#fedbiomed.transport.server.SSLCredentials.certificate","title":"certificate  <code>instance-attribute</code>","text":"<pre><code>certificate = read()\n</code></pre>"},{"location":"developer/api/transport/server/#fedbiomed.transport.server.SSLCredentials.private_key","title":"private_key  <code>instance-attribute</code>","text":"<pre><code>private_key = read()\n</code></pre>"},{"location":"getting-started/basic-example/","title":"Basic Collaborative Learning Setup","text":"<p>Fed-BioMed enables users to quickly create a basic collaborative learning setup within seconds, providing an intuitive framework for exploring federated learning. This article serves as a comprehensive guide, walking you through the process of creating multiple nodes, designing a training plan for collaborative learning, and executing an experiment that performs model training.</p>"},{"location":"getting-started/basic-example/#setup-your-working-directory","title":"Setup your working directory","text":"<p>It is essential to set up a dedicated working directory in your local environment to organize all Fed-BioMed components. Additionally, create a <code>data</code> directory within the working directory to store the dataset that will be used in this guide.</p>"},{"location":"getting-started/basic-example/#create-your-nodes","title":"Create your nodes","text":"<p>The following two commands will create two node components. The directory that components will be initialized will be relative to the path that these commands are executed. Therefore, please make sure that you changed your directory to the working directory that you've created for Fed-BioMed.</p> <pre><code>fedbiomed component create -c node -p ./fbm-node\nfedbiomed component create -c node -p ./fbm-second-node\n</code></pre>"},{"location":"getting-started/basic-example/#deploy-mnist-dataset","title":"Deploy MNIST Dataset","text":"<p>The Fed-BioMed CLI comes with a predefined MNIST dataset configuration that automatically downloads the MNIST dataset and deploys it onto the nodes. The following commands will download the MNIST dataset into the <code>data</code> directory you created and deploy it to <code>my-node</code> and <code>my-second-node</code>.</p> <pre><code>fedbiomed node -p ./fbm-node dataset add --mnist\nfedbiomed node -p ./fbm-second-node dataset add --mnist\n</code></pre>"},{"location":"getting-started/basic-example/#start-your-nodes","title":"Start your Nodes","text":"<p>Please open a separate terminal window and start your nodes by running the appropriate commands for each node in their respective directories. For example:</p> <ol> <li> <p>For <code>fbm-node</code>, run:    <pre><code>fedbiomed node --path ./fbm-node start\n</code></pre></p> </li> <li> <p>For <code>fbm-second-node</code>, run:    <pre><code>fedbiomed node --path ./fbm-second-node start\n</code></pre></p> </li> </ol> <p>Ensure each node is started in its own terminal window so that both can run concurrently.</p>"},{"location":"getting-started/basic-example/#researcher","title":"Researcher","text":"<p>The Researcher component is the central entity that orchestrates the collaborative learning experiment across multiple nodes by allowing users to define the training plan and experiment.</p> <p>To create your Researcher component in the same working directory, execute the following command:</p> <pre><code>fedbiomed component create -c researcher\n</code></pre> <p>The command above will create a <code>fbm-researcher</code> folder in your working directory, where the Researcher component assets will be stored. By starting the Researcher component, you will launch a Jupyter Notebook that is configured to use the Researcher component created in your working directory.</p> <p>To start the Researcher component, run the following command:</p> <pre><code>fedbiomed researcher start\n</code></pre> <p>This will open a Jupyter Notebook, allowing you to interact with the Researcher component and begin configuring your collaborative learning experiment.</p>"},{"location":"getting-started/basic-example/#create-and-train-your-federated-learning-model","title":"Create and train your Federated Learning model","text":"<p>Fed-BioMed does not require you to modify your machine learning models to perform collaborative learning. Instead, it requires you to create a training plan where you can integrate your existing model, optimizer, and training steps. The following example demonstrates how to define a training plan using an existing PyTorch model and training loop.</p> <p> Template of a model definition in Pytorch (left) and in Fed-BioMed using Pytorch (right) </p> Classical Pytorch Fed-BioMed <p> Template of a model training loop in Pytorch (left) and in Fed-BioMed using Pytorch (right)</p> Classical Pytorch Fed-BioMed"},{"location":"getting-started/basic-example/#executing-your-training","title":"Executing your training","text":"<p>Once the Jupyter Notebook has started, you can define your training plan and set up your experiment. After configuring the necessary parameters, you can execute your experiment to initiate the collaborative learning process.</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\n\n\nclass MyRemoteTrainingPlan(TorchTrainingPlan):\n\n    def init_model(self):\n        return self.Net()\n\n    def init_optimizer(self):\n        return torch.optim.SGD(self.parameters(), lr=0.01)\n\n    def init_dependencies(self):\n        return [\"from torchvision import datasets, transforms\"]\n\n    class Net(nn.Module):\n        def __init__(self, model_args: dict = {}):\n            super().__init__()\n            self.in_features = 28*28\n            self.out_features = 10\n            self.fc1 = nn.Linear(self.in_features, 50)\n            self.fc2 = nn.Linear(50, self.out_features)\n\n\n        def forward(self, x):\n            x = x.reshape(-1, 28*28)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.fc2(x)\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_step(self, data, target):\n        output = self.forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n\n    def training_data(self):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        mnist_dataset = datasets.MNIST(self.dataset_path,\n                                       train=True,\n                                       download=False,\n                                       transform=transform)\n        return DataManager(mnist_dataset,  shuffle=True)\n</code></pre> <p>Next, create your training arguments and experiment, then initiate collaborative learning using the two previously started nodes.</p> <pre><code>model_args = {}\n\ntraining_args = {\n    'loader_args': {\n        'batch_size': 48,\n    },\n    'epochs': 20,\n    'dry_run': False,\n}\n\nfrom fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MNIST', '#dataset']\nrounds = 2\n\n# model training\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyRemoteTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\nexp.run()\n\n# model saving\nexp.training_plan().export_model('./my_trained_model')\n</code></pre>"},{"location":"getting-started/basic-example/#going-further","title":"Going Further","text":"<p>Please visit the Configuration Guide to learn more about using the Fed-BioMed CLI to initialize and manage Fed-BioMed components. The Configuration Guide will help you establish the basics of Fed-BioMed federated learning infrastructure needed to follow the tutorials provided in this documentation.</p>"},{"location":"getting-started/configuration/","title":"Fed-BioMed Component configuration","text":"<p>Fed-BioMed components need to be configured before using Fed-BioMed. The Fed-BioMed CLI simplifies this process by automating the setup of default components to enhance the user experience, especially for testing purposes. This article will guide you through the basic configuration steps required for the minimize initialization of Fed-BioMed components.</p> <p>For more detailed configuration instructions, please refer to the Configuring Nodes and Deployment user guides.</p>"},{"location":"getting-started/configuration/#fed-biomed-components","title":"Fed-BioMed Components","text":"<p>Fed-BioMed components are instances provided by Fed-BioMed with distinct responsibilities in a federated learning infrastructure. While they can operate independently and do not directly depend on each other, they are necessary to complete a federated learning infrastructure. These components may also include optional sub-components to enhance usability (e.g., the Node GUI, a user-friendly web application for managing nodes).</p> <p>This guide will be focusing on Node and Researcher components to create a basic infrastructure to follow the examples and the tutorials in the documentation. To find out more about Fed-BioMed architecture and component please refer to Fed-BioMed architecture</p>"},{"location":"getting-started/configuration/#initializing-components","title":"Initializing Components","text":"<p>The Fed-BioMed CLI is configured to initialize components if they do not already exist. Initializing component means creating a specific folder for the component to keep its assets. While Fed-BioMed uses default folder name for each component type it is also possible to use custom paths to address or initialize components with different folder names. For the components that are not existing, the CLI asks for the permission before creating the them.   While this feature allows to create component quickly, CLI also provide a specific option just for initializing/creating components explicitly.</p> <p>It is important to note that, unless specified, the component directory will be generated in the directory where the command is executed. This behavior applies to both the Researcher and Node components.</p> <p>-y option to create components automatically</p> <p>To avoid approving component creation option <code>-y</code> can be used. e.g <code>fedbiomed node -y [ACTION]</code> or <code>fedbiomed researcher -y [ACTION]</code>. <code>-y</code> option should be specified right after component type specification.</p>"},{"location":"getting-started/configuration/#component-directories","title":"Component directories","text":"<p>Component configurations, and other necessary assets to provide components to function properly are located in its own directory of the component. These directories are called component directory, and the directory name (folder name) refers to component name.  While Fed-BioMed components come with default components names, it is also possible to use different folder names. This functionality is useful especially if several components are hosted in the same parent directory of the same file system.</p> <p>First execution of the following command will create will create a component directory called <code>fbm-node</code> (default component name) located in the directory where this command is executed, and it will ask for permission to initialize component directory if it is not existing.</p> <pre><code>fedbiomed node dataset list\n</code></pre> <p>Once the default component is created, Fed-BioMed CLI command execution in the same directory will use the default component named <code>fbm-node</code>. However, it is also possible to indicate component directory to be able to execute Fed-BioMed command from different directories.</p> <pre><code>fedbiomed node --path some/other/directory/fbm-node dataset list\n</code></pre> <p>This functionality is also same for researcher component.</p>"},{"location":"getting-started/configuration/#creating-components-beforehand","title":"Creating components beforehand","text":"<p>Creating components beforehand using the CLI is recommended when creating multiple components before performing any actions on them. A good practice is also to create a separate directory to keep all generated components in one place. For example,  a directory called <code>fbm-components</code> can be created to hold all Fed-BioMed components. This will allow to access these component easily.</p> <p>To create a Node component:</p> <pre><code>fedbiomed component create --component node --path fbm-components/my-node\n</code></pre> <p>To create another Node component:</p> <pre><code>fedbiomed component create --component node --path fbm-components/my-second-node\n</code></pre> <p>The Researcher component is also essential for every Fed-BioMed setup:</p> <pre><code>fedbiomed component create --component researcher --path fbm-components/my-researcher\n</code></pre>"},{"location":"getting-started/configuration/#managing-components","title":"Managing Components","text":"<p>The Fed-BioMed CLI is designed to manage different types of components and their sub-components. It also enables managing multiple components by specifying the path where each component is initialized. This section explains how to address distinct components and minimize errors caused by incorrect component specifications.</p>"},{"location":"getting-started/configuration/#managing-node-component","title":"Managing Node Component","text":"<p>All the actions that are specific to Node component should be declared after the option <code>node</code> of <code>fedbiomed</code> command. For example, <code>fedbiomed node dataset list</code> will list all the datasets deployed on the dataset. You can list all possible options and action by executing <code>fedbiomed node --help</code>.</p> <p>The execution of <code>fedbiomed node</code> without <code>--path</code> option will assume the working directory is the directory where the command is executed, and look for <code>fbm-node</code> folder to chose default node instantiation. If this folder is not existing it will ask permission to create one.  Therefore, it is important to double check the directory that <code>fedbiomed</code> command is going to me executed.</p>"},{"location":"getting-started/configuration/#multi-node-setup","title":"Multi-node setup","text":"<p>As it is mentioned before, <code>--path</code> option allows to chose distinct node initialization. This option accepts relative or absolute paths. While managing multiple components it is highly recommended to use <code>--path</code> option address correct component initialization.</p> <p>You can find an example of multi-node initialization;</p> <p><pre><code>cd my-nodes/\nfedbiomed component create --component node --path ./my-node\nfedbiomed component create --component node --path ./my-second-node\n</code></pre> Here is how these components are chosen for specific actions;</p> <pre><code># List datasets in component initialized in `./my-node`\nfedbiomed node --path ./my-node dataset list\n</code></pre> <p>or,</p> <pre><code>fedbiomed node --path ./my-second-node training-plan list\n</code></pre>"},{"location":"getting-started/configuration/#single-node-setup","title":"Single-node setup","text":"<p>When working with a single-node setup, it is recommended to use default component names to avoid the need for the <code>--path</code> declaration each time a <code>fedbiomed</code> command is executed.</p>"},{"location":"getting-started/configuration/#initializing-a-default-node-component","title":"Initializing a default node component","text":"<p>The following command initializes a default Node component:</p> <pre><code>fedbiomed component create -c node\n</code></pre> <p>After initializing a default component, the <code>fedbiomed node</code> command can be executed without specifying the <code>--path</code> option, unless it is run from a different directory than where <code>fbm-node</code> is located.</p> <p>Example:</p> <pre><code># List datasets deployed on the default node\nfedbiomed node dataset list\n</code></pre>"},{"location":"getting-started/configuration/#initializing-a-default-node-component-in-a-specific-directory","title":"Initializing a default node component in a specific directory","text":"<p>To initialize a default Node component in a specific directory, component path has to be declared, and the folder should be named <code>fbm-node</code>:</p> <pre><code>fedbiomed component create -c node -p /path/to/fbm-node\n</code></pre> <p>If the <code>fedbiomed</code> command is executed from a directory other than <code>/path/to/fbm-node</code>, the <code>--path</code> option must be used to specify the correct Node component:</p> <pre><code># Example with explicit path\nfedbiomed node dataset list --path /path/to/fbm-node\n</code></pre> <p>More about configuring nodes</p> <p>Please visit node user guide to find out more about configuring nodes and deploying dataset.</p>"},{"location":"getting-started/configuration/#managing-researcher-component","title":"Managing researcher component","text":"<p>Unlike the Node component, the Researcher serves as the server in the Fed-BioMed federated learning infrastructure. There can only be one active Researcher that Nodes connect to at a time. The default component name for the Researcher is <code>fbm-researcher</code>.</p> <p>While it is possible to create multiple Researcher components, this is only useful when different Researcher configurations are needed to support separate federated learning setups.</p>"},{"location":"getting-started/configuration/#creating-and-managing-researcher-components","title":"Creating and Managing Researcher Components","text":"<p>The process of creating and selecting a Researcher component is similar to that of a Node. You can specify a particular Researcher component using the <code>--path</code> option.</p> <p>Example: Default component creation</p> <pre><code>fedbiomed component create -c researcher\n</code></pre> <p>Example: using a researcher component from a specific directory</p> <pre><code>fedbiomed researcher --path /path/to/fbm-researcher start\n</code></pre>"},{"location":"getting-started/configuration/#choosing-the-correct-component-initialization","title":"Choosing the Correct Component Initialization","text":"<p>Unlike Node components, the Researcher component runs within a Python session rather than as a standalone process. This session must be correctly configured to use the intended Researcher component. The <code>start</code> command for the Researcher has been designed to ensure to use correct initialization.</p> <p>Example: Default component creation and starting</p> <pre><code># Default component creation\nfedbiomed component create -c researcher\n\n# Start a jupyter notebook for this component\nfedbiomed researcher start\n</code></pre> <p>The commands above will create a default Researcher component (<code>fbm-researcher</code>) and start a Jupyter Notebook that is pre-configured to use the created Researcher.</p> <p>This behavior can also be achieved by specifying the <code>--path</code> option when starting the Researcher from a different directory:</p> <pre><code>fedbiomed researcher --path /path/to/fbm-researcher start\n</code></pre>"},{"location":"getting-started/configuration/#executing-plain-python-scripts-without-fedbiomed-researcher","title":"Executing Plain Python Scripts Without <code>fedbiomed researcher</code>","text":"<p>The Researcher does not need to be started using a Jupyter Notebook. You can use a plain Python script to define and execute experiments. However, in this case, the correct Researcher initialization directory must be set at the beginning of the script.</p> <p>Example: using environment variables in a script</p> <pre><code>import os\nos.environ[\"FBM_RESEARCHER_COMPONENT_ROOT\"] = '/path/to/fbm-researcher'\n\n# Remaining code for the experiment\n# ...\n</code></pre> <p>Example: Setting environment variables in the command Line</p> <p>You can achieve the same behavior by exporting the <code>FBM_RESEARCHER_COMPONENT_ROOT</code> environment variable before running your script:</p> <pre><code>export FBM_RESEARCHER_COMPONENT_ROOT=/path/to/fbm-researcher\npython my-experiment.py\n</code></pre> <p>Alternatively, you can set the environment variable inline:</p> <pre><code>FBM_RESEARCHER_COMPONENT_ROOT=/path/to/fbm-researcher python my-experiment.py\n</code></pre>"},{"location":"getting-started/configuration/#cleaning","title":"Cleaning","text":"<p>A Fed-BioMed instance can handle successive operations like adding and then removing nodes or datasets, conducting sequential experiments. But after testing, thing may get wrong. At this point, you may want to clean all things. Afterwards, you will need to restart from scratch (add datasets to nodes, start nodes, etc...)</p> <p>To clean your Fed-BioMed instance :</p> <ul> <li>stop the researcher : shutdown the notebook kernel (<code>Quit</code> in on the notebook interface or <code>ctrl-C</code> on the console)</li> <li>stop the nodes : interrupt (<code>ctrl-C</code>) on the nodes console</li> <li>remove all configuration files, dataset sharing configuration, temporary files, caches for all Fed-BioMed components with :</li> </ul> <pre><code>$ rm -rf COMPONENT_DIR\n</code></pre> <p>Where <code>COMPONENT_DIR</code> is:</p> <ul> <li>for a node, the parameter provided as <code>fedbiomed node -p COMPONENT_DIR</code> or by default <code>fbm-node</code> if no parameter was given</li> <li>for a researcher, the parameter provided as <code>fedbiomed researcher -p COMPONENT_DIR</code> or by default <code>fbm-researcher</code> if no parameter was given</li> </ul> <p>When you restart a node after cleaning the Fed-BioMed instance, the node doesn't provides any dataset, as the dataset sharing configuration was reset in the cleaning process. Of course, Fed-BioMed did not delete any data, it just stopped sharing them.</p>"},{"location":"getting-started/fedbiomed-architecture/","title":"Fedbiomed Main components","text":"<p>Fed-BioMed has two components ensuring the correct execution of collaborative learning algorithms. These components  are <code>Node</code> and <code>Researcher</code>, and are defined in the following sections:</p>"},{"location":"getting-started/fedbiomed-architecture/#node","title":"<code>Node</code>","text":"<p>In Fed-BioMed, <code>Node</code> provides 2 main functionalities: </p> <ul> <li><code>Node</code> stores datasets, upon which the Federated Learning Model will be trained and Federated Analytics will be performed. Datasets paths towards files and folders in a TinyDB database, as well as other metadata such as datatype, data shape, ... .</li> <li><code>Node</code> trains a model upon a <code>TrainRequest</code> (sent by <code>Researcher</code>), and send it back to the <code>Researcher</code> once training is completed.</li> </ul> <p><code>Node</code> is a client that is responsible for sending replies in response to Researcher requests. Since <code>Node</code> is not a  server, communication between node and researcher is provided through <code>Network</code> component. When a node is started, it directly connects to the <code>Network</code> component.</p> <p>More details about <code>Node</code> installation can be found here.</p>"},{"location":"getting-started/fedbiomed-architecture/#researcher","title":"<code>Researcher</code>","text":"<p>In Fed-BioMed, a <code>Researcher</code> is an entity that orchestrates federated workflow among the nodes. It is the server (gRPC) that each node participating federated experiment is connected.  <code>Researcher</code> component is also the entity that the end-user connect and uses for federated experiment. It provides Python APIs to define the experiment and all the elements needed for an experiment. In order to define an experiment, user has to provide 3 elements using <code>Researcher</code> component:</p> <ul> <li>a <code>TrainingPlan</code>(containing a model, method to load/pre-process data and dependencies)</li> <li>a <code>Strategy</code>, which defines how nodes are selected / sampled while training a Federated Mode</li> <li>an <code>Aggregator</code>, which purpose is to aggregate the local model coming from each node into an aggregated model. Aggregation is performed on <code>Researcher</code> side. <code>Researcher</code> can be run using plain Python scripts or Jupyter Notebook (thus in an interactive fashion).</li> </ul> <p><code>Researcher</code> orchestrates the training by submitting training tasks to <code>Nodes</code>. Connected nodes subscribe the submitted tasks/request through RPC calls. After the task execution is completed <code>Node</code> answers back with an appropriate reply for each task submitted and subscribed by them.</p> <p>More details about <code>Researcher</code> and its installation can be found here.</p>"},{"location":"getting-started/fedbiomed-architecture/#fed-biomed-architecture","title":"Fed-BioMed Architecture","text":"<p>Relationship between each main component aforementioned is detailed in the figure below:</p> <p> Fed-BioMed basic architecture</p> <p>As shown in the diagram, <code>Researcher</code> is a central component in Fed-BioMed that submits federated queries (model training, federated analytics etc.) to the nodes and, collects the results. The network infrastructure is based on nodes collecting tasks from researcher rather than researcher sending requests to the nodes. The tasks created by end-user is stored by <code>Researcher</code> and submitted to nodes once they send task-collect request to the <code>Researcher</code> component. <code>Nodes</code> are in charge of running the model training or executing any other tasks that is submitted by the <code>Researcher</code>, and sending the results back to the <code>Researcher</code> by creating a reply request. Large files such as model parameters are exchanged using streaming to avoid memory issues. </p>"},{"location":"getting-started/fedbiomed-architecture/#node-configuration","title":"<code>Node</code> configuration","text":"<p>For information on how to configure <code>Node</code>, please follow <code>Node</code> configuration steps</p>"},{"location":"getting-started/fedbiomed-architecture/#researcher-configuration","title":"<code>Researcher</code> configuration","text":"<p>For information on how to configure <code>Researcher</code>, please follow <code>Researcher</code> configuration steps</p>"},{"location":"getting-started/fedbiomed-workflow/","title":"Fed-BioMed Workflow","text":"<p>We present in the following page a short step-by-step illustration detailing the workflow of Fed-BioMed.</p> <p>The steps are:</p> <ol> <li>Setting up the <code>Nodes</code></li> <li>Deploying dataset on <code>Nodes</code> </li> <li>Write a Federated Model</li> <li>Run and monitor a Federated Model</li> <li>Model retrieval and evaluation</li> </ol>"},{"location":"getting-started/fedbiomed-workflow/#step-1-setting-up-the-nodes","title":"Step 1: Setting up the <code>Nodes</code>.","text":"<p>In order to run Fed-BioMed, you need to start first one or several <code>Nodes</code>. When starting the <code>Nodes</code>, each of them will try to connect to the <code>Researcher</code>, as shown in the diagram below (Diagram 1). The connection will fail if the researcher component is not up and running, and the nodes will retry to connect every 2 seconds until they create a successful connection with the researcher server.</p> <p>Nodes have only out-bound requests</p> <p>Nodes only send out-bound connection requests and doesn't have in-bound connections. Shortly, nodes are client rather than a server.</p> <p> Diagram 1: <code>Nodes</code> in Fed-BioMed, and long polling RPC calls to researcher server.</p>"},{"location":"getting-started/fedbiomed-workflow/#step-2-deploying-a-dataset-on-the-nodes","title":"Step 2: Deploying a dataset on the <code>Nodes</code>.","text":"<p>The nodes store the datasets locally in the file system where they run. Each dataset needs to be registered/deployed using the Node GUI or Node CLI. This process identifies the dataset to be able to use in training.</p>"},{"location":"getting-started/fedbiomed-workflow/#step-21-loading-a-dataset-into-a-node","title":"Step 2.1: Loading a dataset into a <code>Node</code>.","text":"<p>Fed-BioMed supports standard data sources, such as .csv files and image folders, and provides specific tools for loading medical data formats, such as medical imaging, signals and genomics information (Diagram 2).</p> <p> Diagram 2: loading data into a <code>Node</code>. Different data types are available, especially for medical datasets.</p> <p>After a dataset is deployed, <code>Node</code> will be able to train the models submitted by the <code>Researcher</code>. The user (researcher) must specify the tag that identifies the dataset. Please refer to Diagram 4 to see the example of dataset identifiers as well as the tags associated.</p> <p> Diagram 3: <code>Nodes</code> with respective datasets loaded.</p>"},{"location":"getting-started/fedbiomed-workflow/#step-22-retrieving-nodes-dataset-information-on-the-researcher-side","title":"Step 2.2: Retrieving <code>Nodes</code> dataset information on the <code>Researcher</code> side.","text":"<p>It is possible for the <code>Researcher</code> to obtain information about the dataset of each <code>Node</code>, as shown in the diagram 4 below.  Diagram 4: <code>Node</code> datasets information that <code>Researcher</code> can retrieve. The researcher can access datasets' metadata such as datasets name, dataset data_type, dataset tags, description and shape stored on each node.</p>"},{"location":"getting-started/fedbiomed-workflow/#step-3-write-a-federated-model-trainingplan-aggregator-and-strategy","title":"Step 3: Write a federated Model (<code>TrainingPlan</code>, <code>Aggregator</code> and <code>Strategy</code>)","text":"<p>To create a Federated Model <code>Experiment</code> in Fed-BioMed, three principal ingredients must be provided:</p> <ol> <li>a <code>Training Plan</code>, which is basically a Python class, containing the model definition and related objects, such as cost function and optimizer, and eventually methods for pre-processing (e.g., data standardization and/or imputation), and post-processing (run after that the training of the model on the <code>Node</code> is completed).</li> <li>an <code>Aggregator</code> that defines how the model parameters obtained on each node after training are aggregated once received by the <code>Researcher</code>. Examples of <code>Aggregator</code> can be <code>FedProx</code> or <code>SCAFFOLD</code>.</li> <li>a <code>Strategy</code> that handles both node sampling and node management (e.g., how to deal with non responding nodes).</li> </ol> <p> Diagram 5: the ingredients needed to train a Federated Model in Fed-BioMed.</p>"},{"location":"getting-started/fedbiomed-workflow/#step-4-how-to-run-and-monitor-an-experiment","title":"Step 4: How to run and monitor an <code>Experiment</code>","text":""},{"location":"getting-started/fedbiomed-workflow/#running-an-experiment","title":"Running an <code>Experiment</code>","text":"<p>The following Diagram 6 provides a technical description of the training process within Fed-BioMed:</p> <ol> <li> <p>After the nodes are started they sent constant request to the researcher in order to retrieve the training task (request).</p> </li> <li> <p>The user/researcher creates and experiment with training plan and the required arguments for the training round.</p> </li> <li>The global model along with the training arguments is packaged as <code>TrainRequest</code> and sent to the <code>Nodes</code>. The package contains the training plan, experiment info, dataset identifier (tag) and the arguments for the training round.</li> <li>Each <code>Node</code> trains the model on the data locally stored.</li> <li>The resulting optimized local models are sent back to the <code>Researcher</code> as <code>TrainReply</code>;</li> <li>The shared local models are aggregated to form a new aggregated global model using the <code>Aggregator</code>.</li> </ol> <p> Diagram 6: Showcasing an iteration of federated training in Fed-BioMed.</p> <p> Diagram 7: Alternate view of an iteration of federated training in Fed-BioMed.</p>"},{"location":"getting-started/fedbiomed-workflow/#monitoring-an-experiment","title":"Monitoring an <code>Experiment</code>.","text":"<p>The loss evolution is sent back to the <code>Researcher</code> at each evaluation step during the training. The <code>Researcher</code> can keep track of the loss using Tensorboard, as shown in Diagram 8.</p> <p> Diagram 8: model training monitoring facility available in Fed-BioMed</p>"},{"location":"getting-started/fedbiomed-workflow/#step-5-retrieving-the-model-and-performing-model-evaluation","title":"Step 5: retrieving the model and performing model evaluation","text":"<p>Once federated training is complete, the <code>Researcher</code> can retrieve the final global model, as well as other relevant information such as the timing between each connection, loss and the testing metrics value (if a validation dataset is provided). Fed-BioMed provides a number of standard metrics, such as accuracy for classification, or mean squared error for regression, and allows the definition of custom ones.</p> <p> Diagram 9: model and results collected after training a model using Fed-BioMed framework.</p>"},{"location":"getting-started/fedbiomed-workflow/#going-further","title":"Going Further","text":"<p>Installation Guide</p> <p>Detailed steps on how to install Fed-BioMed on your computer.</p> <p>Tutorials</p> <p>More tutorials, examples and how-to.</p> <p><code>Nodes</code> configuration Guide</p> <p>Provides an exhaustive overview of Fed-BioMed <code>Nodes</code>.</p> <p><code>Researcher</code> configuration Guide</p> <p>Provides additional info on Fed-BioMed <code>Researcher</code>.</p>"},{"location":"getting-started/installation/","title":"Fed-BioMed software installation","text":"<p>This tutorial gives steps for installing Fed-BioMed components (node, researcher).</p> <p>Deployment</p> <p>Deployment documentation explains other available setups.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System requirements","text":"<p>Fed-BioMed is developed and tested under  Linux and MacOS distributions.</p> <p>It has not been fully tested on Windows under WSL2. Using Fed-BioMed on Windows can raise unexpected error. Check specific installation guidelines for Windows 11.</p>"},{"location":"getting-started/installation/#hardware-requirements","title":"Hardware requirements","text":"<p>16GB RAM recommended, 8GB RAM minimum for handling PyTorch &amp; ML packages, for each Fed-BioMed component.</p>"},{"location":"getting-started/installation/#software-packages","title":"Software packages","text":"<p>Fed-BioMed is a Python-based framework that can be installed using the <code>pip</code> package manager. Therefore, please ensure that Python is installed on your system.</p> <p>Fed-BioMed does not support a wide range of Python versions. Currently, the required Python version is 3.10. To configure a specific Python version in your workspace, it is recommended to use tools such as Conda or pyenv.</p> <p>Docker</p> <p>Docker is only needed for advanced usage scenarios with additional VPN protection of Fed-BioMed communications.</p>"},{"location":"getting-started/installation/#install-fed-biomed","title":"Install Fed-BioMed","text":"<p>The command below will perform a complete installation of Fed-BioMed. This installation allows you to test all Fed-BioMed functionalities.</p> <pre><code>pip install fedbiomed[node,researcher,gui]\n</code></pre> <p>Fed-BioMed consists of different components, each requiring specific dependencies. These components are <code>node</code> and <code>researcher</code>. In the context of Federated Learning, these components are typically used in different locations and environments. To avoid installing unnecessary packages that may not be used, the dependencies for these components have been made optional in the pip package.</p> <p>If you only need to install the <code>node</code> or the <code>researcher</code> component, you can use one of the following commands:</p> <p>For <code>node</code> only installation: <pre><code>pip install fedbiomed[node]\n</code></pre></p> <p>For <code>researcher</code> only installation: <pre><code>pip install fedbiomed[researcher]\n</code></pre></p> <p>For installing optional node GUI: <pre><code>pip install fedbiomed[gui]\n</code></pre></p> <p>For installing optional dependency to FLamby: <pre><code>pip install git+https://github.com/owkin/FLamby@main\n</code></pre></p> <p>Fed-BioMed is provided under Apache 2.0 License.</p>"},{"location":"getting-started/installation/#the-next-step","title":"The Next Step","text":"<p>After the steps above are completed you will be ready to start Fed-BioMed components. In the following tutorial you will learn how to launch components and add data in Fed-BioMed to prepare an experiment.</p>"},{"location":"getting-started/what-is-fedbiomed/","title":"What is Fed-BioMed","text":"<p>Fed-BioMed, an open-source collaborative learning framework</p>"},{"location":"getting-started/what-is-fedbiomed/#what-is-fed-biomed","title":"What is Fed-BioMed?","text":"<p>Fed-BioMed is an open source project focused on empowering biomedical research using non-centralized approaches for statistical analysis.  The project is currently based on Python and PyTorch, and enables developing and deploying federated learning and federated analytics in real-world machine learning application. </p> <p>The goal of Fed-BioMed is to provide a simplified and secured environment to:</p> <ul> <li>Easily deploy state-of-the art collaborative learning frameworks, </li> <li>Provide a friendly user interface to share data for collaborative learning experiments,</li> <li>Allow researchers to easily deploy their models and analysis methods,</li> <li>Foster research and collaborations in collaborative learning.</li> </ul> <p>Fed-BioMed is an ongoing initiative, and the code is available on GitHub.</p>"},{"location":"getting-started/what-is-fedbiomed/#what-is-federated-learning","title":"What is Federated Learning ?","text":""},{"location":"getting-started/what-is-fedbiomed/#introduction","title":"Introduction","text":"<p>Standard machine learning approaches require to have a centralized dataset in order to train a model. In certain scenarios like in the biomedical field, this is not straightforward due to several reasons like:</p> <ul> <li>Privacy concerns:<ul> <li>General Data Protection Regulation (GDPR): General Data Protection Regulation (GDPR) \u2013 Official Legal Text</li> <li>Californian Consumer Privacy Act (CCPA): California Consumer Privacy Act (CCPA) | State of California - Department of Justice - Office of the Attorney General</li> <li>Health Insurance Portability and Accountability (HIPAA): Health Information Privacy regulation | U.S. Department of Health and Human Service | HHS official website</li> <li>Family Educational Rights and Privacy Act (FERPA): Family Educational Rights and Privacy Act (FERPA) | U.S. Department of Education official website</li> </ul> </li> <li>Ethical committee approval</li> <li>Transferring data to a centralized location</li> </ul> <p>This slows down research in healthcare and limits the generalization of certain models.</p>"},{"location":"getting-started/what-is-fedbiomed/#federated-learning","title":"Federated Learning","text":"<p>Federated learning (FL) is a machine learning procedure whose goal is to train a model without having data centralized. The goal of FL is to train higher quality models by having access to more data than centralized approaches, as well as to keep data securely decentralized. </p>"},{"location":"getting-started/what-is-fedbiomed/#infrastructure-of-a-federated-learning-setting-in-healthcare","title":"Infrastructure of a federated learning setting in healthcare","text":"<p>A common scenario of federated learning in healthcare is shown as follows:</p> <p></p> <p>Hospitals (a.k.a. clients or nodes) across several geographical locations hold data of interest for a researcher. These data can be \"made available\" for local training but, only the model is authorized to be shared with a third thrusted party (e.g. research center). Once all the models are gathered, different techniques are proposed for aggregating them as a single global model. Then, the Aggregated model can be used as purposed (e.g. training a neural network for segmentation).</p>"},{"location":"getting-started/what-is-fedbiomed/#theoretical-background","title":"Theoretical background","text":"<p>One of the critical points in FL is to know how to aggregate the models submitted by the clients. The main problem relies on finding the best set of parameters that define your model in function of the submissions made by the clients.</p> <p>In a canonical form:</p> \\[ \\min_w F(w) ,\\quad \\textrm{where} F(w):=\\sum_{k=1}^{m} p_k F_k(w) \\] <p>Where \\(m\\) is the total number of nodes, \\(p_k&gt;=0\\), and \\(\\sum_k p_k=1\\) , and \\(F_k\\) is the local objective function for the \\(k\\)-th node. The impact (contribution) of each node to the aggregation of the global model is given by \\(p_k\\).</p> <p>One of the first proposed methodologies in FL for model aggregation was Federated Averaging <code>FedAVG</code> by (MacMahan et al, 2016), the idea behind it was to define the contribution of each node as \\(p_k=\\frac{n_k}{n}\\) where \\(n_k\\) is the number of datapoints in the node \\(k\\) and \\(n\\) is the total number of observations studied.</p>"},{"location":"getting-started/what-is-fedbiomed/#challenges-in-federated-learning","title":"Challenges in federated learning","text":"<p>The main challenges in FL are associated to:</p> <ul> <li> <p>Communication efficiency: number of iterations between nodes and central location to train an optimal model.</p> </li> <li> <p>Data heterogeneity: how to build generalized models with heterogeneous data?</p> </li> <li> <p>Security: adversarial attacks and data leakage.</p> </li> </ul>"},{"location":"getting-started/what-is-fedbiomed/#references","title":"References","text":"<ol> <li> <p>Kone\u010dn\u00fd, J., McMahan, et al. (2016). Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492.</p> </li> <li> <p>Li, T., Sahu, et al. (2018). Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127.</p> </li> <li> <p>Li, T., Sahu, A. K., Talwalkar, A., &amp; Smith, V. (2020). Federated learning: Challenges, methods, and future directions. IEEE Signal Processing Magazine, 37(3), 50-60.</p> </li> </ol>"},{"location":"news/","title":"News","text":"News"},{"location":"news/CAL_oncology/","title":"CAL oncology","text":""},{"location":"news/CAL_oncology/#fed-biomed-serving-oncology-research","title":"Fed-BioMed serving oncology research","text":"<p>Professor Olivier Humbert, from Centre Antoine Lacassagne, presents his research and teaching projects using artificial intelligence for medicine in this interview.</p> <p>Fed-BioMed is the technology empowering the federated learning project !</p>"},{"location":"news/CHB/","title":"Federated Learning Experiment with CHB","text":""},{"location":"news/CHB/#fed-biomed-at-hospital-centre-henri-becquerel-of-rouen","title":"Fed-BioMed at Hospital Centre Henri Becquerel of Rouen","text":"<p>A big thanks to Romain Modzelewski, Nathan Lapel, and Bastien Houis for the successful deployment of Fed-BioMed in Centre Henri-Becquerel!</p> <p>Fed-BioMed is currently providing a secure federated learning environment between Inria, and the hospitals Centre Antoine Lacassagne (Nice) and Centre Henri-Becquerel (Rouen).</p> <p>Looking forward to the next steps of this collaborative AI project in healthcare!</p>"},{"location":"news/OSE-2022/","title":"OSE 2022","text":""},{"location":"news/OSE-2022/#fed-biomed-open-source-experience-2022","title":"Fed-BioMed @ Open Source Experience 2022","text":"<p>Fed-BioMed participated to Open Source Experience 2022 meeting of the European open source community, Paris, Nov 8-9.</p> <p>View Francesco Cremonesi's introduction to Fed-BioMed:</p>"},{"location":"news/Release-01-2022/","title":"Release 01 2022","text":""},{"location":"news/Release-01-2022/#fed-biomed-v33-new-release","title":"Fed-BioMed v3.3 new release","text":"<p>Fed-BioMed v3.3 is now public. Here are some key new features:</p> <ul> <li>researcher: MONAI support and example notebooks</li> <li>researcher: real time monitoring with Tensorboard</li> <li>client: simplified inclusion of dataset in nodes</li> <li>security: model manager to register and check authorised training models on the node</li> <li>improved support for macOSX </li> <li>new tutorials and notebooks</li> </ul> <p>All the new specifics are in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-01-2023/","title":"Release 01 2023","text":""},{"location":"news/Release-01-2023/#fed-biomed-v41-new-release","title":"Fed-BioMed v4.1 new release","text":"<p>Fed-BioMed v4.1 is now available. Here are some key new features:</p> <ul> <li>Introducing Scaffold <code>Aggregation</code> method for PyTorch, focused to cope with the client drift issue, useful when dealing with heterogenous datasets</li> <li>Adding <code>num_updates</code> as a new <code>training_args</code> Argument: <code>num_updates</code> allows you to iterate your model over a specific number of updates, regardless of the size of data accross each <code>Node</code>. It is an alternative to number of epochs <code>epochs</code></li> <li>Adding more integration tests / introducing nightly tests in order to improve code quality</li> <li>improving <code>Researcher</code> log message, by introducing <code>Round</code> number</li> <li>Bug fixes (FedProx <code>Aggregation</code> method, percentage completion logged when using Opacus, and other minor fixes)</li> </ul> <p>More details about the new features can be found in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-02-2022/","title":"Release 02 2022","text":""},{"location":"news/Release-02-2022/#fed-biomed-v34-new-release","title":"Fed-BioMed v3.4 new release","text":"<p>Fed-BioMed v3.4 is now available. Here are some key new features:</p> <ul> <li>node-side differential privacy using Opacus notebook</li> <li>node GUI for managing shared data</li> <li>single Nvidia GPU training acceleration for PyTorch </li> <li>new tutorials and notebooks</li> <li>refactoring and testing for robustness and code maturity</li> </ul> <p>All the new specifics are in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-02-2025/","title":"Release 02 2025","text":""},{"location":"news/Release-02-2025/#fed-biomed-v54-new-release","title":"Fed-BioMed v5.4 new release","text":"<p>Fed-BioMed v6.0.0 is now available !</p> <p>Fed-BioMed is now availbale through <code>pip</code> The main feature provided by this release is the new cli command, and a new and simpler way to download Fed-BioMed through <code>pip</code>: a simple <code>pip install fedbiomed</code> or <code>pip install fedbiomed --upgrade</code> is now enough to get the latest changes of <code>Fed-BioMed</code>. In order to run <code>Fed-BioMed</code>, just write <code>fedbiomed node start</code> or <code>fedbiomed researcher start</code>.</p> <p>Bug fixes and other updates are also included in the release.</p> <p>More details about the fixes &amp; the new features can be found in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-04-2023/","title":"Release 04 2023","text":""},{"location":"news/Release-04-2023/#fed-biomed-v43-new-release","title":"Fed-BioMed v4.3 new release","text":"<p>Fed-BioMed v4.3 is now available.</p> <p>It introduces Secure Aggregation functionality which further protects the federated learning process. Secure Aggregation encrypts model parameters sent by the nodes to the researcher. The researcher then computes aggregated model parameters but cannot access individual node's model parameters in cleartext.</p> <p>Fed-BioMed Secure Aggregation uses Joye-Libert additively homomorphic encryption scheme (based on fault-tolerant-secure-agg implementation) and Shamir multi-party computation protocol (from MP-SPDZ software).</p> <p>Bug fixes and misc updates are also included in the release.</p> <p>More details about the new features can be found in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-05-2022/","title":"Release 05 2022","text":""},{"location":"news/Release-05-2022/#fed-biomed-v35-new-release","title":"Fed-BioMed v3.5 new release","text":"<p>Fed-BioMed v3.5 is now available. Here are some key new features:</p> <ul> <li>FedProx optimization scheme support for PyTorch</li> <li>model evaluation/validation</li> <li>NIFTI and MedNIST datasets</li> <li>API documentation</li> <li>better VPN/containers support</li> <li>etc.</li> </ul> <p>More details about the new features can be found in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-05-2024/","title":"Release 05 2024","text":""},{"location":"news/Release-05-2024/#fed-biomed-v52-new-release","title":"Fed-BioMed v5.2 new release","text":"<p>We are thrilled to announce that Fed-BioMed v5.2.0 is now available !</p> <p>It mainly brings fixes for network related gRPC and secure aggregation known issues. It also introduces a complete refactoring of the researcher component's code, an end-to-end test facility providing advanced testing in order to provide a high level quality software. Besides, it features a concrete-ml example, enabling advanced state-of-the-art protection mechanisms on top of secure aggregation.</p> <p>Bug fixes and miscellaneous updates are also included in the release.</p> <p>More details about the fixes &amp; the new features can be found in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-09-2024/","title":"Release 09 2024","text":""},{"location":"news/Release-09-2024/#fed-biomed-v53-new-release","title":"Fed-BioMed v5.3 new release","text":"<p>Fed-BioMed v5.3.0 is now available !</p> <p>Making secure aggregation for enhanced privacy of your medical data super fast (minimal overhead) and easy to use (no setup), with Low-Overhead Masking (LOM) and in-application Diffie-Hellman key negotiation. Don't miss Riccardo Taiello's presentation at MICCAI DeCaF 2024 !</p> <p>Bug fixes and miscellaneous updates are also included in the release.</p> <p>More details about the fixes &amp; the new features can be found in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-11-2022/","title":"Release 11 2022","text":""},{"location":"news/Release-11-2022/#fed-biomed-v40-new-major-release","title":"Fed-BioMed v4.0 new major release","text":"<p>Fed-BioMed v4.0 is now available. Here are some key new features:</p> <ul> <li>Improved ML security: Differential Privacy (local and central) for Pytorch</li> <li>Improved GUI security: user accounts management and authentication</li> <li>Improved Node-side security: training plan approval mechanism</li> <li>Support for biomedical data: MedicalFolderDataset supporting imaging + csv data</li> <li>Integration with open biomedical datasets: IXI brain imaging, and FLamby repository</li> <li>New tutorials: medical image segmentation, filtering datasets based on number of samples</li> <li>Several backend improvements: mini-batch SGD for scikit-learn, validation of training arguments, redesign of training plan, etc..</li> </ul> <p>More details about the new features can be found in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-11-2024/","title":"Release 11 2024","text":""},{"location":"news/Release-11-2024/#fed-biomed-v54-new-release","title":"Fed-BioMed v5.4 new release","text":"<p>Fed-BioMed v5.4.0 is now available !</p> <p>Making Joye-Libert secure aggregation super easy to use: no more setup, no more dependency to MP-SPDZ library. Uses in-application Shamir Additive Secret Sharing (ASS) negotiation inside Fed-BioMed node to node overlay channel for key negotiation in honest but curious scenario.</p> <p>Bug fixes and other updates are also included in the release.</p> <p>More details about the fixes &amp; the new features can be found in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-12-2023/","title":"Release 12 2023","text":""},{"location":"news/Release-12-2023/#fed-biomed-v50-new-release","title":"Fed-BioMed v5.0 new release","text":"<p>Fed-BioMed v5.0 is now available !</p> <p>It brings a fully redesigned communication architecture based on proven gRPC library for improved robustness, performance and security of the framework.</p> <p>Bug fixes and misc updates are also included in the release.</p> <p>More details about the new features can be found in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/federated-pet/","title":"Federated pet","text":""},{"location":"news/federated-pet/#fed-biomed-for-federated-pet-project","title":"Fed-BioMed for Federated-PET project","text":"<p>Federated-PET is an oncology research project lead by Pr Olivier Humbert (Centre Antoine Lacassagne/Universit\u00e9 C\u00f4te d\u2019Azur/3IA C\u00f4te d\u2019Azur). Federated-PET focuses on predicting the response to immunotherapy of patients followed for lung cancer and personalize the therapeutic strategy to improve the quality and life expectancy of patients. Federated-PET groups 8 hospitals and 4 research centers.</p> <p>We are thrilled to announce that Federated-PET uses Fed-BioMed as its federated learning platform.</p> <p>Read the project presentations on Centre Antoine Lacassagne website and Universit\u00e9 C\u00f4te d'Azur website</p>"},{"location":"news/vivatech-2023/","title":"Vivatech 2023","text":""},{"location":"news/vivatech-2023/#fed-biomed-viva-technology-2023","title":"Fed-BioMed @ Viva Technology 2023","text":"<p>Fed-BioMed will participate to Viva Technology 2023 Europe Startup and Tech event.</p> <p>Come and meet us on the Inria booth on Friday June 16 !</p>"},{"location":"news/waicf-2024/","title":"Waicf 2024","text":""},{"location":"news/waicf-2024/#fed-biomed-waicf-2024","title":"Fed-BioMed @ WAICF 2024","text":"<p>Fed-BioMed participated to World Artificial Intelligence Cannes Forum on 8-9 February 2024, as a project of 3IA C\u00f4te d'Azur on booth A14.</p> <p>Thanks again for visiting us and sharing great ideas and perspectives !</p>"},{"location":"news/welcome_08-2021/","title":"Welcome to Sergen and Yannick!","text":""},{"location":"news/welcome_08-2021/#a-warm-welcome-to-sergen-cansiz-and-yannick-bouillard-in-the-fed-biomed-team","title":"A warm welcome to Sergen Cansiz and Yannick Bouillard in the Fed-BioMed team!","text":"<p>09/2021.</p> <p>Sergen Cansiz and Yannick Bouillard are the new research engineers of Fed-BioMed. They will take care of the development of the platform and of the deployment of Fed-BioMed in the hospitals of our collaborating partners.</p>"},{"location":"pages/CAL/","title":"CAL Fed-BioMed Experiment","text":"<p>We are proud to announce the successful deployment and testing of Fed-BioMed into the Hospital Antoine Lacassagne (CAL) of Nice! CAL is the first partner among the network of oncology centers of the UNICANCER consortium, in which Fed-BioMed is providing a secured federated learning infrastructure for IA in healthcare. A big thanks to Olivier Humbert and Hamid Laceb of CAL, and to the Fed-BioMed team!</p>"},{"location":"pages/about-us/","title":"About","text":""},{"location":"pages/about-us/#about-us","title":"About us","text":"<p>Collaborative learning for Healthcare</p>"},{"location":"pages/about-us/#what-is-fed-biomed","title":"What is Fed-BioMed?","text":"<p>Fed-BioMed is an open-source research and development initiative aiming at translating collaborative learning into real-world medical research applications, including federated learning and federated analytics.</p> <p>Fed-BioMed provides:</p> <ul> <li>A demonstrated framework for deploying collaborative learning in hospital networks,</li> <li>Easy deployment of state-of-the art collaborative learning methods,</li> <li>User-friendly tools for data managment and client participation to collaborative learning,</li> <li>A framework-agnostic environment for easily deploying machine learning methods,</li> <li>Clear solutions compliant with data providers' privacy, and nodes governance requirements.</li> </ul> <p>Fed-BioMed is an ongoing initiative, and the code is available on GitHub.</p>"},{"location":"pages/about-us/#contributors","title":"Contributors:","text":"<p>Fed-BioMed software was originally developed by Inria (Institut National de Recherche en Informatique et Automatique), and of Universit\u00e9 C\u00f4te d\u2019Azur (UCA).</p> <p>Authors come from academic research, private companies and open source community.</p>"},{"location":"pages/funding/","title":"Acknowledgements","text":"<p>Fed-BioMed is grateful for the support from the Agence nationale de la recherche (ANR), from the Inria National Artificial Intelligence Research Programme, from the Marie Sklodowska-Curie EU program and from the Universit\u200c\u00e9 Cote d'Azur.  </p>"},{"location":"pages/publications/","title":"Publications","text":"<ul> <li> <p>Irene Balelli, Santiago Silva and Marco Lorenzi. A Probabilistic Framework for Modeling the Variability Across Federated Datasets of Heterogeneous Multi-View Observations. In Proceedings of The 27th international conference on Information Processing in Medical Imaging (IPMI), 2021. link</p> </li> <li> <p>Yann Fraboni, Richard Vidal and Marco Lorenzi. Free-rider Attacks on Model Aggregation in Federated Learning. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, PMLR 130:1846-1854, 2021. link</p> </li> <li> <p>Santiago Silva, Andre Altmann, Boris  Gutman, and Marco Lorenzi. Fed-BioMed: A General Open-Source Frontend Framework for Federated Learning in Healthcare. Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning. Springer, Cham, 2020. 201-210. link</p> </li> <li> <p>Santiago Silva, Boris Gutman, Barbara Bardoni, Paul M Thompson, Andre Altmann and Marco Lorenzi. Multivariate Learning in Distributed Biomedical Databases: Meta-analysis of Large-scale Brain Imaging Data. IEEE International Symposium on Biomedical Imaging (ISBI), Venice, 2019. link</p> </li> <li> <p>Marco Lorenzi, Boris Gutman, Paul M. Thompson, Daniel C. Alexander, Sebastien Ourselin, Andre Altmann. Secure multivariate large-scale multi-centric analysis through on-line learning: an imaging genetics case study. Proceedings of the 12th International Symposium on Medical Information Processing and Analysis. link</p> </li> </ul>"},{"location":"pages/roadmap/","title":"Fed-BioMed Road Map","text":""},{"location":"support/troubleshooting/","title":"Troubleshooting and FAQ","text":"<p>This page contains some frequently encountered problems, and their solutions.</p>"},{"location":"support/troubleshooting/#missing-python-interpreter-in-jupyter-notebook","title":"Missing Python interpreter in Jupyter notebook","text":"<p>If Fed-BioMed is installed in a virtual environment such as <code>virtualenv</code> or <code>conda</code>, executing <code>fedbiomed researcher -p &lt;component-dir&gt;</code> may not use the correct Python interpreter. This issue can be resolved by installing the necessary packages for the virtual environment tool, either automatically or by manually registering a Python kernel.</p> <p>For Conda users, installing <code>nb_conda_kernels</code> will automatically detect the Python interpreters from created virtual environments and allow you to select them.</p> <p>For other tools, you may need to install <code>ipykernel</code> and register the Python interpreter manually. After activating your virtual environment, execute the following command: <pre><code>python -m ipykernel install --user --name=&lt;project-kernel-name&gt;\n</code></pre> This will make the Python environment used by the virtual environment selectable in Jupyter Notebook.</p> <p>Another option is to add the Python path to the PATH variable: <pre><code>export PATH=&lt;path/to/python3&gt;:$PATH\n</code></pre></p>"},{"location":"support/troubleshooting/#tkinter-error-on-macos-or-missing-python-tk-package","title":"Tkinter Error on macOS or Missing <code>python-tk</code> Package","text":"<p>Fed-BioMed uses the <code>itk</code> module in its CLI to launch a file explorer for selecting files and folders. However, some systems or Python environment managers may not include <code>python-tk</code>, which can cause failures\u2014for example, when adding datasets into nodes via the Fed-BioMed CLI. If you're using tools like <code>pyenv</code> to install different Python versions without <code>python-tk</code> being present on your system, this will result in a Python installation without the correct <code>tk</code> module.</p> <p>You can verify if <code>itk</code> or <code>python-tk</code> is correctly installed by running the following test code:</p> <pre><code>import tkinter\ntkinter._test()\n</code></pre> <p>If the necessary modules are not installed, you may encounter an exit error on macOS similar to this:</p> <pre><code>macOS 14 (1407) or later required, have instead 14 (1406)!\n</code></pre> <p>If this occurs, you can install <code>python-tk</code> by running <code>brew install python-tk</code></p> <p>After installing, reinstall the Python version using <code>pyenv</code> to ensure the correct setup.</p> <p>This issue may also occur if you're using a <code>conda</code> virtual environment. To ensure <code>tk</code> is correctly installed in <code>conda</code>, run:</p>"},{"location":"support/troubleshooting/#conda-install-c-conda-forge-tk","title":"<pre><code>conda install -c conda-forge tk\n</code></pre>","text":""},{"location":"support/troubleshooting/#tkinter-error-on-linux","title":"Tkinter error on Linux","text":"<p>Similar to macOS, on Linux you may encounter errors due to the lack of the tkinter module installed on your local machine. On Linux systems, please run sudo apt-get install python3-tk before creating a pyenv for a specific Python version, or before using any other virtual environment tool.</p> <p>On Fedora, please use sudo dnf install tk-devel.</p>"},{"location":"support/troubleshooting/#missing-gmph-mpfrh-and-mpch","title":"Missing <code>gmp.h</code>, <code>mpfr.h</code> and <code>mpc.h</code>","text":"<p>Some Fed-BioMed secure aggregation modules uses <code>gmpy2</code> for big integer operation. This module requires to have <code>libgmp3-dev</code> and <code>libmpfr-dev</code> and <code>libmpc-dev</code> installed on Linux debian distributions (and equivalents on different Linux distributions). In case of missing <code>gmp.h</code>,  <code>mpfr.h</code>, or <code>mpc.h</code> module errors please install <code>apt-get install libgmp3-dev libmpfr-dev libmpc-dev</code>.</p> <p>On macOS please install: <code>brew link gmp mpfr mpc</code>.</p>"},{"location":"tutorials/advanced/breakpoints/","title":"Breakpoints (experiment saving facility)","text":"<p>An experiment can crash or stop during training due to unexpected events : software component crash (researcher/node), host server failure, network outage, etc. </p> <p>If an experiment crashes during training, one can launch an experiment with the same parameters and re-do a similar training. But if the experiment already ran for a long time, we don't want to lose the previous training effort. This is where breakpoints help.</p> <p>Breakpoints can also be used for a wide variety of reasons such as: the model start to oscillate at some point, the model get over-trained. In such cases, you may want to revert to a previous round's results using a breakpoint, and continue from this breakpoint.</p> <p>A Fed-BioMed breakpoint is a researcher side function that saves an intermediate status and training results of an experiment to disk files. It can later be used to create an experiment from these saved status and results, to continue and complete the partially run experiment.</p>"},{"location":"tutorials/advanced/breakpoints/#basic-use-continue-from-last-breakpoint-of-last-experiment","title":"Basic use : continue from last breakpoint of last experiment","text":"<p>Note</p> <p>Before running the following cells, please make sure you have already a running node. Please follow the following tutorials explaining how to launch Pytorch training plans \" and Scikit-Learn training plans. Don't forget to specify the data under <code>#dummy_tag</code> tag.</p> <p>By default, a Fed-BioMed experiment does not save breakpoints. To create an experiment that save breakpoints after each training round completes, use the <code>save_breakpoints=True</code> option :</p> <pre><code>exp = Experiment(tags=[ '#dummy_tag' ],\n                training_plan_class=MyTrainingPlan,\n                round_limit=2,\n                save_breakpoints=True)\n</code></pre> <p>or use the <code>set_breakpoint()</code> setter <pre><code>exp.set_save_breakpoints(True)\n</code></pre></p> <p>If the experiment crashes or is stopped during training, and at least one round was completed, one can later continue from the last experiment breakpoint.</p> <p>First step is to stop and restart all Fed-BioMed components still running (researcher, nodes, and sometimes network), as they often have lost their state due to the crash beyond the software automatic recovery capability.</p> <p>Second step is then to create an experiment on the researcher with status loaded from the last breakpoint of the previously running experiment :</p> <pre><code>new_exp = Experiment.load_breakpoint()\n</code></pre> <p>Optionally check the experiment folder and current training round for the loaded breakpoint :</p> <pre><code>print(f'Experimentation folder {new_exp.experimentation_folder()}')\nprint(f'Number of experimentation rounds completed {new_exp.round_current()}')\n</code></pre> <p>Then continue and complete experiment loaded from the breakpoint :</p> <pre><code>new_exp.run()\n</code></pre>"},{"location":"tutorials/advanced/breakpoints/#continue-from-a-specific-breakpoint-and-experiment","title":"Continue from a specific breakpoint and experiment","text":"<p>In some cases, it is needed to indicate which breakpoint should be used to create the experiment, because the last breakpoint of the last experiment cannot be automatically guessed, or because it is desirable to select and continue from an older breakpoint or experiment.</p> <p>Fed-BioMed saves experiment results and breakpoints with the following file tree structure : <pre><code>&lt;researcher-component-path&gt;/var/experiments\n|\n\\-- Experiment_0\n|\n\\-- Experiment_1\n|    |\n|    \\-- breakpoint_0\n|    \\-- breakpoint_1\n|\n\\-- Experiment_2\n|    |\n|    \\-- breakpoint_0\n|\n\\-- Experiment_3\n</code></pre></p> <p>Result files and breakpoints for each experiment are saved in a distinct folder under <code>&lt;researcher-component-path&gt;/var/experiments</code>. By default, each experiment is assigned a folder named <code>Experiment_{num}</code> with a unique increasing num. When an experiment saves breakpoint, breakpoint for round round is saved under <code>Experiment_{num}/breakpoint_{round}</code>.</p> <p>When loading last breakpoint of last experiment, it selects the highest existing num experiment and then the highest round for this experiment. In this example, automatic selection of last breakpoint fails because last experiment (<code>Experiment_3</code>) has not saved breakpoints or could not complete its first round.</p> <p>To load a specific breakpoint, indicate the path for this breakpoint (absolute or relative to the current directory). For example to load breakpoint after round 1 for <code>Experiment_1</code>:</p> <pre><code>new_exp = Experiment.load_breakpoint(\n    \"&lt;researcher-component-path&gt;/var/experiments/Experiment_1/breakpoint_1\")\n</code></pre> <p>or, if the current working directory is relative to <code>&lt;researcher-component-path&gt;</code> :</p> <pre><code># works if current directory is &lt;researcher-component-path&gt;\nnew_exp = Experiment.load_breakpoint(\n    \"./var/experiments/Experiment_1/breakpoint_1\")\n</code></pre> <p>Optionally check experimentation loaded from the breakpoint :</p> <pre><code>print(f'Experimentation path {new_exp.experimentation_path()}')\n</code></pre>"},{"location":"tutorials/advanced/breakpoints/#limitations","title":"Limitations","text":"<p>Breakpoints currently do not support copying to another path, as they use absolute path. For example this doesn't work in the current version :</p> <p><pre><code>!mv ./var/experiments/Experiment_1 ./var/experiments/mydir\n</code></pre> <pre><code>new_exp = Experiment.load_breakpoint(\"./var/experiments/mydir\")\n</code></pre></p> <p>Info</p> <p>Breakpoints currently do not save tensorboard monitoring status and tensorboard logs. If you continue from a breakpoint, tensorboard monitoring is not restarted and logs from pre-breakpoint run are not restored.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/","title":"In Depth Experiment Configuration","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\n\n# Here we define the training plan to be used.\n# You can use any class name (here 'MyTrainingPlan')\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    # Defines and return model\n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n\n    # Defines and return optimizer\n    def init_optimizer(self, optimizer_args):\n        return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n\n    # Declares and return dependencies\n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\"]\n        return deps\n\n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            self.fc1 = nn.Linear(9216, 128)\n            self.fc2 = nn.Linear(128, 10)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.dropout2(x)\n            x = self.fc2(x)\n\n\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_data(self):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        loader_arguments = { 'shuffle': True}\n        return DataManager(dataset=dataset1, **loader_arguments)\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn import torch.nn.functional as F from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms   # Here we define the training plan to be used. # You can use any class name (here 'MyTrainingPlan') class MyTrainingPlan(TorchTrainingPlan):      # Defines and return model     def init_model(self, model_args):         return self.Net(model_args = model_args)      # Defines and return optimizer     def init_optimizer(self, optimizer_args):         return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])      # Declares and return dependencies     def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\"]         return deps      class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.conv1 = nn.Conv2d(1, 32, 3, 1)             self.conv2 = nn.Conv2d(32, 64, 3, 1)             self.dropout1 = nn.Dropout(0.25)             self.dropout2 = nn.Dropout(0.5)             self.fc1 = nn.Linear(9216, 128)             self.fc2 = nn.Linear(128, 10)          def forward(self, x):             x = self.conv1(x)             x = F.relu(x)             x = self.conv2(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = self.dropout1(x)             x = torch.flatten(x, 1)             x = self.fc1(x)             x = F.relu(x)             x = self.dropout2(x)             x = self.fc2(x)               output = F.log_softmax(x, dim=1)             return output      def training_data(self):         # Custom torch Dataloader for MNIST data         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         loader_arguments = { 'shuffle': True}         return DataManager(dataset=dataset1, **loader_arguments)      def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss <p>After running the cells above, your training plan class will be ready, and it will be declared in the experiment as training plan which going to be sent to the nodes to perform federated training.</p> <p>After building an empty experiment you won't be able to perform federated training, since it is not fully configured. That's why the output of the initialization of <code>Experiment</code> will always remind you that the experiment is not fully configured.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nexp = Experiment()\n</pre> from fedbiomed.researcher.federated_workflows import Experiment exp = Experiment() In\u00a0[\u00a0]: Copied! <pre>exp.info()\n</pre> exp.info() <p>Based on the output, some arguments are defined with default values, while others are not. Model arguments, training arguments, tags, round limit, training data etc. have no default value, and therefore are required to be set in order to run an experiment. However, these arguments are related to each other. For example, to be able to define your federated training data you need to define the <code>tags</code> first, and then while setting your training data argument, experiment will be able to send search request to the nodes to receive information about the datasets. These relations between the arguments will be explained in the following steps.</p> In\u00a0[\u00a0]: Copied! <pre>exp.set_training_plan_class(training_plan_class=MyTrainingPlan)\n</pre> exp.set_training_plan_class(training_plan_class=MyTrainingPlan) <p>If you set your training plan path first, setter will log a debug message which will inform you about the training plan is not defined yet. This is because the training plan class has not been set yet</p> In\u00a0[\u00a0]: Copied! <pre># Model arguments should be an empty Dict, since our model does not require \n# any argument for initialization\nmodel_args = {}\n\n# Training Arguments\ntraining_args = {\n    'loader_args': { 'batch_size': 48, },\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 1, \n    'test_ratio': 0.2,\n 'test_batch_size': 256,\n 'test_on_local_updates': True,\n 'test_on_global_updates': True,\n    'dry_run': False,  \n    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n\nexp.set_model_args(model_args=model_args)\nexp.set_training_args(training_args=training_args)\n</pre> # Model arguments should be an empty Dict, since our model does not require  # any argument for initialization model_args = {}  # Training Arguments training_args = {     'loader_args': { 'batch_size': 48, },     'optimizer_args': {         'lr': 1e-3     },     'epochs': 1,      'test_ratio': 0.2,  'test_batch_size': 256,  'test_on_local_updates': True,  'test_on_global_updates': True,     'dry_run': False,       'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples }  exp.set_model_args(model_args=model_args) exp.set_training_args(training_args=training_args) In\u00a0[\u00a0]: Copied! <pre>tags = ['#MNIST', '#dataset']\nexp.set_tags(tags = tags)\n</pre> tags = ['#MNIST', '#dataset'] exp.set_tags(tags = tags) <p>To see the tags that are set, you can run <code>tags()</code> method of experiment object.</p> In\u00a0[\u00a0]: Copied! <pre>exp.tags()\n</pre> exp.tags() In\u00a0[\u00a0]: Copied! <pre>training_data = exp.set_training_data(training_data=None, from_tags=True)\n</pre> training_data = exp.set_training_data(training_data=None, from_tags=True) <p>Since the training data setter will send search request to the nodes, the output will inform you about selected nodes for training. It means that those nodes have the dataset, and they will be able to train your model defined in the training plan class.</p> <p><code>set_training_data</code> will return a <code>FederatedDataSet</code> object. You can either use the return value of the setter or the getter for training data which is <code>training_data()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>training_data = exp.training_data()\n</pre> training_data = exp.training_data() <p>To inspect the result in detail you can call the method <code>data()</code> of the <code>FederatedDataSet</code> object. This will return a python dictionary that includes information about the datasets that has been found in the nodes.</p> In\u00a0[\u00a0]: Copied! <pre>training_data.data()\n</pre> training_data.data() <p>As it is mentioned before, setting training data once doesn't mean that you can't change it, for you can create a new <code>FederatedDataSet</code> with a <code>dict</code> that includes the information about the datasets. This will allow you to select the datasets that will be used for federated training.</p> <p>Since the dataset information will be provided, there will be no need to send request to the nodes</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.datasets import FederatedDataSet \n\ntr_data = training_data.data()\ntr_data = training_data.data()\nfederated_dataset = FederatedDataSet(tr_data)\nexp.set_training_data(training_data = federated_dataset)\n</pre> from fedbiomed.researcher.datasets import FederatedDataSet   tr_data = training_data.data() tr_data = training_data.data() federated_dataset = FederatedDataSet(tr_data) exp.set_training_data(training_data = federated_dataset) <p>Or, you can directly use <code>tr_data</code> in <code>set_training_data()</code></p> In\u00a0[\u00a0]: Copied! <pre>exp.set_training_data(training_data = tr_data)\n</pre> exp.set_training_data(training_data = tr_data) <p>         If you change the tags for the dataset by using <code>set_tags</code> and if there is already a defined training data in your experiment object, you have to update your training data by running <code>exp.set_training_data(training_data=None)</code>.       </p> In\u00a0[\u00a0]: Copied! <pre>exp.aggregator()\n</pre> exp.aggregator()  <p>Let's supposed that you have created your own aggregator: then, you can set it as follows:</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.aggregators.fedavg import FedAverage\nexp.set_aggregator(aggregator=FedAverage())\n</pre> from fedbiomed.researcher.aggregators.fedavg import FedAverage exp.set_aggregator(aggregator=FedAverage()) In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.optimizers.optimizer import Optimizer\nfrom fedbiomed.common.optimizers.declearn import AdamModule\n\nlr = .9\nfed_opt = Optimizer(lr=lr, modules=[AdamModule()])\n\nexp.set_agg_optimizer(fed_opt)\n</pre> from fedbiomed.common.optimizers.optimizer import Optimizer from fedbiomed.common.optimizers.declearn import AdamModule  lr = .9 fed_opt = Optimizer(lr=lr, modules=[AdamModule()])  exp.set_agg_optimizer(fed_opt) In\u00a0[\u00a0]: Copied! <pre>exp.set_strategy(node_selection_strategy=None)\n</pre> exp.set_strategy(node_selection_strategy=None) <p>Or, you can directly pass <code>DefaultStrategy</code> (or any Strategy class) as an argument</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\nexp.set_strategy(node_selection_strategy=DefaultStrategy())\n\n# To make sure the strategy has been set\nexp.strategy()\n</pre> from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy exp.set_strategy(node_selection_strategy=DefaultStrategy())  # To make sure the strategy has been set exp.strategy() In\u00a0[\u00a0]: Copied! <pre>exp.set_round_limit(round_limit=2)\nexp.round_limit()\n</pre> exp.set_round_limit(round_limit=2) exp.round_limit() In\u00a0[\u00a0]: Copied! <pre>exp.set_test_ratio(0.25)\nexp.set_test_on_local_updates(True)\nexp.set_test_on_global_updates(True)\n</pre> exp.set_test_ratio(0.25) exp.set_test_on_local_updates(True) exp.set_test_on_global_updates(True) <p>!!! note \"Displaying validation on Tensorboard\" It is possible to display results from validation metric into Tensorboard, for each <code>Round</code>. Please visit the Fed-BioMed user guide for more details.</p> In\u00a0[\u00a0]: Copied! <pre>exp.info()\n</pre> exp.info() <p>If the experiment is ready, you will see the message that says <code>Experiment can be run now (fully defined)</code> at the bottom of the output. So now, we can run the experiment</p> In\u00a0[\u00a0]: Copied! <pre>exp.run_once()\n</pre> exp.run_once() <p>After running the experiment for once, you can check the current round. It returns <code>1</code> which means only one round has been run.</p> In\u00a0[\u00a0]: Copied! <pre>exp.round_current()\n</pre> exp.round_current() <p>Now, let's run the experiment with <code>run_once()</code> again.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run_once()\n</pre> exp.run_once() <p>Since the round limit has been set to <code>2</code> the round limit had been reached. If you try to run <code>run()</code> or <code>run_once()</code> the experiment will indicate that the round limit has been reached.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run_once()\n</pre> exp.run_once() In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>After this point, if you would like to run the experiment you can increase round limit with <code>set_round_limit(round)</code></p> In\u00a0[\u00a0]: Copied! <pre>exp.set_round_limit(4)\nprint('Round Limit    : ' , exp.round_limit())\nprint('Current Round  : ' , exp.round_current())\n</pre> exp.set_round_limit(4) print('Round Limit    : ' , exp.round_limit()) print('Current Round  : ' , exp.round_current()) <p>The round limit of the experiment has been set to <code>4</code> and the completed number of rounds is <code>2</code>. It means if you run the experiment with method <code>run()</code> without passing any argument, it will run the experiment for <code>2</code> rounds.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>Let's check the current round status of the experiment.</p> In\u00a0[\u00a0]: Copied! <pre>print('Round Limit    : ' , exp.round_limit())\nprint('Current Round  : ' , exp.round_current())\n</pre> print('Round Limit    : ' , exp.round_limit()) print('Current Round  : ' , exp.round_current()) <p>Another way to run your experiment if the round limit is reached is by passing <code>rounds</code> argument to the method <code>run()</code>. For example, following cell will run the experiment for <code>2</code> more rounds.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run(rounds=2, increase=True) # increase is True by default\n</pre> exp.run(rounds=2, increase=True) # increase is True by default <p>If the argument <code>increase</code> is <code>False</code>, it will not increase the round limit automatically.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run(rounds=2, increase=False)\n</pre> exp.run(rounds=2, increase=False) In\u00a0[\u00a0]: Copied! <pre>print('Round Limit    : ' , exp.round_limit())\nprint('Current Round  : ' , exp.round_current())\n</pre> print('Round Limit    : ' , exp.round_limit()) print('Current Round  : ' , exp.round_current()) <p>It is also possible to increase number of rounds while running the experiment with <code>run_once()</code> by passing <code>increase</code> argument as <code>True</code></p> In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) In\u00a0[\u00a0]: Copied! <pre>print('Round Limit    : ' , exp.round_limit())\nprint('Current Round  : ' , exp.round_current())\n</pre> print('Round Limit    : ' , exp.round_limit()) print('Current Round  : ' , exp.round_current()) In\u00a0[\u00a0]: Copied! <pre># Training Arguments\ntraining_args = {\n    'loader_args': { 'batch_size': 64, },\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 1, \n    'dry_run': False,  \n    'batch_maxnum': 50\n}\n\nexp.set_training_args(training_args=training_args)\n</pre> # Training Arguments training_args = {     'loader_args': { 'batch_size': 64, },     'optimizer_args': {         'lr': 1e-3     },     'epochs': 1,      'dry_run': False,       'batch_maxnum': 50 }  exp.set_training_args(training_args=training_args) In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True)"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#in-depth-experiment-configuration","title":"In Depth Experiment Configuration\u00b6","text":""},{"location":"tutorials/advanced/in-depth-experiment-configuration/#introduction","title":"Introduction\u00b6","text":"<p>The Experiment class provides an interface that you can manage your experiment with backward compatibility. It means that even if your Experiment has been built/defined you will be able to configure its parameters, and allow you to run your notebooks created using previous Fed-BioMed versions (&lt;3.4). This feature will provide more control over your experiment even after you have been running your experiment for several rounds. In this tutorial, the experiment interface will be explained using MNIST basic example.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#1-configuring-fed-biomed-environment","title":"1. Configuring Fed-BioMed Environment\u00b6","text":"<p>Before running this notebook, you need to configure your environment by completing the following steps:</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#11-creating-the-node-component","title":"1.1 Creating the Node component\u00b6","text":"<p>Simply put, a <code>Node</code> can be created by running:</p> <pre>fedbiomed component create -c node\n</pre> <p>It will create a folder <code>fbm-node</code> (<code>Node</code>'s default name) in the directory where the command has been executed. Ths folder contains all files required to launch a Fed-BioMed <code>Node</code> and is called a <code>component</code>.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#12-deploying-mnist-dataset-in-the-node","title":"1.2. Deploying MNIST Dataset in the Node\u00b6","text":"<p>Please run following command to add MNIST dataset into your Node. This command will deploy MNIST dataset in your default node  in the directory where the command is executed.</p> <p>After running following command, please select data type <code>2) default</code>, use default <code>tags</code> and select the folder where MNIST dataset will be saved.</p> <pre>fedbiomed node dataset add\n</pre>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#13-starting-the-node","title":"1.3. Starting the Node\u00b6","text":"<p>After you have successfully completed previous step, please run following command to start your node.</p> <pre>fedbiomed node start\n</pre>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#2-creating-a-training-plan","title":"2. Creating a Training Plan\u00b6","text":"<p>Before declaring an experiment, the training plan that will be used for federated training should be defined. The training plan below is the same training plan that is created in the Basic MNIST tutorial. We recommend you to follow Basic MNIST tutorial on PyTorch Framework to understand following steps.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#3-creating-an-experiment-step-by-step","title":"3. Creating an Experiment Step by Step\u00b6","text":"<p>The experiment class can be created without passing any argument. This will just build an empty experiment object. Afterwards, you will be able to define your arguments using setters provided by <code>Experiment</code> class.</p> <p>It is always possible to create a fully configured experiment by passing all arguments during the initialization. You can also create your experiment with some arguments and set the other arguments afterwards.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#31-building-an-empty-experiment","title":"3.1. Building an Empty Experiment\u00b6","text":""},{"location":"tutorials/advanced/in-depth-experiment-configuration/#32-displaying-current-status-of-experiment","title":"3.2. Displaying Current Status of Experiment\u00b6","text":"<p>As an addition to output of the initialization, to find out more about the current status of the experiment, you can call the <code>info()</code> method of your experiment object. This method will print the information about your experiment and what you should complete to be able to start your federated training.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#33-setting-training-plan-for-the-experiment","title":"3.3. Setting Training Plan for The Experiment\u00b6","text":"<p>The training plan that is going to be used for the experiment can be set using the method <code>set_training_plan_class</code>.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#34-setting-model-and-training-arguments","title":"3.4. Setting Model and Training Arguments\u00b6","text":"<p>In the previous step, the training plan has been defined for the experiment. Now, you can define your model arguments and training arguments that will be used respectively for building your model class and training your model on the node side. The methods <code>set_model_args</code> and <code>set_training_args</code> of the experiment class will allow you to set these arguments.</p> <p>There isn't any requirement on the order of defining training plan class and mode/training arguments. It is also possible to         define model/training arguments first and training plan class after.     </p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#35-setting-tags","title":"3.5. Setting Tags\u00b6","text":"<p>The tags for the dataset search request can be set using <code>set_tags</code> method of experiment object.</p> <p>Setting tags does not mean sending dataset search request. Search request is sent while setting training data. <code>tags</code> is the argument that is required for the search request.</p> <p>The arguments <code>tags</code> of <code>set_tags</code> method should be an array of tags which are in <code>string</code> type or just a tag in <code>string</code> type.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#36-setting-nodes","title":"3.6. Setting Nodes\u00b6","text":"<p>The <code>nodes</code> arguments indicates the nodes that are going to be used for the experiment. By default, it is equal to <code>None</code> which means every nodes up and running will be part of the experiment as long as they have the dataset that is going to be used for training (and that has been registered under the tags). If the <code>nodes</code> argument has been set in advance when configuring <code>Experiment</code>, the search request for the dataset search will be sent only to nodes that have been indicated. You can set nodes using the method <code>exp.set_nodes(noes=nodes)</code>. This method takes <code>nodes</code> argument which should be an array of node ids which are of type <code>string</code> or just a single node id passed as a <code>string</code>.</p> <p>Since each node id is created randomly to the node when they are configured, we won't be setting <code>nodes</code> for this experiment, so it is possible to run this notebook regardless of the environment.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#37-setting-training-data","title":"3.7. Setting Training Data\u00b6","text":"<p>Training data is a <code>FederatedDataset</code> instance which comes from the module <code>fedbiomed.researcher.datasets</code>. There are several ways to define your training data.</p> <ol> <li>You can run <code>set_training_data(training_data=None, from_tags=True)</code>. This will send search request to the nodes to get dataset information by using the <code>tags</code> which are defined before.</li> <li>You can provide <code>training_data</code> argument which is an instance of <code>FederatedDataSet</code>.</li> <li>You can provide <code>training_data</code> argument as python dictionary <code>dict</code> and setter will create a <code>FederatedDataSet</code> object by itself.</li> </ol> <p>While using the last option please make sure that your <code>dict</code> object is configured accordingly to <code>FederatedDataSet</code> schema. Otherwise, you might get error while running your experiment.</p> <p>A <code>FederatedDataSet</code> object must have one unique dataset per node to ensure training uses only one dataset for each node. This is checked and enforced when creating a <code>FederatedDataSet</code></p> <p>If you run <code>set_training_data(training_data=None)</code>, this means that no training data is defined yet for the experiment (<code>training_data</code> is set to <code>None</code>).</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#38-setting-an-aggregator","title":"3.8. Setting an Aggregator\u00b6","text":"<p>An aggregator is one of the required arguments for the experiment. It is used for aggregating model parameters that are received from the nodes after every round (ie once training is done on each node). By default, when the experiment is initialized without passing any aggregator, it will automatically use the default <code>FedAverage</code> aggregator class. However, it is also possible to set a different aggregation algorithm with the method <code>set_aggregator</code>. Currently, Fed-BioMed has only <code>FedAverage</code> but it is possible to create custom aggregator classes.</p> <p>You can get the current aggregator by running <code>exp.aggregator()</code>. It will return the aggregator object that will be used for aggregation.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#39-setting-an-optimizer","title":"3.9. Setting an Optimizer\u00b6","text":"<p>As well as for the <code>Nodes</code>, it is possible to set an <code>Optimizer</code> on <code>Researcher</code> side (ie in the <code>Experiment</code>). Such <code>optimizer</code> will update the global model, that is the model resulting from the <code>Aggregation</code>.</p> <p>The method <code>set_agg_optimizer</code> can be used to set such <code>optimizer</code>.</p> <p>Please bear in mind that only <code>declearn</code> based <code>Optimizers</code> can be passed in the <code>Experiment</code>. You can load them through <code>Fed-BioMed</code> (<code>from fedbiomed.common.optimizers.declearn</code>) as shown below:</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#310-setting-node-selection-strategy","title":"3.10. Setting Node Selection Strategy\u00b6","text":"<p>Node selection Strategy is also one of the required arguments for the experiment. It is used for selecting nodes before each round of training. Since the strategy will be used for selecting nodes, thus, training data should be already set before setting any strategies. Then, strategy will be able to select for training nodes that are currently available regarding their dataset.</p> <p>By default, <code>set_strategy(node_selection_strategy=None)</code> will use the default <code>DefaultStrategy</code> strategy. It is the default strategy in Fed-BioMed that selects for the training all the nodes available regardless their datasets. However, it is also possible to set different strategies. Currently, Fed-BioMed only provides <code>DefaultStrategy</code> but you can create your custom strategy classes.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#311-setting-round-limit","title":"3.11. Setting Round Limit\u00b6","text":"<p><code>round_limit</code> argument is the limit that indicates max number of rounds of the training. By default, it is <code>None</code> and it needs to be set before running your experiment. You can set the round limit with the method <code>set_round_limit</code>. <code>round_limit</code> can  be changed after running one or several rounds of training. You can always execute <code>exp.round_limit()</code> to see current round limit.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#312-setting-validation-facility","title":"3.12. Setting validation facility\u00b6","text":"<p>When training a Federated Learning model, model validation can prove useful, especially if you want to get an idea on how well your model performs on data, according to one or several metrics. <code>Fed-BioMed</code> comes with a validation facility, with the possibility to test the model against a selection of data sampled randomly among the dataset of each <code>Nodes</code>.</p> <p>Validation can be done in two different ways:</p> <ul> <li>At the begining of each <code>Round</code>, just before model training occurs, but after model aggregation: **Test on gloabl updates`;</li> <li>At the end of each <code>Round</code>, after training the model: Test on local updates.</li> </ul> <p>To use the <code>Fed-BioMed</code> validation facility, you have to activate in your <code>Experiment</code>:</p> <ul> <li>either activate <code>set_test_on_local_updates</code> or/and <code>set_test_on_global_updates</code></li> <li>specify a <code>test_ratio</code>, ie a percentage of data from the <code>Node</code> dataset, that will be used for validating the model.</li> </ul> <p>For more details, especially on how to use a specific validation metric, please visit the Fed-BioMed user guide</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#313-controlling-experiment-status-before-starting-training-rounds","title":"3.13. Controlling Experiment Status Before Starting Training Rounds\u00b6","text":"<p>Now, let's see if our experiment is ready for the training.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#4-running-the-experiment","title":"4. Running The Experiment\u00b6","text":"<p>As long as <code>info()</code> says that the experiment is fully defined you will be able to run your experiment. Experiment has two methods: <code>run()</code> and <code>run_once()</code> for running training rounds.</p> <ul> <li><p><code>run()</code> runs the experiment rounds from current round to round limit. If the round limit is reached it will indicate that the round limit has been reached. However, the method <code>run</code> takes 2 arguments as <code>round</code> and <code>increase</code>.</p> <ul> <li><code>round</code> is an integer that indicates number of rounds that are going to be run. If the experiment is at round <code>0</code>, the round limit is <code>4</code>, and if you pass <code>round</code> as 3, it will run the experiment only for <code>3</code> rounds.</li> <li><code>increase</code> is a boolean that indicates whether round limit should be increased if the given <code>round</code> passes over the round limit. For example, if the current round is <code>3</code>, the round limit is <code>4</code>, and the <code>round</code> argument is <code>2</code>, the experiment will increase round limit to <code>5</code></li> </ul> </li> <li><p><code>run_once()</code> runs the experiment for single round of training. If the round limit is reached it will indicate that the round limit has been reached. However, if it is executed as <code>run_once(increase=True)</code> when the round limit is reached, it increases the round limit for one round.</p> </li> </ul>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#41-running-the-experiment-once","title":"4.1. Running the Experiment once\u00b6","text":""},{"location":"tutorials/advanced/in-depth-experiment-configuration/#42-changing-training-arguments-for-the-next-round","title":"4.2. Changing Training Arguments for the Next Round\u00b6","text":"<p>The method <code>set_training_args()</code> allows you to change the training arguments even if you've already run your experiment several times. Thanks to the method <code>set_training_args()</code> you will be able to configure your training from one round to another. For example, we can change our <code>batch_size</code> to <code>64</code> and <code>batch_maxnum</code> to <code>50</code> for the next round.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#conclusions","title":"Conclusions\u00b6","text":"<p>The <code>Experiment</code> class is the interface and the orchestrator of the whole processes behind federated training on the researcher side. It allows you to manage your federated training experiment easily. It has been extended with setter and getter methods to ease its declaration. This also provides more control before, during or after the training rounds. The purpose of the experiment class is to provide a robust interface for end-user to make them able to easily perform their federated training on Fed-BioMed nodes.</p>"},{"location":"tutorials/advanced/training-with-gpu/","title":"PyTorch model training using a GPU","text":"<p>This example demonstrates using a Nvidia GPU for training a model.</p> <p>The nodes for this example need to run on a machine providing a Nvidia GPU with enough GPU memory (and from a not-too-old model, so that it is supported by PyTorch).</p> <p>If GPU doesn't have enough memory you will get a out of memory error at run time.</p> <p>You can check Fed-BioMed GPU documentation for some background about using GPUs with Fed-BioMed.</p> <p>All this part is the same as when running a model using CPU : model in unchanged</p> <p>Declare a training plan class to send for training on the node</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\n\n# Here we define the training plan to be used.\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    # Defines and return model\n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n\n    # Defines and return optimizer\n    def init_optimizer(self, optimizer_args):\n        return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n\n    # Declares and returns dependencies\n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\"]\n        return deps\n\n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            self.fc1 = nn.Linear(9216, 128)\n            self.fc2 = nn.Linear(128, 10)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.dropout2(x)\n            x = self.fc2(x)\n\n\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_data(self):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        loader_arguments = { 'shuffle': True}\n        return DataManager(dataset=dataset1, **loader_arguments)\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms   # Here we define the training plan to be used. class MyTrainingPlan(TorchTrainingPlan):      # Defines and return model     def init_model(self, model_args):         return self.Net(model_args = model_args)      # Defines and return optimizer     def init_optimizer(self, optimizer_args):         return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])      # Declares and returns dependencies     def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\"]         return deps      class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.conv1 = nn.Conv2d(1, 32, 3, 1)             self.conv2 = nn.Conv2d(32, 64, 3, 1)             self.dropout1 = nn.Dropout(0.25)             self.dropout2 = nn.Dropout(0.5)             self.fc1 = nn.Linear(9216, 128)             self.fc2 = nn.Linear(128, 10)          def forward(self, x):             x = self.conv1(x)             x = F.relu(x)             x = self.conv2(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = self.dropout1(x)             x = torch.flatten(x, 1)             x = self.fc1(x)             x = F.relu(x)             x = self.dropout2(x)             x = self.fc2(x)               output = F.log_softmax(x, dim=1)             return output      def training_data(self):         # Custom torch Dataloader for MNIST data         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         loader_arguments = { 'shuffle': True}         return DataManager(dataset=dataset1, **loader_arguments)      def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'loader_args': { 'batch_size': 48, },\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'use_gpu': True, # Activates GPU\n    'epochs': 1,\n    'dry_run': False,  \n    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n</pre> model_args = {}  training_args = {     'loader_args': { 'batch_size': 48, },     'optimizer_args': {         'lr': 1e-3     },     'use_gpu': True, # Activates GPU     'epochs': 1,     'dry_run': False,       'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MNIST', '#dataset']\nrounds = 2\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['#MNIST', '#dataset'] rounds = 2  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) <p>Let's start the experiment.</p> <p>By default, this function doesn't stop until all the <code>round_limit</code> rounds are done for all the nodes</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>You have completed training a <code>TorchTrainingPlan</code> using a GPU for acceleration.</p>"},{"location":"tutorials/advanced/training-with-gpu/#pytorch-model-training-using-a-gpu","title":"PyTorch model training using a GPU\u00b6","text":""},{"location":"tutorials/advanced/training-with-gpu/#introduction","title":"Introduction\u00b6","text":""},{"location":"tutorials/advanced/training-with-gpu/#set-up-the-nodes-up","title":"Set up the nodes up\u00b6","text":"<p>We need at least 1 node, let's test using 3 nodes.</p> <ol> <li>For each node, add the MNIST dataset :</li> </ol> <pre>fedbiomed node --path node-1 dataset add\nfedbiomed node --path node-2 dataset add\nfedbiomed node --path node-3 dataset add\n</pre> <ul> <li>Select option 2 (default) to add MNIST to the node</li> <li>Confirm default tags by hitting \"y\" and ENTER</li> <li>Pick the folder where MNIST is already downloaded (or where to download MNIST)</li> </ul> <ol> <li>Check that your data has been added by executing</li> </ol> <pre>fedbiomed node --path node-1 dataset list\nfedbiomed node --path node-2 dataset list\nfedbiomed node --path node-3 dataset list\n</pre> <ol> <li>Run the first node using</li> </ol> <pre>fedbiomed node --path node-1 start --gpu\n</pre> <p>so that the node offers to use GPU for training, with the default GPU device.</p> <ol> <li>Run the second node using</li> </ol> <pre>fedbiomed node --path node-2 start --gpu-only --gpunum 1\n</pre> <p>so that the node enforces use of GPU for training even if the researcher doesn't request it, and requests using the 2nd GPU (device 1) but will fallback to default device if you don't have 2 GPUs on this machine.</p> <ol> <li>Run the third node using</li> </ol> <pre>fedbiomed node --path node-3 start\n</pre> <p>so that the node doesn't offer to use GPU for training (default behaviour).</p> <ol> <li>Wait until you get <code>Starting task manager</code> for each node, it means you are online.</li> </ol>"},{"location":"tutorials/advanced/training-with-gpu/#define-the-training-plan","title":"Define the training plan\u00b6","text":""},{"location":"tutorials/advanced/training-with-gpu/#define-the-experiment-parameters","title":"Define the experiment parameters\u00b6","text":"<p><code>training_args</code> are used by the researcher to request the nodes to use GPU for training, if the node has a GPU and offers to use it.</p>"},{"location":"tutorials/advanced/training-with-gpu/#declare-and-run-the-experiment","title":"Declare and run the experiment\u00b6","text":"<p>All this part is the same as when running a model using CPU : experiment declaration and running is unchanged</p>"},{"location":"tutorials/concrete-ml/concrete-ml/","title":"End-to-end Privacy Preserving Training and Inference on Medical Data","text":"<p>You need to download the FLamby dataset that we will use. For licensing reasons, these are not included directly in the FLamby installation.</p> <p>To download the fed_heart dataset in <code>${FEDBIOMED_DIR}/data</code> (where <code>${FEDBIOMED_DIR}</code> is the base directory of Fed-BioMed):</p> <ol> <li><code>pip install wget</code></li> <li><code>python ${FEDBIOMED_DIR}/docs/tutorials/concrete-ml/download.py --output-folder ${FEDBIOMED_DIR}/data</code></li> </ol> In\u00a0[\u00a0]: Copied! <pre>import os\nimport torch \nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom flamby.datasets.fed_heart_disease import FedHeartDisease\nfrom fedbiomed.common.data import FlambyDataset, DataManager\nfrom concrete.ml.torch.compile import compile_torch_model\nfrom torch.nn.modules.loss import _Loss\nimport torch.nn as nn\nfrom fedbiomed.researcher.config import config\ntensorboard_dir = config.vars['TENSORBOARD_RESULTS_DIR']\n</pre> import os import torch  from torch.utils.data import DataLoader import numpy as np from fedbiomed.common.training_plans import TorchTrainingPlan from flamby.datasets.fed_heart_disease import FedHeartDisease from fedbiomed.common.data import FlambyDataset, DataManager from concrete.ml.torch.compile import compile_torch_model from torch.nn.modules.loss import _Loss import torch.nn as nn from fedbiomed.researcher.config import config tensorboard_dir = config.vars['TENSORBOARD_RESULTS_DIR'] In\u00a0[\u00a0]: Copied! <pre>FEDBIOMED_DIR = os.getenv('FEDBIOMED_DIR')\nDATASET_TEST_PATH = f\"{FEDBIOMED_DIR}/data\"\n</pre> FEDBIOMED_DIR = os.getenv('FEDBIOMED_DIR') DATASET_TEST_PATH = f\"{FEDBIOMED_DIR}/data\" In\u00a0[\u00a0]: Copied! <pre>%load_ext tensorboard\n</pre> %load_ext tensorboard In\u00a0[\u00a0]: Copied! <pre>class FedHeartTrainingPlan(TorchTrainingPlan):\n    \n    class Baseline(nn.Module):\n        \n        def __init__(self):\n            super().__init__()\n            self.fc1 = nn.Linear(13, 16)\n            self.fc2 = nn.Linear(16, 2)\n            self.act = nn.LeakyReLU()\n        def forward(self, x):\n            x = self.act(self.fc1(x))\n            x = self.fc2(x)\n            return x\n        \n    class BaselineLoss(_Loss):\n        def __init__(self):\n            super().__init__()\n            self.ce = torch.nn.CrossEntropyLoss()\n\n        def forward(self, prediction: torch.Tensor, target: torch.Tensor):\n            target = torch.squeeze(target, dim=1).type(torch.long)\n            return self.ce(prediction, target)\n    \n    def init_model(self, model_args):\n        return self.Baseline()\n\n    def init_optimizer(self, optimizer_args):\n        return torch.optim.AdamW(self.model().parameters(), lr=optimizer_args[\"lr\"])\n\n    def init_dependencies(self):\n        return [\"from flamby.datasets.fed_heart_disease import FedHeartDisease\",\n                \"from torch.nn.modules.loss import _Loss\",\n                \"from fedbiomed.common.data import FlambyDataset, DataManager\"]\n\n    def training_step(self, data, target):\n        logits = self.model().forward(data)\n        return self.BaselineLoss().forward(logits, target)\n\n    def training_data(self, batch_size=2):\n        dataset = FlambyDataset()\n        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset, **train_kwargs)\n</pre> class FedHeartTrainingPlan(TorchTrainingPlan):          class Baseline(nn.Module):                  def __init__(self):             super().__init__()             self.fc1 = nn.Linear(13, 16)             self.fc2 = nn.Linear(16, 2)             self.act = nn.LeakyReLU()         def forward(self, x):             x = self.act(self.fc1(x))             x = self.fc2(x)             return x              class BaselineLoss(_Loss):         def __init__(self):             super().__init__()             self.ce = torch.nn.CrossEntropyLoss()          def forward(self, prediction: torch.Tensor, target: torch.Tensor):             target = torch.squeeze(target, dim=1).type(torch.long)             return self.ce(prediction, target)          def init_model(self, model_args):         return self.Baseline()      def init_optimizer(self, optimizer_args):         return torch.optim.AdamW(self.model().parameters(), lr=optimizer_args[\"lr\"])      def init_dependencies(self):         return [\"from flamby.datasets.fed_heart_disease import FedHeartDisease\",                 \"from torch.nn.modules.loss import _Loss\",                 \"from fedbiomed.common.data import FlambyDataset, DataManager\"]      def training_step(self, data, target):         logits = self.model().forward(data)         return self.BaselineLoss().forward(logits, target)      def training_data(self, batch_size=2):         dataset = FlambyDataset()         train_kwargs = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset, **train_kwargs) In\u00a0[\u00a0]: Copied! <pre>batch_size = 8\nnum_updates = 10\nnum_rounds = 50\n</pre> batch_size = 8 num_updates = 10 num_rounds = 50 In\u00a0[\u00a0]: Copied! <pre>training_args = {\n    'optimizer_args': {\n        'lr': 5e-4,\n    },\n    'loader_args': {\n        'batch_size': batch_size,\n    },\n    'num_updates': num_updates,\n    'dry_run': False,\n    'log_interval': 2,\n    'test_ratio' : 0.0,\n    'test_on_global_updates': False,\n    'test_on_local_updates': False,\n    'random_seed':42,\n}\n\nmodel_args = {}\n</pre> training_args = {     'optimizer_args': {         'lr': 5e-4,     },     'loader_args': {         'batch_size': batch_size,     },     'num_updates': num_updates,     'dry_run': False,     'log_interval': 2,     'test_ratio' : 0.0,     'test_on_global_updates': False,     'test_on_local_updates': False,     'random_seed':42, }  model_args = {} In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['heart']\n\nexp_sec_agg = Experiment(tags=tags,\n                 training_plan_class=FedHeartTrainingPlan,\n                 training_args=training_args,\n                 model_args=model_args,\n                 round_limit=num_rounds,\n                 aggregator=FedAverage(),\n                 secagg=True,\n                 tensorboard=True\n                )\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['heart']  exp_sec_agg = Experiment(tags=tags,                  training_plan_class=FedHeartTrainingPlan,                  training_args=training_args,                  model_args=model_args,                  round_limit=num_rounds,                  aggregator=FedAverage(),                  secagg=True,                  tensorboard=True                 ) In\u00a0[\u00a0]: Copied! <pre>tensorboard --logdir \"$tensorboard_dir\"\n</pre> tensorboard --logdir \"$tensorboard_dir\" <p>In our example, the training loss curve for the 4 nodes looks like this: </p> In\u00a0[\u00a0]: Copied! <pre>exp_sec_agg.run()\n</pre> exp_sec_agg.run() In\u00a0[\u00a0]: Copied! <pre>fed_sec_agg_model = exp_sec_agg.training_plan().model()\nfed_sec_agg_model.eval()\n</pre> fed_sec_agg_model = exp_sec_agg.training_plan().model() fed_sec_agg_model.eval() In\u00a0[\u00a0]: Copied! <pre>test_dataset = FedHeartDisease(center=0,pooled=True, train=False, data_path=DATASET_TEST_PATH)\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n</pre> test_dataset = FedHeartDisease(center=0,pooled=True, train=False, data_path=DATASET_TEST_PATH) test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False) In\u00a0[\u00a0]: Copied! <pre>def test_torch(net, test_loader):\n    \"\"\"Test the network: measure accuracy on the test set.\"\"\"\n\n    # Freeze normalization layers\n    net.eval()\n\n    all_y_pred = np.zeros((len(test_loader)), dtype=np.int64)\n    all_targets = np.zeros((len(test_loader)), dtype=np.int64)\n\n    # Iterate over the batches\n    idx = 0\n    for data, target in test_loader:\n        # Accumulate the ground truth labels\n        endidx = idx + target.shape[0]\n        all_targets[idx:endidx] = target.numpy()\n\n        # Run forward and get the predicted class id\n        logits = torch.sigmoid(net(data))\n        output = logits.argmax(1).detach().numpy()\n        all_y_pred[idx:endidx] = output\n\n        idx += target.shape[0]\n\n    # Print out the accuracy as a percentage\n    n_correct = np.sum(all_targets == all_y_pred)\n    print(\n        f\"Test accuracy over plaintext model: \"\n        f\"{n_correct / len(test_loader) * 100:.2f}%\"\n    )\n</pre> def test_torch(net, test_loader):     \"\"\"Test the network: measure accuracy on the test set.\"\"\"      # Freeze normalization layers     net.eval()      all_y_pred = np.zeros((len(test_loader)), dtype=np.int64)     all_targets = np.zeros((len(test_loader)), dtype=np.int64)      # Iterate over the batches     idx = 0     for data, target in test_loader:         # Accumulate the ground truth labels         endidx = idx + target.shape[0]         all_targets[idx:endidx] = target.numpy()          # Run forward and get the predicted class id         logits = torch.sigmoid(net(data))         output = logits.argmax(1).detach().numpy()         all_y_pred[idx:endidx] = output          idx += target.shape[0]      # Print out the accuracy as a percentage     n_correct = np.sum(all_targets == all_y_pred)     print(         f\"Test accuracy over plaintext model: \"         f\"{n_correct / len(test_loader) * 100:.2f}%\"     )  In\u00a0[\u00a0]: Copied! <pre>test_torch(fed_sec_agg_model, test_dataloader)\n</pre> test_torch(fed_sec_agg_model, test_dataloader) <p>In our example we reach an accuracy over the plaintext model of 77.56%</p> In\u00a0[\u00a0]: Copied! <pre>def test_with_concrete(quantized_module, test_loader, use_sim):\n    \"\"\"Test a neural network that is quantized and compiled with Concrete ML.\"\"\"\n\n    # Casting the inputs into int64 is recommended\n    all_y_pred = np.zeros((len(test_loader)), dtype=np.int64)\n    all_targets = np.zeros((len(test_loader)), dtype=np.int64)\n\n    # Iterate over the test batches and accumulate predictions and ground truth labels in a vector\n    idx = 0\n    for data, target in test_loader:\n        data = data.numpy()\n        target = target.numpy()\n\n        fhe_mode = \"simulate\" if use_sim else \"execute\"\n\n        # Quantize the inputs and cast to appropriate data type\n        logits = torch.tensor(quantized_module.forward(data, fhe=fhe_mode), requires_grad=False)\n\n        endidx = idx + target.shape[0]\n\n        all_targets[idx:endidx] = target\n\n        # Get the predicted class id and accumulate the predictions\n        y_pred = torch.sigmoid(logits).argmax(1).numpy()\n        all_y_pred[idx:endidx] = y_pred\n\n        # Update the index\n        idx += target.shape[0]\n    n_correct = np.sum(all_targets == all_y_pred)\n    print(\n        f\"Test accuracy over encrypted model: \"\n        f\"{n_correct / len(test_loader) * 100:.2f}%\"\n    )\n</pre> def test_with_concrete(quantized_module, test_loader, use_sim):     \"\"\"Test a neural network that is quantized and compiled with Concrete ML.\"\"\"      # Casting the inputs into int64 is recommended     all_y_pred = np.zeros((len(test_loader)), dtype=np.int64)     all_targets = np.zeros((len(test_loader)), dtype=np.int64)      # Iterate over the test batches and accumulate predictions and ground truth labels in a vector     idx = 0     for data, target in test_loader:         data = data.numpy()         target = target.numpy()          fhe_mode = \"simulate\" if use_sim else \"execute\"          # Quantize the inputs and cast to appropriate data type         logits = torch.tensor(quantized_module.forward(data, fhe=fhe_mode), requires_grad=False)          endidx = idx + target.shape[0]          all_targets[idx:endidx] = target          # Get the predicted class id and accumulate the predictions         y_pred = torch.sigmoid(logits).argmax(1).numpy()         all_y_pred[idx:endidx] = y_pred          # Update the index         idx += target.shape[0]     n_correct = np.sum(all_targets == all_y_pred)     print(         f\"Test accuracy over encrypted model: \"         f\"{n_correct / len(test_loader) * 100:.2f}%\"     )  In\u00a0[\u00a0]: Copied! <pre># concrete ml is using the traceback, \n# while fed-biomed for logging reasons fixs it to 3, to use concrete-ml we reset to the default value\nimport sys\nsys.tracebacklimit = 1000\n</pre> # concrete ml is using the traceback,  # while fed-biomed for logging reasons fixs it to 3, to use concrete-ml we reset to the default value import sys sys.tracebacklimit = 1000 In\u00a0[\u00a0]: Copied! <pre>n_bits = 6\ncompile_set = np.random.randint(0, 10, (100, 13)).astype(float)\nq_module = compile_torch_model(fed_sec_agg_model, compile_set, rounding_threshold_bits=6)\n</pre> n_bits = 6 compile_set = np.random.randint(0, 10, (100, 13)).astype(float) q_module = compile_torch_model(fed_sec_agg_model, compile_set, rounding_threshold_bits=6) In\u00a0[\u00a0]: Copied! <pre>test_with_concrete(q_module, test_dataloader, True)\n</pre> test_with_concrete(q_module, test_dataloader, True) <p>In our example we reach an accuracy over the encrypted model of 76.38%</p> <p>The loss of accuracy due to encryption during inference is neglectable!</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/concrete-ml/concrete-ml/#end-to-end-privacy-preserving-training-and-inference-on-medical-data","title":"End-to-end Privacy Preserving Training and Inference on Medical Data\u00b6","text":""},{"location":"tutorials/concrete-ml/concrete-ml/#introduction","title":"Introduction\u00b6","text":"<p>In this tutorial, we will demonstrate the process of privacy-preserving training using Fed-BioMed, which leverages federated learning with secure aggregation. Subsequently, we will deploy the final model obtained through federated learning in a privacy-preserving manner using the Concrete-ML library. This approach allows us to achieve privacy-preserving inference through a software-as-a-service (SaaS) model.</p> <p>The selected dataset originates from a medical task assigned by Flamby, specifically the FedHeart Disease dataset. For more detailed information about the dataset, please refer to the provided link.</p>"},{"location":"tutorials/concrete-ml/concrete-ml/#install-concrete-ml","title":"Install Concrete-ML\u00b6","text":"<p>This tutorial assumes you have Concrete-ML installed in your Fed-BioMed researcher environment.</p> <p>If needed, you may install it with: <code>pip install concrete-ml</code></p>"},{"location":"tutorials/concrete-ml/concrete-ml/#note-for-macos-users-with-arm-chips","title":"Note for MacOS users with ARM chips\u00b6","text":"<p>If you have a recent Mac machine with Apple Silicon (ARM chips), then you may experience kernel failure in Section 3.2 of this notebook.</p> <p>To overcome this issue, you need to rebuild your conda environment with a native python executable, before installing Concrete-ML.</p> <p>After setting <code>export CONDA_SUBDIR=osx-arm64</code> you can re install <code>fedbiomed</code> and Concrete-ML.</p>"},{"location":"tutorials/concrete-ml/concrete-ml/#task-identify-patients-with-heart-disease","title":"Task: identify patients with heart disease\u00b6","text":"<p>We rely on the FedHeart dataset and task from FLamby, in which tabular patient data as input is used to predict the presence or absence of heart disease. More details can be found in FLamby's paper.</p>"},{"location":"tutorials/concrete-ml/concrete-ml/#flamby-configuration-download-fedheart","title":"FLamby configuration - Download FedHeart\u00b6","text":""},{"location":"tutorials/concrete-ml/concrete-ml/#1-fed-biomed","title":"1. Fed-BioMed\u00b6","text":"<p>Configuring the Fed-BioMed training plan involves specifying the machine learning model, defining the loss function, and identifying the necessary dependencies. This ensures a clear and well-defined setup for the training process.</p>"},{"location":"tutorials/concrete-ml/concrete-ml/#2-federated-learning-training-with-secagg","title":"2. Federated Learning Training with SecAgg\u00b6","text":"<p>Nodes Configuration: The FLamby Fed-Heart benchmark relies on 4 nodes. For each node in the range <code>i in [0...3]</code> (<code>0</code> and <code>3</code> are both included):</p> <ol> <li>Open a new terminal.</li> <li>Run the command: <code>fedbiomed node --path my-node-{i} dataset add</code></li> <li>Select <code>6) flamby</code>.</li> <li>Enter the dataset name: <code>flamby</code> (optional).</li> <li>Set tags to: <code>heart</code> (important).</li> <li>Description: Enter <code>none</code> (optional).</li> <li>Select <code>1) fed_heart_disease</code>.</li> <li>Specify a center ID between 0 and 3: <code>{i}</code>.</li> <li>Description: Enter <code>none</code> (optional).</li> <li>Run the command: <code>fedbiomed node --path my-node-{i}.ini start</code>.</li> </ol>"},{"location":"tutorials/concrete-ml/concrete-ml/#3-inference","title":"3. Inference\u00b6","text":"<p>We now have access to the weights of the final model, after secure encrypted training.</p>"},{"location":"tutorials/concrete-ml/concrete-ml/#31-inference-with-torch-using-the-plaintext-model","title":"3.1. Inference with torch using the plaintext model\u00b6","text":"<p>First, we establish a baseline by evaluating the plaintext (unencrypted) model on a held-out test dataset.</p>"},{"location":"tutorials/concrete-ml/concrete-ml/#32-inference-with-concrete-ml-using-the-encrypted-model","title":"3.2. Inference with Concrete-ML using the encrypted model\u00b6","text":"<p>Using Zama's Concrete-ML library, we now show that a similar performance can be achieved by performing inference on the encrypted model.</p> <p>We have therefore achieved fully secure, end-to-end encrypted training and inference.</p>"},{"location":"tutorials/concrete-ml/download/","title":"Download","text":"In\u00a0[\u00a0]: Copied! <pre>import argparse\nimport hashlib\nimport os\nimport sys\n</pre> import argparse import hashlib import os import sys In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport wget\n</pre> import pandas as pd import wget In\u00a0[\u00a0]: Copied! <pre>from flamby.utils import accept_license, create_config, write_value_in_config\n</pre> from flamby.utils import accept_license, create_config, write_value_in_config In\u00a0[\u00a0]: Copied! <pre>def main(output_folder, debug=False):\n    \"\"\"Download the heart disease dataset.\n\n    Parameters\n    ----------\n    output_folder : str\n        The folder where to download the dataset.\n    \"\"\"\n\n    # location of the files in the UCI archive\n    accept_license(\n        \"https://archive-beta.ics.uci.edu/ml/datasets/heart+disease\", \"fed_heart_disease\"\n    )\n    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/\"\n    centers = [\"cleveland\", \"hungarian\", \"switzerland\", \"va\"]\n    md5_hashes = [\n        \"2d91a8ff69cfd9616aa47b59d6f843db\",\n        \"22e96bee155b5973568101c93b3705f6\",\n        \"9a87f7577310b3917730d06ba9349e20\",\n        \"4249d03ca7711e84f4444768c9426170\",\n    ]\n\n    os.makedirs(output_folder, exist_ok=True)\n\n    print(\n        \"This dataset is licensed under a Creative Commons Attribution 4.0 \"\n        \"International (CC BY 4.0) license.\"\n    )\n\n    print(\"See https://archive-beta.ics.uci.edu/ml/datasets/heart+disease.\\n\")\n\n    print(\n        \"Creators of the dataset:\\n\"\n        \"  1. Hungarian Institute of Cardiology.\"\n        \" Budapest: Andras Janosi, M.D.\\n\"\n        \"  2. University Hospital,\"\n        \" Zurich, Switzerland: William Steinbrunn, M.D.\\n\"\n        \"  3. University Hospital,\"\n        \" Basel, Switzerland: Matthias Pfisterer, M.D.\\n\"\n        \"  4. V.A. Medical Center, Long Beach and\"\n        \" Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\\n\"\n    )\n\n    print(\n        \"To cite this dataset, cite the following:\"\n        \" Janosi, Andras, Steinbrunn, William, Pfisterer, Matthias, Detrano,\"\n        \" Robert &amp; M.D., M.D.. (1988). Heart Disease.\"\n        \" UCI Machine Learning Repository.\\n\"\n    )\n\n    # Creating config file with path to dataset\n    dict, config_file = create_config(output_folder, debug, \"fed_heart_disease\")\n    if dict[\"download_complete\"]:\n        print(\"You have already downloaded the heart disease dataset, aborting.\")\n        sys.exit()\n\n    # get status of download\n    downloaded_status_file_path = os.path.join(output_folder, \"download_status_file.csv\")\n    if not (os.path.exists(downloaded_status_file_path)):\n        downloaded_status_file = pd.DataFrame()\n        downloaded_status_file[\"Status\"] = [\"Not found\"] * 4\n\n        downloaded_status_file.to_csv(downloaded_status_file_path, index=False)\n    else:\n        downloaded_status_file = pd.read_csv(downloaded_status_file_path)\n\n    # for each center, check if downloaded and download if necessary\n    for i, center in enumerate(centers):\n        file_status_ok = downloaded_status_file.loc[i, \"Status\"] == \"Downloaded\"\n\n        if not file_status_ok:\n            fname = wget.download(\n                base_url + \"processed.\" + center + \".data\", out=output_folder\n            )\n\n            hash_md5 = hashlib.md5()\n            with open(fname, \"rb\") as f:\n                hash_md5.update(f.read())\n\n                if hash_md5.hexdigest() == md5_hashes[i]:\n                    downloaded_status_file.loc[i, \"Status\"] = \"Downloaded\"\n                else:\n                    downloaded_status_file.loc[i, \"Status\"] = \"Corrupted\"\n\n        print()\n\n    # We assert we have everything and write it\n    if all((downloaded_status_file[\"Status\"] == \"Downloaded\").tolist()):\n        write_value_in_config(config_file, \"download_complete\", True)\n        write_value_in_config(config_file, \"preprocessing_complete\", True)\n        downloaded_status_file.to_csv(downloaded_status_file_path, index=False)\n\n    else:\n        print(\"Downloading failed.\")\n</pre> def main(output_folder, debug=False):     \"\"\"Download the heart disease dataset.      Parameters     ----------     output_folder : str         The folder where to download the dataset.     \"\"\"      # location of the files in the UCI archive     accept_license(         \"https://archive-beta.ics.uci.edu/ml/datasets/heart+disease\", \"fed_heart_disease\"     )     base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/\"     centers = [\"cleveland\", \"hungarian\", \"switzerland\", \"va\"]     md5_hashes = [         \"2d91a8ff69cfd9616aa47b59d6f843db\",         \"22e96bee155b5973568101c93b3705f6\",         \"9a87f7577310b3917730d06ba9349e20\",         \"4249d03ca7711e84f4444768c9426170\",     ]      os.makedirs(output_folder, exist_ok=True)      print(         \"This dataset is licensed under a Creative Commons Attribution 4.0 \"         \"International (CC BY 4.0) license.\"     )      print(\"See https://archive-beta.ics.uci.edu/ml/datasets/heart+disease.\\n\")      print(         \"Creators of the dataset:\\n\"         \"  1. Hungarian Institute of Cardiology.\"         \" Budapest: Andras Janosi, M.D.\\n\"         \"  2. University Hospital,\"         \" Zurich, Switzerland: William Steinbrunn, M.D.\\n\"         \"  3. University Hospital,\"         \" Basel, Switzerland: Matthias Pfisterer, M.D.\\n\"         \"  4. V.A. Medical Center, Long Beach and\"         \" Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\\n\"     )      print(         \"To cite this dataset, cite the following:\"         \" Janosi, Andras, Steinbrunn, William, Pfisterer, Matthias, Detrano,\"         \" Robert &amp; M.D., M.D.. (1988). Heart Disease.\"         \" UCI Machine Learning Repository.\\n\"     )      # Creating config file with path to dataset     dict, config_file = create_config(output_folder, debug, \"fed_heart_disease\")     if dict[\"download_complete\"]:         print(\"You have already downloaded the heart disease dataset, aborting.\")         sys.exit()      # get status of download     downloaded_status_file_path = os.path.join(output_folder, \"download_status_file.csv\")     if not (os.path.exists(downloaded_status_file_path)):         downloaded_status_file = pd.DataFrame()         downloaded_status_file[\"Status\"] = [\"Not found\"] * 4          downloaded_status_file.to_csv(downloaded_status_file_path, index=False)     else:         downloaded_status_file = pd.read_csv(downloaded_status_file_path)      # for each center, check if downloaded and download if necessary     for i, center in enumerate(centers):         file_status_ok = downloaded_status_file.loc[i, \"Status\"] == \"Downloaded\"          if not file_status_ok:             fname = wget.download(                 base_url + \"processed.\" + center + \".data\", out=output_folder             )              hash_md5 = hashlib.md5()             with open(fname, \"rb\") as f:                 hash_md5.update(f.read())                  if hash_md5.hexdigest() == md5_hashes[i]:                     downloaded_status_file.loc[i, \"Status\"] = \"Downloaded\"                 else:                     downloaded_status_file.loc[i, \"Status\"] = \"Corrupted\"          print()      # We assert we have everything and write it     if all((downloaded_status_file[\"Status\"] == \"Downloaded\").tolist()):         write_value_in_config(config_file, \"download_complete\", True)         write_value_in_config(config_file, \"preprocessing_complete\", True)         downloaded_status_file.to_csv(downloaded_status_file_path, index=False)      else:         print(\"Downloading failed.\") In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--output-folder\",\n        type=str,\n        help=\"Where to store the downloaded data.\",\n        required=True,\n    )\n\n    args = parser.parse_args()\n    main(args.output_folder)\n</pre> if __name__ == \"__main__\":     parser = argparse.ArgumentParser()     parser.add_argument(         \"--output-folder\",         type=str,         help=\"Where to store the downloaded data.\",         required=True,     )      args = parser.parse_args()     main(args.output_folder)"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/","title":"FLamby integration in Fed-BioMed","text":"In\u00a0[\u00a0]: Copied! <pre>! pip install wget nibabel  # monai comes already packaged within fed-biomed\n</pre> ! pip install wget nibabel  # monai comes already packaged within fed-biomed In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\nfrom fedbiomed.common.data.flamby_dataset import FlambyDataset\nfrom fedbiomed.common.data import DataManager\n\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args):\n        return Baseline()\n\n    def init_optimizer(self, optimizer_args):\n        return Optimizer(self.model().parameters(), lr=optimizer_args[\"lr\"])\n\n    def init_dependencies(self):\n        return [\"from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\",\n                \"from fedbiomed.common.data.flamby_dataset import FlambyDataset\",\n                \"from fedbiomed.common.data import DataManager\"]\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        return BaselineLoss().forward(output, target)\n\n    def training_data(self):\n        dataset = FlambyDataset()\n        loader_arguments = { 'shuffle': True}\n        return DataManager(dataset, **loader_arguments)\n</pre> from fedbiomed.common.training_plans import TorchTrainingPlan from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer from fedbiomed.common.data.flamby_dataset import FlambyDataset from fedbiomed.common.data import DataManager   class MyTrainingPlan(TorchTrainingPlan):     def init_model(self, model_args):         return Baseline()      def init_optimizer(self, optimizer_args):         return Optimizer(self.model().parameters(), lr=optimizer_args[\"lr\"])      def init_dependencies(self):         return [\"from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\",                 \"from fedbiomed.common.data.flamby_dataset import FlambyDataset\",                 \"from fedbiomed.common.data import DataManager\"]      def training_step(self, data, target):         output = self.model().forward(data)         return BaselineLoss().forward(output, target)      def training_data(self):         dataset = FlambyDataset()         loader_arguments = { 'shuffle': True}         return DataManager(dataset, **loader_arguments) In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'loader_args': { 'batch_size': 8, },\n    'optimizer_args': {\n        \"lr\" : 1e-3\n    },\n    'epochs': 1,\n    'dry_run': False,\n    'batch_maxnum': 2 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n</pre> model_args = {}  training_args = {     'loader_args': { 'batch_size': 8, },     'optimizer_args': {         \"lr\" : 1e-3     },     'epochs': 1,     'dry_run': False,     'batch_maxnum': 2 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['flixi']\nrounds = 1\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['flixi'] rounds = 1  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') In\u00a0[0]: Copied! <pre>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom flamby.datasets.fed_heart_disease import Baseline, BaselineLoss, Optimizer\nfrom fedbiomed.common.data.flamby_dataset import FlambyDataset\nfrom fedbiomed.common.data import DataManager\n\nclass FedHeartTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args):\n        return Baseline()\n\n    def init_optimizer(self, optimizer_args):\n        return Optimizer(self.model().parameters(), lr=optimizer_args[\"lr\"])\n\n    def init_dependencies(self):\n        return [\"from flamby.datasets.fed_heart_disease import Baseline, BaselineLoss, Optimizer\",\n                \"from fedbiomed.common.data.flamby_dataset import FlambyDataset\",\n                \"from fedbiomed.common.data import DataManager\"]\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        return BaselineLoss().forward(output, target)\n\n    def training_data(self):\n        dataset = FlambyDataset()\n        train_kwargs = { 'shuffle': True}\n        return DataManager(dataset, **train_kwargs)\n</pre> from fedbiomed.common.training_plans import TorchTrainingPlan from flamby.datasets.fed_heart_disease import Baseline, BaselineLoss, Optimizer from fedbiomed.common.data.flamby_dataset import FlambyDataset from fedbiomed.common.data import DataManager  class FedHeartTrainingPlan(TorchTrainingPlan):     def init_model(self, model_args):         return Baseline()      def init_optimizer(self, optimizer_args):         return Optimizer(self.model().parameters(), lr=optimizer_args[\"lr\"])      def init_dependencies(self):         return [\"from flamby.datasets.fed_heart_disease import Baseline, BaselineLoss, Optimizer\",                 \"from fedbiomed.common.data.flamby_dataset import FlambyDataset\",                 \"from fedbiomed.common.data import DataManager\"]      def training_step(self, data, target):         output = self.model().forward(data)         return BaselineLoss().forward(output, target)      def training_data(self):         dataset = FlambyDataset()         train_kwargs = { 'shuffle': True}         return DataManager(dataset, **train_kwargs) In\u00a0[\u00a0]: Copied! <pre>training_args = {\n    'loader_args': { 'batch_size': 4, },\n    'optimizer_args': {\n        'lr': 0.001,\n    },\n    'epochs': 1,\n    'dry_run': False,\n    'log_interval': 2,\n    'batch_maxnum': 8,\n    'test_ratio' : 0.0,\n    'test_on_global_updates': False,\n    'test_on_local_updates': False,\n}\n\nmodel_args = {}\n</pre> training_args = {     'loader_args': { 'batch_size': 4, },     'optimizer_args': {         'lr': 0.001,     },     'epochs': 1,     'dry_run': False,     'log_interval': 2,     'batch_maxnum': 8,     'test_ratio' : 0.0,     'test_on_global_updates': False,     'test_on_local_updates': False, }  model_args = {} In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['flheart']\nnum_rounds = 1\n\nexp = Experiment(tags=tags,\n                 training_plan_class=FedHeartTrainingPlan,\n                 training_args=training_args,\n                 model_args=model_args,\n                 round_limit=num_rounds,\n                 aggregator=FedAverage(),\n                )\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['flheart'] num_rounds = 1  exp = Experiment(tags=tags,                  training_plan_class=FedHeartTrainingPlan,                  training_args=training_args,                  model_args=model_args,                  round_limit=num_rounds,                  aggregator=FedAverage(),                 ) In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom torch.optim import AdamW\nfrom torch import nn\nimport torch.nn.functional as F\nfrom unet import UNet\nfrom monai.transforms import Compose, NormalizeIntensity, Resize\nfrom fedbiomed.common.data.flamby_dataset import FlambyDataset\n\nclass UNetTrainingPlan(TorchTrainingPlan):\n\n    class MyUNet(nn.Module):\n        CHANNELS_DIMENSION = 1\n\n        def __init__(self, model_args):\n            super().__init__()\n            self.unet = UNet(\n            in_channels = model_args.get('in_channels',1),\n            out_classes = model_args.get('out_classes',2),\n            dimensions = model_args.get('dimensions',2),\n            num_encoding_blocks = model_args.get('num_encoding_blocks',5),\n            out_channels_first_layer = model_args.get('out_channels_first_layer',64),\n            normalization = model_args.get('normalization', None),\n            pooling_type = model_args.get('pooling_type', 'max'),\n            upsampling_type = model_args.get('upsampling_type','conv'),\n            preactivation = model_args.get('preactivation',False),\n            residual = model_args.get('residual',False),\n            padding = model_args.get('padding',0),\n            padding_mode = model_args.get('padding_mode','zeros'),\n            activation = model_args.get('activation','ReLU'),\n            initial_dilation = model_args.get('initial_dilation',None),\n            dropout = model_args.get('dropout',0),\n            monte_carlo_dropout = model_args.get('monte_carlo_dropout',0)\n        )\n\n        def forward(self, x):\n            x = self.unet.forward(x)\n            x = F.softmax(x, dim=UNetTrainingPlan.MyUNet.CHANNELS_DIMENSION)\n            return x\n\n    def init_model(self, model_args):\n        return UNetTrainingPlan.MyUNet(model_args)\n\n    def init_dependencies(self):\n        return [\"from torch import nn\",\n               'import torch.nn.functional as F',\n               'from torch.optim import AdamW',\n               'from unet import UNet',\n               'from monai.transforms import Compose, NormalizeIntensity, Resize',\n               'from fedbiomed.common.data.flamby_dataset import FlambyDataset']\n\n    def init_optimizer(self, optimizer_args):\n        return AdamW(self.model().parameters(),\n                     lr=optimizer_args[\"lr\"],\n                     betas=optimizer_args[\"betas\"],\n                     eps=optimizer_args[\"eps\"])\n\n    @staticmethod\n    def get_dice_loss(output, target, epsilon=1e-9):\n        SPATIAL_DIMENSIONS = 2, 3, 4\n        p0 = output\n        g0 = target\n        p1 = 1 - p0\n        g1 = 1 - g0\n        tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n        fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n        fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n        num = 2 * tp\n        denom = 2 * tp + fp + fn + epsilon\n        dice_score = num / denom\n        return 1. - dice_score\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss = UNetTrainingPlan.get_dice_loss(output, target)\n        avg_loss = loss.mean()\n        return avg_loss\n\n    def testing_step(self, data, target):\n        prediction = self.model().forward(data)\n        loss = UNetTrainingPlan.get_dice_loss(prediction, target)\n        avg_loss = loss.mean()  # average per batch\n        return avg_loss\n\n    def training_data(self):\n        dataset = FlambyDataset()\n        transform = Compose([Resize((48,60,48)), NormalizeIntensity()])\n        dataset.init_transform(transform)\n        train_kwargs = { 'shuffle': True}\n        return DataManager(dataset, **train_kwargs)\n</pre> from fedbiomed.common.training_plans import TorchTrainingPlan from torch.optim import AdamW from torch import nn import torch.nn.functional as F from unet import UNet from monai.transforms import Compose, NormalizeIntensity, Resize from fedbiomed.common.data.flamby_dataset import FlambyDataset  class UNetTrainingPlan(TorchTrainingPlan):      class MyUNet(nn.Module):         CHANNELS_DIMENSION = 1          def __init__(self, model_args):             super().__init__()             self.unet = UNet(             in_channels = model_args.get('in_channels',1),             out_classes = model_args.get('out_classes',2),             dimensions = model_args.get('dimensions',2),             num_encoding_blocks = model_args.get('num_encoding_blocks',5),             out_channels_first_layer = model_args.get('out_channels_first_layer',64),             normalization = model_args.get('normalization', None),             pooling_type = model_args.get('pooling_type', 'max'),             upsampling_type = model_args.get('upsampling_type','conv'),             preactivation = model_args.get('preactivation',False),             residual = model_args.get('residual',False),             padding = model_args.get('padding',0),             padding_mode = model_args.get('padding_mode','zeros'),             activation = model_args.get('activation','ReLU'),             initial_dilation = model_args.get('initial_dilation',None),             dropout = model_args.get('dropout',0),             monte_carlo_dropout = model_args.get('monte_carlo_dropout',0)         )          def forward(self, x):             x = self.unet.forward(x)             x = F.softmax(x, dim=UNetTrainingPlan.MyUNet.CHANNELS_DIMENSION)             return x      def init_model(self, model_args):         return UNetTrainingPlan.MyUNet(model_args)      def init_dependencies(self):         return [\"from torch import nn\",                'import torch.nn.functional as F',                'from torch.optim import AdamW',                'from unet import UNet',                'from monai.transforms import Compose, NormalizeIntensity, Resize',                'from fedbiomed.common.data.flamby_dataset import FlambyDataset']      def init_optimizer(self, optimizer_args):         return AdamW(self.model().parameters(),                      lr=optimizer_args[\"lr\"],                      betas=optimizer_args[\"betas\"],                      eps=optimizer_args[\"eps\"])      @staticmethod     def get_dice_loss(output, target, epsilon=1e-9):         SPATIAL_DIMENSIONS = 2, 3, 4         p0 = output         g0 = target         p1 = 1 - p0         g1 = 1 - g0         tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)         fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)         fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)         num = 2 * tp         denom = 2 * tp + fp + fn + epsilon         dice_score = num / denom         return 1. - dice_score      def training_step(self, data, target):         output = self.model().forward(data)         loss = UNetTrainingPlan.get_dice_loss(output, target)         avg_loss = loss.mean()         return avg_loss      def testing_step(self, data, target):         prediction = self.model().forward(data)         loss = UNetTrainingPlan.get_dice_loss(prediction, target)         avg_loss = loss.mean()  # average per batch         return avg_loss      def training_data(self):         dataset = FlambyDataset()         transform = Compose([Resize((48,60,48)), NormalizeIntensity()])         dataset.init_transform(transform)         train_kwargs = { 'shuffle': True}         return DataManager(dataset, **train_kwargs) In\u00a0[\u00a0]: Copied! <pre>model_args = {\n    'in_channels': 1,\n    'out_classes': 2,\n    'dimensions': 3,\n    'num_encoding_blocks': 3,\n    'out_channels_first_layer': 8,\n    'normalization': 'batch',\n    'upsampling_type': 'linear',\n    'padding': True,\n    'activation': 'PReLU',\n}\n\ntraining_args = {\n    'loader_args': { 'batch_size': 16, },\n    'optimizer_args': {\n        'lr': 0.001,\n        'betas': (0.9, 0.999),\n        'eps': 1e-08\n    },\n    'epochs': 1,\n    'dry_run': False,\n    'log_interval': 2,\n    'test_ratio' : 0.0,\n    'test_on_global_updates': False,\n    'test_on_local_updates': False,\n    'batch_maxnum': 2 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n</pre> model_args = {     'in_channels': 1,     'out_classes': 2,     'dimensions': 3,     'num_encoding_blocks': 3,     'out_channels_first_layer': 8,     'normalization': 'batch',     'upsampling_type': 'linear',     'padding': True,     'activation': 'PReLU', }  training_args = {     'loader_args': { 'batch_size': 16, },     'optimizer_args': {         'lr': 0.001,         'betas': (0.9, 0.999),         'eps': 1e-08     },     'epochs': 1,     'dry_run': False,     'log_interval': 2,     'test_ratio' : 0.0,     'test_on_global_updates': False,     'test_on_local_updates': False,     'batch_maxnum': 2 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples }   In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['flixi']\nnum_rounds = 1\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=UNetTrainingPlan,\n                 training_args=training_args,\n                 round_limit=num_rounds,\n                 aggregator=FedAverage(),\n                )\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['flixi'] num_rounds = 1  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=UNetTrainingPlan,                  training_args=training_args,                  round_limit=num_rounds,                  aggregator=FedAverage(),                 ) In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True)"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#flamby-integration-in-fed-biomed","title":"FLamby integration in Fed-BioMed\u00b6","text":"<p>This notebook showcases some examples of the integration between FLamby and Fed-BioMed.</p> <p>For a thorough understanding, please visit the Tutorials section of our documentation.</p> <p>This tutorial assumes that you know and understand the basics of Fed-BioMed, that you have already set up the network component, and are familiar with flow of adding data through the node CLI interface. For an introduction to Fed-BioMed, please follow our PyTorch MNIST tutorial.</p>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#downloading-flamby-datasets","title":"Downloading FLamby datasets\u00b6","text":"<p>Before using FLamby, you need to download the FLamby datasets that you plan to use. For licensing reasons, these are not including directly in the FLamby installation.</p> <p>To download the <code>fed_ixi</code> dataset in <code>${FEDBIOMED_DIR}/data</code>, follow FLamby download instructions. In a nutshell:</p> <ul> <li>execute on the researcher</li> </ul> <pre>#use the environment where Fed-BioMed node is installed\npip install nibabel\n</pre> <ul> <li>then execute on each node (where <code>${FEDBIOMED_DIR}</code> is the base directory of Fed-BioMed):</li> </ul> <pre>#use the environment where Fed-BioMed node is installed\npip install nibabel\n# if a conda environment is used\npython $(find $CONDA_PREFIX -path */fed_ixi/dataset_creation_scripts/download.py) -o ${FEDBIOMED_DIR}/data\n# if a virtualenv environment is used \npython $(find $VIRTUAL_ENV -path */fed_ixi/dataset_creation_scripts/download.py) -o ${FEDBIOMED_DIR}/data\n</pre> <p>To download the <code>fed_heart_disease</code> dataset in <code>${FEDBIOMED_DIR}/data</code>, follow FLamby download instructions. In a nutshell:</p> <ul> <li>execute on the researcher</li> </ul> <pre>#use the environment where Fed-BioMed researcher is installed\npip install wget\n</pre> <ul> <li>then execute on each node (where <code>${FEDBIOMED_DIR}</code> is the base directory of Fed-BioMed):</li> </ul> <pre>#use the environment where Fed-BioMed node is installed\npip install wget\n# if a conda environment is used\npython $(find $CONDA_PREFIX -path */fed_heart_disease/dataset_creation_scripts/download.py) --output-folder ${FEDBIOMED_DIR}/data\n# if a virtual environment is used\npython $(find $VIRTUAL_ENV -path */fed_heart_disease/dataset_creation_scripts/download.py) --output-folder ${FEDBIOMED_DIR}/data\n</pre>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>If you haven't done so already, install the additional dependencies required by the flamby datasets/features that you intend on using.</p> <p>You may check out which dependencies are needed by each dataset directly from Flamby's <code>setup.py</code> file. In our case we'll be using the federated IXI and federated heart disease datasets, hence we'll need wget, monai and nibabel.</p>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#running-a-flamby-experiment-in-a-federated-setting-with-fed-biomed","title":"Running a FLamby experiment in a federated setting with Fed-BioMed\u00b6","text":"<p>Before running a federated experiment, we need to add a FLamby dataset to a node. From a terminal, <code>cd</code> to the Fed-BioMed root installation directory and run</p> <pre>fedbiomed node dataset add\n</pre> <p>Then follow these instructions:</p> <ul> <li>Select option 6 (<code>flamby</code>) when prompted about the data type</li> <li>type any name for the database (suggested <code>flamby-ixi</code>), press Enter to continue</li> <li>type <code>flixi</code> when prompted for tags, press Enter to continue</li> <li>type any description (suggested <code>flamby-ixi</code>), press Enter to continue</li> <li>select option 3 (<code>fed_ixi</code>) when prompted for the FLamby dataset to be configured</li> <li>type a number in the given range, press Enter to continue</li> <li>type any description for the data loading plan (suggested <code>flamby-ixi-dlp</code>), press Enter to continue</li> </ul> <p>Optionally, repeat the instructions above for the <code>fed_heart_disease</code> dataset, using <code>flheart</code> for tags.</p> <p>Finally, start the node with</p> <pre>fedbiomed node start\n</pre>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#basic-example-fed-ixi","title":"Basic example: Fed-IXI\u00b6","text":"<p>The first example will use the model, optimizer and loss function provided by FLamby for the IXI dataset.</p> <p>The instructions for using FLamby are:</p> <ul> <li>define a <code>TorchTrainingPlan</code></li> <li>in the <code>training_data</code> function, instantiate a <code>FlambyDataset</code></li> <li>make sure to include the necessary dependencies in the <code>init_dependencies</code> function</li> </ul>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#basic-example-fed-heart-disease","title":"Basic example: Fed-Heart-Disease\u00b6","text":"<p>We showcase similar functionalities as the above fed-ixi case, but with FLamby's Heart Disease dataset.</p>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#complex-example-fed-ixi-with-data-preprocessing-and-custom-training-elements","title":"Complex example: Fed-IXI with data preprocessing and custom training elements\u00b6","text":"<p>This example demonstrates how to define transformations for data preprocessing and provide a customized model, optimizer, and loss function. Incidentally, it also shows how to use <code>model_args</code> and <code>training_args</code> to parametrize the model, optimizer, and training loop.</p>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#definition-of-preprocessing-transforms","title":"Definition of preprocessing transforms\u00b6","text":"<p>This is achieved in the <code>training_data</code> function. After instantiating the <code>FlambyDataset</code>, you may use the <code>init_transform</code> function to attach a preprocessing transformation for your data. Note that the transform that you define must be of type <code>torchvision.transforms.Compose</code> or <code>monai.transforms.Compose</code>.</p>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#definition-of-custom-model-optimizer-and-loss","title":"Definition of custom model, optimizer and loss\u00b6","text":"<p>This is achieved just like any <code>TorchTrainingPlan</code>, through the functions <code>init_model</code>, <code>init_optimizer</code>, and <code>training_step</code>.</p>"},{"location":"tutorials/flamby/flamby/","title":"FLamby integration in Fed-BioMed general concepts","text":"<p>Fed-BioMed supports easy integration with Owkin's FLamby. FLamby is a benchmark and dataset suite for cross-silo federated learning with natural partitioning, focused on healthcare applications. FLamby may be used as either a dataset suite or as a fully-fledged benchmark to compare the performance of ML algorithms against a set of standardized approaches and data.</p> <p>Fed-BioMed integration with FLamby is only supported with the PyTorch framework. Hence, to use FLamby you must declare a <code>TorchTrainingPlan</code> for your experiment. Fed-BioMed provides a <code>FlambyDataset</code> class that, together with a correctly configured <code>DataLoadingPlan</code>, takes care of all the boilerplate necessary for loading a FLamby dataset in a federated experiment.</p> <p>Summary</p> <p>To use FLamby in your Fed-BioMed experiment, follow these simple rules:</p> <ul> <li>use a <code>TorchTrainingPlan</code></li> <li>create a <code>FlambyDataset</code> in your <code>training_data</code> function</li> <li>Make sure to properly configure a <code>DataLoadingPlan</code> when loading the data to the node</li> </ul>"},{"location":"tutorials/flamby/flamby/#installing-flamby-and-downloading-the-datasets","title":"Installing FLamby and downloading the datasets","text":"<p>If you wish to use Flamby, you need to install the package with</p> <pre><code>pip install  git+https://github.com/owkin/FLamby@main\n</code></pre> <p>Additionally, you will need to manually:</p> <ul> <li>install any dependencies required by the FLamby datasets that you wish to use.</li> <li>download those datasets.</li> </ul> <p>To install the dependencies:</p> <ul> <li>check in FLamby's setup.py the dependencies <code>&lt;PACKAGES&gt;</code> for the dataset you wish to use. For example, dataset <code>tcga</code> needs <code>lifelines</code>.</li> <li>install the dependencies by executing on the researcher (where <code>${FEDBIOMED_DIR}</code> is Fed-BioMed's base directory) <pre><code># use the python environment for [development](../docs/developer/development-environment.md)\npip install &lt;PACKAGES&gt;\n</code></pre></li> <li>install dependencies by executing on each node <pre><code># use the python environment for [development](../docs/developer/development-environment.md)\npip install &lt;PACKAGES&gt;\n</code></pre></li> </ul> <p>To download the dataset named <code>&lt;DATASET&gt;</code> (eg <code>fed_ixi</code> for IXI):</p> <ul> <li> <p>download the dataset by executing on each node</p> <ul> <li> <ol> <li>if you are using a conda virtual environment: <pre><code># use the python environment for [development](../docs/developer/development-environment.md)\npython $(find $CONDA_PREFIX -path */&lt;DATASET&gt;/dataset_creation_scripts/download.py) -o ${FEDBIOMED_DIR}/data\n</code></pre></li> </ol> </li> <li> <ol> <li>if you are using a venv virtual environment: <pre><code># use the python environment for [development](../docs/developer/development-environment.md)\npython $(find $VIRTUAL_ENV -path */&lt;DATASET&gt;/dataset_creation_scripts/download.py) -o ${FEDBIOMED_DIR}/data\n</code></pre></li> </ol> </li> </ul> </li> </ul>"},{"location":"tutorials/flamby/flamby/#defining-the-training-plan","title":"Defining the Training Plan","text":"<p>In Fed-BioMed, researchers create a training plan  to define various aspects of their federated ML experiment, such as the model, the data, the optimizer, and others. To leverage FLamby functionalities within your Fed-BioMed experiment, you will be required to create a  custom training plan inheriting from <code>fedbiomed.common.data.TorchTrainingPlan</code>. </p> <p>For details on the meaning of the different functions in a training plan, and how to implement them correctly,  please follow the TrainingPlan user guide.  Since FLamby is highly compatible with PyTorch, you may use the models and optimizers provided by FLamby in your  Fed-BioMed experiment seamlessly. See the code below for an example:</p> <pre><code>from fedbiomed.common.data import TorchTrainingPlan\nfrom flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args):\n        return Baseline()\n\n    def init_optimizer(self, optimizer_args):\n        return Optimizer()\n\n    def init_dependencies(self):\n        return [\"from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\"]\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        return BaselineLoss(output, target)\n\n    def training_data(self):\n        # See explanation below\n        pass\n</code></pre> <p>Obviously, you may also plug different definitions for the model, optimizer, and loss function, provided that you respect the conditions and guidelines for <code>TorchTrainingPlan</code>.</p>"},{"location":"tutorials/flamby/flamby/#implementing-the-training_data-function","title":"Implementing the <code>training_data</code> function","text":"<p>Fed-BioMed provides a <code>FlambyDataset</code> class that enables simple integration with FLamby datasets. This class requires  an associated <code>DataLoadingPlan</code> to be properly configured in order to work correctly on the node side. If you follow the data adding process through either the CLI or the GUI, the configuration of the <code>DataLoadingPlan</code> will be done automatically for you. </p> <p>To use Flamby, you need to create a FLamby dataset in your <code>training_data</code> function following the example below: <pre><code>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import FlambyDataset, DataManager\n\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_dependencies(self):\n        return [\"from fedbiomed.common.data import FlambyDataset, DataManager\"]\n\n    def training_data(self):\n        dataset = FlambyDataset()\n        loader_arguments = {'shuffle': True}\n        return DataManager(dataset, **loader_arguments)\n\n    # ... Implement the other functions as needed ...\n</code></pre></p>"},{"location":"tutorials/flamby/flamby/#data-transformations","title":"Data transformations","text":"<p>Functional data transformations can be specified in the <code>training_data</code> function, similarly to the common  <code>TorchTrainingPlan</code> pattern. However, for FLamby you are required to use the special function <code>init_transform</code> of <code>FlambyDataset</code>, as per the example below.</p> <pre><code>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom monai.transforms import Compose, Resize, NormalizeIntensity\nfrom fedbiomed.common.data import FlambyDataset, DataManager\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_dependencies(self):\n        return [\"from fedbiomed.common.data import FlambyDataset, DataManager\",\n                \"from monai.transforms import Compose, Resize, NormalizeIntensity\",\n                ]\n\n    def training_data(self):\n        dataset = FlambyDataset()\n\n        myComposedTransform = Compose([Resize((48,60,48)), NormalizeIntensity()])\n        dataset.init_transform(myComposedTransform)\n\n        train_kwargs = {'shuffle': True}\n        return DataManager(dataset, **train_kwargs)\n\n    # ... Implement the other functions as needed ...\n</code></pre> <p>Tranforms must always be of <code>Compose</code> type</p> <p>Transforms added to a <code>FlambyDataset</code> must always be either of type <code>torch.transforms.Compose</code> or <code>monai.transforms.Compose</code></p> <p>Do not forget to always add your transform as dependencies in the <code>init_dependencies</code> function!</p>"},{"location":"tutorials/medical/download_and_split_ixi/","title":"Download and split ixi","text":"In\u00a0[\u00a0]: Copied! <pre>import hashlib\nimport os\nimport requests\nfrom tqdm import tqdm\nimport argparse\nfrom zipfile import ZipFile\nimport shutil\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n</pre> import hashlib import os import requests from tqdm import tqdm import argparse from zipfile import ZipFile import shutil import pandas as pd from sklearn.model_selection import train_test_split In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.node.config import node_component\n</pre> from fedbiomed.node.config import node_component In\u00a0[\u00a0]: Copied! <pre>def parse_args():\n    parser = argparse.ArgumentParser(description='IXI Sample downloader and splitter')\n    parser.add_argument('-f', '--root_folder', required=True, type=str)\n    parser.add_argument('-F', '--force',  action=argparse.BooleanOptionalAction, required=False, type=bool, default=False)\n    return parser.parse_args()\n</pre> def parse_args():     parser = argparse.ArgumentParser(description='IXI Sample downloader and splitter')     parser.add_argument('-f', '--root_folder', required=True, type=str)     parser.add_argument('-F', '--force',  action=argparse.BooleanOptionalAction, required=False, type=bool, default=False)     return parser.parse_args() In\u00a0[\u00a0]: Copied! <pre>def has_correct_checksum_md5(filename, hash):\n    with open(filename, \"rb\") as f:\n        file_hash = hashlib.md5()\n        while chunk := f.read(8192):\n            file_hash.update(chunk)\n    return str(file_hash.hexdigest()) == hash\n</pre> def has_correct_checksum_md5(filename, hash):     with open(filename, \"rb\") as f:         file_hash = hashlib.md5()         while chunk := f.read(8192):             file_hash.update(chunk)     return str(file_hash.hexdigest()) == hash In\u00a0[\u00a0]: Copied! <pre>def download_file(url, filename):\n    \"\"\"\n    Helper method handling downloading large files from `url` to `filename`. Returns a pointer to `filename`.\n    \"\"\"\n    print('Downloading file from:', url)\n    print('File will be saved as:', filename)\n    chunkSize = 1024\n    r = requests.get(url, stream=True)\n    with open(filename, 'wb') as f:\n        pbar = tqdm(unit=\"B\", total=int(r.headers['Content-Length']))\n        for chunk in r.iter_content(chunk_size=chunkSize):\n            if chunk:  # filter out keep-alive new chunks\n                pbar.update(len(chunk))\n                f.write(chunk)\n    return filename\n</pre> def download_file(url, filename):     \"\"\"     Helper method handling downloading large files from `url` to `filename`. Returns a pointer to `filename`.     \"\"\"     print('Downloading file from:', url)     print('File will be saved as:', filename)     chunkSize = 1024     r = requests.get(url, stream=True)     with open(filename, 'wb') as f:         pbar = tqdm(unit=\"B\", total=int(r.headers['Content-Length']))         for chunk in r.iter_content(chunk_size=chunkSize):             if chunk:  # filter out keep-alive new chunks                 pbar.update(len(chunk))                 f.write(chunk)     return filename In\u00a0[\u00a0]: Copied! <pre>def download_and_extract_ixi_sample(root_folder):\n    url = 'https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/7kd5wj7v7p-3.zip'\n    zip_filename = os.path.join(root_folder, 'notebooks', 'data', '7kd5wj7v7p-3.zip')\n    data_folder = os.path.join(root_folder, 'notebooks', 'data')\n    extracted_folder = os.path.join(data_folder, '7kd5wj7v7p-3', 'IXI_sample')\n\n    # Extract if ZIP exists but not folder\n    if not os.path.exists(zip_filename):\n        # Download if it does not exist\n        download_file(url, zip_filename)\n\n    # Check if extracted folder exists\n    if os.path.isdir(extracted_folder):\n        print(f'Dataset folder already exists in {extracted_folder}')\n        return extracted_folder\n\n    assert has_correct_checksum_md5(zip_filename, 'eecb83422a2685937a955251fa45cb03')\n    with ZipFile(zip_filename, 'r') as zip_obj:\n        zip_obj.extractall(data_folder)\n\n    assert os.path.isdir(extracted_folder)\n    return extracted_folder\n</pre> def download_and_extract_ixi_sample(root_folder):     url = 'https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/7kd5wj7v7p-3.zip'     zip_filename = os.path.join(root_folder, 'notebooks', 'data', '7kd5wj7v7p-3.zip')     data_folder = os.path.join(root_folder, 'notebooks', 'data')     extracted_folder = os.path.join(data_folder, '7kd5wj7v7p-3', 'IXI_sample')      # Extract if ZIP exists but not folder     if not os.path.exists(zip_filename):         # Download if it does not exist         download_file(url, zip_filename)      # Check if extracted folder exists     if os.path.isdir(extracted_folder):         print(f'Dataset folder already exists in {extracted_folder}')         return extracted_folder      assert has_correct_checksum_md5(zip_filename, 'eecb83422a2685937a955251fa45cb03')     with ZipFile(zip_filename, 'r') as zip_obj:         zip_obj.extractall(data_folder)      assert os.path.isdir(extracted_folder)     return extracted_folder In\u00a0[\u00a0]: Copied! <pre>if __name__ == '__main__':\n    args = parse_args()\n    root_folder = os.path.abspath(os.path.expanduser(args.root_folder))\n    assert os.path.isdir(root_folder), f'Folder does not exist: {root_folder}'\n\n    # Centralized dataset\n    centralized_data_folder = download_and_extract_ixi_sample(root_folder)\n\n    # Federated Dataset\n    federated_data_folder = os.path.join(root_folder, 'notebooks', 'data', 'Hospital-Centers')\n    shutil.rmtree(federated_data_folder, ignore_errors=True)\n\n    csv_global = os.path.join(centralized_data_folder, 'participants.csv')\n    allcenters = pd.read_csv(csv_global)\n\n    # Split centers\n    center_names = ['Guys', 'HH', 'IOP']\n    center_dfs = list()\n\n    for center_name in center_names:\n        cfg_folder = os.path.join(args.root_folder, f\"{center_name}\")\n        os.makedirs(cfg_folder, exist_ok=True)\n        cfg_file = os.path.join(cfg_folder, f'{center_name.lower()}.ini')\n\n        print(f'Creating node at: {cfg_file}')\n        node_component.initiate()\n        if node_component.is_component_existing(root_folder):\n            print(f\"**Warning: component {root_folder} already exists\")\n        else:\n            node_component.initiate(root_folder)\n\n        df = allcenters[allcenters.SITE_NAME == center_name]\n        center_dfs.append(df)\n\n        train, test = train_test_split(df, test_size=0.1, random_state=21)\n\n        train_folder = os.path.join(federated_data_folder, center_name, 'train')\n        holdout_folder = os.path.join(federated_data_folder, center_name, 'holdout')\n        if not os.path.exists(train_folder):\n            os.makedirs(train_folder)\n        if not os.path.exists(holdout_folder):\n            os.makedirs(holdout_folder)\n\n        for subject_folder in train.FOLDER_NAME.values:\n            shutil.copytree(\n                src=os.path.join(centralized_data_folder, subject_folder),\n                dst=os.path.join(train_folder, subject_folder),\n                dirs_exist_ok=True\n            )\n\n        train_participants_csv = os.path.join(train_folder, 'participants.csv')\n        train.to_csv(train_participants_csv)\n\n        for subject_folder in test.FOLDER_NAME.values:\n            shutil.copytree(\n                src=os.path.join(centralized_data_folder, subject_folder),\n                dst=os.path.join(holdout_folder, subject_folder),\n                dirs_exist_ok=True\n            )\n        test.to_csv(os.path.join(holdout_folder, 'participants.csv'))\n\n    print(f'Centralized dataset located at: {centralized_data_folder}')\n    print(f'Federated dataset located at: {federated_data_folder}')\n\n    print()\n    print('Please add the data to your nodes executing and using the `ixi-train` tag:')\n    for center_name in center_names:\n        print(f'\\tfedbiomed node --path ./{center_name.lower()} dataset add')\n\n    print()\n    print('Then start your nodes by executing:')\n    for center_name in center_names:\n        print(f'\\tfedbiomed node --path ./{center_name.lower()} start')\n</pre> if __name__ == '__main__':     args = parse_args()     root_folder = os.path.abspath(os.path.expanduser(args.root_folder))     assert os.path.isdir(root_folder), f'Folder does not exist: {root_folder}'      # Centralized dataset     centralized_data_folder = download_and_extract_ixi_sample(root_folder)      # Federated Dataset     federated_data_folder = os.path.join(root_folder, 'notebooks', 'data', 'Hospital-Centers')     shutil.rmtree(federated_data_folder, ignore_errors=True)      csv_global = os.path.join(centralized_data_folder, 'participants.csv')     allcenters = pd.read_csv(csv_global)      # Split centers     center_names = ['Guys', 'HH', 'IOP']     center_dfs = list()      for center_name in center_names:         cfg_folder = os.path.join(args.root_folder, f\"{center_name}\")         os.makedirs(cfg_folder, exist_ok=True)         cfg_file = os.path.join(cfg_folder, f'{center_name.lower()}.ini')          print(f'Creating node at: {cfg_file}')         node_component.initiate()         if node_component.is_component_existing(root_folder):             print(f\"**Warning: component {root_folder} already exists\")         else:             node_component.initiate(root_folder)          df = allcenters[allcenters.SITE_NAME == center_name]         center_dfs.append(df)          train, test = train_test_split(df, test_size=0.1, random_state=21)          train_folder = os.path.join(federated_data_folder, center_name, 'train')         holdout_folder = os.path.join(federated_data_folder, center_name, 'holdout')         if not os.path.exists(train_folder):             os.makedirs(train_folder)         if not os.path.exists(holdout_folder):             os.makedirs(holdout_folder)          for subject_folder in train.FOLDER_NAME.values:             shutil.copytree(                 src=os.path.join(centralized_data_folder, subject_folder),                 dst=os.path.join(train_folder, subject_folder),                 dirs_exist_ok=True             )          train_participants_csv = os.path.join(train_folder, 'participants.csv')         train.to_csv(train_participants_csv)          for subject_folder in test.FOLDER_NAME.values:             shutil.copytree(                 src=os.path.join(centralized_data_folder, subject_folder),                 dst=os.path.join(holdout_folder, subject_folder),                 dirs_exist_ok=True             )         test.to_csv(os.path.join(holdout_folder, 'participants.csv'))      print(f'Centralized dataset located at: {centralized_data_folder}')     print(f'Federated dataset located at: {federated_data_folder}')      print()     print('Please add the data to your nodes executing and using the `ixi-train` tag:')     for center_name in center_names:         print(f'\\tfedbiomed node --path ./{center_name.lower()} dataset add')      print()     print('Then start your nodes by executing:')     for center_name in center_names:         print(f'\\tfedbiomed node --path ./{center_name.lower()} start')"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/","title":"Brain Segmentation","text":"In\u00a0[1]: Copied! <pre>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.logger import logger\nfrom fedbiomed.common.data import DataManager, MedicalFolderDataset\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom unet import UNet\n\nclass UNetTrainingPlan(TorchTrainingPlan):\n\n    def init_model(self, model_args):\n        model = self.Net(model_args)\n        return model\n\n\n    def init_optimizer(self):\n        optimizer = AdamW(self.model().parameters())\n        return optimizer\n\n    def init_dependencies(self):\n        # Here we define the custom dependencies that will be needed by our custom Dataloader\n        deps = [\"from monai.transforms import (Compose, NormalizeIntensity, AddChannel, Resize, AsDiscrete)\",\n               \"import torch.nn as nn\",\n               'import torch.nn.functional as F',\n               \"from fedbiomed.common.data import MedicalFolderDataset\",\n               'import numpy as np',\n               'from torch.optim import AdamW',\n               'from unet import UNet']\n        return deps\n\n\n    class Net(nn.Module):\n        # Init of UNetTrainingPlan\n        def __init__(self, model_args: dict = {}):\n            super().__init__()\n            self.CHANNELS_DIMENSION = 1\n\n            self.unet = UNet(\n                in_channels = model_args.get('in_channels',1),\n                out_classes = model_args.get('out_classes',2),\n                dimensions = model_args.get('dimensions',2),\n                num_encoding_blocks = model_args.get('num_encoding_blocks',5),\n                out_channels_first_layer = model_args.get('out_channels_first_layer',64),\n                normalization = model_args.get('normalization', None),\n                pooling_type = model_args.get('pooling_type', 'max'),\n                upsampling_type = model_args.get('upsampling_type','conv'),\n                preactivation = model_args.get('preactivation',False),\n                residual = model_args.get('residual',False),\n                padding = model_args.get('padding',0),\n                padding_mode = model_args.get('padding_mode','zeros'),\n                activation = model_args.get('activation','ReLU'),\n                initial_dilation = model_args.get('initial_dilation',None),\n                dropout = model_args.get('dropout',0),\n                monte_carlo_dropout = model_args.get('monte_carlo_dropout',0)\n            )\n\n        def forward(self, x):\n            x = self.unet.forward(x)\n            x = F.softmax(x, dim=self.CHANNELS_DIMENSION)\n            return x\n\n    @staticmethod\n    def get_dice_loss(output, target, epsilon=1e-9):\n        SPATIAL_DIMENSIONS = 2, 3, 4\n        p0 = output\n        g0 = target\n        p1 = 1 - p0\n        g1 = 1 - g0\n        tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n        fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n        fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n        num = 2 * tp\n        denom = 2 * tp + fp + fn + epsilon\n        dice_score = num / denom\n        return 1. - dice_score\n\n    @staticmethod\n    def demographics_transform(demographics: dict):\n        \"\"\"Transforms dict of demographics into data type for ML.\n\n        This function is provided for demonstration purposes, but\n        note that if you intend to use demographics data as part\n        of your model's input, you **must** provide a\n        `demographics_transform` function which at the very least\n        converts the demographics dict into a torch.Tensor.\n\n        Must return either a torch Tensor or something Tensor-like\n        that can be easily converted through the torch.as_tensor()\n        function.\"\"\"\n\n        if isinstance(demographics, dict) and len(demographics) == 0:\n            # when input is empty dict, we don't want to transform anything\n            return demographics\n\n        # simple example: keep only some keys\n        keys_to_keep = ['HEIGHT', 'WEIGHT']\n        out = np.array([float(val) for key, val in demographics.items() if key in keys_to_keep])\n\n        # more complex: generate dummy variables for site name\n        # not ideal as it requires knowing the site names in advance\n        # could be better implemented with some preprocess\n        site_names = ['Guys', 'IOP', 'HH']\n        len_dummy_vars = len(site_names) + 1\n        dummy_vars = np.zeros(shape=(len_dummy_vars,))\n        site_name = demographics['SITE_NAME']\n        if site_name in site_names:\n            site_idx = site_names.index(site_name)\n        else:\n            site_idx = len_dummy_vars - 1\n        dummy_vars[site_idx] = 1.\n\n        return np.concatenate((out, dummy_vars))\n\n\n    def training_data(self):\n    # The training_data creates the Dataloader to be used for training in the general class Torchnn of fedbiomed\n        common_shape = (48, 60, 48)\n        training_transform = Compose([AddChannel(), Resize(common_shape), NormalizeIntensity(),])\n        target_transform = Compose([AddChannel(), Resize(common_shape), AsDiscrete(to_onehot=2)])\n\n        dataset = MedicalFolderDataset(\n            root=self.dataset_path,\n            data_modalities='T1',\n            target_modalities='label',\n            transform=training_transform,\n            target_transform=target_transform,\n            demographics_transform=UNetTrainingPlan.demographics_transform)\n        loader_arguments = { 'shuffle': True}\n        return DataManager(dataset, **loader_arguments)\n\n\n    def training_step(self, data, target):\n        #this function must return the loss to backward it\n        img = data[0]['T1']\n        demographics = data[1]\n        output = self.model().forward(img)\n        loss = UNetTrainingPlan.get_dice_loss(output, target['label'])\n        avg_loss = loss.mean()\n        return avg_loss\n\n    def testing_step(self, data, target):\n        img = data[0]['T1']\n        demographics = data[1]\n        target = target['label']\n        prediction = self.model().forward(img)\n        loss = UNetTrainingPlan.get_dice_loss(prediction, target)\n        avg_loss = loss.mean()  # average per batch\n        return avg_loss\n</pre> from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.logger import logger from fedbiomed.common.data import DataManager, MedicalFolderDataset import torch.nn as nn from torch.optim import AdamW from unet import UNet  class UNetTrainingPlan(TorchTrainingPlan):      def init_model(self, model_args):         model = self.Net(model_args)         return model       def init_optimizer(self):         optimizer = AdamW(self.model().parameters())         return optimizer      def init_dependencies(self):         # Here we define the custom dependencies that will be needed by our custom Dataloader         deps = [\"from monai.transforms import (Compose, NormalizeIntensity, AddChannel, Resize, AsDiscrete)\",                \"import torch.nn as nn\",                'import torch.nn.functional as F',                \"from fedbiomed.common.data import MedicalFolderDataset\",                'import numpy as np',                'from torch.optim import AdamW',                'from unet import UNet']         return deps       class Net(nn.Module):         # Init of UNetTrainingPlan         def __init__(self, model_args: dict = {}):             super().__init__()             self.CHANNELS_DIMENSION = 1              self.unet = UNet(                 in_channels = model_args.get('in_channels',1),                 out_classes = model_args.get('out_classes',2),                 dimensions = model_args.get('dimensions',2),                 num_encoding_blocks = model_args.get('num_encoding_blocks',5),                 out_channels_first_layer = model_args.get('out_channels_first_layer',64),                 normalization = model_args.get('normalization', None),                 pooling_type = model_args.get('pooling_type', 'max'),                 upsampling_type = model_args.get('upsampling_type','conv'),                 preactivation = model_args.get('preactivation',False),                 residual = model_args.get('residual',False),                 padding = model_args.get('padding',0),                 padding_mode = model_args.get('padding_mode','zeros'),                 activation = model_args.get('activation','ReLU'),                 initial_dilation = model_args.get('initial_dilation',None),                 dropout = model_args.get('dropout',0),                 monte_carlo_dropout = model_args.get('monte_carlo_dropout',0)             )          def forward(self, x):             x = self.unet.forward(x)             x = F.softmax(x, dim=self.CHANNELS_DIMENSION)             return x      @staticmethod     def get_dice_loss(output, target, epsilon=1e-9):         SPATIAL_DIMENSIONS = 2, 3, 4         p0 = output         g0 = target         p1 = 1 - p0         g1 = 1 - g0         tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)         fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)         fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)         num = 2 * tp         denom = 2 * tp + fp + fn + epsilon         dice_score = num / denom         return 1. - dice_score      @staticmethod     def demographics_transform(demographics: dict):         \"\"\"Transforms dict of demographics into data type for ML.          This function is provided for demonstration purposes, but         note that if you intend to use demographics data as part         of your model's input, you **must** provide a         `demographics_transform` function which at the very least         converts the demographics dict into a torch.Tensor.          Must return either a torch Tensor or something Tensor-like         that can be easily converted through the torch.as_tensor()         function.\"\"\"          if isinstance(demographics, dict) and len(demographics) == 0:             # when input is empty dict, we don't want to transform anything             return demographics          # simple example: keep only some keys         keys_to_keep = ['HEIGHT', 'WEIGHT']         out = np.array([float(val) for key, val in demographics.items() if key in keys_to_keep])          # more complex: generate dummy variables for site name         # not ideal as it requires knowing the site names in advance         # could be better implemented with some preprocess         site_names = ['Guys', 'IOP', 'HH']         len_dummy_vars = len(site_names) + 1         dummy_vars = np.zeros(shape=(len_dummy_vars,))         site_name = demographics['SITE_NAME']         if site_name in site_names:             site_idx = site_names.index(site_name)         else:             site_idx = len_dummy_vars - 1         dummy_vars[site_idx] = 1.          return np.concatenate((out, dummy_vars))       def training_data(self):     # The training_data creates the Dataloader to be used for training in the general class Torchnn of fedbiomed         common_shape = (48, 60, 48)         training_transform = Compose([AddChannel(), Resize(common_shape), NormalizeIntensity(),])         target_transform = Compose([AddChannel(), Resize(common_shape), AsDiscrete(to_onehot=2)])          dataset = MedicalFolderDataset(             root=self.dataset_path,             data_modalities='T1',             target_modalities='label',             transform=training_transform,             target_transform=target_transform,             demographics_transform=UNetTrainingPlan.demographics_transform)         loader_arguments = { 'shuffle': True}         return DataManager(dataset, **loader_arguments)       def training_step(self, data, target):         #this function must return the loss to backward it         img = data[0]['T1']         demographics = data[1]         output = self.model().forward(img)         loss = UNetTrainingPlan.get_dice_loss(output, target['label'])         avg_loss = loss.mean()         return avg_loss      def testing_step(self, data, target):         img = data[0]['T1']         demographics = data[1]         target = target['label']         prediction = self.model().forward(img)         loss = UNetTrainingPlan.get_dice_loss(prediction, target)         avg_loss = loss.mean()  # average per batch         return avg_loss  In\u00a0[2]: Copied! <pre>model_args = {\n    'in_channels': 1,\n    'out_classes': 2,\n    'dimensions': 3,\n    'num_encoding_blocks': 3,\n    'out_channels_first_layer': 8,\n    'normalization': 'batch',\n    'upsampling_type': 'linear',\n    'padding': True,\n    'activation': 'PReLU',\n}\n\ntraining_args = {\n    'loader_args': { 'batch_size': 16, }, \n    'epochs': 2, \n    'dry_run': False,\n    'log_interval': 2,\n    'test_ratio' : 0.1,\n    'test_on_global_updates': True,\n    'test_on_local_updates': True,\n}\n</pre> model_args = {     'in_channels': 1,     'out_classes': 2,     'dimensions': 3,     'num_encoding_blocks': 3,     'out_channels_first_layer': 8,     'normalization': 'batch',     'upsampling_type': 'linear',     'padding': True,     'activation': 'PReLU', }  training_args = {     'loader_args': { 'batch_size': 16, },      'epochs': 2,      'dry_run': False,     'log_interval': 2,     'test_ratio' : 0.1,     'test_on_global_updates': True,     'test_on_local_updates': True, } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['ixi-train']\nnum_rounds = 3\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=UNetTrainingPlan,\n                 training_args=training_args,\n                 round_limit=num_rounds,\n                 aggregator=FedAverage(),\n                 tensorboard=True\n                )\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['ixi-train'] num_rounds = 3  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=UNetTrainingPlan,                  training_args=training_args,                  round_limit=num_rounds,                  aggregator=FedAverage(),                  tensorboard=True                 ) In\u00a0[\u00a0]: Copied! <pre>%load_ext tensorboard\nfrom fedbiomed.researcher.config import config\ntensorboard_dir = config.vars['TENSORBOARD_RESULTS_DIR']\n%tensorboard --logdir \"$tensorboard_dir\"\n</pre> %load_ext tensorboard from fedbiomed.researcher.config import config tensorboard_dir = config.vars['TENSORBOARD_RESULTS_DIR'] %tensorboard --logdir \"$tensorboard_dir\" <p>On a Macbook Pro from 2015 with a 2,5 GHz Quad-Core Intel Core i7 processor and 16GB of DRAM, training for 3 rounds of 2 epochs each took about 30 minutes. The final training curves look like this:</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') In\u00a0[6]: Copied! <pre>local_training_plan = UNetTrainingPlan()\nlocal_model = local_training_plan.init_model(model_args)\n</pre> local_training_plan = UNetTrainingPlan() local_model = local_training_plan.init_model(model_args) In\u00a0[7]: Copied! <pre>for dependency_statement in local_training_plan.init_dependencies():\n    exec(dependency_statement)\n</pre> for dependency_statement in local_training_plan.init_dependencies():     exec(dependency_statement) In\u00a0[8]: Copied! <pre>local_model.load_state_dict(exp.aggregated_params()[exp.round_current()-1]['params'])\n</pre> local_model.load_state_dict(exp.aggregated_params()[exp.round_current()-1]['params']) Out[8]: <pre>&lt;All keys matched successfully&gt;</pre> In\u00a0[274]: Copied! <pre>from torch.utils.data import DataLoader\nimport os\n\nresearcher_path = os.path.dirname(config.root)\ndata_loaders = []\n\ndatasets = [{\n    'dataset_path' : f'{researcher_path}/guys/data/holdout/',\n    'dataset_parameters': {\n        'tabular_file': f'{researcher_path}/guys/data/holdout/participants.csv',\n        'index_col': 14\n        }\n    },\n    {\n    'dataset_path' : f'{researcher_path}/hh/data/holdout/',\n    'dataset_parameters': {\n        'tabular_file': f'{researcher_path}/hh/data/holdout/participants.csv',\n        'index_col': 14\n        }\n    },\n    {\n    'dataset_path' : f'{researcher_path}/iop/data/holdout/',\n    'dataset_parameters': {\n        'tabular_file': f'{researcher_path}/iop/data/holdout/participants.csv',\n        'index_col': 14\n        }\n    },\n    \n\n]\n\n\nfor dataset in datasets:\n    local_training_plan.dataset_path = dataset['dataset_path']\n    val_data_manager = local_training_plan.training_data()\n    val_data_manager._dataset.set_dataset_parameters(dataset['dataset_parameters'])\n    data_loaders.append(DataLoader(val_data_manager._dataset))\n</pre> from torch.utils.data import DataLoader import os  researcher_path = os.path.dirname(config.root) data_loaders = []  datasets = [{     'dataset_path' : f'{researcher_path}/guys/data/holdout/',     'dataset_parameters': {         'tabular_file': f'{researcher_path}/guys/data/holdout/participants.csv',         'index_col': 14         }     },     {     'dataset_path' : f'{researcher_path}/hh/data/holdout/',     'dataset_parameters': {         'tabular_file': f'{researcher_path}/hh/data/holdout/participants.csv',         'index_col': 14         }     },     {     'dataset_path' : f'{researcher_path}/iop/data/holdout/',     'dataset_parameters': {         'tabular_file': f'{researcher_path}/iop/data/holdout/participants.csv',         'index_col': 14         }     },       ]   for dataset in datasets:     local_training_plan.dataset_path = dataset['dataset_path']     val_data_manager = local_training_plan.training_data()     val_data_manager._dataset.set_dataset_parameters(dataset['dataset_parameters'])     data_loaders.append(DataLoader(val_data_manager._dataset)) In\u00a0[327]: Copied! <pre>import torch\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter1d\n\n\nlocal_model.eval()\nlosses = []\nlabels = []\n\nfor i, dl in enumerate(data_loaders):\n    \n    losses_ = []\n    labels.append(f\"Center {i+1}\")\n    with torch.no_grad():\n        for i, ((images, demographics), targets) in enumerate(dl):\n            \n            image = images['T1']\n            target = targets['label']\n            prediction = local_model.forward(image)\n            loss = UNetTrainingPlan.get_dice_loss(prediction, target)\n            losses_.append(loss.mean())\n    losses.append(losses_)\n\n    \nplt.subplot(111)    \nbxplt = plt.boxplot(losses,\n                    vert=True,\n                    patch_artist=True)   \nplt.title(\"Mean `dice loss` values on validation images\") \nplt.gca().xaxis.set_ticklabels(labels)\n\ncolors = ['pink', 'lightblue', 'lightgreen']\nfor patch, color in zip(bxplt['boxes'], colors):\n        patch.set_facecolor(color) \nplt.show()\n</pre> import torch import matplotlib.pyplot as plt from scipy.ndimage import gaussian_filter1d   local_model.eval() losses = [] labels = []  for i, dl in enumerate(data_loaders):          losses_ = []     labels.append(f\"Center {i+1}\")     with torch.no_grad():         for i, ((images, demographics), targets) in enumerate(dl):                          image = images['T1']             target = targets['label']             prediction = local_model.forward(image)             loss = UNetTrainingPlan.get_dice_loss(prediction, target)             losses_.append(loss.mean())     losses.append(losses_)       plt.subplot(111)     bxplt = plt.boxplot(losses,                     vert=True,                     patch_artist=True)    plt.title(\"Mean `dice loss` values on validation images\")  plt.gca().xaxis.set_ticklabels(labels)  colors = ['pink', 'lightblue', 'lightgreen'] for patch, color in zip(bxplt['boxes'], colors):         patch.set_facecolor(color)  plt.show() In\u00a0[328]: Copied! <pre># Visualize training loss\n\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter1d\n\n\nmonitor = exp.monitor()\nmetrics = monitor._metric_store\n\n\ntraining_metrics = [('training', k) for k in list(metrics[list(metrics.keys())[0]]['training'].keys())]\ntesting_global_metrics = [('testing_global_updates', k) for k in list(metrics[list(metrics.keys())[0]]['testing_global_updates'].keys())]\ntesting_local_metrics = [('testing_local_updates', k) for k in list(metrics[list(metrics.keys())[0]]['testing_local_updates'].keys())]\n\n\nmetrics_ = [*training_metrics, *testing_local_metrics, *testing_global_metrics]\n\ncols = len(metrics_)\n\n\nfig, axes = plt.subplots(1, cols, figsize=( cols * 4, len(metrics) * 1.5))\n\nfor i, (node, store) in enumerate(metrics.items()):\n    \n    title = \"\"\n    for k, (for_, m_) in enumerate(metrics_):\n        \n        title = f\"Metrics {for_}\" if title != f\"Metrics {for_}\" else title\n        data = [i  for k, l in store[for_].get(m_, {}).items() for i in l[\"values\"]]\n        smoothed = gaussian_filter1d(data, sigma=1.5)\n        axes[k].plot(smoothed, label=f\"Node {i+1}\")\n        axes[k].set_title(title)\n        axes[k].set_ylabel(m_)\n        axes[k].set_xlabel(\"Iterations\")\n        axes[k].legend()\n        \nfig.tight_layout()\nplt.show()\n</pre> # Visualize training loss  import matplotlib.pyplot as plt from scipy.ndimage import gaussian_filter1d   monitor = exp.monitor() metrics = monitor._metric_store   training_metrics = [('training', k) for k in list(metrics[list(metrics.keys())[0]]['training'].keys())] testing_global_metrics = [('testing_global_updates', k) for k in list(metrics[list(metrics.keys())[0]]['testing_global_updates'].keys())] testing_local_metrics = [('testing_local_updates', k) for k in list(metrics[list(metrics.keys())[0]]['testing_local_updates'].keys())]   metrics_ = [*training_metrics, *testing_local_metrics, *testing_global_metrics]  cols = len(metrics_)   fig, axes = plt.subplots(1, cols, figsize=( cols * 4, len(metrics) * 1.5))  for i, (node, store) in enumerate(metrics.items()):          title = \"\"     for k, (for_, m_) in enumerate(metrics_):                  title = f\"Metrics {for_}\" if title != f\"Metrics {for_}\" else title         data = [i  for k, l in store[for_].get(m_, {}).items() for i in l[\"values\"]]         smoothed = gaussian_filter1d(data, sigma=1.5)         axes[k].plot(smoothed, label=f\"Node {i+1}\")         axes[k].set_title(title)         axes[k].set_ylabel(m_)         axes[k].set_xlabel(\"Iterations\")         axes[k].legend()          fig.tight_layout() plt.show()  In\u00a0[17]: Copied! <pre>val_data_loader = data_loaders[0]\none_batch = next(iter(val_data_loader))\n</pre> val_data_loader = data_loaders[0] one_batch = next(iter(val_data_loader)) <p><code>one_batch</code> contains both input features and labels. Both are 3D images, which can be accessed in the following way (<code>k</code> represents the height in the stack of images):</p> In\u00a0[\u00a0]: Copied! <pre>k = 24\none_batch[1]['label'][..., k].shape\n</pre> k = 24 one_batch[1]['label'][..., k].shape In\u00a0[\u00a0]: Copied! <pre>k = 24\none_batch[0][0]['T1'][..., k].shape\n</pre> k = 24 one_batch[0][0]['T1'][..., k].shape In\u00a0[308]: Copied! <pre>import matplotlib.pyplot as plt\n\n\n# Display only layer 24 of 3D image\nk = 24\nmax_samples = 4\nfig, axes = plt.subplots(max_samples, 3, figsize=(6, max_samples * 1.7 ))\n\n\n# Set labels\naxes[0][0].set_title(\"MRI\")\naxes[0][1].set_title(\"Target\")\naxes[0][2].set_title(\"Predicted\")\n    \nfor b, val in enumerate(val_data_loader):\n    \n    if b &gt;= max_samples:\n        break\n    \n    (image, demographic), target = val\n\n    mri = image['T1']\n    target = target['label']\n\n    with torch.no_grad():\n        prediction = local_model.forward(mri)\n \n    image_mri = mri[0, 0, ..., k]\n    axes[b][0].imshow(image_mri)\n    \n    image_target = target[0, 0, ..., k] \n    axes[b][1].imshow(image_target)\n\n    pred = prediction[0, 0, ..., k]\n    axes[b][2].imshow(pred)\n    \n    \nfig.tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt   # Display only layer 24 of 3D image k = 24 max_samples = 4 fig, axes = plt.subplots(max_samples, 3, figsize=(6, max_samples * 1.7 ))   # Set labels axes[0][0].set_title(\"MRI\") axes[0][1].set_title(\"Target\") axes[0][2].set_title(\"Predicted\")      for b, val in enumerate(val_data_loader):          if b &gt;= max_samples:         break          (image, demographic), target = val      mri = image['T1']     target = target['label']      with torch.no_grad():         prediction = local_model.forward(mri)       image_mri = mri[0, 0, ..., k]     axes[b][0].imshow(image_mri)          image_target = target[0, 0, ..., k]      axes[b][1].imshow(image_target)      pred = prediction[0, 0, ..., k]     axes[b][2].imshow(pred)           fig.tight_layout() plt.show()"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#brain-segmentation","title":"Brain Segmentation\u00b6","text":"<p>This tutorial will show how to use Fed-BioMed to perform image segmentation on 3D medical MRI images of brains, using the publicly available IXI dataset. It uses a 3D U-Net model for the segmentation, trained on data from 3 separate centers.</p> <p>Here we display a very complex case, using advanced Fed-BioMed functionalities such as:</p> <ul> <li>loading a <code>MedicalFolderDataset</code></li> <li>implementing a custom Node Selection Strategy</li> <li>setting a non-default Optimizer</li> <li>monitoring training loss with Tensorboard</li> </ul> <p>This tutorial is based on TorchIO's tutorial.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#automatic-download-and-wrangling-for-the-impatient","title":"Automatic download and wrangling for the impatient\u00b6","text":"<p>If you're not interested in the details, you may simply execute the <code>download_and_split_ixi.py</code> script provided by us, as explained below</p> <p>From your folder execute:</p> <pre>febiomed component create -c researcher\npip install tqdm\nfbm-researcher/notebooks/medical-image-segmentation/download_and_split_ixi.py -f fbm-researcher\n</pre> <p>After successfully running the command, follow the instructions printed to add the datasets and run the nodes. Datasets are located in the nodes' <code>data</code> folder (e.g. for <code>Guys</code>, it is located into <code>guys/data</code>). The tag used for this experiment is <code>ixi-train</code>.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#details-about-data-preparation","title":"Details about data preparation\u00b6","text":"<p>If you just want to run the notebook, you may skip this section and skip to <code>Define a new strategy</code>.</p> <p>First, download the IXI dataset from the Mendeley archive.</p> <p>In this tutorial we are going to use the <code>MedicalFolderDataset</code> class provided by the Fed-BioMed library to load medical images in NIFTI format. Using this dataset class for image segmentation problems guarantees maximum compatibility with the rest of the Fed-BioMed functionalities and features.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#folder-structure-for-medicalfolderdataset","title":"Folder structure for MedicalFolderDataset\u00b6","text":"<p>The <code>MedicalFolderDataset</code> is heavily inspired by PyTorch's <code>ImageFolder</code> Dataset, and requires you to manually prepare the image folders in order to respect a precise structure. The format assumes that you are dealing with imaging data, possibly acquired through multiple modalities, for different study subjects. Hence, you should provide one folder per subject, containing multiple subfolders for each image acquisition modality. Optionally, you may provide a <code>csv</code> file containing additional tabular data associated with each subject. This file is typically used for demographics data, and by default is called <code>participants.csv</code>.</p> <pre>_ root-folder\n |_ participants.csv\n |_ subject-1\n | |_ modality-1\n | |_ modality-2\n |_ subject-2\n | |_ modality-1\n | |_ modality-2\n |_ subject-3\n | |_ modality-1\n . .\n . .\n . .\n</pre>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#folder-structure-for-this-tutorial","title":"Folder structure for this tutorial\u00b6","text":"<p>In the specific case of this tutorial, we encourage you to further divide your images into additional subfolders, according to two criteria: the hospital that generated the data (there are three: Guys, HH and IOP) and a random train/holdout split.</p> <p>!!! info Note that each subject's folder will have a name with the following structure: <code>IXI&lt;SUBJECT_ID&gt;-&lt;HOSPITAL&gt;-&lt;RANDOM_ID&gt;</code>, for example <code>IXI002-Guys-0828</code>. In conclusion, combining the splits above with the structure required by the <code>MedicalFolderDataset</code>, your folder tree should look like this:</p> <pre>_root-folder\n |_ Guys\n | |_ train\n | | |_ participants.csv\n | | |_ IXI002-Guys-0828\n | | | |_ T1                &lt;-- T1 is the first imaging modality\n | | | |_ T2\n | | | |_ label\n | | |_ IXI022-Guys-0701\n | | | |_ T1\n | | | |_ T2\n . . .\n . . .\n . . .\n | |_ holdout\n | | |_ participants.csv\n | | |_ IXI004-Guys-0321\n | | | |_ T1\n | | | |_ T2\n | | | |_ label\n | | | |_ T2\n . . .\n . . .\n . . .\n |_ HH\n | |_ train\n . . .\n . . .\n . . .\n | |_ holdout\n . . .\n . . .\n . . .\n |_ IOP\n . . .\n . . .\n</pre>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#add-the-ixi-dataset-to-the-federated-nodes","title":"Add the IXI dataset to the federated nodes\u00b6","text":"<p>For each of the three hospitals, create a federated node and add the corresponding train dataset by selecting the <code>medical-folder</code> data type, and inputting <code>ixi-train</code> as the tag. Then start the nodes.</p> <p>         Dataset for demograhics of the subjects     </p> <p>         After selecting the folder that contains the patients for training the CLI will ask for CSV file where demographics of the patient are stored. These CSV files are named as `participants.csv`, and you can find these CSV files in the folder where the subject folders are located e.g `Guys/train/participant.csv`.     </p> <p>If you don't know how to add datasets to a node, or start a node, please read our user guide or follow the basic tutorial.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#create-a-training-plan","title":"Create a Training Plan\u00b6","text":"<p>We create a training plan that incorporates the UNet model. We rely on the unet package for simplicity. Please refer to the original package for more details about UNet: P\u00e9rez-Garc\u00eda, Fernando. (2020). fepegar/unet: PyTorch implementation of 2D and 3D U-Net (v0.7.5). Zenodo. https://doi.org/10.5281/zenodo.3697931</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#define-the-model-via-the-init_model-function","title":"Define the model via the <code>init_model</code> function\u00b6","text":"<p>The <code>init_model</code> function must return a UNet instance. Please refer to the TrainingPlan documentation for more details.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#define-the-loss-function-via-the-training_step-function","title":"Define the loss function via the <code>training_step</code> function\u00b6","text":"<p>Loss function is computed based on the Dice Loss.</p> <p>Carole H Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and M Jorge Cardoso. Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations. In Deep learning in medical image analysis and multimodal learning for clinical decision support, pages 240\u2013248. Springer, 2017.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#define-data-loading-and-transformations-via-the-training_data-function","title":"Define data loading and transformations via the <code>training_data</code> function\u00b6","text":"<p>Within the <code>training_data</code> function, we create an instance of <code>MedicalFolderDataset</code> and pass it to Fed-BioMed's <code>DataManager</code> class.</p> <p>To preprocess images, we define the image transformations for the input images and the labels leveraging MONAI's transforms. Note that we also include the correct dependencies in the <code>init_dependencies</code> function.</p> <p>Additionally, we define a transformation for the demographics data contained in the associated <code>csv</code> file. In order to be able to use information extracted from the demographics data as inputs to UNet, we must convert it to a <code>torch.Tensor</code> object. To achieve this, we exploit the <code>demographics_transform</code> argument of the <code>MedicalFolderDataset</code>. The transformation defined in this tutorial is just for illustration purposes, it does little more than just extracting some variables from the tabular data and converting them to the appropriate format.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#define-training-step","title":"Define training step\u00b6","text":"<p>Here we take as input one batch of (data, target), train the model and compute the loss function.</p> <p>Note that the <code>MedicalFolderDataset</code> class returns <code>data</code> as a tuple of <code>(images, demographics)</code>, where:</p> <ul> <li><code>images</code> is a <code>dict</code> of <code>{modality: image</code>} (after image transformations)</li> <li><code>demographics</code> is a <code>dict</code> of <code>{column_name: values}</code> where the column names are taken from the demographics csv file while the <code>target</code> is a <code>dict</code> of <code>{modality: image</code>} (after target transformations).</li> </ul> <p>In our case, the modality used is <code>T1</code> for the input images, while the modality used for the target is <code>label</code>. In this tutorial, we ignore the values of the demographics data during training because the UNet model only takes images as input. However, the code is provided for illustration purposes as it shows the recommended way to handle the associated tabular data.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#prepare-the-experiment","title":"Prepare the experiment\u00b6","text":""},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#tensorboard-setup","title":"Tensorboard setup\u00b6","text":""},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#run-the-experiment","title":"Run the experiment\u00b6","text":""},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#validate-on-a-local-holdout-set","title":"Validate on a local holdout set\u00b6","text":"<p>To ensure consistency and simplify our life, we try to reuse the already-available code as much as possible. Note that this process assumes that the held-out data is stored locally on the machine.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#create-an-instance-of-the-global-model","title":"Create an instance of the global model\u00b6","text":"<p>First, we create an instance of the model using the parameters from the latest aggregation round.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#define-a-validation-data-loader","title":"Define a validation data loader\u00b6","text":"<p>We extract the validation data loader from the training plan as well. This requires some knowledge about the internals of the <code>MedicalFolderDataset</code> class. At the end of the process, calling the <code>split</code> function with a ratio of 0 will return a data loader that loads all of the data.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#compute-the-loss-on-validation-images","title":"Compute the loss on validation images\u00b6","text":""},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#visualize-training-loss-and-testing-metrics","title":"Visualize Training Loss and Testing Metrics\u00b6","text":""},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#visualize-predictions","title":"Visualize Predictions\u00b6","text":"<p>As a bonus, we visualize the outputs of our model on the holdout dataset.</p>"},{"location":"tutorials/monai/","title":"Fed-BioMed using MONAI: a step-by-step tutorial","text":"<p>MONAI is a PyTorch-based framework for deep learning in healthcare imaging.  </p> <ol> <li> <p>2D Image Classification with MedNIST</p> </li> <li> <p>2D Image Regression with MedNIST</p> </li> </ol>"},{"location":"tutorials/monai/01_monai-2d-image-classification/","title":"Federated 2d image classification with MONAI","text":"<p>We are now ready to start the researcher by using the environment where Fed-BioMed researcher is installed, and open the Jupyter notebook with <code>fedbiomed researcher start</code>.</p> <p>We can first quesry the network for the mednist dataset. In this case, the nodes are sharing the respective partitions unsing the same tag <code>mednist</code>:</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.requests import Requests\nfrom fedbiomed.researcher.config import config\nreq = Requests(config)\nreq.list(verbose=True)\n</pre> from fedbiomed.researcher.requests import Requests from fedbiomed.researcher.config import config req = Requests(config) req.list(verbose=True)  <p>The code for network and data loader of the MONAI tutorial can now be deployed in Fed-BioMed. We first import the necessary modules from <code>fedbiomed</code> and <code>monai</code> libraries:</p> <p>We can now define the training plan. Note that we can simply use the standard <code>TorchTrainingPlan</code> natively provided in Fed-BioMed. We reuse the <code>MedNISTDataset</code> data loader defined in the original MONAI tutorial, which is returned by the method <code>training_data</code>, which also implements the data parsing from the nodes <code>dataset_path</code>. Following the MONAI tutorial, the model is the <code>DenseNet121</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\nfrom monai.apps import download_and_extract\nfrom monai.config import print_config\nfrom monai.data import decollate_batch\nfrom monai.metrics import ROCAUCMetric\nfrom monai.networks.nets import DenseNet121\nfrom monai.transforms import (\n    Activations,\n    AddChannel,\n    AsDiscrete,\n    Compose,\n    LoadImage,\n    RandFlip,\n    RandRotate,\n    RandZoom,\n    ScaleIntensity,\n    EnsureType,\n)\nfrom monai.utils import set_determinism\n\n\n\n# Here we define the training plan to be used. \n# You can use any class name (here 'MyTrainingPlan')\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    # Declare dependencies\n    def init_dependencies(self):\n        deps = [\"import numpy as np\",\n                \"import os\",\n                \"from monai.apps import download_and_extract\",\n                \"from monai.config import print_config\",\n                \"from monai.data import decollate_batch\",\n                \"from monai.metrics import ROCAUCMetric\",\n                \"from monai.networks.nets import DenseNet121\",\n                \"from monai.transforms import ( Activations, AddChannel, AsDiscrete, Compose, LoadImage, RandFlip, RandRotate, RandZoom, ScaleIntensity, EnsureType, )\",\n                \"from monai.utils import set_determinism\"]\n        \n        return deps\n    \n    # Define and return model\n    def init_model(self):\n\n        model = DenseNet121(spatial_dims=2, in_channels=1,\n                    out_channels = self.model_args()[\"num_class\"])\n        \n        return model \n        \n    class MedNISTDataset(torch.utils.data.Dataset):\n            def __init__(self, image_files, labels, transforms):\n                self.image_files = image_files\n                self.labels = labels\n                self.transforms = transforms\n\n            def __len__(self):\n                return len(self.image_files)\n\n            def __getitem__(self, index):\n                return self.transforms(self.image_files[index]), self.labels[index]\n    \n    def parse_data(self, path):\n        \n        class_names = sorted(x for x in os.listdir(path)\n                     if os.path.isdir(os.path.join(path, x)))\n        num_class = len(class_names)\n        image_files = [\n                        [\n                            os.path.join(path, class_names[i], x)\n                            for x in os.listdir(os.path.join(path, class_names[i]))\n                        ]\n                        for i in range(num_class)\n                      ]\n        \n        return image_files, num_class\n    \n    def training_data(self):\n        self.image_files, num_class = self.parse_data(self.dataset_path)\n        \n        if self.model_args()[\"num_class\"] != num_class:\n                raise Exception('number of available classes does not match declared classes')\n        \n        num_each = [len(self.image_files[i]) for i in range(self.model_args()[\"num_class\"])]\n        image_files_list = []\n        image_class = []\n        \n        for i in range(self.model_args()[\"num_class\"]):\n            image_files_list.extend(self.image_files[i])\n            image_class.extend([i] * num_each[i])\n        num_total = len(image_class)\n        \n        \n        length = len(image_files_list)\n        indices = np.arange(length)\n        np.random.shuffle(indices)\n\n        val_split = int(1. * length) \n        train_indices = indices[:val_split]\n\n        train_x = [image_files_list[i] for i in train_indices]\n        train_y = [image_class[i] for i in train_indices]\n\n\n        train_transforms = Compose(\n            [\n                LoadImage(image_only=True),\n                AddChannel(),\n                ScaleIntensity(),\n                RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),\n                RandFlip(spatial_axis=0, prob=0.5),\n                RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n                EnsureType(),\n            ]\n        )\n\n        val_transforms = Compose(\n            [LoadImage(image_only=True), AddChannel(), ScaleIntensity(), EnsureType()])\n\n        y_pred_trans = Compose([EnsureType(), Activations(softmax=True)])\n        y_trans = Compose([EnsureType(), AsDiscrete(to_onehot=num_class)])\n                \n        train_ds = self.MedNISTDataset(train_x, train_y, train_transforms)\n        \n        return DataManager(dataset=train_ds,  shuffle=True)\n    \n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.cross_entropy(output, target)\n        return loss\n</pre> import os import numpy as np import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms  from monai.apps import download_and_extract from monai.config import print_config from monai.data import decollate_batch from monai.metrics import ROCAUCMetric from monai.networks.nets import DenseNet121 from monai.transforms import (     Activations,     AddChannel,     AsDiscrete,     Compose,     LoadImage,     RandFlip,     RandRotate,     RandZoom,     ScaleIntensity,     EnsureType, ) from monai.utils import set_determinism    # Here we define the training plan to be used.  # You can use any class name (here 'MyTrainingPlan') class MyTrainingPlan(TorchTrainingPlan):      # Declare dependencies     def init_dependencies(self):         deps = [\"import numpy as np\",                 \"import os\",                 \"from monai.apps import download_and_extract\",                 \"from monai.config import print_config\",                 \"from monai.data import decollate_batch\",                 \"from monai.metrics import ROCAUCMetric\",                 \"from monai.networks.nets import DenseNet121\",                 \"from monai.transforms import ( Activations, AddChannel, AsDiscrete, Compose, LoadImage, RandFlip, RandRotate, RandZoom, ScaleIntensity, EnsureType, )\",                 \"from monai.utils import set_determinism\"]                  return deps          # Define and return model     def init_model(self):          model = DenseNet121(spatial_dims=2, in_channels=1,                     out_channels = self.model_args()[\"num_class\"])                  return model               class MedNISTDataset(torch.utils.data.Dataset):             def __init__(self, image_files, labels, transforms):                 self.image_files = image_files                 self.labels = labels                 self.transforms = transforms              def __len__(self):                 return len(self.image_files)              def __getitem__(self, index):                 return self.transforms(self.image_files[index]), self.labels[index]          def parse_data(self, path):                  class_names = sorted(x for x in os.listdir(path)                      if os.path.isdir(os.path.join(path, x)))         num_class = len(class_names)         image_files = [                         [                             os.path.join(path, class_names[i], x)                             for x in os.listdir(os.path.join(path, class_names[i]))                         ]                         for i in range(num_class)                       ]                  return image_files, num_class          def training_data(self):         self.image_files, num_class = self.parse_data(self.dataset_path)                  if self.model_args()[\"num_class\"] != num_class:                 raise Exception('number of available classes does not match declared classes')                  num_each = [len(self.image_files[i]) for i in range(self.model_args()[\"num_class\"])]         image_files_list = []         image_class = []                  for i in range(self.model_args()[\"num_class\"]):             image_files_list.extend(self.image_files[i])             image_class.extend([i] * num_each[i])         num_total = len(image_class)                           length = len(image_files_list)         indices = np.arange(length)         np.random.shuffle(indices)          val_split = int(1. * length)          train_indices = indices[:val_split]          train_x = [image_files_list[i] for i in train_indices]         train_y = [image_class[i] for i in train_indices]           train_transforms = Compose(             [                 LoadImage(image_only=True),                 AddChannel(),                 ScaleIntensity(),                 RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),                 RandFlip(spatial_axis=0, prob=0.5),                 RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),                 EnsureType(),             ]         )          val_transforms = Compose(             [LoadImage(image_only=True), AddChannel(), ScaleIntensity(), EnsureType()])          y_pred_trans = Compose([EnsureType(), Activations(softmax=True)])         y_trans = Compose([EnsureType(), AsDiscrete(to_onehot=num_class)])                          train_ds = self.MedNISTDataset(train_x, train_y, train_transforms)                  return DataManager(dataset=train_ds,  shuffle=True)          def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.cross_entropy(output, target)         return loss  <p>We now set the model and training parameters. Note that we use only 1 epoch for this experiment, and perform the training on ~26% of the locally available training data.</p> In\u00a0[\u00a0]: Copied! <pre>model_args = {\n    'num_class': 6,  \n}\n\ntraining_args = {\n    'loader_args': { 'batch_size': 20, }, \n    'optimizer_args': {\n        'lr': 1e-5\n    }, \n    'epochs': 1, \n    'dry_run': False,  \n    'batch_maxnum':250 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n</pre> model_args = {     'num_class': 6,   }  training_args = {     'loader_args': { 'batch_size': 20, },      'optimizer_args': {         'lr': 1e-5     },      'epochs': 1,      'dry_run': False,       'batch_maxnum':250 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } <p>The experiment can be now defined, by providing the <code>mednist</code> tag, and running the local training on nodes with training plan defined in <code>training_plan_path</code>, standard <code>aggregator</code> (FedAvg) and <code>client_selection_strategy</code> (all nodes used). Federated learning is going to be perfomed through 3 optimization rounds.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MEDNIST', '#dataset']\nrounds = 3\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None\n                )\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['#MEDNIST', '#dataset'] rounds = 3  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None                 ) <p>Let's start the experiment.</p> <p>By default, this function doesn't stop until all the <code>round_limit</code> rounds are done for all the clients</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') <p>Once the federated model is obtained, it is possible to test it locally on an independent testing partition. The test dataset is available at this link:</p> <p>https://drive.google.com/file/d/1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD/</p> In\u00a0[\u00a0]: Copied! <pre>!pip install gdown\n</pre> !pip install gdown In\u00a0[\u00a0]: Copied! <pre>import PIL\nimport torch\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\nfrom monai.config import print_config\nfrom monai.data import decollate_batch\nfrom monai.metrics import ROCAUCMetric\nfrom monai.networks.nets import DenseNet121\n\nfrom monai.transforms import (\n    Activations,\n    AddChannel,\n    AsDiscrete,\n    Compose,\n    LoadImage,\n    RandFlip,\n    RandRotate,\n    RandZoom,\n    ScaleIntensity,\n    EnsureType,\n)\nfrom monai.utils import set_determinism\n\nprint_config()\n</pre> import PIL import torch import numpy as np from sklearn.metrics import classification_report  from monai.config import print_config from monai.data import decollate_batch from monai.metrics import ROCAUCMetric from monai.networks.nets import DenseNet121  from monai.transforms import (     Activations,     AddChannel,     AsDiscrete,     Compose,     LoadImage,     RandFlip,     RandRotate,     RandZoom,     ScaleIntensity,     EnsureType, ) from monai.utils import set_determinism  print_config() <p>Download the testing dataset on the local temporary folder.</p> In\u00a0[\u00a0]: Copied! <pre>import gdown\nimport zipfile\nimport tempfile\nimport os\nfrom fedbiomed.researcher.config import config\n\ntmp_dir = tempfile.TemporaryDirectory(dir=config.vars['TMP_DIR']+os.sep)\n\nresource = \"https://drive.google.com/uc?id=1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD\"\nbase_dir = tmp_dir.name\ntest_file = os.path.join(base_dir, \"MedNIST_testing.zip\")\n\ngdown.download(resource, test_file, quiet=False)\n\nzf = zipfile.ZipFile(test_file)\n\nfor file in zf.infolist():\n    zf.extract(file, base_dir)\n    \ndata_dir = os.path.join(base_dir, \"MedNIST_testing\")\n</pre> import gdown import zipfile import tempfile import os from fedbiomed.researcher.config import config  tmp_dir = tempfile.TemporaryDirectory(dir=config.vars['TMP_DIR']+os.sep)  resource = \"https://drive.google.com/uc?id=1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD\" base_dir = tmp_dir.name test_file = os.path.join(base_dir, \"MedNIST_testing.zip\")  gdown.download(resource, test_file, quiet=False)  zf = zipfile.ZipFile(test_file)  for file in zf.infolist():     zf.extract(file, base_dir)      data_dir = os.path.join(base_dir, \"MedNIST_testing\") <p>Parse the data and create the testing data loader:</p> In\u00a0[\u00a0]: Copied! <pre>class_names = sorted(x for x in os.listdir(data_dir)\n                     if os.path.isdir(os.path.join(data_dir, x)))\nnum_class = len(class_names)\nimage_files = [\n    [\n        os.path.join(data_dir, class_names[i], x)\n        for x in os.listdir(os.path.join(data_dir, class_names[i]))\n    ]\n    for i in range(num_class)\n]\n\nnum_each = [len(image_files[i]) for i in range(num_class)]\nimage_files_list = []\n\nimage_class = []\nfor i in range(num_class):\n    image_files_list.extend(image_files[i])\n    image_class.extend([i] * num_each[i])\nnum_total = len(image_class)\nimage_width, image_height = PIL.Image.open(image_files_list[0]).size\n\nprint(f\"Total image count: {num_total}\")\nprint(f\"Image dimensions: {image_width} x {image_height}\")\nprint(f\"Label names: {class_names}\")\nprint(f\"Label counts: {num_each}\")\n</pre> class_names = sorted(x for x in os.listdir(data_dir)                      if os.path.isdir(os.path.join(data_dir, x))) num_class = len(class_names) image_files = [     [         os.path.join(data_dir, class_names[i], x)         for x in os.listdir(os.path.join(data_dir, class_names[i]))     ]     for i in range(num_class) ]  num_each = [len(image_files[i]) for i in range(num_class)] image_files_list = []  image_class = [] for i in range(num_class):     image_files_list.extend(image_files[i])     image_class.extend([i] * num_each[i]) num_total = len(image_class) image_width, image_height = PIL.Image.open(image_files_list[0]).size  print(f\"Total image count: {num_total}\") print(f\"Image dimensions: {image_width} x {image_height}\") print(f\"Label names: {class_names}\") print(f\"Label counts: {num_each}\") In\u00a0[\u00a0]: Copied! <pre>length = len(image_files_list)\nindices = np.arange(length)\nnp.random.shuffle(indices)\n\n\ntest_split = int(0.1 * length)\ntest_indices = indices[:test_split]\n\ntest_x = [image_files_list[i] for i in test_indices]\ntest_y = [image_class[i] for i in test_indices]\n\nval_transforms = Compose(\n    [LoadImage(image_only=True), AddChannel(), ScaleIntensity(), EnsureType()])\n\ny_pred_trans = Compose([EnsureType(), Activations(softmax=True)])\ny_trans = Compose([EnsureType(), AsDiscrete(to_onehot=num_class)])\n</pre> length = len(image_files_list) indices = np.arange(length) np.random.shuffle(indices)   test_split = int(0.1 * length) test_indices = indices[:test_split]  test_x = [image_files_list[i] for i in test_indices] test_y = [image_class[i] for i in test_indices]  val_transforms = Compose(     [LoadImage(image_only=True), AddChannel(), ScaleIntensity(), EnsureType()])  y_pred_trans = Compose([EnsureType(), Activations(softmax=True)]) y_trans = Compose([EnsureType(), AsDiscrete(to_onehot=num_class)]) In\u00a0[\u00a0]: Copied! <pre>class MedNISTDataset(torch.utils.data.Dataset):\n    def __init__(self, image_files, labels, transforms):\n        self.image_files = image_files\n        self.labels = labels\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, index):\n        return self.transforms(self.image_files[index]), self.labels[index]\n\n\ntest_ds = MedNISTDataset(test_x, test_y, val_transforms)\ntest_loader = torch.utils.data.DataLoader(\n    test_ds, batch_size=300)\n</pre> class MedNISTDataset(torch.utils.data.Dataset):     def __init__(self, image_files, labels, transforms):         self.image_files = image_files         self.labels = labels         self.transforms = transforms      def __len__(self):         return len(self.image_files)      def __getitem__(self, index):         return self.transforms(self.image_files[index]), self.labels[index]   test_ds = MedNISTDataset(test_x, test_y, val_transforms) test_loader = torch.utils.data.DataLoader(     test_ds, batch_size=300) <p>Define testing metric:</p> In\u00a0[\u00a0]: Copied! <pre>auc_metric = ROCAUCMetric()\n</pre> auc_metric = ROCAUCMetric() <p>To test the federated model we need to create a model instance and assign to it the model parameters estimated at the last federated optimization round.</p> In\u00a0[\u00a0]: Copied! <pre>model = exp.training_plan().model()\nmodel.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n</pre> model = exp.training_plan().model() model.load_state_dict(exp.aggregated_params()[rounds - 1]['params']) <p>Compute the testing performance:</p> In\u00a0[\u00a0]: Copied! <pre>y_true = []\ny_pred = []\nwith torch.no_grad():\n    for test_data in test_loader:\n        test_images, test_labels = (\n            test_data[0],\n            test_data[1],\n        )\n        pred = model(test_images).argmax(dim=1)\n        for i in range(len(pred)):\n            y_true.append(test_labels[i].item())\n            y_pred.append(pred[i].item())\n</pre> y_true = [] y_pred = [] with torch.no_grad():     for test_data in test_loader:         test_images, test_labels = (             test_data[0],             test_data[1],         )         pred = model(test_images).argmax(dim=1)         for i in range(len(pred)):             y_true.append(test_labels[i].item())             y_pred.append(pred[i].item())  In\u00a0[\u00a0]: Copied! <pre>print(classification_report(\n    y_true, y_pred, target_names=class_names, digits=4))\n</pre> print(classification_report(     y_true, y_pred, target_names=class_names, digits=4)) <p>In spite of the relatively small training performed on the data shared in the 3 nodes, the performance of the federated model seems pretty good. Well done!</p>"},{"location":"tutorials/monai/01_monai-2d-image-classification/#federated-2d-image-classification-with-monai","title":"Federated 2d image classification with MONAI\u00b6","text":""},{"location":"tutorials/monai/01_monai-2d-image-classification/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial shows how to deploy in Fed-BioMed the 2d image classification example provided in the project MONAI (https://monai.io/):</p> <p>https://github.com/Project-MONAI/tutorials/blob/master/2d_classification/mednist_tutorial.ipynb</p> <p>Being MONAI based on PyTorch, the deployment within Fed-BioMed follows seamlessly the same general structure of general PyTorch training plans.</p> <p>Following the MONAI example, this tutorial is based on the MedNIST dataset.</p>"},{"location":"tutorials/monai/01_monai-2d-image-classification/#creating-mednist-nodes","title":"Creating MedNIST nodes\u00b6","text":"<p>MedNIST provides an artificial 2d classification dataset created by gathering different medical imaging datasets from TCIA, the RSNA Bone Age Challenge, and the NIH Chest X-ray dataset. The dataset is kindly made available by Dr. Bradley J. Erickson M.D., Ph.D. (Department of Radiology, Mayo Clinic) under the Creative Commons CC BY-SA 4.0 license.</p> <p>To proceed with the tutorial, we created an iid partitioning of the MedNIST dataset between 3 clients. Each client has 3000 image samples for each class. The training partitions are availables at the following link:</p> <p>https://drive.google.com/file/d/1vLIcBdtdAhh6K-vrgCFy_0Y55dxOWZwf/view</p> <p>The dataset owned by each client has structure:</p> <p>\u2514\u2500\u2500 client_*/</p> <pre><code>\u251c\u2500\u2500 AbdomenCT/\n\n\u2514\u2500\u2500 BreastMRI/\n\n\u2514\u2500\u2500 CXR/\n\n\u2514\u2500\u2500 ChestCT/\n\n\u2514\u2500\u2500 Hand/\n\n\u2514\u2500\u2500 HeadCT/   </code></pre> <p>To create the federated dataset, we follow the standard procedure for node creation/population of Fed-BioMed. Following command will create a default node.</p> <p><code>fedbiomed node start</code></p> <p>We then populate the node with the data of first client:</p> <p><code>fedbiomed node dataset add</code></p> <p>We select option 3 (images) to add MedNIST partition of client 1, by just picking the folder of client 1. Assign tag <code>#MEDNIST, #dataset</code> to the data when asked (default tag).</p> <p>We can further check that the data has been added by executing <code>fedbiomed node dataset list</code></p> <p>Following the same procedure, we create the other two nodes with the datasets of client 2 and client 3 respectively using option <code>--path</code> to use different installation folder the those nodes.</p>"},{"location":"tutorials/monai/01_monai-2d-image-classification/#running-fed-biomed-researcher","title":"Running Fed-BioMed Researcher\u00b6","text":""},{"location":"tutorials/monai/01_monai-2d-image-classification/#create-an-experiment-to-train-a-model-on-the-data-found","title":"Create an experiment to train a model on the data found\u00b6","text":""},{"location":"tutorials/monai/01_monai-2d-image-classification/#warning","title":"WARNING:\u00b6","text":"<p>For running this experiment, you need a computer with the following specifications:</p> <ul> <li>more than 16 GB of RAM</li> <li>2.5 GHz processor or higher, with at least 4 cores</li> </ul> <p>If your computer specification are lower, you can reduce the number of data passed when training model (set <code>batchnum</code> from 250 to 25) and the number of <code>rounds</code> (from 3 to 1) but model performances may decrease dramatically</p>"},{"location":"tutorials/monai/01_monai-2d-image-classification/#testing","title":"Testing\u00b6","text":""},{"location":"tutorials/monai/02_monai-2d-image-registration/","title":"Federated 2d XRay registration with MONAI","text":"<p>We are now ready to start the researcher by using the environment where Fed-BioMed researcher is installed, and open the Jupyter notebook with <code>fedbiomed researcher start</code>.</p> <p>We can first query the network for the <code>Mednist</code> dataset. In this case, the nodes are sharing the respective partitions using the same tag <code>mednist</code>:</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.requests import Requests\nfrom fedbiomed.researcher.config import config\nreq = Requests(config)\nreq.list(verbose=True)\n</pre> from fedbiomed.researcher.requests import Requests from fedbiomed.researcher.config import config req = Requests(config) req.list(verbose=True) <p>The code for network and data loader of the MONAI tutorial can now be deployed in Fed-BioMed. We first import the necessary modules from <code>fedbiomed</code> and <code>monai</code> libraries:</p> <p>We can now define the training plan. Note that we use the standard <code>TorchTrainingPlan</code> natively provided in Fed-BioMed. We reuse the <code>MedNISTDataset</code> data loader defined in the original MONAI tutorial, which is returned by the method <code>training_data</code>, which also implements the data parsing from the nodes <code>dataset_path</code>. We should also properly define the <code>training_routine</code>, following the MONAI tutorial. According to the MONAI tutorial, the model is the <code>GlobalNet</code> and the loss is <code>MSELoss</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import mse_loss\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.logger import logger\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\nfrom typing import Union, List\n#from torch.utils.data import Dataset, DataLoader\nimport monai\nfrom monai.utils import set_determinism, first\nfrom monai.transforms import (\n    EnsureChannelFirstD,\n    Compose,\n    LoadImageD,\n    RandRotateD,\n    RandZoomD,\n    ScaleIntensityRanged,\n    EnsureTypeD,\n)\nfrom monai.data import DataLoader, Dataset, CacheDataset\nfrom monai.config import print_config, USE_COMPILED\nfrom monai.networks.nets import GlobalNet\nfrom monai.networks.blocks import Warp\nfrom monai.apps import MedNISTDataset\n\n\n# Here we define the training plan to be used. \nclass MyMonaiTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args = None):\n        model_= GlobalNet(\n            image_size=(64, 64),\n            spatial_dims=2,\n            in_channels=2,  # moving and fixed\n            num_channel_initial=16,\n            depth=3)\n\n        if USE_COMPILED:\n            model_.warp_layer = Warp(3, \"border\")\n        else:\n            model_.warp_layer = Warp(\"bilinear\", \"border\")\n\n        return model_\n\n    def init_dependencies(self):\n        return [\"import numpy as np\",\n                \"import monai\",\n                \"from torch.nn.functional import mse_loss\",\n                \"from monai.utils import set_determinism, first\",\n                \"from monai.transforms import (EnsureChannelFirstD,Compose,LoadImageD,RandRotateD,RandZoomD,ScaleIntensityRanged,EnsureTypeD,)\",\n                \"from monai.data import DataLoader, Dataset, CacheDataset\",\n                \"from monai.networks.nets import GlobalNet\",\n                \"from monai.config import USE_COMPILED\",\n                \"from monai.networks.blocks import Warp\",\n                \"from monai.apps import MedNISTDataset\",]\n\n    def init_optimizer(self, optimizer_args):\n        lr = optimizer_args.get('lr', 1e-5)\n        return torch.optim.Adam(self.model().parameters(), lr)\n        \n    def training_data(self):\n        # Custom torch Dataloader for MedNIST data\n        data_path = self.dataset_path\n        # The following line is needed if client structure does not contain the \"/MedNIST\" folder\n        MedNISTDataset.dataset_folder_name = \"\"\n        train_data = MedNISTDataset(root_dir=data_path, section=\"training\", download=False, transform=None)\n        training_datadict = [\n            {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}\n            for item in train_data.data if item[\"label\"] == 4  # label 4 is for xray hands\n        ]\n        train_transforms = Compose(\n            [\n                LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],\n                                     a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),\n                RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),\n                RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1,\n                          monaiprob=1.0, mode=\"bicubic\", align_corners=False),\n                EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),\n            ]\n        )\n        train_ds = CacheDataset(data=training_datadict[:1000], transform=train_transforms,\n                                cache_rate=1.0, num_workers=0)\n        dl = self.MednistDataLoader(train_ds)\n        \n        return DataManager(dl,  shuffle=True, num_workers=0)\n\n    def training_step(self, moving, fixed):\n        ddf = self.model().forward(torch.cat((moving, fixed), dim=1))\n        pred_image = self.model().warp_layer(moving, ddf)\n        loss = mse_loss(pred_image, fixed)\n        return loss\n    \n    class MednistDataLoader(monai.data.Dataset):\n        # Custom DataLoader that inherits from monai's Dataset object\n        def __init__(self, dataset):\n            self.dataset = dataset\n\n        def __len__(self):\n            return len(self.dataset)\n\n        def __getitem__(self, idx):\n            return (self.dataset[idx][\"moving_hand\"],\n                    self.dataset[idx][\"fixed_hand\"])\n</pre> import os import numpy as np import torch import torch.nn as nn from torch.nn.functional import mse_loss from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.logger import logger from fedbiomed.common.data import DataManager from torchvision import datasets, transforms from typing import Union, List #from torch.utils.data import Dataset, DataLoader import monai from monai.utils import set_determinism, first from monai.transforms import (     EnsureChannelFirstD,     Compose,     LoadImageD,     RandRotateD,     RandZoomD,     ScaleIntensityRanged,     EnsureTypeD, ) from monai.data import DataLoader, Dataset, CacheDataset from monai.config import print_config, USE_COMPILED from monai.networks.nets import GlobalNet from monai.networks.blocks import Warp from monai.apps import MedNISTDataset   # Here we define the training plan to be used.  class MyMonaiTrainingPlan(TorchTrainingPlan):     def init_model(self, model_args = None):         model_= GlobalNet(             image_size=(64, 64),             spatial_dims=2,             in_channels=2,  # moving and fixed             num_channel_initial=16,             depth=3)          if USE_COMPILED:             model_.warp_layer = Warp(3, \"border\")         else:             model_.warp_layer = Warp(\"bilinear\", \"border\")          return model_      def init_dependencies(self):         return [\"import numpy as np\",                 \"import monai\",                 \"from torch.nn.functional import mse_loss\",                 \"from monai.utils import set_determinism, first\",                 \"from monai.transforms import (EnsureChannelFirstD,Compose,LoadImageD,RandRotateD,RandZoomD,ScaleIntensityRanged,EnsureTypeD,)\",                 \"from monai.data import DataLoader, Dataset, CacheDataset\",                 \"from monai.networks.nets import GlobalNet\",                 \"from monai.config import USE_COMPILED\",                 \"from monai.networks.blocks import Warp\",                 \"from monai.apps import MedNISTDataset\",]      def init_optimizer(self, optimizer_args):         lr = optimizer_args.get('lr', 1e-5)         return torch.optim.Adam(self.model().parameters(), lr)              def training_data(self):         # Custom torch Dataloader for MedNIST data         data_path = self.dataset_path         # The following line is needed if client structure does not contain the \"/MedNIST\" folder         MedNISTDataset.dataset_folder_name = \"\"         train_data = MedNISTDataset(root_dir=data_path, section=\"training\", download=False, transform=None)         training_datadict = [             {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}             for item in train_data.data if item[\"label\"] == 4  # label 4 is for xray hands         ]         train_transforms = Compose(             [                 LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),                 EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),                 ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],                                      a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),                 RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),                 RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1,                           monaiprob=1.0, mode=\"bicubic\", align_corners=False),                 EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),             ]         )         train_ds = CacheDataset(data=training_datadict[:1000], transform=train_transforms,                                 cache_rate=1.0, num_workers=0)         dl = self.MednistDataLoader(train_ds)                  return DataManager(dl,  shuffle=True, num_workers=0)      def training_step(self, moving, fixed):         ddf = self.model().forward(torch.cat((moving, fixed), dim=1))         pred_image = self.model().warp_layer(moving, ddf)         loss = mse_loss(pred_image, fixed)         return loss          class MednistDataLoader(monai.data.Dataset):         # Custom DataLoader that inherits from monai's Dataset object         def __init__(self, dataset):             self.dataset = dataset          def __len__(self):             return len(self.dataset)          def __getitem__(self, idx):             return (self.dataset[idx][\"moving_hand\"],                     self.dataset[idx][\"fixed_hand\"])  <p>We now set the model and training parameters. Note that in this case, no model argument is required.</p> In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'loader_args': { 'batch_size': 16, },\n    'epochs': 3,\n    'dry_run': False,  \n    'batch_maxnum':250, # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n    'optimizer_args': {\n        'lr': 1e-5,\n    },\n    'use_gpu': True # Training on GPU\n}\n</pre> model_args = {}  training_args = {     'loader_args': { 'batch_size': 16, },     'epochs': 3,     'dry_run': False,       'batch_maxnum':250, # Fast pass for development : only use ( batch_maxnum * batch_size ) samples     'optimizer_args': {         'lr': 1e-5,     },     'use_gpu': True # Training on GPU } <p>The experiment can be now defined, by providing the <code>mednist</code> tag, and running the local training on nodes with training plan defined in <code>training_plan_path</code>, standard <code>aggregator</code> (FedAvg) and <code>client_selection_strategy</code> (all nodes used). Federated learning is going to be perfomed through 5 optimization rounds.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MEDNIST']\nrounds = 5\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyMonaiTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None\n                )\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['#MEDNIST'] rounds = 5  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyMonaiTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None                 ) <p>Let's start the experiment.</p> <p>By default, this function doesn't stop until all the <code>round_limit</code> rounds are done for all the clients</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') <p>Once the federated model is obtained, it is possible to test it locally on an independent testing partition. The test dataset is available at this link:</p> <p>https://drive.google.com/file/d/1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD/</p> <p>Following the Monai tutorial, in this section we will create a set of previously unseen pairs of moving vs fixed hands, and use the final federated model to predict the transformation between each pair.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install matplotlib\n!pip install gdown\n</pre> !pip install matplotlib !pip install gdown In\u00a0[\u00a0]: Copied! <pre>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nprint_config()\nset_determinism(42)\n</pre> import torch import numpy as np import matplotlib.pyplot as plt   print_config() set_determinism(42) <p>Download the testing dataset on the local temporary folder.</p> In\u00a0[\u00a0]: Copied! <pre>import gdown\nimport zipfile\nimport tempfile\nimport os\n\nfrom fedbiomed.researcher.config import config\n\ntmp_dir = tempfile.TemporaryDirectory(dir=config.vars['TMP_DIR']+os.sep)\n\nresource = \"https://drive.google.com/uc?id=1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD\"\nbase_dir = tmp_dir.name\ntest_file = os.path.join(base_dir, \"MedNIST_testing.zip\")\n\ngdown.download(resource, test_file, quiet=False)\n\nzf = zipfile.ZipFile(test_file)\n\nfor file in zf.infolist():\n    zf.extract(file, base_dir)\n    \ndata_dir = os.path.join(base_dir, \"MedNIST_testing\")\n</pre> import gdown import zipfile import tempfile import os  from fedbiomed.researcher.config import config  tmp_dir = tempfile.TemporaryDirectory(dir=config.vars['TMP_DIR']+os.sep)  resource = \"https://drive.google.com/uc?id=1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD\" base_dir = tmp_dir.name test_file = os.path.join(base_dir, \"MedNIST_testing.zip\")  gdown.download(resource, test_file, quiet=False)  zf = zipfile.ZipFile(test_file)  for file in zf.infolist():     zf.extract(file, base_dir)      data_dir = os.path.join(base_dir, \"MedNIST_testing\") <p>We redefine our custom dataloader (defined previously in  the <code>TrainingPlan</code>):</p> In\u00a0[\u00a0]: Copied! <pre>from monai.data import DataLoader, CacheDataset\nimport monai\n\nclass MednistDataLoader(monai.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        return (self.dataset[idx][\"moving_hand\"],\n                self.dataset[idx][\"fixed_hand\"])\n</pre> from monai.data import DataLoader, CacheDataset import monai  class MednistDataLoader(monai.data.Dataset):     def __init__(self, dataset):         self.dataset = dataset      def __len__(self):         return len(self.dataset)      def __getitem__(self, idx):         return (self.dataset[idx][\"moving_hand\"],                 self.dataset[idx][\"fixed_hand\"]) <p>Create the testing data loader and pairs of moving vs fixed hands:</p> In\u00a0[\u00a0]: Copied! <pre># Use a GPU if you have one + enough memory available\n#\n#use_cuda = torch.cuda.is_available()\n#device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\ndevice = 'cpu'\n\n\n# recreate model\nmodel = GlobalNet(\n    image_size=(64, 64),\n    spatial_dims=2,\n    in_channels=2,  # moving and fixed\n    num_channel_initial=16,\n    depth=3).to(device)\n\nif USE_COMPILED:\n    warp_layer = Warp(3, \"border\").to(device)\nelse:\n    warp_layer = Warp(\"bilinear\", \"border\").to(device)\n\nMedNISTDataset.dataset_folder_name = \"\"\ntest_data = MedNISTDataset(root_dir=data_dir, section=\"test\", download=False, transform=None)\ntesting_datadict = [\n    {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}\n    for item in test_data.data if item[\"label\"] == 4  # label 4 is for xray hands\n]\ntest_transforms = Compose(\n            [\n                LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],\n                                     a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),\n                RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),\n                RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1, prob=1.0, mode=\"bicubic\", align_corners=False),\n                EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),\n            ]\n        )\nval_ds = CacheDataset(data=testing_datadict[:1000], transform=test_transforms,\n                      cache_rate=1.0, num_workers=0)\nval_dl = MednistDataLoader(val_ds)\nval_loader = DataLoader(val_dl, batch_size=16, num_workers=0)\n</pre> # Use a GPU if you have one + enough memory available # #use_cuda = torch.cuda.is_available() #device = torch.device(\"cuda:0\" if use_cuda else \"cpu\") device = 'cpu'   # recreate model model = GlobalNet(     image_size=(64, 64),     spatial_dims=2,     in_channels=2,  # moving and fixed     num_channel_initial=16,     depth=3).to(device)  if USE_COMPILED:     warp_layer = Warp(3, \"border\").to(device) else:     warp_layer = Warp(\"bilinear\", \"border\").to(device)  MedNISTDataset.dataset_folder_name = \"\" test_data = MedNISTDataset(root_dir=data_dir, section=\"test\", download=False, transform=None) testing_datadict = [     {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}     for item in test_data.data if item[\"label\"] == 4  # label 4 is for xray hands ] test_transforms = Compose(             [                 LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),                 EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),                 ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],                                      a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),                 RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),                 RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1, prob=1.0, mode=\"bicubic\", align_corners=False),                 EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),             ]         ) val_ds = CacheDataset(data=testing_datadict[:1000], transform=test_transforms,                       cache_rate=1.0, num_workers=0) val_dl = MednistDataLoader(val_ds) val_loader = DataLoader(val_dl, batch_size=16, num_workers=0) <p>Create a model instance and assign to it the model parameters estimated at the last federated optimization round. Generate predictions of the transformation between pairs.</p> In\u00a0[\u00a0]: Copied! <pre># extract federated model into PyTorch framework\nmodel = exp.training_plan().model()\nmodel.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n\nfor moving, fixed in val_loader:\n    ddf = model(torch.cat((moving, fixed), dim=1))\n    pred_image = warp_layer(moving, ddf)\n    break\n\nfixed_image = fixed.detach().cpu().numpy()[:, 0]\nmoving_image = moving.detach().cpu().numpy()[:, 0]\npred_image = pred_image.detach().cpu().numpy()[:, 0]\n</pre> # extract federated model into PyTorch framework model = exp.training_plan().model() model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])  for moving, fixed in val_loader:     ddf = model(torch.cat((moving, fixed), dim=1))     pred_image = warp_layer(moving, ddf)     break  fixed_image = fixed.detach().cpu().numpy()[:, 0] moving_image = moving.detach().cpu().numpy()[:, 0] pred_image = pred_image.detach().cpu().numpy()[:, 0] <p>We can finally print some example of predictions from the testing dataset.</p> In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\nbatch_size = 10\nplt.subplots(batch_size, 4, figsize=(12, 20))\nfor b in range(batch_size):\n    # moving image\n    plt.subplot(batch_size, 4, b * 4 + 1)\n    plt.axis('off')\n    plt.title(\"moving image\")\n    plt.imshow(moving_image[b], cmap=\"gray\")\n    # fixed image\n    plt.subplot(batch_size, 4, b * 4 + 2)\n    plt.axis('off')\n    plt.title(\"fixed image\")\n    plt.imshow(fixed_image[b], cmap=\"gray\")\n    # warped moving\n    plt.subplot(batch_size, 4, b * 4 + 3)\n    plt.axis('off')\n    plt.title(\"predicted image\")\n    plt.imshow(pred_image[b], cmap=\"gray\")\n    \n    #error\n    plt.subplot(batch_size, 4, b * 4 + 4)\n    plt.axis('off')\n    plt.title(\"error between predicted \\nand fixed image\")\n    plt.imshow(pred_image[b] - fixed_image[b], cmap=\"gray\")\nplt.axis('off')\nplt.show()\n</pre> %matplotlib inline batch_size = 10 plt.subplots(batch_size, 4, figsize=(12, 20)) for b in range(batch_size):     # moving image     plt.subplot(batch_size, 4, b * 4 + 1)     plt.axis('off')     plt.title(\"moving image\")     plt.imshow(moving_image[b], cmap=\"gray\")     # fixed image     plt.subplot(batch_size, 4, b * 4 + 2)     plt.axis('off')     plt.title(\"fixed image\")     plt.imshow(fixed_image[b], cmap=\"gray\")     # warped moving     plt.subplot(batch_size, 4, b * 4 + 3)     plt.axis('off')     plt.title(\"predicted image\")     plt.imshow(pred_image[b], cmap=\"gray\")          #error     plt.subplot(batch_size, 4, b * 4 + 4)     plt.axis('off')     plt.title(\"error between predicted \\nand fixed image\")     plt.imshow(pred_image[b] - fixed_image[b], cmap=\"gray\") plt.axis('off') plt.show()"},{"location":"tutorials/monai/02_monai-2d-image-registration/#federated-2d-xray-registration-with-monai","title":"Federated 2d XRay registration with MONAI\u00b6","text":""},{"location":"tutorials/monai/02_monai-2d-image-registration/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial shows how to deploy in Fed-BioMed the 2d image registration example provided in the project MONAI (https://monai.io/):</p> <p>https://github.com/Project-MONAI/tutorials/blob/master/2d_registration/registration_mednist.ipynb</p> <p>Being MONAI based on PyTorch, the deployment within Fed-BioMed follows seamlessly the same general structure of general PyTorch training plans.</p> <p>Following the MONAI example, this tutorial is based on the MedNIST dataset&gt;</p>"},{"location":"tutorials/monai/02_monai-2d-image-registration/#image-registration","title":"Image Registration\u00b6","text":"<p>Image registration is the process of transforming and recalibrating different images into one coordinate system. It makes possible to compare several images captured with the same modality.</p> <p>In this tutorial, we are using a UNet-like registration network ( https://arxiv.org/abs/1711.01666 ). Goal of the notebook is to train a model given moving images and fixed images (recalibrated images).</p>"},{"location":"tutorials/monai/02_monai-2d-image-registration/#creating-mednist-nodes","title":"Creating MedNIST nodes\u00b6","text":"<p>MedNIST provides an artificial 2d classification dataset created by gathering different medical imaging datasets from TCIA, the RSNA Bone Age Challenge, and the NIH Chest X-ray dataset. The dataset is kindly made available by Dr. Bradley J. Erickson M.D., Ph.D. (Department of Radiology, Mayo Clinic) under the Creative Commons CC BY-SA 4.0 license.</p> <p>To proceed with the tutorial, we created an iid partitioning of the MedNIST dataset between 3 clients. Each client has 3000 image samples for each class. The training partitions are availables at the following link:</p> <p>https://drive.google.com/file/d/1vLIcBdtdAhh6K-vrgCFy_0Y55dxOWZwf/view</p> <p>The dataset owned by each client has structure:</p> <p>\u2514\u2500\u2500 client_*/</p> <pre><code>\u251c\u2500\u2500 AbdomenCT/\n\n\u2514\u2500\u2500 BreastMRI/\n\n\u2514\u2500\u2500 CXR/\n\n\u2514\u2500\u2500 ChestCT/\n\n\u2514\u2500\u2500 Hand/\n\n\u2514\u2500\u2500 HeadCT/      </code></pre> <p>To create the federated dataset, we follow the standard procedure for node creation/population of Fed-BioMed.</p> <p>we use the environment where Fed-BioMed node is installed</p> <p>we create a first node by using the commands</p> <p><code>fedbiomed node start</code></p> <p>We then populate the node with the data of first client:</p> <p><code>fedbiomed node dataset add</code></p> <p>We select option 3 (images) to add MedNIST partition of client 1, by just picking the folder of client 1. We use <code>mednist</code> as tag to save the selected dataset. We can further check that the data has been added by executing <code>fedbiomed node dataset list</code></p> <p>Following the same procedure, we create the other two nodes with the datasets of client 2 and client 3 respectively.</p>"},{"location":"tutorials/monai/02_monai-2d-image-registration/#running-fed-biomed-researcher","title":"Running Fed-BioMed Researcher\u00b6","text":""},{"location":"tutorials/monai/02_monai-2d-image-registration/#create-an-experiment-to-train-a-model-on-the-data-found","title":"Create an experiment to train a model on the data found\u00b6","text":""},{"location":"tutorials/monai/02_monai-2d-image-registration/#testing","title":"Testing\u00b6","text":""},{"location":"tutorials/optimizers/01-fedopt-and-scaffold/","title":"Advanced optimizers in Fed-BioMed","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\nfrom torchvision.models import densenet121\nfrom fedbiomed.common.optimizers.optimizer import Optimizer\n\n# Here we define the model to be used. \n# we will use the densnet121 model\nclass MyTrainingPlan(TorchTrainingPlan):\n    \n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\",\n                \"from torchvision.models import densenet121\",\n                \"from fedbiomed.common.optimizers.optimizer import Optimizer\"]\n\n        return deps\n    \n    def init_model(self):\n        self.loss_function = torch.nn.CrossEntropyLoss()\n        model = densenet121(pretrained=True)\n        model.classifier =nn.Sequential(nn.Linear(1024,512), nn.Softmax())\n        return model \n    \n    def init_optimizer(self, optimizer_args):\n        # Defines and return a declearn optimizer\n        # equivalent: Optimizer(lr=optimizer_args['lr'], modules=[], regurlarizers=[])\n        return Optimizer(lr=optimizer_args['lr'])\n\n    def training_data(self, batch_size = 48):\n        preprocess = transforms.Compose([transforms.ToTensor(),\n                                        transforms.Normalize(\n                                            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                                        )])\n        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset=train_data, **train_kwargs)\n    \n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = self.loss_function(output, target)\n        return loss\n</pre> import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms from torchvision.models import densenet121 from fedbiomed.common.optimizers.optimizer import Optimizer  # Here we define the model to be used.  # we will use the densnet121 model class MyTrainingPlan(TorchTrainingPlan):          def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\",                 \"from torchvision.models import densenet121\",                 \"from fedbiomed.common.optimizers.optimizer import Optimizer\"]          return deps          def init_model(self):         self.loss_function = torch.nn.CrossEntropyLoss()         model = densenet121(pretrained=True)         model.classifier =nn.Sequential(nn.Linear(1024,512), nn.Softmax())         return model           def init_optimizer(self, optimizer_args):         # Defines and return a declearn optimizer         # equivalent: Optimizer(lr=optimizer_args['lr'], modules=[], regurlarizers=[])         return Optimizer(lr=optimizer_args['lr'])      def training_data(self, batch_size = 48):         preprocess = transforms.Compose([transforms.ToTensor(),                                         transforms.Normalize(                                             mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]                                         )])         train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)         train_kwargs = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset=train_data, **train_kwargs)          def training_step(self, data, target):         output = self.model().forward(data)         loss   = self.loss_function(output, target)         return loss  In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.training_plans import FedSGDClassifier\nfrom fedbiomed.common.data import DataManager\nfrom fedbiomed.common.optimizers.optimizer import Optimizer\nfrom torchvision import datasets, transforms\nimport torch\n\n\nclass MyTrainingPlan(FedSGDClassifier):\n    # Declares and return dependencies\n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\",\n                \"from fedbiomed.common.optimizers.optimizer import Optimizer\",\n                \"import torch\"]\n        return deps\n\n    def training_data(self, batch_size):\n        # in comparison to PyTorch Training Plan, preprocess involves additional steps in order to be used \n        # with sklearn SGDClassifier, which is expecting vectors in lieu of arrays\n        # here we are grayscaling and reshaping images\n        squeezer = lambda x: torch.squeeze(x) # removes extra dimensions\n        preprocess = transforms.Compose([transforms.ToTensor(),\n                                                transforms.Normalize(\n                                                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                                                ),\n                                         transforms.Grayscale(1),\n                                         transforms.Resize((64*64, 1)),\n                                         transforms.Lambda(squeezer)\n                                        ])\n\n        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n        return DataManager(dataset=train_data, batch_size=batch_size)\n\n    # Defines and return a declearn optimizer\n    def init_optimizer(self, optimizer_args):\n        return Optimizer(lr=optimizer_args['lr'])\n</pre> from fedbiomed.common.training_plans import FedSGDClassifier from fedbiomed.common.data import DataManager from fedbiomed.common.optimizers.optimizer import Optimizer from torchvision import datasets, transforms import torch   class MyTrainingPlan(FedSGDClassifier):     # Declares and return dependencies     def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\",                 \"from fedbiomed.common.optimizers.optimizer import Optimizer\",                 \"import torch\"]         return deps      def training_data(self, batch_size):         # in comparison to PyTorch Training Plan, preprocess involves additional steps in order to be used          # with sklearn SGDClassifier, which is expecting vectors in lieu of arrays         # here we are grayscaling and reshaping images         squeezer = lambda x: torch.squeeze(x) # removes extra dimensions         preprocess = transforms.Compose([transforms.ToTensor(),                                                 transforms.Normalize(                                                     mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]                                                 ),                                          transforms.Grayscale(1),                                          transforms.Resize((64*64, 1)),                                          transforms.Lambda(squeezer)                                         ])          train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)         return DataManager(dataset=train_data, batch_size=batch_size)      # Defines and return a declearn optimizer     def init_optimizer(self, optimizer_args):         return Optimizer(lr=optimizer_args['lr']) In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\nfrom torchvision.models import densenet121\nfrom fedbiomed.common.optimizers.optimizer import Optimizer\nfrom fedbiomed.common.optimizers.declearn import AdamModule, RidgeRegularizer\n\n# Here we define the model to be used. \n# we will use the densnet121 model\nclass MyTrainingPlan(TorchTrainingPlan):\n    \n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\",\n                \"from torchvision.models import densenet121\",\n                \"from fedbiomed.common.optimizers.optimizer import Optimizer\",\n                \"fedbiomed.common.optimizers.declearn import AdamModule\",\n                \"fedbiomed.common.optimizers.declearn import RidgeRegularizer\"]\n\n        return deps\n    \n    def init_model(self):\n        self.loss_function = torch.nn.CrossEntropyLoss()\n        model = densenet121(pretrained=True)\n        model.classifier =nn.Sequential(nn.Linear(1024,512), nn.Softmax())\n        return model \n    \n    def init_optimizer(self, optimizer_args):\n        # Defines and return a declearn optimizer\n        # equivalent: Optimizer(lr=optimizer_args['lr'], modules=[], regurlarizers=[])\n        return Optimizer(lr=optimizer_args['lr'], modules=[AdamModule()], regularizers=[RidgeRegularizer()])\n\n    def training_data(self, batch_size = 48):\n        preprocess = transforms.Compose([transforms.ToTensor(),\n                                        transforms.Normalize(\n                                            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                                        )])\n        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset=train_data, **train_kwargs)\n    \n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = self.loss_function(output, target)\n        return loss\n</pre> import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms from torchvision.models import densenet121 from fedbiomed.common.optimizers.optimizer import Optimizer from fedbiomed.common.optimizers.declearn import AdamModule, RidgeRegularizer  # Here we define the model to be used.  # we will use the densnet121 model class MyTrainingPlan(TorchTrainingPlan):          def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\",                 \"from torchvision.models import densenet121\",                 \"from fedbiomed.common.optimizers.optimizer import Optimizer\",                 \"fedbiomed.common.optimizers.declearn import AdamModule\",                 \"fedbiomed.common.optimizers.declearn import RidgeRegularizer\"]          return deps          def init_model(self):         self.loss_function = torch.nn.CrossEntropyLoss()         model = densenet121(pretrained=True)         model.classifier =nn.Sequential(nn.Linear(1024,512), nn.Softmax())         return model           def init_optimizer(self, optimizer_args):         # Defines and return a declearn optimizer         # equivalent: Optimizer(lr=optimizer_args['lr'], modules=[], regurlarizers=[])         return Optimizer(lr=optimizer_args['lr'], modules=[AdamModule()], regularizers=[RidgeRegularizer()])      def training_data(self, batch_size = 48):         preprocess = transforms.Compose([transforms.ToTensor(),                                         transforms.Normalize(                                             mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]                                         )])         train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)         train_kwargs = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset=train_data, **train_kwargs)          def training_step(self, data, target):         output = self.model().forward(data)         loss   = self.loss_function(output, target)         return loss  In\u00a0[\u00a0]: Copied! <pre>lr = 1e-3\nmodel_args = {'n_features': 64*64,\n              'n_classes' : 6,\n              'eta0':lr}\n\ntraining_args = {\n    'loader_args': {\n        'batch_size': 8,\n    },\n    'optimizer_args': {\n        \"lr\" : lr\n    },\n    'dry_run': False,\n    'num_updates': 50\n}\n\ntags =  ['#dataset', '#MEDNIST']\nrounds = 2\n</pre> lr = 1e-3 model_args = {'n_features': 64*64,               'n_classes' : 6,               'eta0':lr}  training_args = {     'loader_args': {         'batch_size': 8,     },     'optimizer_args': {         \"lr\" : lr     },     'dry_run': False,     'num_updates': 50 }  tags =  ['#dataset', '#MEDNIST'] rounds = 2 In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators import FedAverage\nfrom fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n\n\nexp = Experiment()\nexp.set_training_plan_class(training_plan_class=MyTrainingPlan)\nexp.set_model_args(model_args=model_args)\nexp.set_training_args(training_args=training_args)\nexp.set_tags(tags = tags)\nexp.set_aggregator(aggregator=FedAverage())\nexp.set_round_limit(rounds)\nexp.set_training_data(training_data=None, from_tags=True)\nexp.set_strategy(node_selection_strategy=DefaultStrategy())\n\nexp.run(increase=True)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators import FedAverage from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy   exp = Experiment() exp.set_training_plan_class(training_plan_class=MyTrainingPlan) exp.set_model_args(model_args=model_args) exp.set_training_args(training_args=training_args) exp.set_tags(tags = tags) exp.set_aggregator(aggregator=FedAverage()) exp.set_round_limit(rounds) exp.set_training_data(training_data=None, from_tags=True) exp.set_strategy(node_selection_strategy=DefaultStrategy())  exp.run(increase=True) <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') <p>To get and display the content of all <code>OptiModules</code> (respectively the <code>Regularizers</code>) available and compatible with Fed-BioMed , one can use <code>list_optim_modules</code> (resp. <code>list_optim_regularizers</code>), as shown as below:</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.optimizers.declearn import list_optim_modules,  list_optim_regularizers\n\nlist_optim_modules(), list_optim_regularizers()\n</pre> from fedbiomed.common.optimizers.declearn import list_optim_modules,  list_optim_regularizers  list_optim_modules(), list_optim_regularizers()  In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators import FedAverage\nfrom fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\nfrom fedbiomed.common.optimizers.declearn import YogiModule as FedYogi\n\nexp = Experiment()\nexp.set_training_plan_class(training_plan_class=MyTrainingPlan)\nexp.set_model_args(model_args=model_args)\nexp.set_training_args(training_args=training_args)\nexp.set_tags(tags = tags)\nexp.set_aggregator(aggregator=FedAverage())\nexp.set_round_limit(rounds)\nexp.set_training_data(training_data=None, from_tags=True)\nexp.set_strategy(node_selection_strategy=DefaultStrategy())\n\n# here we are adding an Optimizer on Researcher side (FedYogi)\nfed_opt = Optimizer(lr=.8, modules=[FedYogi()])\nexp.set_agg_optimizer(fed_opt)\n\nexp.run(increase=True)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators import FedAverage from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy from fedbiomed.common.optimizers.declearn import YogiModule as FedYogi  exp = Experiment() exp.set_training_plan_class(training_plan_class=MyTrainingPlan) exp.set_model_args(model_args=model_args) exp.set_training_args(training_args=training_args) exp.set_tags(tags = tags) exp.set_aggregator(aggregator=FedAverage()) exp.set_round_limit(rounds) exp.set_training_data(training_data=None, from_tags=True) exp.set_strategy(node_selection_strategy=DefaultStrategy())  # here we are adding an Optimizer on Researcher side (FedYogi) fed_opt = Optimizer(lr=.8, modules=[FedYogi()]) exp.set_agg_optimizer(fed_opt)  exp.run(increase=True) <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\nfrom torchvision.models import densenet121\nfrom fedbiomed.common.optimizers.optimizer import Optimizer\nfrom fedbiomed.common.optimizers.declearn import ScaffoldClientModule\n\n# Here we define the model to be used. \n# we will use the densnet121 model\nclass MyTrainingPlan(TorchTrainingPlan):\n    \n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\",\n                \"from torchvision.models import densenet121\",\n                \"from fedbiomed.common.optimizers.optimizer import Optimizer\",\n                \"from declearn.optimizer.modules import ScaffoldClientModule\"]\n\n        return deps\n    \n    def init_model(self):\n        self.loss_function = torch.nn.CrossEntropyLoss()\n        model = densenet121(pretrained=True)\n        model.classifier =nn.Sequential(nn.Linear(1024,512), nn.Softmax())\n        return model \n    \n    def init_optimizer(self, optimizer_args):\n        # Defines and return a declearn optimizer\n        return Optimizer(lr=optimizer_args['lr'], modules=[ScaffoldClientModule()])\n\n    def training_data(self, batch_size = 48):\n        preprocess = transforms.Compose([transforms.ToTensor(),\n                                        transforms.Normalize(\n                                            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                                        )])\n        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset=train_data, **train_kwargs)\n    \n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = self.loss_function(output, target)\n        return loss\n</pre> import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms from torchvision.models import densenet121 from fedbiomed.common.optimizers.optimizer import Optimizer from fedbiomed.common.optimizers.declearn import ScaffoldClientModule  # Here we define the model to be used.  # we will use the densnet121 model class MyTrainingPlan(TorchTrainingPlan):          def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\",                 \"from torchvision.models import densenet121\",                 \"from fedbiomed.common.optimizers.optimizer import Optimizer\",                 \"from declearn.optimizer.modules import ScaffoldClientModule\"]          return deps          def init_model(self):         self.loss_function = torch.nn.CrossEntropyLoss()         model = densenet121(pretrained=True)         model.classifier =nn.Sequential(nn.Linear(1024,512), nn.Softmax())         return model           def init_optimizer(self, optimizer_args):         # Defines and return a declearn optimizer         return Optimizer(lr=optimizer_args['lr'], modules=[ScaffoldClientModule()])      def training_data(self, batch_size = 48):         preprocess = transforms.Compose([transforms.ToTensor(),                                         transforms.Normalize(                                             mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]                                         )])         train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)         train_kwargs = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset=train_data, **train_kwargs)          def training_step(self, data, target):         output = self.model().forward(data)         loss   = self.loss_function(output, target)         return loss  <p>The <code>Experiment</code> will be defined that way, with an <code>Optimizer</code> configured with <code>ScaffoldServerModule</code> :</p> In\u00a0[\u00a0]: Copied! <pre>lr = 1e-3\nmodel_args = {}\ntraining_args = {\n    'loader_args': {\n        'batch_size': 8,\n    },\n    'optimizer_args': {\n        \"lr\" : lr\n    },\n    'dry_run': False,\n    'num_updates': 50\n}\n\ntags =  ['#dataset', '#MEDNIST']\nrounds = 2\n</pre> lr = 1e-3 model_args = {} training_args = {     'loader_args': {         'batch_size': 8,     },     'optimizer_args': {         \"lr\" : lr     },     'dry_run': False,     'num_updates': 50 }  tags =  ['#dataset', '#MEDNIST'] rounds = 2 In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators import FedAverage\nfrom fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\nfrom fedbiomed.common.optimizers.declearn import ScaffoldServerModule\n\nexp = Experiment()\nexp.set_training_plan_class(training_plan_class=MyTrainingPlan)\nexp.set_model_args(model_args=model_args)\nexp.set_training_args(training_args=training_args)\nexp.set_tags(tags = tags)\nexp.set_aggregator(aggregator=FedAverage())\nexp.set_round_limit(rounds)\nexp.set_training_data(training_data=None, from_tags=True)\nexp.set_strategy(node_selection_strategy=DefaultStrategy())\n\n# here we are adding an Optimizer on Researcher side (FedYogi)\nfed_opt = Optimizer(lr=.8, modules=[ScaffoldServerModule()])\nexp.set_agg_optimizer(fed_opt)\n\nexp.run(increase=True)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators import FedAverage from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy from fedbiomed.common.optimizers.declearn import ScaffoldServerModule  exp = Experiment() exp.set_training_plan_class(training_plan_class=MyTrainingPlan) exp.set_model_args(model_args=model_args) exp.set_training_args(training_args=training_args) exp.set_tags(tags = tags) exp.set_aggregator(aggregator=FedAverage()) exp.set_round_limit(rounds) exp.set_training_data(training_data=None, from_tags=True) exp.set_strategy(node_selection_strategy=DefaultStrategy())  # here we are adding an Optimizer on Researcher side (FedYogi) fed_opt = Optimizer(lr=.8, modules=[ScaffoldServerModule()]) exp.set_agg_optimizer(fed_opt)  exp.run(increase=True) <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') In\u00a0[\u00a0]: Copied! <pre>exp.run(rounds=1, increase=True)\n</pre> exp.run(rounds=1, increase=True) <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model')"},{"location":"tutorials/optimizers/01-fedopt-and-scaffold/#advanced-optimizers-in-fed-biomed","title":"Advanced optimizers in Fed-BioMed\u00b6","text":"<p>Difficulty level: advanced</p>"},{"location":"tutorials/optimizers/01-fedopt-and-scaffold/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial presents on how  to deal with heterogeneous dataset by changing its <code>Optimizer</code>. In <code>Fed-BioMed</code>, one can specify two sort of <code>Optimizer</code>s:</p> <ol> <li>a <code>Optimizer</code> on the <code>Node</code> side, defined on the <code>Training Plan</code></li> <li>a <code>Optimizer</code> on the <code>Researcher</code> side, configured in the <code>Experiment</code></li> </ol> <p>Advanced <code>Optimizer</code> are backed by <code>declearn</code> package, a python package focused on <code>Optimization</code> for Federated Learning. Advanced <code>Optimizer</code> can be used regardless of the machine learning framework (meaning it is compatible with both sklearn and PyTorch)</p> <p>In this tutorial you will learn:</p> <ul> <li>how to use and chain one or several <code>Optimizers</code> on <code>Node</code> and <code>Researcher</code> side</li> <li>how to use fedopt</li> <li>how to use <code>Optimizers</code> that exchange auxiliary variables such as <code>Scaffold</code></li> </ul> <p>For further details you can refer to the <code>Optimizer</code> section in the User Guide as well as the declearn documentation on <code>Optimizers</code>.</p>"},{"location":"tutorials/optimizers/01-fedopt-and-scaffold/#1-configuring-nodes","title":"1. Configuring <code>Nodes</code>\u00b6","text":"<p>Before starting, we need to configure several <code>Nodes</code> and add MedNist dataset to it. Node configuration steps require <code>fedbiomed-node</code> conda environment. Please make sure that you have the necessary conda environment: this is explained in the installation tutorial.</p> <p>Please open a terminal, <code>cd</code> to the base directory of the cloned fedbiomed project and follow the steps below.</p> <ul> <li>Configuration Steps:<ul> <li>Run <code>fedbiomed node dataset add</code> in the terminal</li> <li>It will ask you to select the data type that you want to add. The third option has been configured to add the MedNIST dataset. Please type <code>3</code> and continue.</li> <li>Please use default tags which are <code>#MEDNIST</code> and <code>#dataset</code>.</li> <li>For the next step, please select the directory that you want to download the MNIST dataset.</li> <li>After the download is completed you will see the details of the MNIST dataset on the screen.</li> </ul> </li> </ul> <p>Please run the command below in the same terminal to make sure the MNIST dataset is successfully added to the Node.</p> <pre>$ fedbiomed node --path my-node dataset add\n</pre> <pre>$ fedbiomed node --path my-node start\n</pre> <p>In another terminal, you may proceed by launching a second <code>Node</code>. Please repeat the above configuration steps, but by specifying another configuration file (for instance <code>conf2.ini</code>).</p> <pre>$ fedbiomed node --path my-second-node dataset add\n$ fedbiomed node --path my-second-node start\n</pre>"},{"location":"tutorials/optimizers/01-fedopt-and-scaffold/#2-defining-an-optimizer-on-node-side","title":"2. Defining an <code>Optimizer</code> on <code>Node</code> side\u00b6","text":"<p><code>Optimizers</code> are defined through the <code>init_optimizer</code> method of the <code>training plan</code>. They must be set using <code>Fed-BioMed</code> <code>Optimizer</code> object (ie from <code>fedbiomed.common.optimizers.optimizer.Optimizer</code>)</p>"},{"location":"tutorials/optimizers/01-fedopt-and-scaffold/#21-with-pytorch-framework","title":"2.1 With PyTorch framework\u00b6","text":"<p>In this tutorial we have showcased the use of a PyTorch model with PyTorch native optimizers, such as <code>torch.optim.SGD</code>. In the present tutorial, we will see how to use <code>declearn</code> cross frameworks optimizers</p> <p>PyTorch <code>Training Plan</code></p> <p>Below is a simple implementation of a <code>declearn</code> SGD <code>Optimizer</code> on a PyTorch model. It is equivalent to the following <code>Training Plan</code> (that uses native Pytorch Optimizer <code>torch.optim.SGD</code>):</p> <pre>class MyTrainingPlan(TorchTrainingPlan):\n    ...\n    def init_optimizer(self, optimizer_args):\n        return torch.optim.SGD(self.model().parameters(), lr = optimizer_args['lr'])\n</pre> <p> </p>"},{"location":"tutorials/optimizers/01-fedopt-and-scaffold/#22-sklearn-training-plan","title":"2.2 Sklearn <code>Training Plan</code>\u00b6","text":"<p>For another machine learning framework such as sklearn, <code>init_optimizer</code> method syntax is the same</p>"},{"location":"tutorials/optimizers/01-fedopt-and-scaffold/#23-using-a-more-advanced-optimizer-with-regularizer","title":"2.3 Using a more advanced <code>Optimizer</code> with <code>Regularizer</code>\u00b6","text":"<p><code>Optimizer</code> from <code>fedbiomed.common.optimizers.optimizer</code> with learning rate equals <code>.1</code> can be written as <code>Optimizer(lr=.1, decay=0., modules=[], regularizers=[])</code>, where:</p> <ul> <li><code>decay</code> is the weight decay ;</li> <li><code>modules</code> is a python list containing no, one or several <code>declearn</code> <code>OptiModules</code> ;</li> <li><code>regularizers</code> is a python list containing no, one or several <code>declearn</code> <code>Regularizers</code>.</li> </ul> <p>We will re-use the <code>Pytorch Training Plan</code> already defined above and show how to use a <code>Adam</code> <code>Optimizer</code> with <code>Ridge</code> as the <code>Regularizer</code>. For that, we need to import the <code>Adam</code>  and the <code>Ridge</code> versions of <code>declearn</code> (<code>AdamModule</code> and <code>RidgeRegularizer</code>).</p> <p>Then the <code>Training Plan</code> can be defined as follow (for PyTorch):</p>"},{"location":"tutorials/optimizers/01-fedopt-and-scaffold/#24-create-the-experiment","title":"2.4. Create the <code>Experiment</code>\u00b6","text":"<p>Once the <code>Training Plan</code> has been created with a specific framework model, definition of the <code>Experiment</code> is the same as the one in PyTorch or Scikit-Learn, as shown below:</p> <p>Note: There are a small additional parameters you have to configure in the <code>model_args</code> for scikit-learn, that have been added but that will be ignored for PyTorch model</p>"},{"location":"tutorials/optimizers/01-fedopt-and-scaffold/#3-defining-an-optimizer-on-researcher-side-fedopt","title":"3. Defining an <code>Optimizer</code> on <code>Researcher</code> side: <code>FedOpt</code>\u00b6","text":"<p>In some case, you may want to use Adaptive Federated Optimization, also called <code>FedOpt</code>: the idea behind <code>FedOpt</code> is to optimize also the global model on <code>Researcher</code> side in addition to the <code>Nodes</code> local models, mainly to tackle data heterogeneity. Optimization on <code>Researcher</code> side is done by computing a pseudo gradient, which is the difference of the updates whithin 2 successive <code>Round</code>s.</p> <p>Adaptative Federated Optimization can be done in <code>Fed-BioMed</code> with <code>declearn</code> modules through the use of <code>Experiment.set_agg_optimizer</code> method.</p> <p>Important: Please note that it is not possible to use native framework optimizers on <code>Researcher</code> side (such as <code>torch.optim.Optimizer</code> for instance). Only <code>Fed-BioMed</code>/<code>declearn</code> <code>Optimizer</code> can be used.</p> <p>For instance, if one wants to use <code>FedYogi</code>, using the first <code>Training Plan</code> (that is based on SGD optimizer), the <code>Experiment</code> will be written as:</p>"},{"location":"tutorials/optimizers/01-fedopt-and-scaffold/#4-defining-scaffold-through-optimizer","title":"4. Defining <code>Scaffold</code> through <code>Optimizer</code>\u00b6","text":"<p>In the following subsection, we will present <code>Scaffold</code>: <code>Scaffold</code> purpose is to limit the so called client drift that may happen when dealing with heterogenous dataset accross <code>Node</code>s. For that, <code>Scaffold</code> involves the exchange between <code>Node</code>s and <code>Researcher</code> of additional parameters called correction states, which quantitize how much clients has drifted (drift can be considered as the difference between client's local extrema and global extrema). In <code>Fed-BioMed</code>, additional parameters that are requiered by <code>Optimizers</code> are called <code>auxiliary variables</code>: correction states in <code>Scaffold</code> is one of them.</p> <p><code>declearn</code> comes with <code>Scaffold</code> as 2 <code>OptiModules</code>:</p> <ol> <li>a <code>ScaffoldClientModule</code> on <code>Node</code> side ;</li> <li>a <code>ScaffoldServerModule</code> on <code>Researcher</code> side.</li> </ol> <p>For plain <code>Scaffold</code>, the <code>Training Plan</code> would look like (for a PyTorch model):</p> <p>Important: <code>FedAvg</code> <code>Aggregator</code> in <code>Fed-BioMed</code> refers to the way model weights are aggregated, and should not be confused with the <code>FedAvg</code> algorithm, which is basically a SGD optimizer performed on <code>Node</code> side using <code>FedAvg</code> <code>Aggregtor</code>.</p>"},{"location":"tutorials/optimizers/01-fedopt-and-scaffold/#5-explore-advanced-optimizer-feature-through-declearn-and-the-fed-biomed-user-guide","title":"5. Explore advanced <code>Optimizer</code> feature through <code>declearn</code> and the Fed-BioMed user guide\u00b6","text":"<p>Congrats!</p> <p>In this tutorial, you learned how to conduct your <code>Experiment</code> using advanced cross framework <code>Optimizer</code>provided by <code>declearn</code>. <code>declearn</code> modules offers the possibilty to chain <code>Optimizers</code> and <code>Regularizers</code>, making possible to customize as much as possible your federated <code>Expermient</code>. <code>declearn</code> compatible modules with Fed-BioMed are provided in <code>fedbiomed.common.optimizers.declearn</code></p> <p>For more in depth analysis on <code>declearn</code> <code>Optimizer</code>, please reach the <code>Optimizer</code> section in the <code>User Guide</code></p> <p>Please also check <code>declearn</code> documentation for further details reagrding <code>declearn</code> package.</p>"},{"location":"tutorials/pytorch/","title":"Fed-BioMed using Pytorch Deep Learning framework: a step-by-step tutorial","text":"<p>Pytorch is one of the one of the primary open source machine learning libraries.  </p> <ol> <li> <p>Basic Pytorch example with MNIST</p> </li> <li> <p>Write your own Pytorch training plan</p> </li> <li> <p>Comparing PyTorch federated model vs model trained locally</p> </li> </ol>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/","title":"PyTorch MNIST Basic Example","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install matplotlib\n</pre> !pip install matplotlib In\u00a0[\u00a0]: Copied! <pre>import os\nfrom fedbiomed.researcher.config import config\n</pre> import os from fedbiomed.researcher.config import config In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torchvision import datasets, transforms\n\n\ntransform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\n# Get the test dataset\ntest_set = datasets.MNIST(root = os.path.join(config.vars['TMP_DIR'], 'mnist_testing.tmp'),\n                          download = True, train = False, transform = transform)\n</pre> import torch from torchvision import datasets, transforms   transform = transforms.Compose([             transforms.ToTensor(),             transforms.Normalize((0.1307,), (0.3081,))         ])  # Get the test dataset test_set = datasets.MNIST(root = os.path.join(config.vars['TMP_DIR'], 'mnist_testing.tmp'),                           download = True, train = False, transform = transform)   In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\n# display a few digits from MNIST testing dataset\n\nnb_digits_to_display = 10\n\nplt.figure(figsize=(10,2)) \nplt.title(\"Few images of MNIST dataset\")\nfor i in range(nb_digits_to_display):\n    plt.subplot(1,nb_digits_to_display, i+1)\n    plt.imshow(test_set.data[i].numpy())\n    plt.title(f\"label: {test_set.targets[i].numpy()}\")\n</pre> import matplotlib.pyplot as plt  # display a few digits from MNIST testing dataset  nb_digits_to_display = 10  plt.figure(figsize=(10,2))  plt.title(\"Few images of MNIST dataset\") for i in range(nb_digits_to_display):     plt.subplot(1,nb_digits_to_display, i+1)     plt.imshow(test_set.data[i].numpy())     plt.title(f\"label: {test_set.targets[i].numpy()}\")  In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\n# Here we define the training plan to be used.\n# You can use any class name (here 'MyTrainingPlan')\nclass MyTrainingPlan(TorchTrainingPlan):\n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            self.fc1 = nn.Linear(9216, 128)\n            self.fc2 = nn.Linear(128, 10)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.dropout2(x)\n            x = self.fc2(x)\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n\n    def init_optimizer(self, optimizer_args):\n        return Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n\n    def init_dependencies(self):\n        return [\"from torchvision import datasets, transforms\",\n                \"from torch.optim import Adam\"]\n\n    def training_data(self):\n        transform = transforms.Compose([transforms.ToTensor(),\n                                        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        loader_arguments = { 'shuffle': True}\n        return DataManager(dataset1, **loader_arguments)\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn import torch.nn.functional as F from torch.optim import Adam from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms  # Here we define the training plan to be used. # You can use any class name (here 'MyTrainingPlan') class MyTrainingPlan(TorchTrainingPlan):     class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.conv1 = nn.Conv2d(1, 32, 3, 1)             self.conv2 = nn.Conv2d(32, 64, 3, 1)             self.dropout1 = nn.Dropout(0.25)             self.dropout2 = nn.Dropout(0.5)             self.fc1 = nn.Linear(9216, 128)             self.fc2 = nn.Linear(128, 10)          def forward(self, x):             x = self.conv1(x)             x = F.relu(x)             x = self.conv2(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = self.dropout1(x)             x = torch.flatten(x, 1)             x = self.fc1(x)             x = F.relu(x)             x = self.dropout2(x)             x = self.fc2(x)             output = F.log_softmax(x, dim=1)             return output      def init_model(self, model_args):         return self.Net(model_args = model_args)      def init_optimizer(self, optimizer_args):         return Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])      def init_dependencies(self):         return [\"from torchvision import datasets, transforms\",                 \"from torch.optim import Adam\"]      def training_data(self):         transform = transforms.Compose([transforms.ToTensor(),                                         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         loader_arguments = { 'shuffle': True}         return DataManager(dataset1, **loader_arguments)      def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss  <p>         Fed-BioMed nodes can be configured to accept only approved training plans. Under this configuration, the training plan files that are sent by a researcher must be approved by the node in advance. For more details, you can visit the tutorial for working with approved training plan file and user guide for managing nodes. </p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.metrics import MetricTypes\nmodel_args = {}\n\ntraining_args = {\n    'loader_args': { 'batch_size': 48, },\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 1, \n    'dry_run': False,  \n    'batch_maxnum': 100, # Fast pass for development : only use ( batch_maxnum * batch_size ) samples,\n    'test_ratio' : 0.25,\n    'test_metric': MetricTypes.F1_SCORE,\n    'test_on_global_updates': True,\n    'test_on_local_updates': True,\n    'test_batch_size': 0,\n    'shuffle_testing_dataset': True,\n}\n\ntags =  ['#MNIST', '#dataset']\nrounds = 4\n</pre> from fedbiomed.common.metrics import MetricTypes model_args = {}  training_args = {     'loader_args': { 'batch_size': 48, },     'optimizer_args': {         'lr': 1e-3     },     'epochs': 1,      'dry_run': False,       'batch_maxnum': 100, # Fast pass for development : only use ( batch_maxnum * batch_size ) samples,     'test_ratio' : 0.25,     'test_metric': MetricTypes.F1_SCORE,     'test_on_global_updates': True,     'test_on_local_updates': True,     'test_batch_size': 0,     'shuffle_testing_dataset': True, }  tags =  ['#MNIST', '#dataset'] rounds = 4  <p>Training plan class should be passed to the experiment. The experiment will be responsible for uploading the training plan file to the file repository. Afterwards, the nodes will receive training request that includes the URL where the training plan class is stored.</p> <p>Finally, you should indicate which method should be chosen to aggregate model parameters after every round. The basic federation scheme is federated averaging, implemented in Fed-BioMed in the class  <code>FedAverage</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage   exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) <p>As an output, you should see the log message from the node which dispose of the MNIST dataset. It means that the search request that contains <code>#MNIST, #dataset</code> tags has been successfully received by the node. In the example displayed here, we received messages only from one <code>Node</code>, which we created before. Obviously, it is non-sensical to train a <code>Federated Learning</code> model with only one <code>Node</code>, but this tutorial is just here for the sake of demonstration.</p> <p>The experiment also receives loss values during training on each node. In Fed-BioMed, it is possible to use a tensorboard to display loss values during training. Please refer to Fed-BioMed's tensorboard documentation for how to enable the tensorboard.</p> <p>Now, let's run the experiment.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') <p>After running the experiment, according to the provided arguments 4 training rounds should be completed on the node that you created. You can check the node id from the output and compare it with the node id which is defined in the config.ini file. After the process is finished, you are ready to inspect the model parameters.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the training rounds : \", exp.training_replies().keys())\n</pre> print(\"\\nList the training rounds : \", exp.training_replies().keys()) <p>Now, let's see how training details can be accessed from <code>training_replies()</code>. The following parameters will be inspected;</p> <ul> <li><code>rtime_training</code> : Real-time (clock time) spent in the training function on the node</li> <li><code>ptime_training</code>: Process time (user and system CPU) spent in the training function on the node</li> <li><code>rtime_total</code>   : Real-time (clock time) spent in the researcher between sending training requests and handling the responses</li> </ul> <p>Note: The following code accesses the training replies of the last round of the experiment.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the training rounds : \", exp.training_replies().keys())\n\nprint(\"\\nList the nodes for the last training round and their timings : \")\nround_data = exp.training_replies()[rounds - 1]\nfor r in round_data.values():\n    print(\"\\t- {id} :\\\n    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = r['node_id'],\n        rtraining = r['timing']['rtime_training'],\n        ptraining = r['timing']['ptime_training'],\n        rtotal = r['timing']['rtime_total']))\n</pre> print(\"\\nList the training rounds : \", exp.training_replies().keys())  print(\"\\nList the nodes for the last training round and their timings : \") round_data = exp.training_replies()[rounds - 1] for r in round_data.values():     print(\"\\t- {id} :\\     \\n\\t\\trtime_training={rtraining:.2f} seconds\\     \\n\\t\\tptime_training={ptraining:.2f} seconds\\     \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = r['node_id'],         rtraining = r['timing']['rtime_training'],         ptraining = r['timing']['ptime_training'],         rtotal = r['timing']['rtime_total'])) In\u00a0[\u00a0]: Copied! <pre>print(\"\\nAccess the federated params for the last training round : \")\nprint(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n</pre> print(\"\\nAccess the federated params for the last training round : \") print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())  <p>Finally, to access specific parameters of last round:</p> In\u00a0[\u00a0]: Copied! <pre>print(\"\\t- Parameters of CONV1 layer's biases of last round: \\n\", exp.aggregated_params()[rounds - 1]['params']['conv1.bias'])\n</pre> print(\"\\t- Parameters of CONV1 layer's biases of last round: \\n\", exp.aggregated_params()[rounds - 1]['params']['conv1.bias']) In\u00a0[\u00a0]: Copied! <pre># access the TrainingPlan\nexp.training_plan()\n</pre> # access the TrainingPlan exp.training_plan() In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\nmodel_args = {}\n\ntraining_args = {\n    'loader_args': { 'batch_size': 48, },\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 1,\n    'dry_run': False,  \n    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n\ntags =  ['#MNIST', '#dataset']\nrounds = 4\n\nexpN2 = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  model_args = {}  training_args = {     'loader_args': { 'batch_size': 48, },     'optimizer_args': {         'lr': 1e-3     },     'epochs': 1,     'dry_run': False,       'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples }  tags =  ['#MNIST', '#dataset'] rounds = 4  expN2 = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) <p>You can see from the output that the search request (done when initializing <code>Experiment</code>) has been sent to 2 <code>Nodes</code> by the experiment.</p> <p>Now, let's run the experiment.</p> In\u00a0[\u00a0]: Copied! <pre>expN2.run()\n</pre> expN2.run() In\u00a0[\u00a0]: Copied! <pre>fed_model = expN2.training_plan().model()\nfed_model.load_state_dict(expN2.aggregated_params()[rounds - 1]['params'])\n</pre> fed_model = expN2.training_plan().model() fed_model.load_state_dict(expN2.aggregated_params()[rounds - 1]['params']) In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport torch.nn.functional as F\n# Test function \ndef testing_accuracy(model, data_loader,):\n    model.eval()\n    test_loss = 0\n    device = 'cpu'\n\n    y_pred = []\n    y_actu = []\n    with torch.no_grad():\n        for data, target in data_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            y_pred.extend(torch.flatten(pred).tolist()) \n            y_actu.extend(target.tolist())\n           \n    y_pred = pd.Series(y_pred, name='Actual')\n    y_actu = pd.Series(y_actu, name='Predicted')\n    cm = pd.crosstab(y_actu, y_pred)\n    correct = sum([cm.iloc[i,i] for i in range(len(cm))])\n    \n    test_loss /= len(data_loader.dataset)\n    accuracy = 100*correct/len(data_loader.dataset)\n\n    return(test_loss, accuracy, cm)\n</pre> import pandas as pd import torch.nn.functional as F # Test function  def testing_accuracy(model, data_loader,):     model.eval()     test_loss = 0     device = 'cpu'      y_pred = []     y_actu = []     with torch.no_grad():         for data, target in data_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             y_pred.extend(torch.flatten(pred).tolist())              y_actu.extend(target.tolist())                 y_pred = pd.Series(y_pred, name='Actual')     y_actu = pd.Series(y_actu, name='Predicted')     cm = pd.crosstab(y_actu, y_pred)     correct = sum([cm.iloc[i,i] for i in range(len(cm))])          test_loss /= len(data_loader.dataset)     accuracy = 100*correct/len(data_loader.dataset)      return(test_loss, accuracy, cm) <p>We will use the MNIST test dataset for testing our federated model. You can download this dataset set from <code>torchvision.dataset</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torchvision import datasets, transforms\n\n\ntransform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\n# Get the test dataset\n# here we save the dataset under the `tmp` directory whithin Fed-BioMed folder\ntest_set = datasets.MNIST(root = os.path.join(config.vars['TMP_DIR'], 'mnist_testing.tmp'), download = True, train = False, transform = transform)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)\n</pre> import torch from torchvision import datasets, transforms   transform = transforms.Compose([             transforms.ToTensor(),             transforms.Normalize((0.1307,), (0.3081,))         ])  # Get the test dataset # here we save the dataset under the `tmp` directory whithin Fed-BioMed folder test_set = datasets.MNIST(root = os.path.join(config.vars['TMP_DIR'], 'mnist_testing.tmp'), download = True, train = False, transform = transform) test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True) <p>Now, it is time to get performance results</p> In\u00a0[\u00a0]: Copied! <pre>test_results = testing_accuracy(fed_model, test_loader)\n\nprint(\"- Test Loss: \", test_results[0], \"\\n\")\nprint(\"- Accuracy: \", test_results[1], \"\\n\")\nprint(\"- Confusion Matrix: \\n \\n\",  test_results[2] )\n</pre> test_results = testing_accuracy(fed_model, test_loader)  print(\"- Test Loss: \", test_results[0], \"\\n\") print(\"- Accuracy: \", test_results[1], \"\\n\") print(\"- Confusion Matrix: \\n \\n\",  test_results[2] ) <p>We will use <code>matplotlib</code> for plotting results. If you have followed all the steps in this tutorial, you must have installed this library in the section 2. Otherwise, you can run <code>!pip install matplotlib</code> command in a notebook cell.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nconf_matrix = test_results[2].to_numpy()\n\nfig, ax = plt.subplots(figsize=(10,5))\nim = ax.imshow(conf_matrix)\n\nax.set_xticks(np.arange(10))\nax.set_yticks(np.arange(10))\n\nfor i in range(conf_matrix.shape[0]):\n    for j in range(conf_matrix.shape[1]):\n        text = ax.text(j, i, conf_matrix[i, j],\n                       ha=\"center\", va=\"center\", color=\"w\")\n        \nax.set_xlabel('Actual targets')\nax.set_ylabel('Predicted targets')\nax.set_title('Confusion Matrix')\n</pre> import numpy as np import matplotlib.pyplot as plt  conf_matrix = test_results[2].to_numpy()  fig, ax = plt.subplots(figsize=(10,5)) im = ax.imshow(conf_matrix)  ax.set_xticks(np.arange(10)) ax.set_yticks(np.arange(10))  for i in range(conf_matrix.shape[0]):     for j in range(conf_matrix.shape[1]):         text = ax.text(j, i, conf_matrix[i, j],                        ha=\"center\", va=\"center\", color=\"w\")          ax.set_xlabel('Actual targets') ax.set_ylabel('Predicted targets') ax.set_title('Confusion Matrix') In\u00a0[\u00a0]: Copied! <pre>errors = []\n\nfor i in range(rounds):\n    fed_model = expN2.training_plan().model()\n    fed_model.load_state_dict(expN2.aggregated_params()[i]['params'])\n    loss = testing_accuracy(fed_model, test_loader)[0]\n    errors.append(loss)\n</pre> errors = []  for i in range(rounds):     fed_model = expN2.training_plan().model()     fed_model.load_state_dict(expN2.aggregated_params()[i]['params'])     loss = testing_accuracy(fed_model, test_loader)[0]     errors.append(loss) In\u00a0[\u00a0]: Copied! <pre>### Plotting \nplt.plot(errors, label = 'Federated Test Loss')\nplt.xlabel('Round')\nplt.ylabel('Loss')\nplt.title(\"Log Likelihood Loss evolution over number of rounds\")\nplt.legend()\n</pre> ### Plotting  plt.plot(errors, label = 'Federated Test Loss') plt.xlabel('Round') plt.ylabel('Loss') plt.title(\"Log Likelihood Loss evolution over number of rounds\") plt.legend()"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#pytorch-mnist-basic-example","title":"PyTorch MNIST Basic Example\u00b6","text":""},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial focuses on how to train a CNN model with Fed-BioMed nodes using the PyTorch framework on the MNIST dataset. You will learn;</p> <ul> <li>How to prepare your environment to be able to train your model;</li> <li>How to create a training plan class to run it in a single node which works as a different process in your local machine;</li> <li>How to create a federated learning experiment;</li> <li>How to load and inspect your model parameters;</li> <li>How to test your model using test dataset.</li> </ul> <p>Note: In the following steps, we will be running this example using two nodes.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#before-you-start","title":"Before you start\u00b6","text":"<p>Before starting this tutorial please change your directory where you want to keep created Fed-BioMed components. This will allow you to keep Fed-BioMed component in a specific folder, and will ease specifying components using <code>fedbiomed</code> command.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#1-configuring-nodes","title":"1. Configuring Nodes\u00b6","text":"<p>In this tutorial, you will learn how to train your model with a single Fed-BioMed node. Thus, you need to configure a node and add MNIST dataset into it. Please open a terminal and follow the steps below.</p> <ul> <li>Configuration Steps:<ul> <li>Navigate to directory where you want to keep Fed-BioMed node and researcher component folders.</li> <li>Run <code>fedbiomed node dataset add</code> in the terminal. This command will automatically create a default node component folder called 'fbm-node'.</li> <li>It will ask you to select the data type that you want to add. The second option (which is the default) has been configured to add the MNIST dataset. Please type <code>2</code> and continue.</li> <li>Please use default tags which are <code>#MNIST</code> and <code>#dataset</code>.</li> <li>For the next step, please select the directory that you want to download the MNIST dataset. Usually, we may want to save them into the node component folder (here in <code>fbm-node/data/</code> folder).</li> <li>After the download is completed you will see the details of the MNIST dataset on the screen.</li> </ul> </li> </ul> <p>Please run the command below in the same terminal to make sure the MNIST dataset is successfully added to the node.</p> <pre><code>$ fedbiomed node dataset list\n</code></pre> <p>Then launch the <code>Node</code> by executing:</p> <pre><code>$ fedbiomed node start\n</code></pre>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#start-your-notebook","title":"Start your notebook\u00b6","text":"<p>You need to start a jupyter notebook and create a new notebook to be able to follow the tutorial. Please open a new terminal window and run the following command to start your notebook.</p> <pre><code>$ fedbiomed researcher start\n</code></pre> <p>The command above will create a component folder for researcher called \"fbm-researcher\" in the directory where this command is executed.Note: If you are having a problem understanding the steps above, we recommend you to follow the installation tutorial.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#2-what-is-mnist-dataset","title":"2. What is MNIST dataset?\u00b6","text":"<p>MNIST dataset contains 60000 grayscale images (of size 28 * 28 pixels) of handwritten digits between 0 and 9. MNIST is commonly used for image classification task: the goal is to classify each image by assigning it to the correct digit.</p> <p>For a better visual understanding, we display a few samples from MNIST testing dataset.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#3-training-a-model","title":"3. Training a model\u00b6","text":"<p>In this section, you will learn how to train a model in Fed-BioMed, by creating a training plan class that includes special methods to retrieve mode, optimizer, training data as well as training step to execute at each iteration of training. Then the federated training will be launched by wrapping the training plan in an experiment.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#31-creating-a-pytorch-training-plan","title":"3.1 Creating A PyTorch Training Plan\u00b6","text":"<p>A PyTorch training plan is a Python class that inherits from <code>fedbiomed.common.training_plans.TorchTrainingPlan</code> which is an abstract class. The abstract methods <code>init_model</code>, <code>training_data</code> and <code>training_step</code> should be provided in the training plan to retrieve/execute model (<code>nn.Module</code>), data loader and training actions at each iteration.</p> <p>         Please visit training plan user guide for more information and other special methods of training plan.     </p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#32-define-an-experiment","title":"3.2 Define an Experiment\u00b6","text":"<p>An experiment is a class that orchestrates the training processes that run on different nodes.</p> <ul> <li><code>model_arg</code> includes arguments that will pass into <code>init_model</code> method of training plan class. In our case, we don't need to add any arguments.</li> <li><code>training_args</code> includes arguments related to optimizer, data loader and training step/routine such as learning rate, number of epochs.</li> <li><code>tags</code> is a list that includes tags that are going to be used for searching related datasets in nodes. In our case, we saved the MNIST dataset with #MNIST and #dataset tags.</li> <li><code>rounds</code> represents the number of training rounds that will be applied in nodes. In each round every node complete epochs and send model parameters to the experiment.</li> </ul> <p>!!! note \"Validation step on testing dataset\" The training arguments can also include testing parameters to define how the trained model performs for both globally and locally updated model weights after local training. These parameters allow you to reserve a portion of the training dataset for validation. The node retains the initially selected validation subset and uses the same samples for subsequent training rounds. However, if <code>shuffle_testing_dataset</code> is set to true for a given round, the dataset will be reshuffled, creating a new validation subset. Additionally, modifying the test ratio will reset the testing dataset, leading to a newly selected validation set that differs from previous training rounds. For more information please visit testing documentaiton.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#33-what-happens-during-the-initialization-of-an-experiment","title":"3.3 What happens during the initialization of an experiment?\u00b6","text":"<ol> <li>The experiment searches for <code>Nodes</code> whose available datasets have been saved with tags indicated in the <code>tags</code> argument. Nodes are selected based on a node selection strategy.</li> <li>The experiment sets up according to the provided arguments</li> </ol> <p>For more details, you can visit <code>Experiment</code> webpage.</p> <p>Now, let's create our experiment. Since only one node has been created, the experiment will only find a single node for training.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#34-extracting-training-results","title":"3.4 Extracting Training Results\u00b6","text":""},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#timing","title":"Timing\u00b6","text":"<p>Training replies for each round are available via <code>exp.training_replies()</code> (index 0 to (<code>rounds</code> - 1) ). You can display the keys of each round by running the following script.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#federated-parameters","title":"Federated Parameters\u00b6","text":"<p>Federated model parameters for each round are available via <code>exp.aggregated_params()</code> (index 0 to (<code>rounds</code> - 1) ). For example, you can easily view the federated parameters for the last round of the experiment:</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#4-pytorch-mnist-example-with-two-nodes","title":"4. PyTorch MNIST Example with Two Nodes\u00b6","text":"<p>In this section, we will be working on two nodes. Following the previous example, the experiment and training routine will remain unchanged.  Therefore, you just need to configure another node, and add the MNIST dataset with the default tags.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#41-configuring-second-node","title":"4.1 Configuring Second Node\u00b6","text":"<p>While creating a second node you need to be careful with the node that has already been created. To configure the second node, a different config file has to be defined. Please follow the steps below to configure your second node.</p> <ol> <li>Please open a new terminal and cd into the base directory of fedbiomed.</li> <li>In this step, you need to name a new config file using the <code>--path</code> parameter. Instead of downloading a new MNIST dataset, you can use the one that you already downloaded in the previous example. To do so please run <code>fedbiomed node --path ./my-second-node dataset add --mnist /path/to/your/mnist</code>. Otherwise you can still do a <code>fedbiomed node --path ./my-second-node dataset add</code> as shown before. It will create a component directory called <code>my-second-node</code> in the directory where the command is executed.</li> <li>You need to start the new node by indicating the newely created config file. Please run the following command to start the second node <code>fedbiomed node --path ./my-second-node start</code></li> </ol>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#42-defining-an-experiment","title":"4.2 Defining an Experiment\u00b6","text":"<p>Since a training plan has already been created and saved in the previous example, you don't need to repeat this step here again: the same training plan with same model will be used for training. However, you can define a new experiment for testing purposes. The experiment will search the MNIST dataset in available nodes. Training arguments are kept the same as in the previous example.</p> <p>You can also list datasets and select specific nodes to perform traning. You can visit listing datasets and selecting nodes documentation to get more information.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#5-testing-federated-model","title":"5. Testing Federated Model\u00b6","text":"<p>In this section, we will create a test function to obtain accuracy, loss, and confusion matrix using the test MNIST dataset.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#51-aggregated-parameters","title":"5.1 Aggregated Parameters\u00b6","text":"<p><code>training_plan()</code> returns the training plan and <code>training_plan().model()</code> returns the model that is created in the training plan.  It is possible to load specific aggregated parameters which are obtained in every round. Thereafter, it will be ready to make predictions using those parameters. The last round gives the last aggregated model parameters which represents the final model.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#52-creating-a-test-function","title":"5.2 Creating A Test Function\u00b6","text":"<p>Let's create a test function that returns loss, accuracy, and confusion matrix.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#53-creating-heatmap-for-confusion-matrix","title":"5.3 Creating Heatmap for Confusion Matrix\u00b6","text":""},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#54-plotting-loss-for-each-round","title":"5.4 Plotting Loss for Each round\u00b6","text":"<p>In this section, we will plot loss values that are obtained over the test dataset using model parameters of every round.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/","title":"How to Create Your Custom PyTorch Training Plan","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport pandas as pd\nimport shutil\n\nfrom fedbiomed.researcher.config import config\n\n# Celeba folder\nparent_dir = os.path.join(config.root, \"notebooks\", \"data\", \"Celeba\") \nceleba_raw_folder = os.path.join(\"Celeba_raw\", \"raw\")\nimg_dir = os.path.join(parent_dir, celeba_raw_folder, 'img_align_celeba') + os.sep\nout_dir = os.path.join(parent_dir, \"celeba_preprocessed\")\n\n# Read attribute CSV and only load Smilling column\ndf = pd.read_csv(os.path.join(parent_dir, celeba_raw_folder, 'list_attr_celeba.txt'),\n                 sep=\"\\s+\", skiprows=1, usecols=['Smiling'])\n\n# data is on the form : 1 if the person is smiling, -1 otherwise. we set all -1 to 0 for the model to train faster\ndf.loc[df['Smiling'] == -1, 'Smiling'] = 0\n\n# Split csv in 3 parts\nlength = len(df)\ndata_node_1 = df.iloc[:int(length/3)]\ndata_node_2 = df.iloc[int(length/3):int(length/3) * 2]\ndata_node_3 = df.iloc[int(length/3) * 2:]\n\n# Create folder for each node\nif not os.path.exists(os.path.join(out_dir, \"data_node_1\")):\n    os.makedirs(os.path.join(out_dir, \"data_node_1\", \"data\"))\nif not os.path.exists(os.path.join(out_dir, \"data_node_2\")):\n    os.makedirs(os.path.join(out_dir, \"data_node_2\", \"data\"))\nif not os.path.exists(os.path.join(out_dir, \"data_node_3\")):\n    os.makedirs(os.path.join(out_dir, \"data_node_3\", \"data\"))\n\n# Save each node's target CSV to the corect folder\ndata_node_1.to_csv(os.path.join(out_dir, 'data_node_1', 'target.csv'), sep='\\t')\ndata_node_2.to_csv(os.path.join(out_dir, 'data_node_2', 'target.csv'), sep='\\t')\ndata_node_3.to_csv(os.path.join(out_dir, 'data_node_3', 'target.csv'), sep='\\t')\n\n# Copy all images of each node in the correct folder\nfor im in data_node_1.index:\n    shutil.copy(img_dir+im, os.path.join(out_dir,\"data_node_1\", \"data\", im))\nprint(\"data for node 1 succesfully created\")\n\nfor im in data_node_2.index:\n    shutil.copy(img_dir+im, os.path.join(out_dir, \"data_node_2\", \"data\", im))\nprint(\"data for node 2 succesfully created\")\n\nfor im in data_node_3.index:\n    shutil.copy(img_dir+im, os.path.join(out_dir, \"data_node_3\", \"data\", im))\nprint(\"data for node 3 succesfully created\")\n</pre> import os import pandas as pd import shutil  from fedbiomed.researcher.config import config  # Celeba folder parent_dir = os.path.join(config.root, \"notebooks\", \"data\", \"Celeba\")  celeba_raw_folder = os.path.join(\"Celeba_raw\", \"raw\") img_dir = os.path.join(parent_dir, celeba_raw_folder, 'img_align_celeba') + os.sep out_dir = os.path.join(parent_dir, \"celeba_preprocessed\")  # Read attribute CSV and only load Smilling column df = pd.read_csv(os.path.join(parent_dir, celeba_raw_folder, 'list_attr_celeba.txt'),                  sep=\"\\s+\", skiprows=1, usecols=['Smiling'])  # data is on the form : 1 if the person is smiling, -1 otherwise. we set all -1 to 0 for the model to train faster df.loc[df['Smiling'] == -1, 'Smiling'] = 0  # Split csv in 3 parts length = len(df) data_node_1 = df.iloc[:int(length/3)] data_node_2 = df.iloc[int(length/3):int(length/3) * 2] data_node_3 = df.iloc[int(length/3) * 2:]  # Create folder for each node if not os.path.exists(os.path.join(out_dir, \"data_node_1\")):     os.makedirs(os.path.join(out_dir, \"data_node_1\", \"data\")) if not os.path.exists(os.path.join(out_dir, \"data_node_2\")):     os.makedirs(os.path.join(out_dir, \"data_node_2\", \"data\")) if not os.path.exists(os.path.join(out_dir, \"data_node_3\")):     os.makedirs(os.path.join(out_dir, \"data_node_3\", \"data\"))  # Save each node's target CSV to the corect folder data_node_1.to_csv(os.path.join(out_dir, 'data_node_1', 'target.csv'), sep='\\t') data_node_2.to_csv(os.path.join(out_dir, 'data_node_2', 'target.csv'), sep='\\t') data_node_3.to_csv(os.path.join(out_dir, 'data_node_3', 'target.csv'), sep='\\t')  # Copy all images of each node in the correct folder for im in data_node_1.index:     shutil.copy(img_dir+im, os.path.join(out_dir,\"data_node_1\", \"data\", im)) print(\"data for node 1 succesfully created\")  for im in data_node_2.index:     shutil.copy(img_dir+im, os.path.join(out_dir, \"data_node_2\", \"data\", im)) print(\"data for node 2 succesfully created\")  for im in data_node_3.index:     shutil.copy(img_dir+im, os.path.join(out_dir, \"data_node_3\", \"data\", im)) print(\"data for node 3 succesfully created\") <p>Now if you go to the <code>fdm-researcher/notebooks/data/Celaba</code> directory you can see the folder called <code>celeba_preprocessed</code>. There will be three different folders that contain an image dataset for 3 nodes. The next step will be configuring the nodes and deplying the datasets. In the next steps, we will be configuring only two nodes. The dataset for the third node is going to be used for the testing.</p> <p>Create 2 nodes for training :</p> <ul> <li><code>fedbiomed component create -c node --path ./my-first-node</code></li> <li><code>fedbiomed component create -c node --path ./my-second-node</code></li> </ul> <p>Note: You may want to copy the dataset created into <code>Nodes</code>, in the folder <code>my-first-node/data</code> or  <code>./my-second-node/data</code> . This is not required for Fed-BioMed to work, since you can access dataset from outside <code>Node</code> component, but enables you to run the tutorials more easily. Commands for that will be:</p> <ul> <li><code>cp -r fbm-researcher/notebooks/data/Celeba/celeba_preprocessed/data_node_1/ my-first-node/data/</code></li> <li><code>cp -r fbm-researcher/notebooks/data/Celeba/celeba_preprocessed/data_node_2/ my-second-node/data/</code></li> </ul> <p>Add data to each node :</p> <ul> <li><code>fedbiomed node --path ./my-first-node dataset add</code></li> <li><code>fedbiomed node --path ./my-second-node dataset add</code></li> </ul> <p>Next step is to create our <code>Net</code> class based on the methods that have been explained in the previous sections. This class is part of the training plan that will be passed to the Experiment.  Afterwards, the nodes will receive the training plan and perform the training by retrieving training data and passing it to the <code>training_step</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nfrom fedbiomed.common.data import DataManager\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport os\n\n\nclass CelebaTrainingPlan(TorchTrainingPlan):\n\n    # Defines model\n    def init_model(self):\n        model = self.Net()\n        return model\n\n    # Here we define the custom dependencies that will be needed by our custom Dataloader\n    def init_dependencies(self):\n        deps = [\"from torch.utils.data import Dataset\",\n                \"from torchvision import transforms\",\n                \"import pandas as pd\",\n                \"from PIL import Image\",\n                \"import os\",\n                \"import numpy as np\"]\n        return deps\n\n    # Torch modules class\n    class Net(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            #convolution layers\n            self.conv1 = nn.Conv2d(3, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 32, 3, 1)\n            self.conv3 = nn.Conv2d(32, 32, 3, 1)\n            self.conv4 = nn.Conv2d(32, 32, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            # classifier\n            self.fc1 = nn.Linear(3168, 128)\n            self.fc2 = nn.Linear(128, 2)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.max_pool2d(x, 2)\n            x = F.relu(x)\n\n            x = self.conv2(x)\n            x = F.max_pool2d(x, 2)\n            x = F.relu(x)\n\n            x = self.conv3(x)\n            x = F.max_pool2d(x, 2)\n            x = F.relu(x)\n\n            x = self.conv4(x)\n            x = F.max_pool2d(x, 2)\n            x = F.relu(x)\n\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n\n            x = self.dropout2(x)\n            x = self.fc2(x)\n            output = F.log_softmax(x, dim=1)\n            return output\n\n\n    class CelebaDataset(Dataset):\n        \"\"\"Custom Dataset for loading CelebA face images\"\"\"\n\n        # we dont load the full data of the images, we retrieve the image with the get item.\n        # in our case, each image is 218*178 * 3colors. there is 67533 images. this take at leas 7G of ram\n        # loading images when needed takes more time during training but it wont impact the ram usage as much as loading everything\n        def __init__(self, txt_path, img_dir, transform=None):\n            df = pd.read_csv(txt_path, sep=\"\\t\", index_col=0)\n            self.img_dir = img_dir\n            self.txt_path = txt_path\n            self.img_names = df.index.values\n            self.y = df['Smiling'].values\n            self.transform = transform\n            print(\"celeba dataset finished\")\n\n        def __getitem__(self, index):\n            img = np.asarray(Image.open(os.path.join(self.img_dir,\n                                        self.img_names[index])))\n            img = transforms.ToTensor()(img)\n            label = self.y[index]\n            return img, label\n\n        def __len__(self):\n            return self.y.shape[0]\n\n    # The training_data creates the Dataloader to be used for training in the\n    # general class Torchnn of fedbiomed\n    def training_data(self):\n        dataset = self.CelebaDataset(os.path.join(self.dataset_path, \"target.csv\"), os.path.join(self.dataset_path, \"data\"))\n        loader_arguments = { 'shuffle': True}\n        return DataManager(dataset, **loader_arguments)\n\n    # This function must return the loss to backward it\n    def training_step(self, data, target):\n\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan import torch.nn.functional as F from torchvision import transforms from torch.utils.data import Dataset from fedbiomed.common.data import DataManager import pandas as pd import numpy as np from PIL import Image import os   class CelebaTrainingPlan(TorchTrainingPlan):      # Defines model     def init_model(self):         model = self.Net()         return model      # Here we define the custom dependencies that will be needed by our custom Dataloader     def init_dependencies(self):         deps = [\"from torch.utils.data import Dataset\",                 \"from torchvision import transforms\",                 \"import pandas as pd\",                 \"from PIL import Image\",                 \"import os\",                 \"import numpy as np\"]         return deps      # Torch modules class     class Net(nn.Module):          def __init__(self):             super().__init__()             #convolution layers             self.conv1 = nn.Conv2d(3, 32, 3, 1)             self.conv2 = nn.Conv2d(32, 32, 3, 1)             self.conv3 = nn.Conv2d(32, 32, 3, 1)             self.conv4 = nn.Conv2d(32, 32, 3, 1)             self.dropout1 = nn.Dropout(0.25)             self.dropout2 = nn.Dropout(0.5)             # classifier             self.fc1 = nn.Linear(3168, 128)             self.fc2 = nn.Linear(128, 2)          def forward(self, x):             x = self.conv1(x)             x = F.max_pool2d(x, 2)             x = F.relu(x)              x = self.conv2(x)             x = F.max_pool2d(x, 2)             x = F.relu(x)              x = self.conv3(x)             x = F.max_pool2d(x, 2)             x = F.relu(x)              x = self.conv4(x)             x = F.max_pool2d(x, 2)             x = F.relu(x)              x = self.dropout1(x)             x = torch.flatten(x, 1)             x = self.fc1(x)             x = F.relu(x)              x = self.dropout2(x)             x = self.fc2(x)             output = F.log_softmax(x, dim=1)             return output       class CelebaDataset(Dataset):         \"\"\"Custom Dataset for loading CelebA face images\"\"\"          # we dont load the full data of the images, we retrieve the image with the get item.         # in our case, each image is 218*178 * 3colors. there is 67533 images. this take at leas 7G of ram         # loading images when needed takes more time during training but it wont impact the ram usage as much as loading everything         def __init__(self, txt_path, img_dir, transform=None):             df = pd.read_csv(txt_path, sep=\"\\t\", index_col=0)             self.img_dir = img_dir             self.txt_path = txt_path             self.img_names = df.index.values             self.y = df['Smiling'].values             self.transform = transform             print(\"celeba dataset finished\")          def __getitem__(self, index):             img = np.asarray(Image.open(os.path.join(self.img_dir,                                         self.img_names[index])))             img = transforms.ToTensor()(img)             label = self.y[index]             return img, label          def __len__(self):             return self.y.shape[0]      # The training_data creates the Dataloader to be used for training in the     # general class Torchnn of fedbiomed     def training_data(self):         dataset = self.CelebaDataset(os.path.join(self.dataset_path, \"target.csv\"), os.path.join(self.dataset_path, \"data\"))         loader_arguments = { 'shuffle': True}         return DataManager(dataset, **loader_arguments)      # This function must return the loss to backward it     def training_step(self, data, target):          output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss  <p>This group of arguments corresponds respectively to:</p> <ul> <li><code>model_args</code>: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node-side.</li> <li><code>training_args</code>: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node-side.</li> </ul> <p>Note: Typos and/or lack of positional (required) arguments might raise an error.</p> In\u00a0[\u00a0]: Copied! <pre>training_args = {\n    'loader_args': { 'batch_size': 32, }, \n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 1, \n    'dry_run': False,  \n    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n</pre> training_args = {     'loader_args': { 'batch_size': 32, },      'optimizer_args': {         'lr': 1e-3     },     'epochs': 1,      'dry_run': False,       'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#celeba']\nrounds = 3\n\nexp = Experiment(tags=tags,\n                 training_plan_class=CelebaTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['#celeba'] rounds = 3  exp = Experiment(tags=tags,                  training_plan_class=CelebaTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) <p>Let's start the experiment.</p> <p>By default, this function doesn't stop until all the <code>round_limit</code> rounds are done for all the nodes. While the experiment runs you can open the terminals where you have started the nodes and see the training progress. However, the loss values obtained from each node during the training will be printed as output in real time. Since we are working on an image dataset, training might take some time.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') In\u00a0[\u00a0]: Copied! <pre>fed_model = exp.training_plan().model()\nfed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n</pre> fed_model = exp.training_plan().model() fed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params']) In\u00a0[\u00a0]: Copied! <pre>import torch\n\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport os\n\ndef testing_Accuracy(model, data_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n\n    device = \"cpu\"\n\n    correct = 0\n\n    loader_size = len(data_loader)\n    with torch.no_grad():\n        for idx, (data, target) in enumerate(data_loader):\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n            #only uses 10% of the dataset, results are similar but faster\n            if idx &gt;= loader_size / 10:\n                pass\n                break\n\n    \n        pred = output.argmax(dim=1, keepdim=True)\n\n    test_loss /= len(data_loader.dataset)\n    accuracy = 100* correct/(data_loader.batch_size * idx)\n\n    return(test_loss, accuracy)\n</pre> import torch  import torch.nn.functional as F from torchvision import transforms from torch.utils.data import Dataset import pandas as pd import numpy as np from PIL import Image import os  def testing_Accuracy(model, data_loader):     model.eval()     test_loss = 0     correct = 0      device = \"cpu\"      correct = 0      loader_size = len(data_loader)     with torch.no_grad():         for idx, (data, target) in enumerate(data_loader):             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()              #only uses 10% of the dataset, results are similar but faster             if idx &gt;= loader_size / 10:                 pass                 break               pred = output.argmax(dim=1, keepdim=True)      test_loss /= len(data_loader.dataset)     accuracy = 100* correct/(data_loader.batch_size * idx)      return(test_loss, accuracy) <p>We also need to define a custom Dataset class for the test dataset in order to load it using PyTorch's <code>DataLoader</code>. This will be the same class that has been already defined in the training plan.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.config import config\nfrom torch.utils.data import DataLoader\n\n\ntest_dataset_path = os.path.join(config.root,\n                                 \"notebooks\",\n                                 \"data\",\n                                 \"Celeba\",\n                                 \"celeba_preprocessed\",\n                                 \"data_node_3\")\n\nclass CelebaDataset(Dataset):\n    \"\"\"Custom Dataset for loading CelebA face images\"\"\"\n\n    def __init__(self, txt_path, img_dir, transform=None):\n        df = pd.read_csv(txt_path, sep=\"\\t\", index_col=0)\n        self.img_dir = img_dir\n        self.txt_path = txt_path\n        self.img_names = df.index.values\n        self.y = df['Smiling'].values\n        self.transform = transform\n        print(\"celeba dataset finished\")\n\n    def __getitem__(self, index):\n        img = np.asarray(Image.open(os.path.join(self.img_dir,\n                                        self.img_names[index])))\n        img = transforms.ToTensor()(img)\n        label = self.y[index]\n        return img, label\n\n    def __len__(self):\n        return self.y.shape[0]\n\n\ndataset = CelebaDataset(os.path.join(test_dataset_path, \"target.csv\"), os.path.join(test_dataset_path, \"data\"))\ntrain_kwargs = { 'shuffle': True}\ndata_loader = DataLoader(dataset, **train_kwargs)\n</pre> from fedbiomed.researcher.config import config from torch.utils.data import DataLoader   test_dataset_path = os.path.join(config.root,                                  \"notebooks\",                                  \"data\",                                  \"Celeba\",                                  \"celeba_preprocessed\",                                  \"data_node_3\")  class CelebaDataset(Dataset):     \"\"\"Custom Dataset for loading CelebA face images\"\"\"      def __init__(self, txt_path, img_dir, transform=None):         df = pd.read_csv(txt_path, sep=\"\\t\", index_col=0)         self.img_dir = img_dir         self.txt_path = txt_path         self.img_names = df.index.values         self.y = df['Smiling'].values         self.transform = transform         print(\"celeba dataset finished\")      def __getitem__(self, index):         img = np.asarray(Image.open(os.path.join(self.img_dir,                                         self.img_names[index])))         img = transforms.ToTensor()(img)         label = self.y[index]         return img, label      def __len__(self):         return self.y.shape[0]   dataset = CelebaDataset(os.path.join(test_dataset_path, \"target.csv\"), os.path.join(test_dataset_path, \"data\")) train_kwargs = { 'shuffle': True} data_loader = DataLoader(dataset, **train_kwargs) In\u00a0[\u00a0]: Copied! <pre>acc_federated = testing_Accuracy(fed_model, data_loader)\nacc_federated[1]\n</pre> acc_federated = testing_Accuracy(fed_model, data_loader) acc_federated[1]"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#how-to-create-your-custom-pytorch-training-plan","title":"How to Create Your Custom PyTorch Training Plan\u00b6","text":"<p>Fed-BioMed allows you to perform model training without changing your PyTorch training plan class completely. Integrating your PyToch model to Fed-BioMed only requires to add extra attributes and methods to train your model based on a federated approach.  In this tutorial, you will learn how to write/define your <code>TrainingPlan</code> (wrapping your model) in Fed-BioMed for PyTorch framework.</p> <p>Note: Before starting this tutorial we highly recommend you to follow the previous tutorials to understand the basics of Fed-BioMed.</p> <p>In this tutorial, we will be using Celaba (CelebaFaces) dataset to train the model. You can see details of the dataset here. In the following sections, you will have the instructions for downloading and configuring Celeba dataset for Fed-BioMed framework.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#1-fed-biomed-training-plan","title":"1. Fed-BioMed Training Plan\u00b6","text":"<p>In this section, you will learn how to write your custom training plan.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#what-is-training-plan","title":"What is Training Plan?\u00b6","text":"<p>The training plan is the class where all the methods and attributes are defined to train your model on the nodes. Each training plan should inherit the base training plan class of the belonging ML framework that is provided by Fed-BioMed. For more details, you can visit documentation for training plan.  The following code snippet shows a basic training plan that can be defined in Fed-BioMed for PyTorch framework.</p> <pre>from fedbiomed.common.training_plans import TorchTrainingPlan\n\n\nclass CustomTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args):\n        # Define here your model\n        # ...\n        return\n\n    def init_dependencies(self):\n        # Add here the dependencies / third party libraries to be loaded\n        #...\n        return\n    \n    def init_optimizer(self, optimizer_args):\n        # Define here your optimizer\n        #...\n        return\n\n    def training_data(self,  batch_size = 48):\n        # Define here how data are processed before feeding it to the model\n        # ...\n        return\n    \n    def training_step(self, data, target):\n        # Define here the loss function\n        # ...\n        return\n</pre>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#init_model-method-of-training-plan","title":"<code>init_model</code> Method of Training Plan\u00b6","text":"<p><code>init_model</code> method of the training plan is where you initialize your neural network module as in classical PyTorch model class. The network should be defined inside the training plan class and <code>init_model</code> should instantiate this network (<code>Module</code>), and return it.</p> <p>In this tutorial, we will be training a classification model for CelebA image dataset that will be able to predict whether the given face is smiling.</p> <pre>def init_model(self, model_args: dict = {}):\n    return self.Net(model_args)\n\nclass Net(nn.Module):\n\n    def __init__(model_args):\n        super().__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n        self.conv3 = nn.Conv2d(32, 32, 3, 1)\n        self.conv4 = nn.Conv2d(32, 32, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        # Classifier\n        self.fc1 = nn.Linear(3168, 128)\n        self.fc2 = nn.Linear(128, 2)\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n\n        x = self.conv3(x)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n\n        x = self.conv4(x)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n</pre>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#init_dependencies-method","title":"<code>init_dependencies</code> Method\u00b6","text":"<p>Next, you should define the <code>init_dependencies</code> to declare the modules that are used in the training plan. The modules should be supported by the Fed-BioMed.</p> <pre>def init_depedencies(self)\n    # Here we define the custom dependencies that will be needed by our custom Dataloader\n    deps = [\"from torch.utils.data import Dataset, DataLoader\",\n            \"from torchvision import transforms\",\n            \"import pandas as pd\",\n            \"from PIL import Image\",\n            \"import os\",\n            \"import numpy as np\"]\n    return deps\n</pre>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#init_optimizer-method","title":"<code>init_optimizer</code> Method\u00b6","text":"<p>To optimize your model, you will need an optimizer. This is where <code>init_optimizer</code> method comes into play. In this method, you may change the optimizer you want to use, add a PyTorch learning rate <code>Scheduler</code> or provide your custom optimizer.</p> <p><code>init_optimizer</code> takes <code>optimizer_args</code> as argument, an entry from <code>training_args</code> (more details later) which is a dictionary containing parameters that may be needed for optimizer initalization (such as learning rate, <code>Adagrad</code> weights decay, <code>Adam</code> beta parameters, ...). <code>init_optimizer</code> method should return the initialized <code>optimizer</code>, that will be used to optimize model.</p> <p>Defining an <code>optimizer</code> in Fed-BioMed is pretty similar to PyTorch, as shown in the example below (using PyTorch's <code>SGD</code> optimizer):</p> <pre>def init_optimizer(self, optimizer_args):\n    return torch.optim.SGD(self.model().parameters(), lr=optimizer_args['lr'])\n</pre> <p>By default (if this method is not specified in the <code>TrainingPlan</code>), model will be optimized using default <code>Adam</code> optimizer.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#training_data-and-custom-dataset","title":"<code>training_data() and Custom Dataset</code>\u00b6","text":"<p><code>training_data</code> is a method where the data is loaded for training on the node side. During each round of training, each node that particapates federated training builds the model, loads the dataset using the method <code>training_data</code>, and performs the <code>training_step</code> by passing loaded dataset.</p> <p>The dataset that we will be using in this tutorial is a image dataset. Therefore, your custom PyTorch <code>Dataset</code> should be be able to load images by given index . Please see the details of custom PyTorch datasets.</p> <pre>class CelebaDataset(Dataset):\n        \"\"\"Custom Dataset for loading CelebA face images\"\"\"\n\n\n        def __init__(self, txt_path, img_dir, transform=None):\n\n            # Read the csv file that includes classes for each image\n            df = pd.read_csv(txt_path, sep=\"\\t\", index_col=0)\n            self.img_dir = img_dir\n            self.txt_path = txt_path\n            self.img_names = df.index.values\n            self.y = df['Smiling'].values\n            self.transform = transform\n\n        def __getitem__(self, index):\n            img = np.asarray(Image.open(os.path.join(self.img_dir, self.img_names[index])))\n            img = transforms.ToTensor()(img)\n            label = self.y[index]\n            return img, label\n\n        def __len__(self):\n            return self.y.shape[0]\n</pre> <p>Now, you need to define a <code>training_data</code> method that will create a Fed-BioMed DataManager using custom <code>CelebaDataset</code> class.</p> <pre>def training_data(self):\n        # The training_data creates the dataset and returns DataManager to be used for training in the general class Torchnn of Fed-BioMed\n        dataset = self.CelebaDataset(\n            os.path.join(self.dataset_path, \"target.csv\"), os.path.join(self.dataset_path, \"data\")\n            )\n        loader_arguments = { 'shuffle': True}\n        return DataManager(dataset, **loader_arguments)\n</pre>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#training_step","title":"<code>training_step()</code>\u00b6","text":"<p>The last method that needs to be defined is the <code>training_step</code>. This method is responsible for executing the forward method and calculating the loss value for the backward process of the network. To access the <code>forward</code> method of the <code>torch.nn.Module</code> that is defined in the <code>init_model</code>, the getter method <code>model()</code> of training plan class should be used.</p> <pre>def training_step(self, data, target): \n    output = self.model().forward(data)\n    loss   = torch.nn.functional.nll_loss(output, target)\n    return loss\n</pre> <p>You are now ready to create your training plan class. All you need to do is to locate every method that has been explained in the previous sections in your traning plan class. In the next steps we will;</p> <ol> <li>launch the <code>Researcher</code>: <code>fedbiomed researcher start</code></li> <li>download the CelebA dataset and deploy it on the nodes</li> <li>define our complete training</li> <li>create an experiment and run it</li> <li>evaluate our model using a testing dataset</li> </ol>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#2configuring-nodes","title":"2.Configuring Nodes\u00b6","text":"<p>We will be working with CelebA (CelebFaces) dataset. Therefore, please visit here and download the files <code>img/img_align_celeba.zip</code> and <code>Anno/list_attr_celeba.txt</code>. After the download operation is completed;</p> <ul> <li>Please go to <code>fbm-researcher/notebooks/data/Celeba</code> from the folder your are running the Fed-BioMed <code>Researcher</code>.</li> <li>Create <code>Celeba_raw/raw</code> directory and copy the <code>list_attr_celeba.txt</code> file.</li> <li>Extract the zip file <code>img_align_celeba.zip</code></li> </ul> <p>Your folder should be same as the tree below;</p> <pre><code>Celeba\n    README.md\n    create_node_data.py    \n    .gitignore\n \n    Celeba_raw\n        raw\n            list_attr_celeba.txt\n            img_align_celeba.zip\n            img_align_celeba\n              lots of images \n</code></pre> <p>The dataset has to be processed and split to create three distinct datasets for Node 1, Node 2, and Node 3. You can do it easily by running the following script in your notebook. If you are working in a different directory than the <code>fbm-researcher/notebooks</code>, please make sure that you define/modify the correct paths in the following example.</p> <p>Running the following scripts might take some time, please be patient.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#21-configuration-steps","title":"2.1. Configuration Steps\u00b6","text":"<p>It is necessary to previously configure at least a node:</p> <ol> <li><code>fedbiomed node --path ./my-first-node dataset add</code> or <code>fedbiomed node --path ./my-second-node dataset add</code></li> </ol> <ul> <li>For each <code>Node</code>, select option <code>4</code> (images) to add an image dataset to the node</li> <li>Add a name and the tag for the dataset (tag should contain '#celeba' as it is the tag used for this training) and finally add the description</li> <li>Pick a data folder from the 3 generated datasets inside <code>fbm-researcher/data/Celeba/celeba_preprocessed</code> (eg: <code>data_node_1</code>) or direclty in the <code>Node</code> <code>data</code> folder</li> <li>Data must have been added (if you get a warning saying that data must be unique is because it's been already added)</li> </ul> <ol> <li>Check that your data has been added by executing <code>fedbiomed node --path &lt;component-directory&gt; dataset list</code></li> <li>Run the node using <code>fedbiomed node --path &lt;component-directory&gt; start</code>. Wait until you get <code>Starting task manager</code>. it means you are online.</li> </ol> <p>After the steps above are completed, you will be ready to train your classification model on two different nodes.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#3-defining-custom-pytorch-model-and-training-plan","title":"3. Defining Custom PyTorch Model and Training Plan\u00b6","text":""},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#4-training-federated-model","title":"4. Training Federated Model\u00b6","text":"<p>To provide training orchestration over two nodes we need to define an experiment which:</p> <ul> <li>searches nodes serving data for the <code>tags</code>,</li> <li>defines the local training on nodes with the training plan saved in <code>training_plan_path</code>, and federates all local updates at each round with <code>aggregator</code></li> <li>runs training for <code>round_limit</code>.</li> </ul> <p>You can visit user guide to know much more about experiment.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#loading-training-parameters","title":"Loading Training Parameters\u00b6","text":"<p>After all the rounds have been completed, you retrieve the aggregated parameters from the last round and load them.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#5-testing-federated-model","title":"5. Testing Federated Model\u00b6","text":"<p>We will define a testing routine to extract the accuracy metrics on the testing dataset. We will use the dataset that has been extracted into <code>data_node_3</code>.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#conclusions","title":"Conclusions\u00b6","text":"<p>In this tutorial, running a custom model on Fed-BioMed (by wrapping it in a custom training plan) for the PyTorch framework has been explained. Because the examples are designed for the development environment, we have been running nodes in the same host machine. In production, the nodes that you need to use to train your model will serve in remote servers. Please check out how to deploy Nodes in a production environment.</p>"},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/","title":"PyTorch Used Cars Dataset Example","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport pandas as pd\nfrom fedbiomed.researcher.config import config\n# Optional: to be able use different researcher configuration.\n# config.load(root=&lt;different-component-root&gt;) \n\n\nroot_dir = config.root\n\naudi = pd.read_csv(os.path.join(root_dir,'notebooks','data','UsedCars', 'raw', 'audi.csv'))\nbmw = pd.read_csv(os.path.join(root_dir,'notebooks', 'data', 'UsedCars', 'raw', 'bmw.csv'))\nford = pd.read_csv(os.path.join(root_dir,'notebooks', 'data', 'UsedCars', 'raw', \"ford.csv\"))\n</pre> import os import pandas as pd from fedbiomed.researcher.config import config # Optional: to be able use different researcher configuration. # config.load(root=)    root_dir = config.root  audi = pd.read_csv(os.path.join(root_dir,'notebooks','data','UsedCars', 'raw', 'audi.csv')) bmw = pd.read_csv(os.path.join(root_dir,'notebooks', 'data', 'UsedCars', 'raw', 'bmw.csv')) ford = pd.read_csv(os.path.join(root_dir,'notebooks', 'data', 'UsedCars', 'raw', \"ford.csv\")) <p>Drop columns for car <code>model</code> &amp; <code>fuelType</code> as labels are not consistent across files.</p> In\u00a0[\u00a0]: Copied! <pre>audi.drop(columns = ['model','fuelType'], inplace = True)\nbmw.drop(columns = ['model','fuelType'], inplace = True)\nford.drop(columns = ['model','fuelType'], inplace = True)\n</pre> audi.drop(columns = ['model','fuelType'], inplace = True) bmw.drop(columns = ['model','fuelType'], inplace = True) ford.drop(columns = ['model','fuelType'], inplace = True) <p>Label encoding for <code>transmission</code> column</p> In\u00a0[\u00a0]: Copied! <pre>audi['transmission'] = audi['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\nbmw['transmission'] = bmw['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\nford['transmission'] = ford['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n</pre> audi['transmission'] = audi['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3}) bmw['transmission'] = bmw['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3}) ford['transmission'] = ford['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3}) <p>Now, we can save our new CSV files into <code>data/UsedCars</code> directory</p> In\u00a0[\u00a0]: Copied! <pre>audi.to_csv(os.path.join(root_dir, 'notebooks', 'data', 'UsedCars', 'audi_transformed.csv'),header = True,index= False)\nbmw.to_csv(os.path.join(root_dir, 'notebooks', 'data', 'UsedCars', 'bmw_transformed.csv'),header = True,index= False)\nford.to_csv(os.path.join(root_dir, 'notebooks', 'data', 'UsedCars', 'ford_transformed.csv'),header = True,index= False)\n</pre> audi.to_csv(os.path.join(root_dir, 'notebooks', 'data', 'UsedCars', 'audi_transformed.csv'),header = True,index= False) bmw.to_csv(os.path.join(root_dir, 'notebooks', 'data', 'UsedCars', 'bmw_transformed.csv'),header = True,index= False) ford.to_csv(os.path.join(root_dir, 'notebooks', 'data', 'UsedCars', 'ford_transformed.csv'),header = True,index= False) <p>Before creating an experiment, we need to define training plan class.</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\n\n\n# Here we define the training plan to be used for the experiment.\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    # Model\n    def init_model(self):\n        model_args = self.model_args()\n        model = self.Net(model_args)\n        return model\n\n    # Dependencies\n    def init_dependencies(self):\n        deps = [\"from torch.utils.data import Dataset\",\n                \"import pandas as pd\"]\n        return deps\n\n    # network\n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.in_features = model_args['in_features']\n            self.out_features = model_args['out_features']\n            self.fc1 = nn.Linear(self.in_features, 5)\n            self.fc2 = nn.Linear(5, self.out_features)\n\n        def forward(self, x):\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.fc2(x)\n            return x\n\n    def training_step(self, data, target):\n        output = self.model().forward(data).float()\n        criterion = torch.nn.MSELoss()\n        loss   = torch.sqrt(criterion(output, target.unsqueeze(1)))\n        return loss\n\n    class csv_Dataset(Dataset):\n    # Here we define a custom Dataset class inherited from the general torch Dataset class\n    # This class takes as argument a .csv file path and creates a torch Dataset\n        def __init__(self, dataset_path, x_dim):\n            self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)\n            x_train = self.input_file.loc[:,('year','transmission','mileage','tax','mpg','engineSize')].values\n            y_train = self.input_file.loc[:,'price'].values\n            self.X_train = torch.from_numpy(x_train).float()\n            self.Y_train = torch.from_numpy(y_train).float()\n\n        def __len__(self):\n            return len(self.Y_train)\n\n        def __getitem__(self, idx):\n\n            return (self.X_train[idx], self.Y_train[idx])\n\n    def training_data(self):\n    # The training_data creates the Dataloader to be used for training in the general class TorchTrainingPlan of fedbiomed\n        dataset = self.csv_Dataset(self.dataset_path, self.model_args()[\"in_features\"])\n        train_kwargs = { 'shuffle': True}\n        return DataManager(dataset=dataset , **train_kwargs)\n</pre> import pandas as pd import torch import torch.nn as nn import torch.nn.functional as F  from torch.utils.data import Dataset from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager   # Here we define the training plan to be used for the experiment. class MyTrainingPlan(TorchTrainingPlan):      # Model     def init_model(self):         model_args = self.model_args()         model = self.Net(model_args)         return model      # Dependencies     def init_dependencies(self):         deps = [\"from torch.utils.data import Dataset\",                 \"import pandas as pd\"]         return deps      # network     class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.in_features = model_args['in_features']             self.out_features = model_args['out_features']             self.fc1 = nn.Linear(self.in_features, 5)             self.fc2 = nn.Linear(5, self.out_features)          def forward(self, x):             x = self.fc1(x)             x = F.relu(x)             x = self.fc2(x)             return x      def training_step(self, data, target):         output = self.model().forward(data).float()         criterion = torch.nn.MSELoss()         loss   = torch.sqrt(criterion(output, target.unsqueeze(1)))         return loss      class csv_Dataset(Dataset):     # Here we define a custom Dataset class inherited from the general torch Dataset class     # This class takes as argument a .csv file path and creates a torch Dataset         def __init__(self, dataset_path, x_dim):             self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)             x_train = self.input_file.loc[:,('year','transmission','mileage','tax','mpg','engineSize')].values             y_train = self.input_file.loc[:,'price'].values             self.X_train = torch.from_numpy(x_train).float()             self.Y_train = torch.from_numpy(y_train).float()          def __len__(self):             return len(self.Y_train)          def __getitem__(self, idx):              return (self.X_train[idx], self.Y_train[idx])      def training_data(self):     # The training_data creates the Dataloader to be used for training in the general class TorchTrainingPlan of fedbiomed         dataset = self.csv_Dataset(self.dataset_path, self.model_args()[\"in_features\"])         train_kwargs = { 'shuffle': True}         return DataManager(dataset=dataset , **train_kwargs) In\u00a0[\u00a0]: Copied! <pre># model parameters\nmodel_args = {\n    'in_features': 6,\n    'out_features': 1\n}\n\n# training parameters\ntraining_args = {\n    'loader_args': { 'batch_size': 40, },\n    'optimizer_args': {\n          'lr': 1e-3\n    },\n    'epochs': 2,\n#    'batch_maxnum': 2,  # can be used to debugging to limit the number of batches per epoch\n#    'log_interval': 1,  # output a logging message every log_interval batches\n}\n\ntags =  ['#UsedCars']\nrounds = 5\n</pre> # model parameters model_args = {     'in_features': 6,     'out_features': 1 }  # training parameters training_args = {     'loader_args': { 'batch_size': 40, },     'optimizer_args': {           'lr': 1e-3     },     'epochs': 2, #    'batch_maxnum': 2,  # can be used to debugging to limit the number of batches per epoch #    'log_interval': 1,  # output a logging message every log_interval batches }  tags =  ['#UsedCars'] rounds = 5 <p>The other arguments that should be passed to the experiment is the training plan class which is <code>MyTrainingPlan</code>.</p> <p>You should also indicate which method should be chosen to aggregate model parameters after every round. The basic federation scheme is federated averaging, implemented in Fed-BioMed in the class <code>FedAverage</code>. You can also visit aggregation documentation to have more information about aggregation process.</p> <p>Since we are going to use every <code>Node</code> that has <code>UsedCars</code> datasets, the <code>node_selection_strategy</code> should be <code>None</code> which means that every <code>Node</code> will be part of the federated training.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\n\nexp = Experiment(tags=tags,\n                 training_plan_class=MyTrainingPlan,\n                 model_args=model_args,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage   exp = Experiment(tags=tags,                  training_plan_class=MyTrainingPlan,                  model_args=model_args,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) <p>The experiment also receives loss values during training on each node. In Fed-BioMed, it is possible to use a tensorboard to display loss values during training. Please refer to Fed-BioMed's tensorboard documentation to enable tensorboard.</p> <p>Let's start the experiment. By default, this function doesn't stop until all the <code>round_limit</code> rounds are done for all the nodes.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the training rounds : \", exp.training_replies().keys())\n</pre> print(\"\\nList the training rounds : \", exp.training_replies().keys()) <p>Now, let's see how training details can be accessed via <code>training_replies()</code>. The following parameters will be inspected;</p> <ul> <li><code>rtime_training</code> : Real-time (clock time) spent in the training function on the node</li> <li><code>ptime_training</code>: Process time (user and system CPU) spent in the training function on the node</li> <li><code>rtime_total</code>   : Real-time (clock time) spent in the researcher between sending training requests and handling the responses</li> </ul> <p>Note: The following code accesses the training replies of the last round of the experiment.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the training rounds : \", exp.training_replies().keys())\n\nprint(\"\\nList the nodes for the last training round and their timings : \")\nround_data = exp.training_replies()[rounds - 1]\nfor r in round_data.values():\n    print(\"\\t- {id} :\\\n    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = r['node_id'],\n        rtraining = r['timing']['rtime_training'],\n        ptraining = r['timing']['ptime_training'],\n        rtotal = r['timing']['rtime_total']))\nprint('\\n')\n</pre> print(\"\\nList the training rounds : \", exp.training_replies().keys())  print(\"\\nList the nodes for the last training round and their timings : \") round_data = exp.training_replies()[rounds - 1] for r in round_data.values():     print(\"\\t- {id} :\\     \\n\\t\\trtime_training={rtraining:.2f} seconds\\     \\n\\t\\tptime_training={ptraining:.2f} seconds\\     \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = r['node_id'],         rtraining = r['timing']['rtime_training'],         ptraining = r['timing']['ptime_training'],         rtotal = r['timing']['rtime_total'])) print('\\n') In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\nprint(\"\\nAccess the federated params for the last training round :\")\nprint(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n</pre> print(\"\\nList the training rounds : \", exp.aggregated_params().keys()) print(\"\\nAccess the federated params for the last training round :\") print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())  In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().model()\n</pre> exp.training_plan().model() In\u00a0[\u00a0]: Copied! <pre>fed_model = exp.training_plan().model()\nfed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n</pre> fed_model = exp.training_plan().model() fed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params']) In\u00a0[\u00a0]: Copied! <pre>fed_model\n</pre>  fed_model In\u00a0[\u00a0]: Copied! <pre>test_dataset_path = os.path.join(root_dir, 'notebooks', 'data', 'UsedCars', 'ford_transformed.csv')\n</pre> test_dataset_path = os.path.join(root_dir, 'notebooks', 'data', 'UsedCars', 'ford_transformed.csv') In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\n\ndef cal_rmse(actual, prediction):\n    return ((actual- prediction)**2).mean()**0.5\n\ndef testing_rmse(model, data_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    device = 'cpu'\n    preds = []\n    with torch.no_grad():\n        for data, target in data_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            preds.append(output.numpy().flatten())\n    rmse = cal_rmse(data_loader.dataset.Y_train.numpy(),np.hstack(preds))\n    return rmse\n</pre> import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset, DataLoader import pandas as pd  def cal_rmse(actual, prediction):     return ((actual- prediction)**2).mean()**0.5  def testing_rmse(model, data_loader):     model.eval()     test_loss = 0     correct = 0     device = 'cpu'     preds = []     with torch.no_grad():         for data, target in data_loader:             data, target = data.to(device), target.to(device)             output = model(data)             preds.append(output.numpy().flatten())     rmse = cal_rmse(data_loader.dataset.Y_train.numpy(),np.hstack(preds))     return rmse <p>We also need to create a Dataset class for PyTorch data loader.</p> In\u00a0[\u00a0]: Copied! <pre>class csv_Dataset(Dataset):\n        def __init__(self, dataset_path):\n            self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)\n            x_train = self.input_file.loc[:,('year','transmission','mileage','tax','mpg','engineSize')].values\n            y_train = self.input_file.loc[:,'price'].values\n            self.X_train = torch.from_numpy(x_train).float()\n            self.Y_train = torch.from_numpy(y_train).float()\n\n        def __len__(self):            \n            return len(self.Y_train)\n\n        def __getitem__(self, idx):\n\n            return (self.X_train[idx], self.Y_train[idx])\n</pre> class csv_Dataset(Dataset):         def __init__(self, dataset_path):             self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)             x_train = self.input_file.loc[:,('year','transmission','mileage','tax','mpg','engineSize')].values             y_train = self.input_file.loc[:,'price'].values             self.X_train = torch.from_numpy(x_train).float()             self.Y_train = torch.from_numpy(y_train).float()          def __len__(self):                         return len(self.Y_train)          def __getitem__(self, idx):              return (self.X_train[idx], self.Y_train[idx]) In\u00a0[\u00a0]: Copied! <pre>dataset = csv_Dataset(test_dataset_path)\ntrain_kwargs = { 'shuffle': True}\ndata_loader = DataLoader(dataset, **train_kwargs)\n</pre> dataset = csv_Dataset(test_dataset_path) train_kwargs = { 'shuffle': True} data_loader = DataLoader(dataset, **train_kwargs) In\u00a0[\u00a0]: Copied! <pre>rmse = testing_rmse(fed_model, data_loader)\nprint(rmse)\n</pre> rmse = testing_rmse(fed_model, data_loader) print(rmse) In\u00a0[\u00a0]: Copied! <pre>!pip install matplotlib\n</pre> !pip install matplotlib In\u00a0[\u00a0]: Copied! <pre>errors = []\n\nfor i in range(rounds):\n    fed_model = exp.training_plan().model()\n    fed_model.load_state_dict(exp.aggregated_params()[i]['params'])\n    loss = testing_rmse(fed_model, data_loader)\n    errors.append(loss)\n</pre> errors = []  for i in range(rounds):     fed_model = exp.training_plan().model()     fed_model.load_state_dict(exp.aggregated_params()[i]['params'])     loss = testing_rmse(fed_model, data_loader)     errors.append(loss) In\u00a0[\u00a0]: Copied! <pre>### Plotting \nimport matplotlib.pyplot as plt\nplt.plot(errors, label = 'Federated Test Loss')\nplt.xlabel('Round')\nplt.ylabel('Loss')\nplt.legend()\n</pre> ### Plotting  import matplotlib.pyplot as plt plt.plot(errors, label = 'Federated Test Loss') plt.xlabel('Round') plt.ylabel('Loss') plt.legend()"},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#pytorch-used-cars-dataset-example","title":"PyTorch Used Cars Dataset Example\u00b6","text":""},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#1-introduction","title":"1. Introduction\u00b6","text":"<p>This tutorial focuses on how to train a federated regression model on Non-IID dataset using PyTorch framework. We will be working on the Used Cars dataset to perform federated learning. The sections of this tutorial are presented as follows;</p> <ul> <li>Dataset Preparation;</li> <li>Node Configurations;</li> <li>Create an Experiment to Train a Model;</li> <li>Testing Federated Model.</li> </ul>"},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#2-dataset-preparation","title":"2. Dataset Preparation\u00b6","text":"<p>In this tutorial, we will be using the Used Cars dataset. The goal of the model will be to predict the price of the car based on given features.</p> <p>You can download the dataset from here. To be able to download this dataset you need to have a Kaggle account. After downloading, you can create folders for the dataset in the Fed-BioMed <code>Researcher</code> <code>fbm-researcher/notebooks/data</code> directory.</p> <pre>cd /path/to/fedbiomed/directory\nfedbiomed component create -c researcher\nmkdir fbm-researcher/notebooks/data/UsedCars &amp;&amp; mkdir fbm-researcher/notebooks/data/UsedCars/raw\n</pre> <p>You can extract CSV files in the zip file into <code>fbm-researcher/notebooks/data/UsedCars/raw</code>. Your file tree should be like the tree below;</p> <pre>\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 UsedCars\n\u2502       \u2514\u2500\u2500 raw\n\u2502           \u251c\u2500\u2500 audi.csv\n\u2502           \u251c\u2500\u2500 bmw.csv\n\u2502           \u251c\u2500\u2500 cclass.csv\n\u2502           \u251c\u2500\u2500 focus.csv\n\u2502           \u251c\u2500\u2500 ford.csv\n\u2502           \u251c\u2500\u2500 hyundi.csv\n\u2502           \u251c\u2500\u2500 merc.csv\n\u2502           \u251c\u2500\u2500 skoda.csv\n\u2502           \u251c\u2500\u2500 toyota.csv\n\u2502           \u251c\u2500\u2500 unclean cclass.csv\n\u2502           \u251c\u2500\u2500 unclean focus.csv\n\u2502           \u251c\u2500\u2500 vauxhall.csv\n\u2502           \u2514\u2500\u2500 vw.csv\n</pre>"},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#21-selecting-csv-dataset-for-each-node","title":"2.1 Selecting CSV Dataset for Each Node\u00b6","text":"<p>Each CSV dataset contains features for different car brands. It is a good example for applying federated learning through each dataset if we assume that these datasets will be stored in different locations. We will be working on 3 datasets that are <code>audi.csv</code>, <code>bmw.csv</code>, and <code>ford.csv</code>. We will deploy <code>audi.csv</code> and <code>bmw.csv</code> on different nodes and use <code>ford.csv</code> for final testing at the central researcher using the model trained on two nodes.</p>"},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#22-preprocessing","title":"2.2 Preprocessing\u00b6","text":"<p>Before deploying datasets we need to apply some preprocessing to make them ready for the federated training. Since car <code>model</code> and <code>fuelType</code> features are not consistent across the dataset, we can drop them. We also need to apply label encoding for the <code>transmission</code> feature.</p> <p>Note: Dropping and encoding columns can be also done in the <code>training_data</code> method of <code>TrainingPlan</code> but it is always better to deploy clean/prepared datasets in the nodes.</p> <p>Let's starting with loading CSV datasets using <code>pd.read_csv</code> API. Please make sure that you have launched your Jupyter notebook using the command <code>fedbiomed researcher start</code> so you can follow code examples without changing file paths.</p>"},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#3-node-configurations","title":"3. Node Configurations\u00b6","text":"<p>We will deploy the <code>audi_transformed.csv</code> and <code>bmw_transformed.csv</code> datasets on different nodes.</p> <ol> <li><p>Configuring First Node</p> <ul> <li>Run <code>fedbiomed component create -c node --path ./node-audi</code></li> <li>Move <code>audi_transformed.csv</code> from <code>fbm-researcher/notebooks/data/UsedCars</code> to the <code>Node</code> <code>node-audi/data</code> with the following command:<ul> <li><code>mv fbm-researcher/notebooks/data/UsedCars/audi_transformed.csv node-audi/data/</code></li> </ul> </li> <li>Run <code>fedbiomed node --path ./node-audi dataset add</code></li> <li>Select option 1 to add a csv file (audi_transformed.csv)</li> <li>Choose a name for dataset, For example <code>Used-Cars-Audi</code></li> <li>Choose tag for the dataset. This part is important because we will be sending search request to nodes with this specified tag. Therefore please type <code>#UsedCars</code> and enter.</li> <li>Enter a description for the dataset</li> <li>Select the audi_transformed.csv file in the file selection window</li> </ul> </li> <li><p>Configuring Second Node</p> <ul> <li>Run <code>fedbiomed component create -c node --path ./node-bmw</code></li> <li>Move <code>bmw_transformed.csv</code> from <code>fbm-researcher/notebooks/data/UsedCars</code> to the <code>Node</code> <code>node-bmw/data</code> with the following command:<ul> <li><code>mv fbm-researcher/notebooks/data/UsedCars/bmw_transformed.csv node-bmw/data/</code></li> </ul> </li> <li>Run <code>fedbiomed node --path ./node-bmw dataset add</code></li> <li>Select option 1 to add a csv file (bmw_transformed.csv)</li> <li>Choose a name for dataset, For example <code>Used-Cars-BMW</code></li> <li>Since we entered the tag as <code>#UsedCars</code>, we need to use the same tag for this one too.</li> <li>Enter a description for the dataset</li> <li>Select the bmw_trasnformed.csv file in the file selection window</li> </ul> </li> <li><p>Starting Nodes Please run the following command to start the node that has the <code>audi</code> dataset.</p> <pre>fedbiomed node --path ./node-audi start\n</pre> <p>Please open a new terminal window to start the node that has the <code>bmw</code> dataset</p> <pre>fedbiomed node --path ./node-bmw start\n</pre> </li> </ol>"},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#4-create-an-experiment-to-train-a-model","title":"4. Create an Experiment to Train a Model\u00b6","text":""},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#41-defining-arguments-for-the-experiment","title":"4.1 Defining Arguments for The Experiment\u00b6","text":"<p>An experiment is a class that orchestrates the training processes that run on different nodes. The experiment has to be initialized with necessary arguments to inform nodes about the training plan. In this case, first, you need to define <code>model_arg</code>, <code>training_args</code>, <code>tags</code>, and <code>round</code>.</p> <p>Please visit experiment documentation to get detailed information about the experiment class.</p>"},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#42-what-happens-during-the-initialization-of-an-experiment","title":"4.2 What happens during the initialization of an experiment?\u00b6","text":"<ol> <li>The experiment searches for nodes that have datasets that have been saved with the <code>#UsedCars</code> tag.</li> <li>The experiment is set up to manage the training process across the nodes with the given arguments</li> </ol> <p>Note: It is possible to send search requests to only specified nodes with the <code>Nodes</code> argument of the experiment. Please visit listing datasets and selecting nodes documentation for more information.</p>"},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#43-extracting-training-results","title":"4.3 Extracting Training Results\u00b6","text":""},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#timing","title":"Timing\u00b6","text":"<p>Training replies for each round are available in <code>exp.training_replies()</code> (index 0 to (<code>rounds</code> - 1) ). You can display the keys of each round by running the following script.</p>"},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#federated-parameters","title":"Federated Parameters\u00b6","text":"<p>Federated model parameters for each round are available in <code>exp.aggregated_params()</code> (index 0 to (<code>rounds</code> - 1) ). For example, you can easily view the federated parameters for the last round of the experiment:</p>"},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#5-testing-federated-model","title":"5. Testing Federated Model\u00b6","text":"<p>In this section, we will create a test function to obtain RMSE on <code>ford_transformed.csv</code> dataset by using federated model.</p>"},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#51-aggregated-parameters","title":"5.1 Aggregated Parameters\u00b6","text":"<p><code>model_instance</code> returns the model that we have created in the previous section. You can load specific aggregated parameters which are obtained in the round. Thereafter, it will make the predictions using those parameters. The last round gives the last aggregated model parameters which represents the final model.</p>"},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#52-creating-a-test-function","title":"5.2 Creating A Test Function\u00b6","text":"<p>Let's create a test function that returns <code>rmse</code>.</p>"},{"location":"tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/#53-plotting-rmse-values-of-each-round","title":"5.3 Plotting RMSE Values of Each Round\u00b6","text":""},{"location":"tutorials/pytorch/04-Aggregation_in_Fed-BioMed/","title":"PyTorch aggregation methods in Fed-BioMed","text":"In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\nfrom fedbiomed.common.data import DataManager\nfrom fedbiomed.common.data.flamby_dataset import FlambyDataset\n       \n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args):\n        return Baseline()\n\n    def init_optimizer(self, optimizer_args):\n        return Optimizer(self.model().parameters(), lr=optimizer_args[\"lr\"])\n\n    def init_dependencies(self):\n        return [\"from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\",\n                \"from fedbiomed.common.data import DataManager\",\n                \"from fedbiomed.common.data.flamby_dataset import FlambyDataset\"]\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        return BaselineLoss().forward(output, target)\n\n    def training_data(self):\n        dataset = FlambyDataset()\n        loader_arguments = { 'shuffle': True}\n        return DataManager(dataset, **loader_arguments)\n</pre> from fedbiomed.common.training_plans import TorchTrainingPlan from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer from fedbiomed.common.data import DataManager from fedbiomed.common.data.flamby_dataset import FlambyDataset          class MyTrainingPlan(TorchTrainingPlan):     def init_model(self, model_args):         return Baseline()      def init_optimizer(self, optimizer_args):         return Optimizer(self.model().parameters(), lr=optimizer_args[\"lr\"])      def init_dependencies(self):         return [\"from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\",                 \"from fedbiomed.common.data import DataManager\",                 \"from fedbiomed.common.data.flamby_dataset import FlambyDataset\"]      def training_step(self, data, target):         output = self.model().forward(data)         return BaselineLoss().forward(output, target)      def training_data(self):         dataset = FlambyDataset()         loader_arguments = { 'shuffle': True}         return DataManager(dataset, **loader_arguments)  <p>We define hereafter parameters for <code>Experiment</code> to be used with vanilla <code>FedAverage</code></p> In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'random_seed': 1234,\n    'loader_args': { 'batch_size': 8, },\n    'optimizer_args': {\n        \"lr\" : 1e-3\n    },\n    'dry_run': False,\n    'num_updates': 50\n}\n</pre> model_args = {}  training_args = {     'random_seed': 1234,     'loader_args': { 'batch_size': 8, },     'optimizer_args': {         \"lr\" : 1e-3     },     'dry_run': False,     'num_updates': 50 }  <p>Activate Tensorboard</p> In\u00a0[\u00a0]: Copied! <pre>%load_ext tensorboard\n</pre> %load_ext tensorboard In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.config import config\nimport os\n\n\nfedavg_tensorboard_dir = os.path.join(config.root, 'fedavg_runs')\nos.makedirs(fedavg_tensorboard_dir, exist_ok=True)\nconfig.vars['TENSORBOARD_RESULTS_DIR'] = fedavg_tensorboard_dir\n</pre> from fedbiomed.researcher.config import config import os   fedavg_tensorboard_dir = os.path.join(config.root, 'fedavg_runs') os.makedirs(fedavg_tensorboard_dir, exist_ok=True) config.vars['TENSORBOARD_RESULTS_DIR'] = fedavg_tensorboard_dir In\u00a0[\u00a0]: Copied! <pre>tensorboard --logdir \"$fedavg_tensorboard_dir\"\n</pre> tensorboard --logdir \"$fedavg_tensorboard_dir\" <p>We then import <code>FedAverage</code> <code>Aggregator</code> from Fed-BioMed's <code>Aggregators</code></p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators import FedAverage\nfrom fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n\ntags =  ['flixi']\n\nrounds = 3\n\n\nexp_fed_avg = Experiment()\nexp_fed_avg.set_training_plan_class(training_plan_class=MyTrainingPlan)\nexp_fed_avg.set_model_args(model_args=model_args)\nexp_fed_avg.set_training_args(training_args=training_args)\nexp_fed_avg.set_tags(tags = tags)\nexp_fed_avg.set_training_data(training_data=None, from_tags=True)\nexp_fed_avg.set_aggregator(aggregator=FedAverage())\nexp_fed_avg.set_strategy(node_selection_strategy=DefaultStrategy())\nexp_fed_avg.set_round_limit(rounds)\nexp_fed_avg.set_tensorboard(True)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators import FedAverage from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy  tags =  ['flixi']  rounds = 3   exp_fed_avg = Experiment() exp_fed_avg.set_training_plan_class(training_plan_class=MyTrainingPlan) exp_fed_avg.set_model_args(model_args=model_args) exp_fed_avg.set_training_args(training_args=training_args) exp_fed_avg.set_tags(tags = tags) exp_fed_avg.set_training_data(training_data=None, from_tags=True) exp_fed_avg.set_aggregator(aggregator=FedAverage()) exp_fed_avg.set_strategy(node_selection_strategy=DefaultStrategy()) exp_fed_avg.set_round_limit(rounds) exp_fed_avg.set_tensorboard(True) In\u00a0[\u00a0]: Copied! <pre>exp_fed_avg.run(increase=True)\n</pre> exp_fed_avg.run(increase=True) <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp_fed_avg.training_plan().export_model('./trained_model')\n</pre> exp_fed_avg.training_plan().export_model('./trained_model') In\u00a0[\u00a0]: Copied! <pre># let's create a new folder for storing tensorbaord results for FedProx aggregator\nimport os\nfrom fedbiomed.researcher.config import config\n\n\nfedprox_tensorboard_dir = os.path.join(config.root, 'fedprox_runs')\nos.makedirs(fedprox_tensorboard_dir, exist_ok=True)\n\nconfig.vars['TENSORBOARD_RESULTS_DIR'] = fedprox_tensorboard_dir\n</pre> # let's create a new folder for storing tensorbaord results for FedProx aggregator import os from fedbiomed.researcher.config import config   fedprox_tensorboard_dir = os.path.join(config.root, 'fedprox_runs') os.makedirs(fedprox_tensorboard_dir, exist_ok=True)  config.vars['TENSORBOARD_RESULTS_DIR'] = fedprox_tensorboard_dir In\u00a0[\u00a0]: Copied! <pre>%reload_ext tensorboard\n</pre> %reload_ext tensorboard In\u00a0[\u00a0]: Copied! <pre>tensorboard --logdir \"$fedprox_tensorboard_dir\"\n</pre> tensorboard --logdir \"$fedprox_tensorboard_dir\" In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args_fedprox = {\n    'random_seed': 1234,\n    'loader_args': { 'batch_size': 8, },\n    'optimizer_args': {\n        \"lr\" : 1e-3\n    },\n    'dry_run': False,\n    'num_updates': 50, \n    'fedprox_mu': .1  # This parameter indicates that we are going to use FedProx\n}\n</pre> model_args = {}  training_args_fedprox = {     'random_seed': 1234,     'loader_args': { 'batch_size': 8, },     'optimizer_args': {         \"lr\" : 1e-3     },     'dry_run': False,     'num_updates': 50,      'fedprox_mu': .1  # This parameter indicates that we are going to use FedProx } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators import FedAverage\nfrom fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n\ntags =  ['flixi']\nrounds = 3\n\nexp_fedprox = Experiment()\n\n\nexp_fedprox.set_training_plan_class(training_plan_class=MyTrainingPlan)\nexp_fedprox.set_model_args(model_args=model_args)\nexp_fedprox.set_training_args(training_args=training_args_fedprox)\nexp_fedprox.set_tags(tags = tags)\nexp_fedprox.set_training_data(training_data=None, from_tags=True)\nexp_fedprox.set_aggregator(aggregator=FedAverage())\nexp_fedprox.set_strategy(node_selection_strategy=DefaultStrategy())\nexp_fedprox.set_round_limit(rounds)\nexp_fedprox.set_tensorboard(True)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators import FedAverage from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy  tags =  ['flixi'] rounds = 3  exp_fedprox = Experiment()   exp_fedprox.set_training_plan_class(training_plan_class=MyTrainingPlan) exp_fedprox.set_model_args(model_args=model_args) exp_fedprox.set_training_args(training_args=training_args_fedprox) exp_fedprox.set_tags(tags = tags) exp_fedprox.set_training_data(training_data=None, from_tags=True) exp_fedprox.set_aggregator(aggregator=FedAverage()) exp_fedprox.set_strategy(node_selection_strategy=DefaultStrategy()) exp_fedprox.set_round_limit(rounds) exp_fedprox.set_tensorboard(True) In\u00a0[\u00a0]: Copied! <pre>exp_fedprox.run(increase=True)\n</pre> exp_fedprox.run(increase=True) <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp_fedprox.training_plan().export_model('./trained_model')\n</pre> exp_fedprox.training_plan().export_model('./trained_model') In\u00a0[\u00a0]: Copied! <pre># let's create a new folder for storing tensorbaord results for SCAFFOLD aggregator\nscaffold_tensorboard_dir = os.path.join(config.root, 'scaffold_runs')\nos.makedirs(scaffold_tensorboard_dir, exist_ok=True)\n\nconfig.vars['TENSORBOARD_RESULTS_DIR'] = scaffold_tensorboard_dir\n</pre> # let's create a new folder for storing tensorbaord results for SCAFFOLD aggregator scaffold_tensorboard_dir = os.path.join(config.root, 'scaffold_runs') os.makedirs(scaffold_tensorboard_dir, exist_ok=True)  config.vars['TENSORBOARD_RESULTS_DIR'] = scaffold_tensorboard_dir In\u00a0[\u00a0]: Copied! <pre>%reload_ext tensorboard\n</pre> %reload_ext tensorboard In\u00a0[\u00a0]: Copied! <pre>tensorboard --logdir \"$scaffold_tensorboard_dir\"\n</pre> tensorboard --logdir \"$scaffold_tensorboard_dir\" In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.aggregators import Scaffold\nfrom fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n\nserver_lr = .8\nexp_scaffold = Experiment()\n\nexp_scaffold.set_training_plan_class(training_plan_class=MyTrainingPlan)\nexp_scaffold.set_model_args(model_args=model_args)\nexp_scaffold.set_training_args(training_args=training_args)\nexp_scaffold.set_tags(tags = tags)\nexp_scaffold.set_training_data(training_data=None, from_tags=True)\nexp_scaffold.set_aggregator(Scaffold(server_lr=server_lr))\nexp_scaffold.set_strategy(node_selection_strategy=DefaultStrategy())\nexp_scaffold.set_round_limit(rounds)\nexp_scaffold.set_tensorboard(True)\n</pre> from fedbiomed.researcher.aggregators import Scaffold from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy  server_lr = .8 exp_scaffold = Experiment()  exp_scaffold.set_training_plan_class(training_plan_class=MyTrainingPlan) exp_scaffold.set_model_args(model_args=model_args) exp_scaffold.set_training_args(training_args=training_args) exp_scaffold.set_tags(tags = tags) exp_scaffold.set_training_data(training_data=None, from_tags=True) exp_scaffold.set_aggregator(Scaffold(server_lr=server_lr)) exp_scaffold.set_strategy(node_selection_strategy=DefaultStrategy()) exp_scaffold.set_round_limit(rounds) exp_scaffold.set_tensorboard(True)  In\u00a0[\u00a0]: Copied! <pre>exp_scaffold.run(increase=True)\n</pre> exp_scaffold.run(increase=True) <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp_scaffold.training_plan().export_model('./trained_model')\n</pre> exp_scaffold.training_plan().export_model('./trained_model') In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.aggregators import Scaffold, FedAverage\nfrom fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n\nserver_lr = .8\nexp_multi_agg = Experiment()\n\n# selecting how many rounds of each aggregator we will perform\nrounds_scaffold = 3\nrounds_fedavg = 1\n\n\nexp_multi_agg.set_training_plan_class(training_plan_class=MyTrainingPlan)\nexp_multi_agg.set_model_args(model_args=model_args)\nexp_multi_agg.set_training_args(training_args=training_args)\nexp_multi_agg.set_tags(tags = tags)\nexp_multi_agg.set_training_data(training_data=None, from_tags=True)\nexp_multi_agg.set_aggregator(Scaffold(server_lr=server_lr))\nexp_multi_agg.set_strategy(node_selection_strategy=DefaultStrategy())\nexp_multi_agg.set_round_limit(rounds_scaffold + rounds_fedavg)\n\n#exp_multi_agg.run(rounds=rounds_scaffold)\n</pre> from fedbiomed.researcher.aggregators import Scaffold, FedAverage from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy  server_lr = .8 exp_multi_agg = Experiment()  # selecting how many rounds of each aggregator we will perform rounds_scaffold = 3 rounds_fedavg = 1   exp_multi_agg.set_training_plan_class(training_plan_class=MyTrainingPlan) exp_multi_agg.set_model_args(model_args=model_args) exp_multi_agg.set_training_args(training_args=training_args) exp_multi_agg.set_tags(tags = tags) exp_multi_agg.set_training_data(training_data=None, from_tags=True) exp_multi_agg.set_aggregator(Scaffold(server_lr=server_lr)) exp_multi_agg.set_strategy(node_selection_strategy=DefaultStrategy()) exp_multi_agg.set_round_limit(rounds_scaffold + rounds_fedavg)  #exp_multi_agg.run(rounds=rounds_scaffold)  In\u00a0[\u00a0]: Copied! <pre>exp_multi_agg.set_aggregator(FedAverage())\nexp_multi_agg.run(rounds=rounds_fedavg)\n</pre> exp_multi_agg.set_aggregator(FedAverage()) exp_multi_agg.run(rounds=rounds_fedavg) <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp_multi_agg.training_plan().export_model('./trained_model')\n</pre> exp_multi_agg.training_plan().export_model('./trained_model') <p>For more advanced Aggregators and Regularizers, like <code>FedOpt</code>, you may be interested by <code>DecLearn</code> optimizers that are compatible with Fed-BioMed and provide more options for Aggregation and Optimization.</p>"},{"location":"tutorials/pytorch/04-Aggregation_in_Fed-BioMed/#pytorch-aggregation-methods-in-fed-biomed","title":"PyTorch aggregation methods in Fed-BioMed\u00b6","text":"<p>Difficulty level: advanced</p>"},{"location":"tutorials/pytorch/04-Aggregation_in_Fed-BioMed/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial focuses on how to deal with heterogeneous dataset by changing its <code>Aggegator</code>. Fed-BioMed provides different methods for Aggregation. Selecting an appropriate Aggregation method can be critical when being confronted to unbalanced /heterogeneous datasets.</p> <p><code>Aggregators</code> provides a way to merge local models sent by <code>Nodes</code> into a global, more generalized model. Please note that designing <code>Nodes</code> sampling <code>Strategies</code> could also help when working on heterogeneous datasets.</p> <p>For more information about <code>Aggregators</code> object in Fed-BioMed, and on how to create your own <code>Aggregator</code>; please see <code>Aggregators</code> in the User Guide</p>"},{"location":"tutorials/pytorch/04-Aggregation_in_Fed-BioMed/#before-you-start","title":"Before you start\u00b6","text":"<p>For this tutorial, we will be using heterogenous Fed-IXI dataset, provided by FLamby. FLamby comes with a few medical datasets that have heterogenous data properties. Please have a look at the notebooks on how to use FLamby in Fed-BioMed tutorials before starting - you will indeed need to set up FLamby before running this tutorial.</p>"},{"location":"tutorials/pytorch/04-Aggregation_in_Fed-BioMed/#1-defining-an-experiment-using-fedaverage-aggregator","title":"1. Defining an <code>Experiment</code> using <code>FedAverage</code> <code>Aggregator</code>\u00b6","text":"<p>First, let's re-use the <code>TorchTrainingPlan</code> that is defined in the FLamby tutorials. FedAveraging has been introduced by McMahan et al. as the first aggregation method in the Federated Learning literature. It does the weighted sum of all <code>Nodes</code> local models parameters in order to obtain a global model:</p> <p>In this tutorial, we will keep the same <code>TrainingPlan</code> (and thus the same model) for all the <code>Experimentations</code>, we will be changing only <code>Aggregators</code></p>"},{"location":"tutorials/pytorch/04-Aggregation_in_Fed-BioMed/#2-defining-an-experiment-using-fedprox-aggregator","title":"2. Defining an <code>Experiment</code> using <code>FedProx</code> <code>Aggregator</code>\u00b6","text":"<p>In order to improve our results, we can change our <code>Aggregator</code>, by changing <code>FedAverage</code> into <code>FedProx</code>. Since <code>FedProx</code> is a <code>FedAverge</code> aggregator with a regularization term, we are re-using <code>FedAverage</code> <code>Aggregator</code> but we will be adding to the <code>training_args</code> <code>fedprox_mu</code>, that is the regularization parameter.</p>"},{"location":"tutorials/pytorch/04-Aggregation_in_Fed-BioMed/#3-defining-an-experiment-using-scaffold-aggregator","title":"3. Defining an <code>Experiment</code> using <code>SCAFFOLD</code> <code>Aggregator</code>\u00b6","text":"<p><code>Scaffold</code> purpose is to limit the so called client drift that may happen when dealing with heterogenous datasset accross <code>Nodes</code>.</p> <p>In order to use <code>Scaffold</code>, we will have to import another <code>Aggregator</code> from <code>fedbiomed.researcher.aggregators</code> module, as you can see below.</p> <p><code>Scaffold</code> takes <code>server_lr</code> and <code>fds</code> the as arguments.</p> <ul> <li><code>server_lr</code> is the server learning rate (in <code>Scaffold</code>, used to perform a gradient descent on global model's updates</li> <li><code>fds</code> is the <code>Federated Dataset</code> containing information about <code>Nodes</code> connected to the network after issuing a <code>TrainRequest</code></li> </ul> <p>Please note that it is possible to use <code>Scaffold</code> with a regularization parameter as suggested in <code>FedProx</code>. For that, you just have to specify <code>fedprox_mu</code> into the <code>training_args</code> dictionary, as shown in the <code>FedProx</code> example</p> <p>Attention: this version of <code>Scaffold</code> exchanges correction terms that are not protected, even when using Secure Aggregation. Please do not use this version of <code>Scaffold</code> under heavy security  constraints.</p>"},{"location":"tutorials/pytorch/04-Aggregation_in_Fed-BioMed/#4-going-further","title":"4. Going further\u00b6","text":"<p>In this tutorial we presented 3 important <code>Aggregators</code> that can be found in the Federated Learning Literature. If you want to create your custom <code>Aggregator</code>, please check our Aggregation User guide</p> <p>You may have noticed that thanks to Fed-BioMed's modular structure, it is possible to alternate from one aggregator to another while conducting an <code>Experiment</code>. For instance, you may start with <code>SCAFFOLD</code> <code>Aggregator</code> for the 3 first rounds, and then switch to <code>FedAverage</code> <code>Aggregator</code> for the remaining rounds, as shown in the example below:</p>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/","title":"Transfer-learning in Fed-BioMed tutorial","text":"In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.requests import Requests\nfrom fedbiomed.researcher.config import config\nreq = Requests(config)\nreq.list()\n</pre> from fedbiomed.researcher.requests import Requests from fedbiomed.researcher.config import config req = Requests(config) req.list() In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torchvision.models.densenet import DenseNet121_Weights\nimport pandas as pd\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\n\nfrom fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n</pre> import torch import torch.nn as nn from torchvision.models.densenet import DenseNet121_Weights import pandas as pd from fedbiomed.common.training_plans import TorchTrainingPlan  from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  In\u00a0[\u00a0]: Copied! <pre>class MyTrainingPlan1(TorchTrainingPlan):\n\n    def init_model(self, model_args):\n        model = models.densenet121(weights=None)  # here model coefficients are set to random weights\n\n        # add the classifier \n        num_classes = model_args['num_classes'] \n        num_ftrs = model.classifier.in_features\n        model.classifier= nn.Sequential(\n            nn.Linear(num_ftrs, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n      \n        return model\n\n    def init_dependencies(self):\n        return [\n            \"from torchvision import datasets, transforms, models\",\n            \"import torch.optim as optim\",\n            \"from torchvision.models import densenet121\"\n        ]\n\n\n    def init_optimizer(self, optimizer_args):        \n        return optim.Adam(self.model().parameters(), lr=optimizer_args[\"lr\"])\n\n    \n    # training data\n    \n    def training_data(self):\n\n        # Transform images and  do data augmentation \n        preprocess = transforms.Compose([\n                transforms.Resize((224,224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n           ])\n    \n        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n        train_kwargs = { 'shuffle': True}\n        return DataManager(dataset=train_data, **train_kwargs)\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss_func = nn.CrossEntropyLoss()\n        loss   = loss_func(output, target)\n        return loss\n</pre> class MyTrainingPlan1(TorchTrainingPlan):      def init_model(self, model_args):         model = models.densenet121(weights=None)  # here model coefficients are set to random weights          # add the classifier          num_classes = model_args['num_classes']          num_ftrs = model.classifier.in_features         model.classifier= nn.Sequential(             nn.Linear(num_ftrs, 512),             nn.ReLU(inplace=True),             nn.Dropout(0.5),             nn.Linear(512, num_classes)         )                return model      def init_dependencies(self):         return [             \"from torchvision import datasets, transforms, models\",             \"import torch.optim as optim\",             \"from torchvision.models import densenet121\"         ]       def init_optimizer(self, optimizer_args):                 return optim.Adam(self.model().parameters(), lr=optimizer_args[\"lr\"])           # training data          def training_data(self):          # Transform images and  do data augmentation          preprocess = transforms.Compose([                 transforms.Resize((224,224)),                 transforms.ToTensor(),                 transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])            ])              train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)         train_kwargs = { 'shuffle': True}         return DataManager(dataset=train_data, **train_kwargs)      def training_step(self, data, target):         output = self.model().forward(data)         loss_func = nn.CrossEntropyLoss()         loss   = loss_func(output, target)         return loss  In\u00a0[\u00a0]: Copied! <pre>training_args = {\n    'loader_args': { 'batch_size': 32, }, \n    'optimizer_args': {'lr': 1e-3}, \n    'epochs': 1, \n    'dry_run': False,  \n    'batch_maxnum': 100, # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n    'random_seed': 1234\n}\n\nmodel_args = {\n    'num_classes': 6, # adapt this number to the number of classes in your dataset\n}\n</pre> training_args = {     'loader_args': { 'batch_size': 32, },      'optimizer_args': {'lr': 1e-3},      'epochs': 1,      'dry_run': False,       'batch_maxnum': 100, # Fast pass for development : only use ( batch_maxnum * batch_size ) samples     'random_seed': 1234 }  model_args = {     'num_classes': 6, # adapt this number to the number of classes in your dataset } In\u00a0[\u00a0]: Copied! <pre>tags =  ['#MEDNIST', '#dataset']\n\nrounds = 1 # adjsut the number of rounds \n\nexp = Experiment(tags=tags,\n                 training_plan_class=MyTrainingPlan1,\n                 model_args=model_args,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage())\n\n# testing section \nfrom fedbiomed.common.metrics import MetricTypes\nexp.set_test_ratio(.1) \nexp.set_test_on_global_updates(True)\nexp.set_test_metric(MetricTypes.ACCURACY)\n\nexp.set_tensorboard(True)\n</pre> tags =  ['#MEDNIST', '#dataset']  rounds = 1 # adjsut the number of rounds   exp = Experiment(tags=tags,                  training_plan_class=MyTrainingPlan1,                  model_args=model_args,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage())  # testing section  from fedbiomed.common.metrics import MetricTypes exp.set_test_ratio(.1)  exp.set_test_on_global_updates(True) exp.set_test_metric(MetricTypes.ACCURACY)  exp.set_tensorboard(True) In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() In\u00a0[\u00a0]: Copied! <pre>#save model \nexp.training_plan().export_model('./training_plan1_densenet_MedNIST')\n</pre> #save model  exp.training_plan().export_model('./training_plan1_densenet_MedNIST') In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.config import config\ntensorboard_dir = config.vars['TENSORBOARD_RESULTS_DIR']\n</pre> from fedbiomed.researcher.config import config tensorboard_dir = config.vars['TENSORBOARD_RESULTS_DIR'] In\u00a0[\u00a0]: Copied! <pre>%load_ext tensorboard\n</pre> %load_ext tensorboard In\u00a0[\u00a0]: Copied! <pre>%tensorboard --logdir \"$tensorboard_dir\"\n</pre> %tensorboard --logdir \"$tensorboard_dir\" In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the training rounds : \", exp.training_replies().keys())\n\nprint(\"\\nList the nodes for the last training round and their timings : \")\nround_data = exp.training_replies()[rounds - 1]\nfor r in round_data.values():\n    print(\"\\t- {id} :\\\n    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = r['node_id'],\n        rtraining = r['timing']['rtime_training'],\n        ptraining = r['timing']['ptime_training'],\n        rtotal = r['timing']['rtime_total']))\nprint('\\n')\n</pre> print(\"\\nList the training rounds : \", exp.training_replies().keys())  print(\"\\nList the nodes for the last training round and their timings : \") round_data = exp.training_replies()[rounds - 1] for r in round_data.values():     print(\"\\t- {id} :\\     \\n\\t\\trtime_training={rtraining:.2f} seconds\\     \\n\\t\\tptime_training={ptraining:.2f} seconds\\     \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = r['node_id'],         rtraining = r['timing']['rtime_training'],         ptraining = r['timing']['ptime_training'],         rtotal = r['timing']['rtime_total'])) print('\\n') In\u00a0[\u00a0]: Copied! <pre>model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', weights=DenseNet121_Weights.DEFAULT)\ntorch.save(model.state_dict(), 'pretrained_model.pt')\ntorch.save(model.state_dict(), 'pretrained_model2.pt')\n</pre> model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', weights=DenseNet121_Weights.DEFAULT) torch.save(model.state_dict(), 'pretrained_model.pt') torch.save(model.state_dict(), 'pretrained_model2.pt') <p>In this experiment I will unfreeze two last block layers and the classifier layers. Other layers will stay frozen (i.e. they will not change during the experiment).</p> <p>I introduce a new argument in <code>model_args</code> called <code>num_unfrozen_blocks</code>. This argument specifies the number of blocks left unfrozen. In DenseNet model, layers are grouped whithin blocks. There is a total of 12 blocks, containing several layers each. In our experiment, we will consider rather freezing blocks of layer than layers.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.training_plans import TorchTrainingPlan\nclass MyTrainingPlan2(TorchTrainingPlan):\n\n    def init_model(self, model_args):\n        model = models.densenet121(weights=None)\n        # let's unfreeze layers of the last dense block\n        num_unfrozen_layer = model_args['num_unfrozen_blocks']\n        for param in model.features[:-num_unfrozen_layer].parameters():\n            param.requires_grad = False\n\n        # add the classifier \n        num_ftrs = model.classifier.in_features\n        num_classes = model_args['num_classes'] \n        model.classifier = nn.Sequential(\n            nn.Linear(num_ftrs, 512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, num_classes)       \n            )\n        \n        return model\n\n    def init_dependencies(self):\n        return [\n            \"from torchvision import datasets, transforms, models\",\n            \"import torch.optim as optim\"\n        ]\n\n\n    def init_optimizer(self, optimizer_args):        \n        return optim.Adam(self.model().parameters(), lr=optimizer_args[\"lr\"])\n\n    def training_data(self):\n        \n        # Custom torch Dataloader for MedNIST data and transform images and perform data augmentation \n       \n        preprocess = transforms.Compose([\n                transforms.Resize((224,224)),  \n                transforms.ToTensor(),\n                transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n           ])\n        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n        train_kwargs = { 'shuffle': True}\n        return DataManager(dataset=train_data, **train_kwargs)\n\n\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss_func = nn.CrossEntropyLoss()\n        loss   = loss_func(output, target)\n        return loss\n</pre> from fedbiomed.common.training_plans import TorchTrainingPlan class MyTrainingPlan2(TorchTrainingPlan):      def init_model(self, model_args):         model = models.densenet121(weights=None)         # let's unfreeze layers of the last dense block         num_unfrozen_layer = model_args['num_unfrozen_blocks']         for param in model.features[:-num_unfrozen_layer].parameters():             param.requires_grad = False          # add the classifier          num_ftrs = model.classifier.in_features         num_classes = model_args['num_classes']          model.classifier = nn.Sequential(             nn.Linear(num_ftrs, 512),             nn.ReLU(inplace=True),             nn.Linear(512, num_classes)                    )                  return model      def init_dependencies(self):         return [             \"from torchvision import datasets, transforms, models\",             \"import torch.optim as optim\"         ]       def init_optimizer(self, optimizer_args):                 return optim.Adam(self.model().parameters(), lr=optimizer_args[\"lr\"])      def training_data(self):                  # Custom torch Dataloader for MedNIST data and transform images and perform data augmentation                  preprocess = transforms.Compose([                 transforms.Resize((224,224)),                   transforms.ToTensor(),                 transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])            ])         train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)         train_kwargs = { 'shuffle': True}         return DataManager(dataset=train_data, **train_kwargs)        def training_step(self, data, target):         output = self.model().forward(data)         loss_func = nn.CrossEntropyLoss()         loss   = loss_func(output, target)         return loss    In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntraining_args = {\n    'loader_args': { 'batch_size': 32, }, \n    'optimizer_args': {'lr': 1e-4}, # You could decrease the learning rate\n    'epochs': 1, # you can increase the epoch's number =10\n    'dry_run': False,\n    'random_seed': 1234,\n    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\nmodel_args={\n    'num_classes': 6,\n    'num_unfrozen_blocks': 2  \n}\ntags =  ['#MEDNIST', '#dataset']\nrounds = 1  # you can increase the rounds's number \n\nexp = Experiment(tags=tags,\n                 training_plan_class=MyTrainingPlan2,\n                 model_args=model_args,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage())\n\nfrom fedbiomed.common.metrics import MetricTypes\nexp.set_test_ratio(.1)\nexp.set_test_on_global_updates(True)\nexp.set_test_metric(MetricTypes.ACCURACY)\n\nexp.set_tensorboard(True)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  training_args = {     'loader_args': { 'batch_size': 32, },      'optimizer_args': {'lr': 1e-4}, # You could decrease the learning rate     'epochs': 1, # you can increase the epoch's number =10     'dry_run': False,     'random_seed': 1234,     'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } model_args={     'num_classes': 6,     'num_unfrozen_blocks': 2   } tags =  ['#MEDNIST', '#dataset'] rounds = 1  # you can increase the rounds's number   exp = Experiment(tags=tags,                  training_plan_class=MyTrainingPlan2,                  model_args=model_args,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage())  from fedbiomed.common.metrics import MetricTypes exp.set_test_ratio(.1) exp.set_test_on_global_updates(True) exp.set_test_metric(MetricTypes.ACCURACY)  exp.set_tensorboard(True)      In\u00a0[\u00a0]: Copied! <pre># here we load the model we have saved with torch-hub weights\n\nexp.training_plan().import_model('pretrained_model.pt')\n</pre> # here we load the model we have saved with torch-hub weights  exp.training_plan().import_model('pretrained_model.pt') In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the training rounds : \", exp.training_replies().keys())\n\nprint(\"\\nList the nodes for the last training round and their timings : \")\nround_data = exp.training_replies()[rounds - 1]\nfor r in round_data.values():\n    print(\"\\t- {id} :\\\n    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = r['node_id'],\n        rtraining = r['timing']['rtime_training'],\n        ptraining = r['timing']['ptime_training'],\n        rtotal = r['timing']['rtime_total']))\nprint('\\n')\n</pre> print(\"\\nList the training rounds : \", exp.training_replies().keys())  print(\"\\nList the nodes for the last training round and their timings : \") round_data = exp.training_replies()[rounds - 1] for r in round_data.values():     print(\"\\t- {id} :\\     \\n\\t\\trtime_training={rtraining:.2f} seconds\\     \\n\\t\\tptime_training={ptraining:.2f} seconds\\     \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = r['node_id'],         rtraining = r['timing']['rtime_training'],         ptraining = r['timing']['ptime_training'],         rtotal = r['timing']['rtime_total'])) print('\\n')  In\u00a0[\u00a0]: Copied! <pre>#save model \nexp.training_plan().export_model('./training_plan2_densenet_MedNIST')\n</pre> #save model  exp.training_plan().export_model('./training_plan2_densenet_MedNIST') In\u00a0[\u00a0]: Copied! <pre>%reload_ext tensorboard\n</pre> %reload_ext tensorboard In\u00a0[\u00a0]: Copied! <pre>%tensorboard --logdir \"$tensorboard_dir\" --port 6007\n</pre> %tensorboard --logdir \"$tensorboard_dir\" --port 6007 <p>You could import your first model from TrainingPlan1 instead of loading the original DenseNet. You could also retrieve the model's features.</p> In\u00a0[\u00a0]: Copied! <pre># import your model from a file\nmodel_features_ = torch.load('./training_plan2_densenet_MedNIST')\nmodel_features_\n</pre> # import your model from a file model_features_ = torch.load('./training_plan2_densenet_MedNIST') model_features_  In\u00a0[\u00a0]: Copied! <pre># unfrozen layers during transfer learning (MyTrainingPlan2)\nmodel_features = exp.training_plan().model()\nmodel_features.features[:-model_args['num_unfrozen_blocks']]\n</pre> # unfrozen layers during transfer learning (MyTrainingPlan2) model_features = exp.training_plan().model() model_features.features[:-model_args['num_unfrozen_blocks']] In\u00a0[\u00a0]: Copied! <pre># Here we check if Layers of the DenseNet model have changed between the initial model and the model extracted\n# from the training plan (after transfer learning)\nmodel_features = exp.training_plan().model()\n\ntable = pd.DataFrame(columns=[\"Layer name\", \"Layer set to frozen\", \"Is Layer changed?\"])\nref_model = torch.load('pretrained_model.pt')  # reloading model downloaded from pytorch hub\n\n\nremove_norm_layers= lambda name : not any([x in name for x in ('norm', 'batch') ])\n    \n\nlayers = list(ref_model.keys())\nours_layers = model_features.features[:-model_args['num_unfrozen_blocks']]\nours_layers = ['features.'+ x for x in ours_layers.state_dict().keys()]\n\n_counter = 0\nfor i, (layer_name, param) in enumerate(model_features.state_dict().items()):\n    if i &gt;= len(layers):\n        continue\n    l = layers[i]\n\n    if remove_norm_layers(l) :\n        r_tensor = ref_model[l]\n        if 'classifier' in layer_name:\n            table.loc[_counter] = [l, l in ours_layers, \"non comparable\"]\n\n        else:\n            t = model_features.get_parameter(l)\n            _is_close = bool(torch.isclose(r_tensor, t).all())\n            table.loc[_counter] = [l, l in ours_layers, not _is_close, ]\n\n    _counter += 1\n</pre> # Here we check if Layers of the DenseNet model have changed between the initial model and the model extracted # from the training plan (after transfer learning) model_features = exp.training_plan().model()  table = pd.DataFrame(columns=[\"Layer name\", \"Layer set to frozen\", \"Is Layer changed?\"]) ref_model = torch.load('pretrained_model.pt')  # reloading model downloaded from pytorch hub   remove_norm_layers= lambda name : not any([x in name for x in ('norm', 'batch') ])       layers = list(ref_model.keys()) ours_layers = model_features.features[:-model_args['num_unfrozen_blocks']] ours_layers = ['features.'+ x for x in ours_layers.state_dict().keys()]  _counter = 0 for i, (layer_name, param) in enumerate(model_features.state_dict().items()):     if i &gt;= len(layers):         continue     l = layers[i]      if remove_norm_layers(l) :         r_tensor = ref_model[l]         if 'classifier' in layer_name:             table.loc[_counter] = [l, l in ours_layers, \"non comparable\"]          else:             t = model_features.get_parameter(l)             _is_close = bool(torch.isclose(r_tensor, t).all())             table.loc[_counter] = [l, l in ours_layers, not _is_close, ]      _counter += 1 In\u00a0[\u00a0]: Copied! <pre># display comaprison table content\ntable\n</pre> # display comaprison table content table  <p>The table displays all layers, the one modified and untouched during the training. <code>\"non comparable\"</code> means layers that have been modified from original model to our use case. Those layers are the classifiying layers.</p>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#transfer-learning-in-fed-biomed-tutorial","title":"Transfer-learning in Fed-BioMed tutorial\u00b6","text":""},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#goal-of-this-tutoriel","title":"Goal of this tutoriel\u00b6","text":"<p>This tutorial shows how to do 2d images classification example on MedNIST dataset using pretrained PyTorch model.</p> <p>The goal of this tutorial is to provide an example of transfer learning methods with Fed-BioMed for medical images classification.</p>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#about-the-model","title":"About the model\u00b6","text":"<p>The model used is Densenet-121 model(\u201cDensely Connected Convolutional Networks\u201d) pretrained on ImageNet dataset. The Pytorch pretrained model Densenet121 to perform image classification on the MedNIST dataset. The goal of this Densenet121 model is to predict the class of <code>MedNIST</code> medical images.</p>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#about-mednist","title":"About MedNIST\u00b6","text":"<p>MedNIST provides an artificial 2d classification dataset created by gathering different medical imaging datasets from TCIA, the RSNA Bone Age Challenge, and the NIH Chest X-ray dataset. The dataset is kindly made available by Dr. Bradley J. Erickson M.D., Ph.D. (Department of Radiology, Mayo Clinic) under the Creative Commons CC BY-SA 4.0 license.</p> <p>MedNIST dataset is downloaded from the resources provided by the project MONAI</p> <p>The dataset MedNIST has 58954 images of size (3, 64, 64) distributed into 6 classes (10000 images per class except for BreastMRI class which has 8954 images). Classes are AbdomenCT, BreastMRI, CXR, ChestCT, Hand, HeadCT. It has the structure:</p> <p>\u2514\u2500\u2500 MedNIST/</p> <pre><code>\u251c\u2500\u2500 AbdomenCT/\n\n\u2514\u2500\u2500 BreastMRI/\n\n\u2514\u2500\u2500 CXR/\n\n\u2514\u2500\u2500 ChestCT/\n\n\u2514\u2500\u2500 Hand/\n\n\u2514\u2500\u2500 HeadCT/   </code></pre>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#transfer-learning","title":"Transfer-learning\u00b6","text":"<p>Transfer learning is a machine learning technique where a model trained on one task is repurposed or adapted for a second related task. Transfer learning uses a pre-trained neural network on a large dataset, as Imagenet is used to train DenseNet model to perform classification of a wide diversity of images.</p> <p>The objective is that the knowledge gained from learning one task can be useful for learning another task (as we do here, the knowledge of DenseNet model trained on ImageNet is used to classify medical images in 6 categories). This is particularly beneficial when the amount of labeled data for the target task is limited, as the pre-trained model has already learned useful features and representations from a large dataset.</p> <p>Transfer learning is typically applied in one of two ways:</p> <ul> <li><p>(I) Feature Extraction: In this approach, the pre-trained model is used as a fixed feature extractor. The earlier layers of the neural network, which capture general features and patterns, are frozen, and only the later layers are replaced or retrained for the new task.</p> </li> <li><p>(II) Fine-tuning: In this approach, the pre-trained model is further trained or partially trained on the new task. This allows the model to adapt its learned representations to the specifics of the new task while retaining some of the knowledge gained from the original task.</p> </li> </ul> <p>In this example, we load on two nodes a sampled dataset ( 500 images and 1000 images) of MedNIST to illustrate   transfer-learning's effectiveness. The sampled dataset is made with a random selection of images and return a sampled dataset with balanced classes, to avoid classification's bias. We will run two independant TrainingPlan experiments, one without transfer-learning and the second with transfer learning. We will compare these two experiments running on DenseNet model with focus on loss value and accuracy as metrics to evaluate the effectiveness of Transfer-learning methods.</p> <p>Nota: This Transfer-Learning example is not to be confused with Federated Transfer Learning-FTL (see for example this paper). The example only showcases here Transfer Learning on a Federated Learning use case.</p>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#1-load-dataset-or-sampled-dataset","title":"1. Load dataset or sampled dataset\u00b6","text":"<ul> <li>In a new Fed-BioMed environment, run the script python: <code>python fbm-researcher/notebooks/transfer-learning/download_sample_of_mednist.py -n 2</code>, with <code>-n 2</code> the number of <code>Nodes</code> you want to create ( for more details about this script, please run <code>python fbm-researcher/notebooks/transfer-learning/download_sample_of_mednist.py --help</code>)</li> <li>The script will ask for each <code>Nodes</code> created the number of samples you want for your dataset. For example you could: Enter 500 the first time the script ask the number of samples, and 1000 the second time Scripts will output component directories for each of <code>Nodes</code>, with configured database, using the following naming convention: <code>node_MedNIST_&lt;i&gt;_sampled</code> where <code>&lt;i&gt;</code> corresponds to the number of Node created. Components will be created in the directory where this script is executed. Eventually, it will add the dataset to the already created <code>Nodes</code>.</li> <li>Finally launch your Nodes  by running: <code>fedbiomed node --path node_MedNIST_1_sampled start</code>.  In another terminal, run <code>fedbiomed node --path node_MedNIST_2_sampled start</code>.</li> </ul> <p>Wait until you get <code>Starting task manager</code>.</p>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#2-launch-the-researcher","title":"2. Launch the researcher\u00b6","text":"<ul> <li>From the root directory of Fed-BioMed, run : <code>fedbiomed researcher start</code></li> <li>It opens the Jupyter notebook.</li> </ul> <p>To make sure that MedNIST dataset is loaded in the node we can send a request to the network to list the available dataset in the node. The list command should output an entry for mednist data.</p>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#import-of-librairies","title":"Import of librairies\u00b6","text":""},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#i-adapt-the-last-layer-to-your-classifications-goal","title":"I- Adapt the last layer to your classification's goal\u00b6","text":"<p>Here we use the DenseNet model that allows classification through 10000 samples. We could adapt this classification's task to the MedNIST dataset by replacing the last layer with our classifier. The <code>model.classifier</code> layer of the <code>DenseNet-121</code> model classifies images through 6 classes, in the Training Plan, by adapting the num_classes value (can be done in through <code>model_args</code> argument).</p>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#data-augmentation","title":"Data augmentation\u00b6","text":"<p>You could perform data augmentation through the preprocess part if you need. Here I show random flip, rotation and crops. You could do the preprocessing of images by doing only transforms.resize, transforms.to_tensor and transforms.normalize, as mentionned in the code below (commented lines).</p>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#i-run-an-expriment-for-images-classification-without-transfer-learning","title":"I. Run an expriment for image's classification without Transfer-learning\u00b6","text":"<p>Here we propose to run as first experiment a TrainingPlan0 with the untrained DenseNet model. Then, we will compare the loss value from the two other experiments allowing Transfer-learning methods.</p> <p>We don't use the pre-trained weights. It is important to adapt learning rate. I propose you to start with lr=1e-4 and we could adapt learning rate according to the metric's evaluation.</p>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#i-1-define-training-plan-experiment","title":"I -1. Define Training plan experiment\u00b6","text":""},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#i-3-run-your-experiment","title":"I - 3. Run your experiment\u00b6","text":""},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#for-example-at-the-end-of-training-experiment-i-obtained","title":"For example,  At the end of training experiment, I obtained\u00b6","text":"<pre><code>fedbiomed INFO - VALIDATION ON GLOBAL UPDATES \n\t\t\t\t\t NODE_ID: node_mednist_1_sampled \n\t\t\t\t\t Round 2 | Iteration: 1/1 (100%) | Samples: 50/50\n \t\t\t\t\t ACCURACY: 0.740000 \n\t\t\t\t\t ---------\n\nfedbiomed INFO - VALIDATION ON GLOBAL UPDATES \n\t\t\t\t\t NODE_ID: node_mednist_2_sampled \n\t\t\t\t\t Round 2 | Iteration: 1/1 (100%) | Samples: 100/100\n \t\t\t\t\t ACCURACY: 0.780000 \n\t\t\t\t\t ---------\n\t\t\t\t\t \n</code></pre>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#i-4-save-your-model","title":"I - 4. Save your model\u00b6","text":"<p>You could save your model to later use it in a new TrainingPlan This save allows to import the model including your layers's modification and weights values.</p>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#i-5-results-in-tensorboard","title":"I - 5. Results in tensorboard\u00b6","text":""},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#i-6-training-timing","title":"I - 6. Training timing\u00b6","text":""},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#ii-run-an-expriment-for-images-classification-using-transfer-learning","title":"II - Run an expriment for image's classification using Transfer-learning\u00b6","text":""},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#ii-1-downloading-the-pretrained-models-weights","title":"II-1. Downloading the pretrained model's weights\u00b6","text":"<p>Here I download and save the model's weights through Torch.hub using the command below in a file <code>'pretrained_model.pt'</code></p>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#ii-2-adapt-the-last-layer-to-your-classifications-goal","title":"II-2. Adapt the last layer to your classification's goal\u00b6","text":"<p>Here we use the DenseNet model that allows classification through 1500 samples (on 2 nodes). We could adapt this classification's task to the MedNIST dataset by replacing the last layer with our classifier. The <code>model.classifier</code> layer of the <code>DenseNet-121</code> model classifies images through 6 classes, in the Training Plan, by adapting the num_classes value (can be done in through <code>model_args</code> argument).</p> <p>The dataset is defined below, after TrainingPlan as previously shown.</p> <p>You could also import the model you saved to perform your second TrainingPlan experiment (let's see below)</p>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#ii-3-run-your-experiment","title":"II - 3. Run your experiment\u00b6","text":""},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#for-example-at-the-end-of-training-experiment-i-obtained","title":"For example,  At the end of training experiment, I obtained :\u00b6","text":"<pre><code>fedbiomed INFO - VALIDATION ON GLOBAL UPDATES \n\t\t\t\t\t NODE_ID: node_mednist_1_sampled \n\t\t\t\t\t Round 2 | Iteration: 1/1 (100%) | Samples: 50/50\n \t\t\t\t\t ACCURACY: 1.0000\n\t\t\t\t\t ---------\n\nfedbiomed INFO - VALIDATION ON GLOBAL UPDATES \n\t\t\t\t\t NODE_ID: node_mednist_2_sampled \n\t\t\t\t\t Round 2 | Iteration: 1/1 (100%) | Samples: 100/100\n \t\t\t\t\t ACCURACY: 1.0000 \n\t\t\t\t\t ---------\n\t\t\t\t\t \n</code></pre>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#ii-4-export-your-model","title":"II -  4. Export your model\u00b6","text":""},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#ii-5-display-losses-on-tensorboard","title":"II - 5. Display losses on Tensorboard\u00b6","text":""},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#ii-6-save-and-import-your-model-and-parameters","title":"II - 6. Save and Import your model and parameters\u00b6","text":""},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#ii-7-check-model-parameters-changedunchanged","title":"II - 7. check model parameters changed/unchanged\u00b6","text":"<p>Here we are just making sure that the layers that were supoosed to be modified have indeed been modified, between the original model downloaded from pytorch hub and the trained model.</p> <p>We will discard the batch normalization layers, since those may have changed during the transfer learning operation</p> <p>Let's first have a look to the layers in the model that we left unfrozen.</p>"},{"location":"tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/#conclusions","title":"Conclusions\u00b6","text":"<p>Through these experiments, we have observed a better accuracy and a faster decreasing loss value with transfer-learning methods instead of using the untrained model.</p> <p>To conclude with the method of transfer learning, it is depending on how many data you have. You could choose to train more layers and compare the metrics with partial fine-tuning. You choose the method that gives the best metrics for your experiment.</p>"},{"location":"tutorials/scikit-learn/","title":"Scikit-Learn with Fed-BioMed: a step-by-step tutorial","text":"<p>Scikit-Learn is one of the most famous and renowned open source machine learning library in Python. It implements several classical Machine Learning algorithms such as linear regression, SVM, random forests, ... </p> <ol> <li> <p>Classifying MNIST dataset using Scikit-Learn Perceptron</p> </li> <li> <p>Dealing with regression tasks using Scikit-Learn SGDRegressor</p> </li> <li> <p>Others Scikit-Learn models supported in Fed-BioMed</p> </li> </ol>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/","title":"MNIST classification with Scikit-Learn Classifier (Perceptron)","text":"In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.training_plans import FedPerceptron\nfrom fedbiomed.common.data import DataManager\n\n\nclass SkLearnClassifierTrainingPlan(FedPerceptron):\n    def init_dependencies(self):\n        \"\"\"Define additional dependencies.\n        return [\"from torchvision import datasets, transforms\",\n                \"from torch.utils.data import DataLoader\"]\n\n    def training_data(self):\n        \n        In this case, we rely on torchvision functions for preprocessing the images.\n        \"\"\"\n        return [\"from torchvision import datasets, transforms\",]\n\n    def training_data(self):\n        \"\"\"Prepare data for training.\n        \n        This function loads a MNIST dataset from the node's filesystem, applies some\n        preprocessing and converts the full dataset to a numpy array. \n        Finally, it returns a DataManager created with these numpy arrays.\n        \"\"\"\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        \n        X_train = dataset.data.numpy()\n        X_train = X_train.reshape(-1, 28*28)\n        Y_train = dataset.targets.numpy()\n        return DataManager(dataset=X_train, target=Y_train,  shuffle=False)\n</pre> from fedbiomed.common.training_plans import FedPerceptron from fedbiomed.common.data import DataManager   class SkLearnClassifierTrainingPlan(FedPerceptron):     def init_dependencies(self):         \"\"\"Define additional dependencies.         return [\"from torchvision import datasets, transforms\",                 \"from torch.utils.data import DataLoader\"]      def training_data(self):                  In this case, we rely on torchvision functions for preprocessing the images.         \"\"\"         return [\"from torchvision import datasets, transforms\",]      def training_data(self):         \"\"\"Prepare data for training.                  This function loads a MNIST dataset from the node's filesystem, applies some         preprocessing and converts the full dataset to a numpy array.          Finally, it returns a DataManager created with these numpy arrays.         \"\"\"         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)                  X_train = dataset.data.numpy()         X_train = X_train.reshape(-1, 28*28)         Y_train = dataset.targets.numpy()         return DataManager(dataset=X_train, target=Y_train,  shuffle=False)  <p>Provide dynamic arguments for the model and training. These may potentially be changed at every round.</p> In\u00a0[\u00a0]: Copied! <pre>model_args = {'n_features': 28*28,\n              'n_classes' : 10,\n              'eta0':1e-6,\n              'random_state':1234,\n              'alpha':0.1 }\n\ntraining_args = {\n    'epochs': 3, \n    'batch_maxnum': 20,  # can be used to debugging to limit the number of batches per epoch\n#    'log_interval': 1,  # output a logging message every log_interval batches\n    'loader_args': {\n        'batch_size': 4,\n    },\n}\n</pre> model_args = {'n_features': 28*28,               'n_classes' : 10,               'eta0':1e-6,               'random_state':1234,               'alpha':0.1 }  training_args = {     'epochs': 3,      'batch_maxnum': 20,  # can be used to debugging to limit the number of batches per epoch #    'log_interval': 1,  # output a logging message every log_interval batches     'loader_args': {         'batch_size': 4,     }, } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MNIST', '#dataset']\nrounds = 3\n\n# select nodes participating in this experiment\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=SkLearnClassifierTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['#MNIST', '#dataset'] rounds = 3  # select nodes participating in this experiment exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=SkLearnClassifierTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) In\u00a0[\u00a0]: Copied! <pre>exp.run(increase=True)\n</pre> exp.run(increase=True) <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') In\u00a0[\u00a0]: Copied! <pre>import tempfile\nimport os\nfrom fedbiomed.researcher.config import config\n\nfrom torchvision import datasets, transforms\nimport numpy as np\n\n\ntmp_dir_model = tempfile.TemporaryDirectory(dir=config.vars['TMP_DIR']+os.sep)\nmodel_file = os.path.join(tmp_dir_model.name, 'class_export_mnist.py')\n\n# collecting MNIST testing dataset: for that we are downloading the whole dataset on en temporary file\n\ntransform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\ntesting_MNIST_dataset = datasets.MNIST(root = os.path.join(config.vars['TMP_DIR'], 'local_mnist.tmp'),\n                                       download = True,\n                                       train = False,\n                                       transform = transform)\n\ntesting_MNIST_data = testing_MNIST_dataset.data.numpy().reshape(-1, 28*28)\ntesting_MNIST_targets = testing_MNIST_dataset.targets.numpy()\n</pre> import tempfile import os from fedbiomed.researcher.config import config  from torchvision import datasets, transforms import numpy as np   tmp_dir_model = tempfile.TemporaryDirectory(dir=config.vars['TMP_DIR']+os.sep) model_file = os.path.join(tmp_dir_model.name, 'class_export_mnist.py')  # collecting MNIST testing dataset: for that we are downloading the whole dataset on en temporary file  transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))]) testing_MNIST_dataset = datasets.MNIST(root = os.path.join(config.vars['TMP_DIR'], 'local_mnist.tmp'),                                        download = True,                                        train = False,                                        transform = transform)  testing_MNIST_data = testing_MNIST_dataset.data.numpy().reshape(-1, 28*28) testing_MNIST_targets = testing_MNIST_dataset.targets.numpy() In\u00a0[\u00a0]: Copied! <pre># retrieve Sklearn model and losses at the end of each round\n\nfrom sklearn.linear_model import  SGDClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, hinge_loss\n\nfed_perceptron_model = exp.training_plan().model()\nperceptron_args = {key: model_args[key] for key in model_args.keys() if key in fed_perceptron_model.get_params().keys()}\n\n\nlosses = []\naccuracies = []\n\nfor r in range(rounds):\n    fed_perceptron_model = fed_perceptron_model.set_params(**perceptron_args)\n    fed_perceptron_model.classes_ = np.unique(testing_MNIST_dataset.targets.numpy())\n    fed_perceptron_model.coef_ = exp.aggregated_params()[r]['params']['coef_'].copy()\n    fed_perceptron_model.intercept_ = exp.aggregated_params()[r]['params']['intercept_'].copy()  \n\n    prediction = fed_perceptron_model.decision_function(testing_MNIST_data)\n    losses.append(hinge_loss(testing_MNIST_targets, prediction))\n    accuracies.append(fed_perceptron_model.score(testing_MNIST_data,\n                                                testing_MNIST_targets))\n</pre> # retrieve Sklearn model and losses at the end of each round  from sklearn.linear_model import  SGDClassifier from sklearn.metrics import accuracy_score, confusion_matrix, hinge_loss  fed_perceptron_model = exp.training_plan().model() perceptron_args = {key: model_args[key] for key in model_args.keys() if key in fed_perceptron_model.get_params().keys()}   losses = [] accuracies = []  for r in range(rounds):     fed_perceptron_model = fed_perceptron_model.set_params(**perceptron_args)     fed_perceptron_model.classes_ = np.unique(testing_MNIST_dataset.targets.numpy())     fed_perceptron_model.coef_ = exp.aggregated_params()[r]['params']['coef_'].copy()     fed_perceptron_model.intercept_ = exp.aggregated_params()[r]['params']['intercept_'].copy()        prediction = fed_perceptron_model.decision_function(testing_MNIST_data)     losses.append(hinge_loss(testing_MNIST_targets, prediction))     accuracies.append(fed_perceptron_model.score(testing_MNIST_data,                                                 testing_MNIST_targets)) In\u00a0[\u00a0]: Copied! <pre># downloading MNIST dataset\ntraining_MNIST_dataset = datasets.MNIST(root = os.path.join(config.vars['TMP_DIR'], 'local_mnist.tmp'),\n                                       download = True,\n                                       train = True,\n                                       transform = transform)\n\ntraining_MNIST_data = training_MNIST_dataset.data.numpy().reshape(-1, 28*28)\ntraining_MNIST_targets = training_MNIST_dataset.targets.numpy()\n</pre> # downloading MNIST dataset training_MNIST_dataset = datasets.MNIST(root = os.path.join(config.vars['TMP_DIR'], 'local_mnist.tmp'),                                        download = True,                                        train = True,                                        transform = transform)  training_MNIST_data = training_MNIST_dataset.data.numpy().reshape(-1, 28*28) training_MNIST_targets = training_MNIST_dataset.targets.numpy() <p>Local Model training loop : a new model is trained locally, then compared with the remote <code>FedPerceptron</code> model</p> In\u00a0[\u00a0]: Copied! <pre>fed_perceptron_model.get_params()\n</pre> fed_perceptron_model.get_params() In\u00a0[\u00a0]: Copied! <pre>local_perceptron_losses = []\nlocal_perceptron_accuracies = []\nclasses = np.unique(training_MNIST_targets)\nbatch_size = training_args[\"loader_args\"][\"batch_size\"]\n\n# model definition\nlocal_perceptron_model = SGDClassifier()\nperceptron_args = {key: model_args[key] for key in model_args.keys() if key in fed_perceptron_model.get_params().keys()}\nlocal_perceptron_model.set_params(**perceptron_args)\nmodel_param_list = ['coef_', 'intercept_']\n\n# Model initialization\nlocal_perceptron_model.intercept_ = np.zeros((model_args[\"n_classes\"],))\nlocal_perceptron_model.coef_ = np.zeros((model_args[\"n_classes\"], model_args[\"n_features\"]))\n</pre> local_perceptron_losses = [] local_perceptron_accuracies = [] classes = np.unique(training_MNIST_targets) batch_size = training_args[\"loader_args\"][\"batch_size\"]  # model definition local_perceptron_model = SGDClassifier() perceptron_args = {key: model_args[key] for key in model_args.keys() if key in fed_perceptron_model.get_params().keys()} local_perceptron_model.set_params(**perceptron_args) model_param_list = ['coef_', 'intercept_']  # Model initialization local_perceptron_model.intercept_ = np.zeros((model_args[\"n_classes\"],)) local_perceptron_model.coef_ = np.zeros((model_args[\"n_classes\"], model_args[\"n_features\"])) <p>Implementation of mini-batch SGD</p> In\u00a0[\u00a0]: Copied! <pre>for r in range(rounds):\n    for e in range(training_args[\"epochs\"]):\n        \n        tot_samples_processed = 0\n        for idx_batch in range(training_args[\"batch_maxnum\"]):\n            param = {k: getattr(local_perceptron_model, k) for k in model_param_list}\n            grads = {k: np.zeros_like(v) for k, v in param.items()}\n            \n            # for each sample: 1) call partial_fit 2) accumulate the gradients 3) reset the model parameters\n            for sample_idx in range(tot_samples_processed, tot_samples_processed+batch_size):\n                local_perceptron_model.partial_fit(training_MNIST_data[sample_idx:sample_idx+1,:],\n                                                   training_MNIST_targets[sample_idx:sample_idx+1],\n                                                   classes=classes)\n                for key in model_param_list:\n                    grads[key] += getattr(local_perceptron_model, key)\n                    setattr(local_perceptron_model, key, param[key])\n                    \n            tot_samples_processed += batch_size\n\n            # after each epoch, we update the model with the averaged gradients over the batch\n            for key in model_param_list:\n                setattr(local_perceptron_model, key, grads[key] / batch_size)\n                \n    predictions = local_perceptron_model.decision_function(testing_MNIST_data)\n    local_perceptron_losses.append(hinge_loss(testing_MNIST_targets, predictions))\n    local_perceptron_accuracies.append(local_perceptron_model.score(testing_MNIST_data,\n                                                testing_MNIST_targets))\n</pre> for r in range(rounds):     for e in range(training_args[\"epochs\"]):                  tot_samples_processed = 0         for idx_batch in range(training_args[\"batch_maxnum\"]):             param = {k: getattr(local_perceptron_model, k) for k in model_param_list}             grads = {k: np.zeros_like(v) for k, v in param.items()}                          # for each sample: 1) call partial_fit 2) accumulate the gradients 3) reset the model parameters             for sample_idx in range(tot_samples_processed, tot_samples_processed+batch_size):                 local_perceptron_model.partial_fit(training_MNIST_data[sample_idx:sample_idx+1,:],                                                    training_MNIST_targets[sample_idx:sample_idx+1],                                                    classes=classes)                 for key in model_param_list:                     grads[key] += getattr(local_perceptron_model, key)                     setattr(local_perceptron_model, key, param[key])                                  tot_samples_processed += batch_size              # after each epoch, we update the model with the averaged gradients over the batch             for key in model_param_list:                 setattr(local_perceptron_model, key, grads[key] / batch_size)                      predictions = local_perceptron_model.decision_function(testing_MNIST_data)     local_perceptron_losses.append(hinge_loss(testing_MNIST_targets, predictions))     local_perceptron_accuracies.append(local_perceptron_model.score(testing_MNIST_data,                                                 testing_MNIST_targets)) <p>Compare the local and federated models. The two curves should overlap almost identically, although slight numerical errors are acceptable.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,5))\n\nplt.subplot(1,2,1)\nplt.plot(losses, label=\"federated Perceptron losses\")\nplt.plot(local_perceptron_losses, \"--\", color='r', label=\"local Perceptron losses\")\nplt.ylabel('Perceptron Cost Function (Hinge)')\nplt.xlabel('Number of Rounds')\nplt.title('Perceptron loss evolution on test dataset')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(accuracies, label=\"federated Perceptron accuracies\")\nplt.plot(local_perceptron_accuracies, \"--\", color='r',\n         label=\"local Perceptron accuracies\")\nplt.ylabel('Accuracy')\nplt.xlabel('Number of Rounds')\nplt.title('Perceptron accuracy over rounds (on test dataset)')\nplt.legend()\n</pre> import matplotlib.pyplot as plt  plt.figure(figsize=(10,5))  plt.subplot(1,2,1) plt.plot(losses, label=\"federated Perceptron losses\") plt.plot(local_perceptron_losses, \"--\", color='r', label=\"local Perceptron losses\") plt.ylabel('Perceptron Cost Function (Hinge)') plt.xlabel('Number of Rounds') plt.title('Perceptron loss evolution on test dataset') plt.legend()  plt.subplot(1,2,2) plt.plot(accuracies, label=\"federated Perceptron accuracies\") plt.plot(local_perceptron_accuracies, \"--\", color='r',          label=\"local Perceptron accuracies\") plt.ylabel('Accuracy') plt.xlabel('Number of Rounds') plt.title('Perceptron accuracy over rounds (on test dataset)') plt.legend()  <p>In this example, plots appear to be the same: this means that Federated and local <code>Perceptron</code> models are performing equivalently!</p> In\u00a0[\u00a0]: Copied! <pre># federated model predictions\nfed_prediction = fed_perceptron_model.predict(testing_MNIST_data)\nacc = accuracy_score(testing_MNIST_targets, fed_prediction)\nprint('Federated Perceptron Model accuracy :', acc)\n\n# local model predictions\nlocal_prediction = local_perceptron_model.predict(testing_MNIST_data)\nacc = accuracy_score(testing_MNIST_targets, local_prediction)\nprint('Local Perceptron Model accuracy :', acc)\n</pre> # federated model predictions fed_prediction = fed_perceptron_model.predict(testing_MNIST_data) acc = accuracy_score(testing_MNIST_targets, fed_prediction) print('Federated Perceptron Model accuracy :', acc)  # local model predictions local_prediction = local_perceptron_model.predict(testing_MNIST_data) acc = accuracy_score(testing_MNIST_targets, local_prediction) print('Local Perceptron Model accuracy :', acc) In\u00a0[\u00a0]: Copied! <pre>def plot_confusion_matrix(fig, ax, conf_matrix, title, xlabel, ylabel, n_image=0):\n    \n    im = ax[n_image].imshow(conf_matrix)\n\n    ax[n_image].set_xticks(np.arange(10))\n    ax[n_image].set_yticks(np.arange(10))\n\n    for i in range(conf_matrix.shape[0]):\n        for j in range(conf_matrix.shape[1]):\n            text = ax[n_image].text(j, i, conf_matrix[i, j],\n                           ha=\"center\", va=\"center\", color=\"w\")\n\n    ax[n_image].set_xlabel(xlabel)\n    ax[n_image].set_ylabel(ylabel)\n    ax[n_image].set_title(title)\n</pre> def plot_confusion_matrix(fig, ax, conf_matrix, title, xlabel, ylabel, n_image=0):          im = ax[n_image].imshow(conf_matrix)      ax[n_image].set_xticks(np.arange(10))     ax[n_image].set_yticks(np.arange(10))      for i in range(conf_matrix.shape[0]):         for j in range(conf_matrix.shape[1]):             text = ax[n_image].text(j, i, conf_matrix[i, j],                            ha=\"center\", va=\"center\", color=\"w\")      ax[n_image].set_xlabel(xlabel)     ax[n_image].set_ylabel(ylabel)     ax[n_image].set_title(title) In\u00a0[\u00a0]: Copied! <pre>fed_conf_matrix = confusion_matrix(testing_MNIST_targets, fed_prediction)\nlocal_conf_matrix = confusion_matrix(testing_MNIST_targets, local_prediction)\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2,figsize=(10,5))\n\n\n\nplot_confusion_matrix(fig, axs, fed_conf_matrix,\n                      \"Federated Perceptron Confusion Matrix\",\n                      \"Actual values\", \"Predicted values\", n_image=0)\n        \nplot_confusion_matrix(fig, axs, local_conf_matrix,\n                      \"Local Perceptron Confusion Matrix\",\n                      \"Actual values\", \"Predicted values\", n_image=1)\n</pre> fed_conf_matrix = confusion_matrix(testing_MNIST_targets, fed_prediction) local_conf_matrix = confusion_matrix(testing_MNIST_targets, local_prediction)   fig, axs = plt.subplots(nrows=1, ncols=2,figsize=(10,5))    plot_confusion_matrix(fig, axs, fed_conf_matrix,                       \"Federated Perceptron Confusion Matrix\",                       \"Actual values\", \"Predicted values\", n_image=0)          plot_confusion_matrix(fig, axs, local_conf_matrix,                       \"Local Perceptron Confusion Matrix\",                       \"Actual values\", \"Predicted values\", n_image=1)"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#mnist-classification-with-scikit-learn-classifier-perceptron","title":"MNIST classification with Scikit-Learn Classifier (Perceptron)\u00b6","text":"<p>Overview of the tutorial:</p> <p>In this tutorial, we are going to train Scikit-Learn <code>Perceptron</code> as a federated model over a <code>Node</code>.</p> <p>At the end of this tutorial, you will learn:</p> <ul> <li>how to define a Sklearn classifier in Fed-BioMed (especially <code>Perceptron</code> model)</li> <li>how to train it</li> <li>how to evaluate the resulting model</li> </ul> <p>HINT : to reload the notebook,  please click on the following button:</p> <p><code>Kernel</code> -&gt; <code>Restart and clear Output</code></p> <p></p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#1-clean-your-environments","title":"1. Clean your environments\u00b6","text":"<p>Before executing notebook and starting nodes, it is safer to remove all configuration scripts automatically generated by Fed-BioMed. To do so, enter the following in a terminal:</p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#2-setting-the-node-up","title":"2. Setting the node up\u00b6","text":"<ol> <li><code>fedbiomed node dataset add</code></li> </ol> <ul> <li>Select option 2 (default) to add MNIST to the node</li> <li>Confirm default tags by hitting \"y\" and ENTER</li> <li>Pick the folder where MNIST is downloaded (this is due torch issue https://github.com/pytorch/vision/issues/3549)</li> <li>Data must have been added (if you get a warning saying that data must be unique is because it has been already added)</li> </ul> <ol> <li>Check that your data has been added by executing <code>fedbiomed node dataset list</code></li> <li>Run the node using <code>fedbiomed node start</code>. Wait until you get <code>Connected with result code 0</code>. it means your node is working and ready to participate to a Federated training.</li> </ol> <p>More details are given in tutorial : Installation/setting up environment </p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#3-create-sklearn-federated-perceptron-training-plan","title":"3. Create Sklearn Federated Perceptron Training Plan\u00b6","text":"<p>The class <code>FedPerceptron</code> constitutes the Fed-BioMed wrapper for executing Federated Learning using Scikit-Learn Perceptron model based on mini-batch Stochastic Gradient Descent (SGD). As we have done with Pytorch model in previous chapter, we create a new training plan class <code>SkLearnClassifierTrainingPlan</code> that inherits from it. For a refresher on how Training Plans work in Fed-BioMed, please refer to our Training Plan user guide.</p> <p>In scikit-learn Training Plans, you typically need to define only the <code>training_data</code> function, and optionally an <code>init_dependencies</code> function if your code requires additional module imports.</p> <p>The <code>training_data</code> function defines how datasets should be loaded in nodes to make them ready for training. It takes a <code>batch_size</code> argument and returns a <code>DataManager</code> class. For scikit-learn, the <code>DataManager</code> must be instantiated with a <code>dataset</code> and a <code>target</code> argument, both <code>np.ndarrays</code> of the same length.</p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#model-arguments","title":"Model arguments\u00b6","text":"<p><code>model_args</code> is a dictionary with the arguments related to the model, that will be passed to the <code>Perceptron</code> constructor.</p> <p>IMPORTANT For classification tasks, you are required to specify the following two fields:</p> <ul> <li><code>n_features</code>: the number of features in each input sample (in our case, the number of pixels in the images)</li> <li><code>n_classes</code>: the number of classes in the target data</li> </ul> <p>Furthermore, the classes may not be represented by arbitrary values: classes must be identified by integers in the range 0..n_classes</p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#training-arguments","title":"Training arguments\u00b6","text":"<p><code>training_args</code> is a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.</p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#4-train-your-model-on-mnist-dataset","title":"4. Train your model on MNIST dataset\u00b6","text":"<p>MNIST dataset is composed of handwritten digits images, from 0 to 9. The purpose of our classifier is to associate an image to the corresponding represented digit</p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#5-testing-on-mnist-test-dataset","title":"5. Testing on MNIST test dataset\u00b6","text":"<p>Let's assess performance of our classifier with  MNIST testing dataset</p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#6-getting-loss-function","title":"6. Getting Loss function\u00b6","text":"<p>Here we use the <code>aggregated_params()</code> getter to access all model weights at the end of each round to plot the evolution of Perceptron loss funciton, as well as its accuracy.</p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#7-comparison-with-a-local-perceptron-model","title":"7. Comparison with a local <code>Perceptron</code> model\u00b6","text":"<p>In this section, we implement a local <code>Perceptron</code> model, so we can compare remote and local models accuracies.</p> <p>You can use this section as an insight on how things are implemented within the Fed-BioMed network. In particular, looking at the code in the next few cells you may learn how:</p> <ul> <li>we implement mini-batch gradient descent for scikit-learn models</li> <li>we implement <code>Perceptron</code> based on <code>SGDClassifier</code></li> </ul>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#8-getting-accuracy-and-confusion-matrix","title":"8. Getting accuracy and confusion matrix\u00b6","text":""},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#congrats","title":"Congrats !\u00b6","text":"<p>You have figured out how to train your first Federated Sklearn classifier model !</p> <p>If you want to practise more, you can try to deploy such classifier on two or more nodes. As you can see, <code>Perceptron</code> is a limited model: its generalization is <code>SGDCLassifier</code>, provided by Fed-BioMed as a <code>FedSGDCLassifier</code> Training Plan. You can thus try to apply <code>SGDCLassifier</code>, providing more feature such as different cost functions, regularizations and learning rate decays.</p>"},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/","title":"Fed-BioMed to train a federated SGD regressor model","text":"In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.requests import Requests\nfrom fedbiomed.researcher.config import config\nreq = Requests(config)\nreq.list(verbose=True)\n</pre> from fedbiomed.researcher.requests import Requests from fedbiomed.researcher.config import config req = Requests(config) req.list(verbose=True) <p>The class <code>FedSGDRegressor</code> constitutes the Fed-BioMed wrapper for executing Federated Learning using Scikit-Learn <code>SGDRegressor</code> model based on mini-batch Stochastic Gradient Descent (SGD). As we have done with Pytorch model in previous chapter, we create a new training plan class <code>SGDRegressorTrainingPlan</code> that inherits from it. For a refresher on how Training Plans work in Fed-BioMed, please refer to our Training Plan user guide.</p> <p>In scikit-learn Training Plans, you typically need to define only the <code>training_data</code> function, and optionally an <code>init_dependencies</code> function if your code requires additional module imports.</p> <p>The <code>training_data</code> function defines how datasets should be loaded in nodes to make them ready for training. It takes a <code>batch_size</code> argument and returns a <code>DataManager</code> class. For scikit-learn, the <code>DataManager</code> must be instantiated with a <code>dataset</code> and a <code>target</code> argument, both <code>np.ndarrays</code> of the same length.</p> <p>We note that this model performs a common standardization across federated datasets by centering with respect to the same parameters.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom fedbiomed.common.training_plans import FedSGDRegressor\nfrom fedbiomed.common.data import DataManager\n\nclass SGDRegressorTrainingPlan(FedSGDRegressor):\n    def training_data(self):\n        dataset = pd.read_csv(self.dataset_path,delimiter=',')\n        regressors_col = ['AGE', 'WholeBrain.bl',\n                          'Ventricles.bl', 'Hippocampus.bl', 'MidTemp.bl', 'Entorhinal.bl']\n        target_col = ['MMSE.bl']\n\n        # mean and standard deviation for normalizing dataset\n        # it has been computed over the whole dataset\n        scaling_mean = np.array([72.3, 0.7, 0.0, 0.0, 0.0, 0.0])\n        scaling_sd = np.array([7.3e+00, 5.0e-02, 1.1e-02, 1.0e-03, 2.0e-03, 1.0e-03])\n\n        X = (dataset[regressors_col].values-scaling_mean)/scaling_sd\n        y = dataset[target_col]\n        return DataManager(dataset=X, target=y.values.ravel(), )\n</pre> import numpy as np from fedbiomed.common.training_plans import FedSGDRegressor from fedbiomed.common.data import DataManager  class SGDRegressorTrainingPlan(FedSGDRegressor):     def training_data(self):         dataset = pd.read_csv(self.dataset_path,delimiter=',')         regressors_col = ['AGE', 'WholeBrain.bl',                           'Ventricles.bl', 'Hippocampus.bl', 'MidTemp.bl', 'Entorhinal.bl']         target_col = ['MMSE.bl']          # mean and standard deviation for normalizing dataset         # it has been computed over the whole dataset         scaling_mean = np.array([72.3, 0.7, 0.0, 0.0, 0.0, 0.0])         scaling_sd = np.array([7.3e+00, 5.0e-02, 1.1e-02, 1.0e-03, 2.0e-03, 1.0e-03])          X = (dataset[regressors_col].values-scaling_mean)/scaling_sd         y = dataset[target_col]         return DataManager(dataset=X, target=y.values.ravel(), ) <p>Provide dynamic arguments for the model and training. These may potentially be changed at every round.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.metrics import MetricTypes\nRANDOM_SEED = 1234\n\nmodel_args = {\n    'eta0':0.05,\n    'n_features': 6,\n    'random_state': RANDOM_SEED\n}\n\ntraining_args = {\n    'epochs': 1,\n    'test_ratio':.2,\n    'test_metric': MetricTypes.MEAN_SQUARE_ERROR,\n    'test_on_local_updates': True,\n    'test_on_global_updates': True,\n    'loader_args': { 'batch_size': 30, },\n#    'batch_maxnum': 2,  # can be used to debugging to limit the number of batches per epoch\n#    'log_interval': 1,  # output a logging message every log_interval batches\n}\n</pre> from fedbiomed.common.metrics import MetricTypes RANDOM_SEED = 1234  model_args = {     'eta0':0.05,     'n_features': 6,     'random_state': RANDOM_SEED }  training_args = {     'epochs': 1,     'test_ratio':.2,     'test_metric': MetricTypes.MEAN_SQUARE_ERROR,     'test_on_local_updates': True,     'test_on_global_updates': True,     'loader_args': { 'batch_size': 30, }, #    'batch_maxnum': 2,  # can be used to debugging to limit the number of batches per epoch #    'log_interval': 1,  # output a logging message every log_interval batches } <p>The experiment can be now defined, by providing the <code>adni</code> tag, and running the local training on nodes with training plan defined in <code>training_plan_path</code>, standard <code>aggregator</code> (FedAvg) and <code>client_selection_strategy</code> (all nodes used). Federated learning is going to be performed through 10 optimization rounds.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['adni']\n\n# Add more rounds for results with better accuracy\n#\n#rounds = 40\nrounds = 10\n\n# select nodes participating in this experiment\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=SGDRegressorTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['adni']  # Add more rounds for results with better accuracy # #rounds = 40 rounds = 10  # select nodes participating in this experiment exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=SGDRegressorTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) In\u00a0[\u00a0]: Copied! <pre># start federated training\nexp.run(increase=True)\n</pre> # start federated training exp.run(increase=True) <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') In\u00a0[\u00a0]: Copied! <pre>!pip install matplotlib\n!pip install gdown\n</pre> !pip install matplotlib !pip install gdown <p>Download the testing dataset on the local temporary folder.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport gdown\nimport tempfile\nimport zipfile\nimport pandas as pd\nimport numpy as np\n\nfrom fedbiomed.researcher.config import config\n\nresource = \"https://drive.google.com/uc?id=19kxuI146WA2fhcOU2_AvF8dy-ppJkzW7\"\n\ntmpdir = tempfile.TemporaryDirectory(dir=config.vars['TMP_DIR'])\nbase_dir = tmpdir.name\n\ntest_file = os.path.join(base_dir, \"test_data.zip\")\ngdown.download(resource, test_file, quiet=False)\n\nzf = zipfile.ZipFile(test_file)\n\nfor file in zf.infolist():\n    zf.extract(file, base_dir)\n\n# loading testing dataset\ntest_data = pd.read_csv(os.path.join(base_dir,'adni_validation.csv'))\n</pre> import os import gdown import tempfile import zipfile import pandas as pd import numpy as np  from fedbiomed.researcher.config import config  resource = \"https://drive.google.com/uc?id=19kxuI146WA2fhcOU2_AvF8dy-ppJkzW7\"  tmpdir = tempfile.TemporaryDirectory(dir=config.vars['TMP_DIR']) base_dir = tmpdir.name  test_file = os.path.join(base_dir, \"test_data.zip\") gdown.download(resource, test_file, quiet=False)  zf = zipfile.ZipFile(test_file)  for file in zf.infolist():     zf.extract(file, base_dir)  # loading testing dataset test_data = pd.read_csv(os.path.join(base_dir,'adni_validation.csv')) In\u00a0[\u00a0]: Copied! <pre>from sklearn.linear_model import SGDRegressor\nimport matplotlib.pyplot as plt\n</pre> from sklearn.linear_model import SGDRegressor import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>Here we extract the relevant regressors and target from the testing data</p> In\u00a0[\u00a0]: Copied! <pre>regressors_col = ['AGE', 'WholeBrain.bl', 'Ventricles.bl', 'Hippocampus.bl', 'MidTemp.bl', 'Entorhinal.bl']\ntarget_col = ['MMSE.bl']\nX_test = test_data[regressors_col].values\ny_test = test_data[target_col].values\n</pre> regressors_col = ['AGE', 'WholeBrain.bl', 'Ventricles.bl', 'Hippocampus.bl', 'MidTemp.bl', 'Entorhinal.bl'] target_col = ['MMSE.bl'] X_test = test_data[regressors_col].values y_test = test_data[target_col].values <p>To inspect the model evolution across FL rounds, we export <code>exp.aggregated_params()</code> containing models parameters collected at the end of each round. The MSE should be decreasing at each iteration with the federated parameters.</p> In\u00a0[\u00a0]: Copied! <pre>scaling_mean = np.array([72.3, 0.7, 0.0, 0.0, 0.0, 0.0])\nscaling_sd = np.array([7.3e+00, 5.0e-02, 1.1e-02, 1.0e-03, 2.0e-03, 1.0e-03])\n\ntesting_error = []\n\n\n# we create here several instances of SGDRegressor using same sklearn arguments\n# we have used for Federated Learning training\nfed_model = exp.training_plan().model()\nregressor_args = {key: model_args[key] for key in model_args.keys() if key in fed_model.get_params().keys()}\n\nfor i in range(rounds):\n    fed_model.coef_ = exp.aggregated_params()[i]['params']['coef_'].copy()\n    fed_model.intercept_ = exp.aggregated_params()[i]['params']['intercept_'].copy()\n    mse = np.mean((fed_model.predict((X_test-scaling_mean)/scaling_sd) - y_test)**2)\n    testing_error.append(mse)\n\nplt.plot(testing_error)\nplt.title('FL testing loss')\nplt.xlabel('FL round')\nplt.ylabel('testing loss (MSE)')\n</pre> scaling_mean = np.array([72.3, 0.7, 0.0, 0.0, 0.0, 0.0]) scaling_sd = np.array([7.3e+00, 5.0e-02, 1.1e-02, 1.0e-03, 2.0e-03, 1.0e-03])  testing_error = []   # we create here several instances of SGDRegressor using same sklearn arguments # we have used for Federated Learning training fed_model = exp.training_plan().model() regressor_args = {key: model_args[key] for key in model_args.keys() if key in fed_model.get_params().keys()}  for i in range(rounds):     fed_model.coef_ = exp.aggregated_params()[i]['params']['coef_'].copy()     fed_model.intercept_ = exp.aggregated_params()[i]['params']['intercept_'].copy()     mse = np.mean((fed_model.predict((X_test-scaling_mean)/scaling_sd) - y_test)**2)     testing_error.append(mse)  plt.plot(testing_error) plt.title('FL testing loss') plt.xlabel('FL round') plt.ylabel('testing loss (MSE)') <p>We finally inspect the predictions of the final federated model on the testing data.</p> In\u00a0[\u00a0]: Copied! <pre>y_predicted = fed_model.predict((X_test-scaling_mean)/scaling_sd)\nplt.scatter(y_predicted, y_test, label='model prediction')\nplt.xlabel('predicted')\nplt.ylabel('target')\nplt.title('Federated model testing prediction')\n\nfirst_diag = np.arange(np.min(y_test.flatten()),\n                       np.max(y_test.flatten()+1))\nplt.scatter(first_diag, first_diag, label='correct Target')\nplt.legend()\n</pre> y_predicted = fed_model.predict((X_test-scaling_mean)/scaling_sd) plt.scatter(y_predicted, y_test, label='model prediction') plt.xlabel('predicted') plt.ylabel('target') plt.title('Federated model testing prediction')  first_diag = np.arange(np.min(y_test.flatten()),                        np.max(y_test.flatten()+1)) plt.scatter(first_diag, first_diag, label='correct Target') plt.legend()"},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#fed-biomed-to-train-a-federated-sgd-regressor-model","title":"Fed-BioMed to train a federated SGD regressor model\u00b6","text":""},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#data","title":"Data\u00b6","text":"<p>This tutorial shows how to deploy in Fed-BioMed to solve a federated regression problem with scikit-learn.</p> <p>In this tutorial we are using the wrapper of Fed-BioMed for the SGD regressor. The goal of the notebook is to train a model on a realistic dataset of (synthetic) medical information mimicking the ADNI dataset.</p>"},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#creating-nodes","title":"Creating nodes\u00b6","text":"<p>To proceed with the tutorial, we create 3 clients with corresponding dataframes of clinical information in .csv format. Each client has 300 data points composed by several features corresponding to clinical and medical imaging information. The data is entirely synthetic and randomly sampled to mimick the variability of the real ADNI dataset. The training partitions are available at the following link:</p> <p>https://drive.google.com/file/d/1R39Ir60oQi8ZnmHoPz5CoGCrVIglcO9l/view?usp=sharing</p> <p>The federated task we aim at solve is to predict a clinical variable (the mini-mental state examination, MMSE) from a combination of demographic and imaging features. The regressors variables are the following features:</p> <pre>['SEX', 'AGE', 'PTEDUCAT', 'WholeBrain.bl', 'Ventricles.bl', 'Hippocampus.bl', 'MidTemp.bl', 'Entorhinal.bl']\n</pre> <p>and the target variable is:</p> <pre>['MMSE.bl']\n</pre> <p>To create the federated dataset, we follow the standard procedure for node creation/population of Fed-BioMed.</p> <p>we create a first node by using the commands</p> <pre>$ fedbiomed node -p my-node start\n</pre> <p>We then populate the node with the data of first client:</p> <pre>$ fedbiomed node -d my-node dataset add`\n</pre> <p>We select option 1 (csv) to add the .csv partition of client 1, by just picking the .csv of client 1. We use <code>adni</code> as tag to save the selected dataset. We can further check that the data has been added by executing <code>fedbiomed node -d my-node dataset list</code></p> <p>Following the same procedure, we create the other two nodes with the datasets of client 2 and client 3 respectively.</p>"},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#fed-biomed-researcher","title":"Fed-BioMed Researcher\u00b6","text":"<p>We are now ready to start the researcher environment with the following command. This command will activate researcher environment and start Jupyter Notebook.</p> <pre>$ fedbiomed researcher start\n</pre> <p>We can first query the network for the <code>adni</code> dataset. In this case, the nodes are sharing the respective partitions using the same tag <code>adni</code>:</p>"},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#create-an-experiment-to-train-a-model-on-the-data-found","title":"Create an experiment to train a model on the data found\u00b6","text":""},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#model-arguments","title":"Model arguments\u00b6","text":"<p><code>model_args</code> is a dictionary with the arguments related to the model, that will be passed to the <code>SGDRegressor</code> constructor. In this case, these include <code>n_features</code>, <code>random_state</code> and <code>eta0</code>.</p> <p>IMPORTANT For regression tasks, you are required to specify the following field:</p> <ul> <li><code>n_features</code>: the number of features in each input sample (in our case, the number of pixels in the images)</li> </ul>"},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#training-arguments","title":"Training arguments\u00b6","text":"<p><code>training_args</code> is a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.</p>"},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#testing","title":"Testing\u00b6","text":"<p>Once the federated model is obtained, it is possible to test it locally on an independent testing partition. The test dataset is available at this link:</p> <p>https://drive.google.com/uc?id=19kxuI146WA2fhcOU2_AvF8dy-ppJkzW7</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/","title":"Implementing other Scikit Learn models for Federated Learning","text":"In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.training_plans import FedSGDRegressor, FedPerceptron, FedSGDClassifier\n\nSelectedTrainingPlan = FedPerceptron\n\n\nclass SkLearnTrainingPlan(SelectedTrainingPlan):\n    def init_dependencies(self):\n        # The method for declaring dependencies that are used generally in this training plan.\n        # E.g, `import numpy as np`should be added dependency array if it is used in the training_data method.\n        deps= [\"import numpy as np\",\n               \"import pandas as pd\"]\n        return deps\n\n    def training_data(self):\n        # Define here how data are handled and /or shuffled\n        # First you need to instantiate the dataset. This will be typically something like\n        # raw_dataset = pd.read_csv(self.dataset_path)\n        # X = raw_dataset[feature_columns]\n        # y = raw_dataset[target_column(s)]\n\n        return DataManager(dataset=X.values, target=y.values,  shuffle=True, drop_last=False)\n</pre> from fedbiomed.common.training_plans import FedSGDRegressor, FedPerceptron, FedSGDClassifier  SelectedTrainingPlan = FedPerceptron   class SkLearnTrainingPlan(SelectedTrainingPlan):     def init_dependencies(self):         # The method for declaring dependencies that are used generally in this training plan.         # E.g, `import numpy as np`should be added dependency array if it is used in the training_data method.         deps= [\"import numpy as np\",                \"import pandas as pd\"]         return deps      def training_data(self):         # Define here how data are handled and /or shuffled         # First you need to instantiate the dataset. This will be typically something like         # raw_dataset = pd.read_csv(self.dataset_path)         # X = raw_dataset[feature_columns]         # y = raw_dataset[target_column(s)]          return DataManager(dataset=X.values, target=y.values,  shuffle=True, drop_last=False) <p>Training a Scikit Learn model is pretty similar to training a Pytorch model. The only difference is the selection of model hyperparameters (contained in <code>model_args</code>) and training parameters (in <code>training_args</code>). Initializing the class <code>Experiment</code> will allow the <code>Researcher</code> to search for active nodes tagged with defined tags.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\n\ntags =  ['#MNIST', '#dataset']\nrounds = 5\n\n# select nodes participating to this experiment\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=SkLearnTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment  tags =  ['#MNIST', '#dataset'] rounds = 5  # select nodes participating to this experiment exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=SkLearnTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre>  exp.run() <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') In\u00a0[\u00a0]: Copied! <pre>exp.aggregated_params()\n</pre> exp.aggregated_params() <p>More algorithms from Scikit-Learn are coming soon ! Stay Tuned !</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#implementing-other-scikit-learn-models-for-federated-learning","title":"Implementing other Scikit Learn models for Federated Learning\u00b6","text":"<p>In this tutorial, you will learn how to define and run any Scikit Learn Supervised and Unsupervised model, as well as Data reduction methods, in Fed-BioMed.</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#1-introduction","title":"1. Introduction\u00b6","text":"<p>Like in previous tutorials with Pytorch, you can implement custom Scikit Learn models in Fed-BioMed. In this tutorial, we are summarizing all the steps to set up a Scikit Learn model in Fed-BioMed.</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#current-scikit-learn-methods-implemented-in-fed-biomed","title":"Current Scikit-Learn Methods implemented in Fed-BioMed\u00b6","text":"<ul> <li>Classifiers:<ul> <li>SGDClassifier</li> <li>Perceptron</li> </ul> </li> </ul> <ul> <li>Regressor:<ul> <li>SGDRegressor</li> </ul> </li> </ul> <ul> <li>Clustering:<ul> <li>Coming Soon!</li> </ul> </li> </ul> <p>Check out our User Guide for further information about Scikit Learn models available in Fed-BioMed.</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#2-scikit-learn-training-plan","title":"2. Scikit-Learn training plan\u00b6","text":"<p>As you could have seen in the previous tutorials concerning Scikit-Learn, you should define a \"Scikit-Learn training plan\". We provide here a template to create a TrainingPlan for Scikit Learn. As for PyTorch training plan, every Scikit-Learn Training Plan class should be inherited from one of the <code>\"FedPerceptron\", \"FedSGDRegressor\", \"FedSGDClassifier\"</code> classes.</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#training-plan-for-supervised-learning-regressor-and-classifier","title":"Training Plan for supervised Learning (Regressor and Classifier)\u00b6","text":"<p>A template of a Supervised Learning algorithm for Scikit-Learn models. Each supported SkLearn model can be imported from the module <code>fedbiomed.common.training_plan</code>. Currently Fed-BioMed support following SkLearn models <code> \"FedPerceptron\", \"FedSGDRegressor\", \"FedSGDClassifier\"</code>.</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#31-arguments-for-model-definition-and-model-training","title":"3.1 Arguments for model definition and model training:\u00b6","text":"<ul> <li><p><code>tags</code>: a list containing tags that will be used for finding models. Same as for PyTorch models.</p> </li> <li><p><code>model_args</code>: a Python dictionary containing all arguments related to the model (ie all Scikit Learn model parameters). In addition, it MUST include the following fields:</p> <ul> <li><code>n_features</code>: number of features in the dataset</li> <li><code>n_classes</code>: number of classes (for classification or clustering algorithms only, ignored if a Regression algorithm is used). </li> </ul> </li> <li><p><code>training_plan_class</code>: the Scikit-Learn training Plan class. Same as for Pytorch models.</p> </li> <li><p><code>training_args</code>: a dictionary containing training parameter. For the moment, it contains the following entries:</p> <ul> <li><code>epochs</code>: the number of epoch to be performed locally (ie on each node). </li> </ul> </li> <li><p><code>round_limit</code>: the number of rounds (ie global aggregations) to be performed. Same as for PyTorch models.</p> </li> <li><p><code>aggregator</code>: the aggregation strategy, here Federated Average. More information on User Guide/Aggregators. Same as for PyTorch models.</p> </li> <li><p><code>node_selection_startegy</code>: how to select/sample nodes among all available nodes. Same as for Pytorch models.</p> </li> </ul>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#32-training-the-model","title":"3.2 Training the model\u00b6","text":"<p>Calling the <code>run</code> method from <code>Experiment</code> will train the Federated Model.</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#33-retrieve-model-weights-for-each-federated-round","title":"3.3 Retrieve model weights for each Federated round.\u00b6","text":"<p>The history of each round is accessed via <code>aggregated_params()</code> attribute of <code>Experiment</code> class. In fact, aggregated model at each round is contained in a dictionary, where each key corresponds to  a specific round. Each key is mapping an aggregated model obtained through the round.</p> <p>To extract all the history, enter :</p>"},{"location":"tutorials/security/differential-privacy-with-opacus-on-fedbiomed/","title":"Using Differential Privacy with OPACUS on Fed-BioMed","text":"<p>In this notebook we show how <code>opacus</code> (https://opacus.ai/) can be used in Fed-BioMed. Opacus is a library which allows to train PyTorch models with differential privacy. We will train the basic MNIST example using two nodes.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\n# Here we define the training plan to be used in the experiment. \nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\",\n                \"import torch.nn.functional as F\"]\n        \n        return deps\n    \n    def init_model(self):\n        model = nn.Sequential(nn.Conv2d(1, 32, 3, 1),\n                                  nn.ReLU(),\n                                  nn.Conv2d(32, 64, 3, 1),\n                                  nn.ReLU(),\n                                  nn.MaxPool2d(2),\n                                  nn.Dropout(0.25),\n                                  nn.Flatten(),\n                                  nn.Linear(9216, 128),\n                                  nn.ReLU(),\n                                  nn.Dropout(0.5),\n                                  nn.Linear(128, 10),\n                                  nn.LogSoftmax(dim=1))\n        return model\n    \n\n    \n    def training_data(self):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        loader_arguments = { 'shuffle': True}\n        return DataManager(dataset1, **loader_arguments)\n    \n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn import torch.nn.functional as F from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms  # Here we define the training plan to be used in the experiment.  class MyTrainingPlan(TorchTrainingPlan):     def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\",                 \"import torch.nn.functional as F\"]                  return deps          def init_model(self):         model = nn.Sequential(nn.Conv2d(1, 32, 3, 1),                                   nn.ReLU(),                                   nn.Conv2d(32, 64, 3, 1),                                   nn.ReLU(),                                   nn.MaxPool2d(2),                                   nn.Dropout(0.25),                                   nn.Flatten(),                                   nn.Linear(9216, 128),                                   nn.ReLU(),                                   nn.Dropout(0.5),                                   nn.Linear(128, 10),                                   nn.LogSoftmax(dim=1))         return model                def training_data(self):         # Custom torch Dataloader for MNIST data         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         loader_arguments = { 'shuffle': True}         return DataManager(dataset1, **loader_arguments)          def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss  <p>This group of arguments correspond respectively:</p> <ul> <li><code>model_args</code>: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side. For instance, the privacy parameters should be passed here.</li> <li><code>training_args</code>: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.</li> </ul> <p>NOTE: typos and/or lack of positional (required) arguments will raise error. \ud83e\udd13</p> <p>In the cell below, we are going to define <code>dp_args</code> inside the <code>training_args</code> dictionary. Based on the given paremeters node will perform Opacus's differeantal privacy.</p> <ul> <li><p><code>noise_multiplier</code> - <code>sigma</code>: The ratio of the standard deviation of the Gaussian noise to the L2-sensitivity of the function to which the noise is added (How much noise to add)</p> </li> <li><p><code>max_grad_norm</code> - <code>clip</code>: The maximum norm of the per-sample gradients. Any gradient with norm higher than this will be clipped to this value.</p> </li> <li><p><code>type</code>: Differential privacy type as one of <code>local</code> or <code>central</code></p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'loader_args': { 'batch_size': 48, },\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 1, \n    'dry_run': False, \n    'dp_args': # DP Arguments for differential privacy\n        {\n            \"type\": \"local\", \n            \"sigma\": 0.4, \n            \"clip\": 0.005\n        },\n    'batch_maxnum': 50 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n</pre> model_args = {}  training_args = {     'loader_args': { 'batch_size': 48, },     'optimizer_args': {         'lr': 1e-3     },     'epochs': 1,      'dry_run': False,      'dp_args': # DP Arguments for differential privacy         {             \"type\": \"local\",              \"sigma\": 0.4,              \"clip\": 0.005         },     'batch_maxnum': 50 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MNIST', '#dataset']\nrounds = 3\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['#MNIST', '#dataset'] rounds = 3  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) <p>Let's start the experiment.</p> <p>By default, this function doesn't stop until all the <code>rounds</code> are done for all the nodes</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Federated parameters for each round are available in <code>exp.aggregated_params()</code> (index 0 to (<code>rounds</code> - 1) ).</p> <p>For example you can view the federated parameters for the last round of the experiment :</p> In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n\nprint(\"\\nAccess the federated params for the last training round :\")\nprint(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n</pre> print(\"\\nList the training rounds : \", exp.aggregated_params().keys())  print(\"\\nAccess the federated params for the last training round :\") print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys()) <p>We define a little testing routine to extract the accuracy metrics on the testing dataset</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn.functional as F\n\n\ndef testing_Accuracy(model, data_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    device = 'cpu'\n\n    correct = 0\n    \n    with torch.no_grad():\n        for data, target in data_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n        pred = output.argmax(dim=1, keepdim=True)\n\n    test_loss /= len(data_loader.dataset)\n    accuracy = 100* correct/len(data_loader.dataset)\n\n    return(test_loss, accuracy)\n</pre> import torch import torch.nn.functional as F   def testing_Accuracy(model, data_loader):     model.eval()     test_loss = 0     correct = 0     device = 'cpu'      correct = 0          with torch.no_grad():         for data, target in data_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()          pred = output.argmax(dim=1, keepdim=True)      test_loss /= len(data_loader.dataset)     accuracy = 100* correct/len(data_loader.dataset)      return(test_loss, accuracy) In\u00a0[\u00a0]: Copied! <pre>from torchvision import datasets, transforms\nfrom fedbiomed.researcher.config import config\nimport os\n\nlocal_mnist = os.path.join(config.vars['TMP_DIR'], 'local_mnist')\n\ntransform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\ntest_set = datasets.MNIST(root = local_mnist, download = True, train = False, transform = transform)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)\n</pre> from torchvision import datasets, transforms from fedbiomed.researcher.config import config import os  local_mnist = os.path.join(config.vars['TMP_DIR'], 'local_mnist')  transform = transforms.Compose([             transforms.ToTensor(),             transforms.Normalize((0.1307,), (0.3081,))         ])  test_set = datasets.MNIST(root = local_mnist, download = True, train = False, transform = transform) test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True) In\u00a0[\u00a0]: Copied! <pre>fed_model = exp.training_plan().model()\nfed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n\nacc_federated = testing_Accuracy(fed_model, test_loader)\n\nprint('\\nAccuracy federated training:  {:.4f}'.format(acc_federated[1]))\n\nprint('\\nError federated training:  {:.4f}'.format(acc_federated[0]))\n</pre> fed_model = exp.training_plan().model() fed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])  acc_federated = testing_Accuracy(fed_model, test_loader)  print('\\nAccuracy federated training:  {:.4f}'.format(acc_federated[1]))  print('\\nError federated training:  {:.4f}'.format(acc_federated[0]))"},{"location":"tutorials/security/differential-privacy-with-opacus-on-fedbiomed/#using-differential-privacy-with-opacus-on-fed-biomed","title":"Using Differential Privacy with OPACUS on Fed-BioMed\u00b6","text":""},{"location":"tutorials/security/differential-privacy-with-opacus-on-fedbiomed/#setting-the-node-up","title":"Setting the node up\u00b6","text":"<p>It is necessary to previously configure a node:</p> <ol> <li>You can create a node by adding a dataset, <code>fedbiomed node dataset add</code></li> </ol> <ul> <li>Select option 2 (default)</li> <li>Confirm default tags by hitting \"y\" and ENTER</li> <li>Pick the folder where MNIST is downloaded (this is due torch issue https://github.com/pytorch/vision/issues/3549)</li> <li>Data must have been added (if you get a warning saying that data must be unique is because it's been already added)</li> </ul> <p>This process will create a default node component in the directory where the command is executed, or use the existing one. 2. Check that your data has been added by executing <code>fedbiomed node dataset list</code> 3. Run the node using <code>fedbiomed node start</code>. Wait until you get <code>Starting task manager</code>. it means you are online.</p>"},{"location":"tutorials/security/differential-privacy-with-opacus-on-fedbiomed/#defining-a-training-plan-and-parameters","title":"Defining a Training Plan and Parameters\u00b6","text":""},{"location":"tutorials/security/differential-privacy-with-opacus-on-fedbiomed/#declare-and-run-the-experiment","title":"Declare and run the experiment\u00b6","text":""},{"location":"tutorials/security/differential-privacy-with-opacus-on-fedbiomed/#testing","title":"Testing\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/","title":"Local and Central DP with Fed-BioMed: MONAI 2d image registration","text":"<p>We are now ready to start the researcher by using the environment where Fed-BioMed researcher is installed, and open the Jupyter notebook with <code>fedbiomed researcher start</code>.</p> <p>We can first quesry the network for the mednist dataset. In this case, the nodes are sharing the respective partitions unsing the same tag <code>mednist</code>:</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.requests import Requests\nfrom fedbiomed.researcher.config import config\nreq = Requests(config)\nreq.list(verbose=True)\n</pre> from fedbiomed.researcher.requests import Requests from fedbiomed.researcher.config import config req = Requests(config) req.list(verbose=True) <p>The code for network and data loader of the MONAI tutorial can now be deployed in Fed-BioMed. We first import the necessary modules from <code>fedbiomed</code> and <code>monai</code> libraries:</p> <p>We can now define the training plan. Note that we use the standard <code>TorchTrainingPlan</code> natively provided in Fed-BioMed. We reuse the <code>MedNISTDataset</code> data loader defined in the original MONAI tutorial, which is returned by the method <code>training_data</code>, which also implements the data parsing from the nodes <code>dataset_path</code>. We should also properly define the <code>training_routine</code>, following the MONAI tutorial. According to the MONAI tutorial, the model is the <code>GlobalNet</code> and the loss is <code>MSELoss</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport torch\nfrom torch.nn import MSELoss\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\n\n#from torch.utils.data import Dataset, DataLoader\nimport monai\nfrom monai.utils import set_determinism, first\nfrom monai.transforms import (\n    EnsureChannelFirstD,\n    Compose,\n    LoadImageD,\n    RandRotateD,\n    RandZoomD,\n    ScaleIntensityRanged,\n    EnsureTypeD,\n)\nfrom monai.data import DataLoader, Dataset, CacheDataset\nfrom monai.config import print_config, USE_COMPILED\nfrom monai.networks.nets import GlobalNet\nfrom monai.networks.blocks import Warp\nfrom monai.apps import MedNISTDataset\n\n\n# Here we define the training plan to be used. \nclass MyTrainingPlan(TorchTrainingPlan):\n        \n    # Dependencies for training plan\n    def init_dependencies(self):\n        deps = [\"import numpy as np\",\n            \"import monai\",\n            \"from torch.nn import MSELoss\",\n            \"from monai.utils import set_determinism, first\",\n            \"from monai.transforms import (EnsureChannelFirstD,Compose,LoadImageD,RandRotateD,RandZoomD,ScaleIntensityRanged,EnsureTypeD,)\",\n            \"from monai.data import DataLoader, Dataset, CacheDataset\",\n            \"from monai.networks.nets import GlobalNet\",\n            \"from monai.config import USE_COMPILED\",\n            \"from monai.networks.blocks import Warp\",\n            \"from monai.apps import MedNISTDataset\" ]\n        return deps \n    \n    # Model for training\n    def init_model(self):\n        \n        # Define model related attributes \n        self.image_loss = MSELoss()\n        if USE_COMPILED:\n            self.warp_layer = Warp(3, \"border\")\n        else:\n            self.warp_layer = Warp(\"bilinear\", \"border\")\n        \n        # Define model \n        model = GlobalNet(image_size=(64, 64),\n                          spatial_dims=2,\n                          in_channels=2,  # moving and fixed\n                          num_channel_initial=16,\n                          depth=3)\n        \n        return model \n    \n    # Optimizer for training\n    def init_optimizer(self, optimizer_args):\n        optimizer = torch.optim.Adam(self.model().parameters(), lr=optimizer_args[\"lr\"])\n        \n        return optimizer\n\n\n    def training_data(self):\n        # Custom torch Dataloader for MedNIST data\n        data_path = self.dataset_path\n        # The following line is needed if client structure does not contain the \"/MedNIST\" folder\n        MedNISTDataset.dataset_folder_name = \"\"\n        train_data = MedNISTDataset(root_dir=data_path, section=\"training\", download=False, transform=None)\n        training_datadict = [\n            {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}\n            for item in train_data.data if item[\"label\"] == 4  # label 4 is for xray hands\n        ]\n        train_transforms = Compose(\n            [\n                LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],\n                                     a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),\n                RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),\n                RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1,\n                          monaiprob=1.0, mode=\"bicubic\", align_corners=False),\n                EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),\n            ]\n        )\n        train_ds = CacheDataset(data=training_datadict, transform=train_transforms,\n                                cache_rate=1.0, num_workers=0)\n        dl = self.MednistDataLoader(train_ds)\n        \n        return DataManager(dl,  shuffle=True, num_workers=0)\n    \n    def training_step(self, moving, fixed):\n        ddf = self.model().forward(torch.cat((moving, fixed), dim=1))\n        pred_image = self.warp_layer(moving, ddf)\n        loss = self.image_loss(pred_image, fixed)\n        return loss\n    \n    class MednistDataLoader(monai.data.Dataset):\n        # Custom DataLoader that inherits from monai's Dataset object\n        def __init__(self, dataset):\n            self.dataset = dataset\n\n        def __len__(self):\n            return len(self.dataset)\n\n        def __getitem__(self, idx):\n            return (self.dataset[idx][\"moving_hand\"],\n                    self.dataset[idx][\"fixed_hand\"])\n</pre> import numpy as np import torch from torch.nn import MSELoss from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager  #from torch.utils.data import Dataset, DataLoader import monai from monai.utils import set_determinism, first from monai.transforms import (     EnsureChannelFirstD,     Compose,     LoadImageD,     RandRotateD,     RandZoomD,     ScaleIntensityRanged,     EnsureTypeD, ) from monai.data import DataLoader, Dataset, CacheDataset from monai.config import print_config, USE_COMPILED from monai.networks.nets import GlobalNet from monai.networks.blocks import Warp from monai.apps import MedNISTDataset   # Here we define the training plan to be used.  class MyTrainingPlan(TorchTrainingPlan):              # Dependencies for training plan     def init_dependencies(self):         deps = [\"import numpy as np\",             \"import monai\",             \"from torch.nn import MSELoss\",             \"from monai.utils import set_determinism, first\",             \"from monai.transforms import (EnsureChannelFirstD,Compose,LoadImageD,RandRotateD,RandZoomD,ScaleIntensityRanged,EnsureTypeD,)\",             \"from monai.data import DataLoader, Dataset, CacheDataset\",             \"from monai.networks.nets import GlobalNet\",             \"from monai.config import USE_COMPILED\",             \"from monai.networks.blocks import Warp\",             \"from monai.apps import MedNISTDataset\" ]         return deps           # Model for training     def init_model(self):                  # Define model related attributes          self.image_loss = MSELoss()         if USE_COMPILED:             self.warp_layer = Warp(3, \"border\")         else:             self.warp_layer = Warp(\"bilinear\", \"border\")                  # Define model          model = GlobalNet(image_size=(64, 64),                           spatial_dims=2,                           in_channels=2,  # moving and fixed                           num_channel_initial=16,                           depth=3)                  return model           # Optimizer for training     def init_optimizer(self, optimizer_args):         optimizer = torch.optim.Adam(self.model().parameters(), lr=optimizer_args[\"lr\"])                  return optimizer       def training_data(self):         # Custom torch Dataloader for MedNIST data         data_path = self.dataset_path         # The following line is needed if client structure does not contain the \"/MedNIST\" folder         MedNISTDataset.dataset_folder_name = \"\"         train_data = MedNISTDataset(root_dir=data_path, section=\"training\", download=False, transform=None)         training_datadict = [             {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}             for item in train_data.data if item[\"label\"] == 4  # label 4 is for xray hands         ]         train_transforms = Compose(             [                 LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),                 EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),                 ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],                                      a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),                 RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),                 RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1,                           monaiprob=1.0, mode=\"bicubic\", align_corners=False),                 EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),             ]         )         train_ds = CacheDataset(data=training_datadict, transform=train_transforms,                                 cache_rate=1.0, num_workers=0)         dl = self.MednistDataLoader(train_ds)                  return DataManager(dl,  shuffle=True, num_workers=0)          def training_step(self, moving, fixed):         ddf = self.model().forward(torch.cat((moving, fixed), dim=1))         pred_image = self.warp_layer(moving, ddf)         loss = self.image_loss(pred_image, fixed)         return loss          class MednistDataLoader(monai.data.Dataset):         # Custom DataLoader that inherits from monai's Dataset object         def __init__(self, dataset):             self.dataset = dataset          def __len__(self):             return len(self.dataset)          def __getitem__(self, idx):             return (self.dataset[idx][\"moving_hand\"],                     self.dataset[idx][\"fixed_hand\"]) <p>Finally we import the required modules for running any experiment</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage <p>We first train our model in a non-private way. We set the model and training parameters. In particular, we are going to perform 2 epochs over 3 rounds for this experiment. Moreover the training is performed on ~26% of the locally available training data. We are also trying to use GPU if available.</p> In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'loader_args': { 'batch_size': 16, }, \n    'optimizer_args': {\n        'lr': 1e-5\n    },\n    'use_gpu': True,\n    'epochs': 4, \n    'dry_run': False\n#    'batch_maxnum': 2,  # can be used to debugging to limit the number of batches per epoch\n#    'log_interval': 1,  # output a logging message every log_interval batches\n}\n\ntags =  ['#MEDNIST', '#dataset']\nrounds = 5\n</pre> model_args = {}  training_args = {     'loader_args': { 'batch_size': 16, },      'optimizer_args': {         'lr': 1e-5     },     'use_gpu': True,     'epochs': 4,      'dry_run': False #    'batch_maxnum': 2,  # can be used to debugging to limit the number of batches per epoch #    'log_interval': 1,  # output a logging message every log_interval batches }  tags =  ['#MEDNIST', '#dataset'] rounds = 5 <p>The experiment can be now defined, by providing the <code>mednist</code> tag, and running the local training on nodes with training plan defined in <code>training_plan_path</code>, standard <code>aggregator</code> (FedAvg) and <code>client_selection_strategy</code> (all nodes used). Federated learning is going to be perfomed through 3 optimization rounds.</p> In\u00a0[\u00a0]: Copied! <pre>exp = Experiment(tags=tags,\n                 training_plan_class=MyTrainingPlan,\n                 model_args=model_args,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None\n                )\n</pre> exp = Experiment(tags=tags,                  training_plan_class=MyTrainingPlan,                  model_args=model_args,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None                 ) <p>Let's start the experiment.</p> <p>By default, this function doesn't stop until all the <code>round_limit</code> rounds are done for all the clients</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') In\u00a0[\u00a0]: Copied! <pre>import urllib.request\nresponse = urllib.request.urlopen('https://raw.githubusercontent.com/tensorflow/privacy/7eea74a6a1cf15e2d2bd890722400edd0e470db8/research/hyperparameters_2022/rdp_accountant.py')\nrdp_accountant = response.read()\nexec(rdp_accountant)\n\ndef get_iterations(target_delta, sigma, q, max_epsilon, max_N):\n    \"\"\"Computes max number of iterations given budget parameters\n\n    Args:\n        target_delta: If not `None`, the delta for which we compute the corresponding epsilon.\n        sigma: sigma to be used in Gaussian DP mechanism\n        q: training sample ratio\n        max_epsilon: Maximum budget allowed\n         max_N: Maximum number of iterations\n\n    Returns:\n        An integer number of iterations, and the evolution of the budget\n    Raises:\n        ValueError: If target_eps and target_delta are messed up.\n    \"\"\"\n\n    orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n    rdp = compute_rdp(q=q,\n                      noise_multiplier=sigma,\n                      steps=1,\n                      orders=orders)\n    epsilon_range = [get_privacy_spent(orders, i * rdp, target_delta=target_delta) for i in range(max_N)]\n    max_training_steps = int(np.sum(np.array([x[0] for x in epsilon_range]) &lt; max_epsilon))\n    return max_training_steps, [x[0] for x in epsilon_range][:max_training_steps]\n</pre> import urllib.request response = urllib.request.urlopen('https://raw.githubusercontent.com/tensorflow/privacy/7eea74a6a1cf15e2d2bd890722400edd0e470db8/research/hyperparameters_2022/rdp_accountant.py') rdp_accountant = response.read() exec(rdp_accountant)  def get_iterations(target_delta, sigma, q, max_epsilon, max_N):     \"\"\"Computes max number of iterations given budget parameters      Args:         target_delta: If not `None`, the delta for which we compute the corresponding epsilon.         sigma: sigma to be used in Gaussian DP mechanism         q: training sample ratio         max_epsilon: Maximum budget allowed          max_N: Maximum number of iterations      Returns:         An integer number of iterations, and the evolution of the budget     Raises:         ValueError: If target_eps and target_delta are messed up.     \"\"\"      orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))     rdp = compute_rdp(q=q,                       noise_multiplier=sigma,                       steps=1,                       orders=orders)     epsilon_range = [get_privacy_spent(orders, i * rdp, target_delta=target_delta) for i in range(max_N)]     max_training_steps = int(np.sum(np.array([x[0] for x in epsilon_range]) &lt; max_epsilon))     return max_training_steps, [x[0] for x in epsilon_range][:max_training_steps] <p>In order to perform DP training (both local and central) we need to provide to the model and training schemes:</p> <ul> <li><code>clip</code>: defining the maximal L2 norm of gradients</li> <li><code>sigma</code>: defining the strength of Gaussian noise to be added (either to gradients in case of LDP or to the final local model in case of CDP)</li> </ul> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.requests import Requests\nfrom fedbiomed.researcher.config import config\nreq = Requests(config)\nquery_nodes = req.list()\n</pre> from fedbiomed.researcher.requests import Requests from fedbiomed.researcher.config import config req = Requests(config) query_nodes = req.list() In\u00a0[\u00a0]: Copied! <pre>query_nodes\n</pre> query_nodes In\u00a0[\u00a0]: Copied! <pre>min_dataset_size = min([dataset['shape'][0] for i in query_nodes for dataset in query_nodes[i] if dataset['tags'] == ['#MEDNIST', '#dataset']]) #see training data in model\ntot_dataset_size = sum([dataset['shape'][0] for i in query_nodes for dataset in query_nodes[i] if dataset['tags'] == ['#MEDNIST', '#dataset']]) #see training data in model\n</pre> min_dataset_size = min([dataset['shape'][0] for i in query_nodes for dataset in query_nodes[i] if dataset['tags'] == ['#MEDNIST', '#dataset']]) #see training data in model tot_dataset_size = sum([dataset['shape'][0] for i in query_nodes for dataset in query_nodes[i] if dataset['tags'] == ['#MEDNIST', '#dataset']]) #see training data in model In\u00a0[\u00a0]: Copied! <pre>q = training_args['loader_args']['batch_size']/min_dataset_size\nsigma = 0.4\nclip = 0.005\ndelta = .1/min_dataset_size\nmax_epsilon = 10.\nmax_N = int(1e2)\n</pre> q = training_args['loader_args']['batch_size']/min_dataset_size sigma = 0.4 clip = 0.005 delta = .1/min_dataset_size max_epsilon = 10. max_N = int(1e2) In\u00a0[\u00a0]: Copied! <pre>N, eps_list = get_iterations(delta, sigma, q, max_epsilon, max_N)\n</pre> N, eps_list = get_iterations(delta, sigma, q, max_epsilon, max_N) In\u00a0[\u00a0]: Copied! <pre>max_rounds = N/(training_args['epochs'])\n</pre> max_rounds = N/(training_args['epochs']) In\u00a0[\u00a0]: Copied! <pre>assert training_args['epochs']*rounds&lt;=max_rounds, 'Number of rounds not compatible with privacy budget'\n\nprint(f'The maximal number of FL rounds for ({max_epsilon},{delta})-LDP training is {max_rounds}')\nprint('The selected number of FL rounds, '+str(rounds)+\n      ',implies ('+str(eps_list[training_args['epochs']*rounds-1])+','+str(delta)+',)-LDP')\n</pre> assert training_args['epochs']*rounds&lt;=max_rounds, 'Number of rounds not compatible with privacy budget'  print(f'The maximal number of FL rounds for ({max_epsilon},{delta})-LDP training is {max_rounds}') print('The selected number of FL rounds, '+str(rounds)+       ',implies ('+str(eps_list[training_args['epochs']*rounds-1])+','+str(delta)+',)-LDP') <p>We are now going to repeat the same training but with private SGD: at each epoch gradients are clipped and perturbed according to the provided privacy parameters.</p> <p>In order to perform DP-training we should provide an additional argument to training: the dictionalry <code>'DP_args'</code> containing necessary parameters for DP. If we want to perform LDP, we should specify: <code>'type' : 'local'</code>.</p> In\u00a0[\u00a0]: Copied! <pre>model_args = {}\nLDP = {'dp_args': {'type' : 'local', 'sigma': sigma, 'clip': clip}}\ntraining_args.update(LDP)\ntraining_args\n</pre> model_args = {} LDP = {'dp_args': {'type' : 'local', 'sigma': sigma, 'clip': clip}} training_args.update(LDP) training_args In\u00a0[\u00a0]: Copied! <pre>exp_LDP = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None\n                )\n</pre> exp_LDP = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None                 ) In\u00a0[\u00a0]: Copied! <pre>exp_LDP.run()\n</pre> exp_LDP.run() In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nnum_clients = len([dataset['shape'][0] for i in query_nodes for dataset in query_nodes[i] if dataset['tags'] == tags])\n\n# Here we use the same parameters as LDP to evaluate the number of rounds, \n# since we are performing record-level DP\n\nq = training_args['loader_args']['batch_size']/min_dataset_size \nsigma = 0.4#/(np.sqrt(num_clients)*training_args['loader_args']['batch_size'])\nclip = 0.005\ndelta = .1/min_dataset_size\nmax_epsilon = 10.\nmax_N = int(1e2)\n\nN, eps_list = get_iterations(delta, sigma, q, max_epsilon, max_N)\n</pre> import numpy as np num_clients = len([dataset['shape'][0] for i in query_nodes for dataset in query_nodes[i] if dataset['tags'] == tags])  # Here we use the same parameters as LDP to evaluate the number of rounds,  # since we are performing record-level DP  q = training_args['loader_args']['batch_size']/min_dataset_size  sigma = 0.4#/(np.sqrt(num_clients)*training_args['loader_args']['batch_size']) clip = 0.005 delta = .1/min_dataset_size max_epsilon = 10. max_N = int(1e2)  N, eps_list = get_iterations(delta, sigma, q, max_epsilon, max_N) In\u00a0[\u00a0]: Copied! <pre>max_rounds = N/(training_args['epochs'])\nprint(max_rounds)\n</pre> max_rounds = N/(training_args['epochs']) print(max_rounds) In\u00a0[\u00a0]: Copied! <pre>assert rounds&lt;=max_rounds, 'Number of rounds not compatible with privacy budget'\n\nprint(f'The maximal number of allowed rounds for ({max_epsilon},{delta})-CDP training is {max_rounds}')\nprint(f'The selected number of training rounds, '+str(rounds)+\n      ',implies ('+str(eps_list[rounds-1])+','+str(delta)+',)-CDP')\n</pre> assert rounds&lt;=max_rounds, 'Number of rounds not compatible with privacy budget'  print(f'The maximal number of allowed rounds for ({max_epsilon},{delta})-CDP training is {max_rounds}') print(f'The selected number of training rounds, '+str(rounds)+       ',implies ('+str(eps_list[rounds-1])+','+str(delta)+',)-CDP') <p>If we want to perform CDP, we should update the <code>'DP_args'</code> dictionary by setting:  <code>'type' : 'central'</code>. Otherwise we are going to keep the same privacy parameters.</p> In\u00a0[\u00a0]: Copied! <pre>CDP = {'dp_args': {'type' : 'central', 'sigma': sigma/np.sqrt(num_clients), 'clip': clip}}\ntraining_args.update(CDP)\ntraining_args\n</pre> CDP = {'dp_args': {'type' : 'central', 'sigma': sigma/np.sqrt(num_clients), 'clip': clip}} training_args.update(CDP) training_args In\u00a0[\u00a0]: Copied! <pre>exp_CDP = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None\n                )\n</pre> exp_CDP = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None                 ) In\u00a0[\u00a0]: Copied! <pre>exp_CDP.run()\n</pre> exp_CDP.run() <p>We are now going to test and compare locally the three final federated models on an independent testing partition. The test dataset is available at this link:</p> <p>https://drive.google.com/file/d/1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD/</p> In\u00a0[\u00a0]: Copied! <pre>!pip install matplotlib -q\n!pip install gdown -q\n</pre> !pip install matplotlib -q !pip install gdown -q In\u00a0[\u00a0]: Copied! <pre>import os\nimport tempfile\nimport PIL\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gdown\nimport zipfile\nimport matplotlib.pyplot as plt\n\nprint_config()\nset_determinism(42)\n</pre> import os import tempfile import PIL import torch import numpy as np import matplotlib.pyplot as plt import gdown import zipfile import matplotlib.pyplot as plt  print_config() set_determinism(42) <p>Download the testing dataset on the local temporary folder.</p> In\u00a0[\u00a0]: Copied! <pre>import gdown\nimport zipfile\nimport tempfile\nimport os\nfrom fedbiomed.researcher.config import config\n\ntmp_dir = tempfile.TemporaryDirectory(dir=config.vars['TMP_DIR']+os.sep)\n\nresource = \"https://drive.google.com/uc?id=1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD\"\nbase_dir = tmp_dir.name\ntest_file = os.path.join(base_dir, \"MedNIST_testing.zip\")\n\ngdown.download(resource, test_file, quiet=False)\n\nzf = zipfile.ZipFile(test_file)\n\nfor file in zf.infolist():\n    zf.extract(file, base_dir)\n    \ndata_dir = os.path.join(base_dir, \"MedNIST_testing\")\n</pre> import gdown import zipfile import tempfile import os from fedbiomed.researcher.config import config  tmp_dir = tempfile.TemporaryDirectory(dir=config.vars['TMP_DIR']+os.sep)  resource = \"https://drive.google.com/uc?id=1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD\" base_dir = tmp_dir.name test_file = os.path.join(base_dir, \"MedNIST_testing.zip\")  gdown.download(resource, test_file, quiet=False)  zf = zipfile.ZipFile(test_file)  for file in zf.infolist():     zf.extract(file, base_dir)      data_dir = os.path.join(base_dir, \"MedNIST_testing\") <p>We redefine our custom dataloader (defined previously in  the <code>TrainingPlan</code>):</p> In\u00a0[\u00a0]: Copied! <pre>from monai.data import DataLoader, Dataset, CacheDataset\nimport monai\n\nclass MednistDataLoader(monai.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        return (self.dataset[idx][\"moving_hand\"],\n                self.dataset[idx][\"fixed_hand\"])\n</pre> from monai.data import DataLoader, Dataset, CacheDataset import monai  class MednistDataLoader(monai.data.Dataset):     def __init__(self, dataset):         self.dataset = dataset      def __len__(self):         return len(self.dataset)      def __getitem__(self, idx):         return (self.dataset[idx][\"moving_hand\"],                 self.dataset[idx][\"fixed_hand\"]) <p>Create the testing data loader and pairs of moving vs fixed hands:</p> In\u00a0[\u00a0]: Copied! <pre># Use a GPU if you have one + enough memory available\n#\n#use_cuda = torch.cuda.is_available()\n#device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\ndevice = 'cpu'\n\n\n# recreate model\nmodel = GlobalNet(\n    image_size=(64, 64),\n    spatial_dims=2,\n    in_channels=2,  # moving and fixed\n    num_channel_initial=16,\n    depth=3).to(device)\n\nif USE_COMPILED:\n    warp_layer = Warp(3, \"border\").to(device)\nelse:\n    warp_layer = Warp(\"bilinear\", \"border\").to(device)\n\nMedNISTDataset.dataset_folder_name = \"\"\ntest_data = MedNISTDataset(root_dir=data_dir, section=\"test\", download=False, transform=None)\ntesting_datadict = [\n    {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}\n    for item in test_data.data if item[\"label\"] == 4  # label 4 is for xray hands\n]\ntest_transforms = Compose(\n            [\n                LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],\n                                     a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),\n                RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),\n                RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1, prob=1.0, mode=\"bicubic\", align_corners=False),\n                EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),\n            ]\n        )\nval_ds = CacheDataset(data=testing_datadict[:1000], transform=test_transforms,\n                      cache_rate=1.0, num_workers=0)\nval_dl = MednistDataLoader(val_ds)\nval_loader = DataLoader(val_dl, batch_size=16, num_workers=0)\n</pre> # Use a GPU if you have one + enough memory available # #use_cuda = torch.cuda.is_available() #device = torch.device(\"cuda:0\" if use_cuda else \"cpu\") device = 'cpu'   # recreate model model = GlobalNet(     image_size=(64, 64),     spatial_dims=2,     in_channels=2,  # moving and fixed     num_channel_initial=16,     depth=3).to(device)  if USE_COMPILED:     warp_layer = Warp(3, \"border\").to(device) else:     warp_layer = Warp(\"bilinear\", \"border\").to(device)  MedNISTDataset.dataset_folder_name = \"\" test_data = MedNISTDataset(root_dir=data_dir, section=\"test\", download=False, transform=None) testing_datadict = [     {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}     for item in test_data.data if item[\"label\"] == 4  # label 4 is for xray hands ] test_transforms = Compose(             [                 LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),                 EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),                 ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],                                      a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),                 RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),                 RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1, prob=1.0, mode=\"bicubic\", align_corners=False),                 EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),             ]         ) val_ds = CacheDataset(data=testing_datadict[:1000], transform=test_transforms,                       cache_rate=1.0, num_workers=0) val_dl = MednistDataLoader(val_ds) val_loader = DataLoader(val_dl, batch_size=16, num_workers=0) <p>To test the federated models we need to create model instances and assign to it the models parameters estimated at the last federated optimization rounds. Then, we generate predictions of the transformation between pairs. In addition, we evaluate the structural similarity index for each model.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install torchmetrics -q\n\nfrom torchmetrics.functional import structural_similarity_index_measure\n\n# Non private training\nmodel = exp.training_plan().model()\nmodel.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n\n# training with LDP\nmodel_LDP = exp_LDP.training_plan().model()\nmodel_LDP.load_state_dict(exp_LDP.aggregated_params()[rounds - 1]['params'])\n\n# training with CDP\nmodel_CDP = exp_CDP.training_plan().model()\nmodel_CDP.load_state_dict(exp_CDP.aggregated_params()[rounds - 1]['params'])\n\nfor moving, fixed in val_loader:\n    # Non private training\n    ddf = model(torch.cat((moving, fixed), dim=1))\n    pred_image = warp_layer(moving, ddf)\n    \n    # training with LDP\n    ddf_LDP = model_LDP(torch.cat((moving, fixed), dim=1))\n    pred_image_LDP = warp_layer(moving, ddf_LDP)\n    \n    # training with CDP\n    ddf_CDP = model_CDP(torch.cat((moving, fixed), dim=1))\n    pred_image_CDP = warp_layer(moving, ddf_CDP)\n    \n    # ssim predicted vs ground truth\n    # Non private training\n    SSIM = structural_similarity_index_measure(pred_image, fixed)\n    # training with LDP\n    SSIM_LDP = structural_similarity_index_measure(pred_image_LDP, fixed)\n    # training with CDP\n    SSIM_CDP = structural_similarity_index_measure(pred_image_CDP, fixed)\n    \n    break\n\nfixed_image = fixed.detach().cpu().numpy()[:, 0]\nmoving_image = moving.detach().cpu().numpy()[:, 0]\npred_image = pred_image.detach().cpu().numpy()[:, 0]\npred_image_LDP = pred_image_LDP.detach().cpu().numpy()[:, 0]\npred_image_CDP = pred_image_CDP.detach().cpu().numpy()[:, 0]\n</pre> !pip install torchmetrics -q  from torchmetrics.functional import structural_similarity_index_measure  # Non private training model = exp.training_plan().model() model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])  # training with LDP model_LDP = exp_LDP.training_plan().model() model_LDP.load_state_dict(exp_LDP.aggregated_params()[rounds - 1]['params'])  # training with CDP model_CDP = exp_CDP.training_plan().model() model_CDP.load_state_dict(exp_CDP.aggregated_params()[rounds - 1]['params'])  for moving, fixed in val_loader:     # Non private training     ddf = model(torch.cat((moving, fixed), dim=1))     pred_image = warp_layer(moving, ddf)          # training with LDP     ddf_LDP = model_LDP(torch.cat((moving, fixed), dim=1))     pred_image_LDP = warp_layer(moving, ddf_LDP)          # training with CDP     ddf_CDP = model_CDP(torch.cat((moving, fixed), dim=1))     pred_image_CDP = warp_layer(moving, ddf_CDP)          # ssim predicted vs ground truth     # Non private training     SSIM = structural_similarity_index_measure(pred_image, fixed)     # training with LDP     SSIM_LDP = structural_similarity_index_measure(pred_image_LDP, fixed)     # training with CDP     SSIM_CDP = structural_similarity_index_measure(pred_image_CDP, fixed)          break  fixed_image = fixed.detach().cpu().numpy()[:, 0] moving_image = moving.detach().cpu().numpy()[:, 0] pred_image = pred_image.detach().cpu().numpy()[:, 0] pred_image_LDP = pred_image_LDP.detach().cpu().numpy()[:, 0] pred_image_CDP = pred_image_CDP.detach().cpu().numpy()[:, 0] In\u00a0[\u00a0]: Copied! <pre>print('---&gt; Results for non-private training')\nprint(f'SSIM = {SSIM}')\n\nprint('---&gt; Results for training with LDP')\nprint(f'SSIM = {SSIM_LDP})')\n\nprint('---&gt; Results for training with CDP')\nprint(f'SSIM = {SSIM_CDP})')\n</pre> print('---&gt; Results for non-private training') print(f'SSIM = {SSIM}')  print('---&gt; Results for training with LDP') print(f'SSIM = {SSIM_LDP})')  print('---&gt; Results for training with CDP') print(f'SSIM = {SSIM_CDP})') <p>Finally, we can print some example of predictions of all models from the testing dataset.</p> In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\nbatch_size = 10\nplt.subplots(batch_size, 5, figsize=(12, 25))\nfor b in range(batch_size):\n    # moving image\n    plt.subplot(batch_size, 5, b * 5 + 1)\n    plt.axis('off')\n    plt.title(\"moving image\")\n    plt.imshow(moving_image[b], cmap=\"gray\")\n    # fixed image\n    plt.subplot(batch_size, 5, b * 5 + 2)\n    plt.axis('off')\n    plt.title(\"fixed image\")\n    plt.imshow(fixed_image[b], cmap=\"gray\")\n    # warped moving\n    plt.subplot(batch_size, 5, b * 5 + 3)\n    plt.axis('off')\n    plt.title(\"predicted image\")\n    plt.imshow(pred_image[b], cmap=\"gray\")\n    # warped moving LDP\n    plt.subplot(batch_size, 5, b * 5 + 4)\n    plt.axis('off')\n    plt.title(\"predicted image (LDP)\")\n    plt.imshow(pred_image_LDP[b], cmap=\"gray\")\n    # warped moving CDP\n    plt.subplot(batch_size, 5, b * 5 + 5)\n    plt.axis('off')\n    plt.title(\"predicted image (CDP)\")\n    plt.imshow(pred_image_CDP[b], cmap=\"gray\")\nplt.axis('off')\nplt.show()\n</pre> %matplotlib inline batch_size = 10 plt.subplots(batch_size, 5, figsize=(12, 25)) for b in range(batch_size):     # moving image     plt.subplot(batch_size, 5, b * 5 + 1)     plt.axis('off')     plt.title(\"moving image\")     plt.imshow(moving_image[b], cmap=\"gray\")     # fixed image     plt.subplot(batch_size, 5, b * 5 + 2)     plt.axis('off')     plt.title(\"fixed image\")     plt.imshow(fixed_image[b], cmap=\"gray\")     # warped moving     plt.subplot(batch_size, 5, b * 5 + 3)     plt.axis('off')     plt.title(\"predicted image\")     plt.imshow(pred_image[b], cmap=\"gray\")     # warped moving LDP     plt.subplot(batch_size, 5, b * 5 + 4)     plt.axis('off')     plt.title(\"predicted image (LDP)\")     plt.imshow(pred_image_LDP[b], cmap=\"gray\")     # warped moving CDP     plt.subplot(batch_size, 5, b * 5 + 5)     plt.axis('off')     plt.title(\"predicted image (CDP)\")     plt.imshow(pred_image_CDP[b], cmap=\"gray\") plt.axis('off') plt.show()"},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#local-and-central-dp-with-fed-biomed-monai-2d-image-registration","title":"Local and Central DP with Fed-BioMed: MONAI 2d image registration\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial shows how to deploy in Fed-BioMed the 2d image registration example provided in the project MONAI (https://monai.io/), trained with Differential Privacy (DP). We are going to compare results of:</p> <ul> <li>non private training</li> <li>train with Local Differential Privacy (LDP)</li> <li>train with Central Differential Privacy (CDP)</li> </ul> <p>In order to enforce differential privacy during training (both local and central) we will rely on the Opacus library (https://opacus.ai/).</p>"},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#image-registration","title":"Image Registration\u00b6","text":"<p>Image registration is the process of transforming and recalibrating different images into one coordinate system. It makes possible to compare several images captured with the same modality.</p> <p>In this tutorial, we are using a UNet-like registration network ( https://arxiv.org/abs/1711.01666 ). Goal of the notebook is to train a model given moving images and fixed images (recalibrated images).</p>"},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#creating-mednist-nodes","title":"Creating MedNIST nodes\u00b6","text":"<p>MedNIST provides an artificial 2d classification dataset created by gathering different medical imaging datasets from TCIA, the RSNA Bone Age Challenge, and the NIH Chest X-ray dataset. The dataset is kindly made available by Dr. Bradley J. Erickson M.D., Ph.D. (Department of Radiology, Mayo Clinic) under the Creative Commons CC BY-SA 4.0 license.</p> <p>To proceed with the tutorial, we created an iid partitioning of the MedNIST dataset between 3 clients. Each client has 3000 image samples for each class. The training partitions are availables at the following link:</p> <p>https://drive.google.com/file/d/1vLIcBdtdAhh6K-vrgCFy_0Y55dxOWZwf/view</p> <p>The dataset owned by each client has structure:</p> <p>\u2514\u2500\u2500 client_*/</p> <pre><code>\u251c\u2500\u2500 AbdomenCT/\n\n\u2514\u2500\u2500 BreastMRI/\n\n\u2514\u2500\u2500 CXR/\n\n\u2514\u2500\u2500 ChestCT/\n\n\u2514\u2500\u2500 Hand/\n\n\u2514\u2500\u2500 HeadCT/   </code></pre> <p>To create the federated dataset, we follow the standard procedure for node creation/population of Fed-BioMed.</p> <p>we use the environment where Fed-BioMed node is installed</p> <p>we create a first node by using the commands</p> <p><code>fedbiomed node start</code></p> <p>We then poulate the node with the data of first client:</p> <p><code>fedbiomed node dataset add</code></p> <p>We select option 3 (images) to add MedNIST partition of client 1, by just picking the folder of client 1. Assign tag <code>mednist</code> to the data when asked.</p> <p>We can further check that the data has been added by executing <code>fedbiomed node dataset list</code></p> <p>Following the same procedure, we create the other two nodes with the datasets of client 2 and client 3 respectively.</p>"},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#running-fed-biomed-researcher","title":"Running Fed-BioMed Researcher\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#create-an-experiment-to-train-a-model-on-the-data-found","title":"Create an experiment to train a model on the data found\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#non-private-training","title":"Non-private training\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#training-with-dp","title":"Training with DP\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#download-and-execute-rdp-accountant-module","title":"Download and execute RDP Accountant Module\u00b6","text":"<p>Following actions will download RDP module to calculate privacy budget and create a function called <code>get_iterations</code> which is going to be used for calculating the number training iterations that respects the privacy budget. The result of the function will be used for finding max number of rounds for the experiment.</p>"},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#dp-parameters","title":"DP parameters\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#ldp","title":"LDP\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#dimensioning-the-training-parameters-with-ldp","title":"Dimensioning the training parameters with LDP\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#update-training-parameters-for-ldp","title":"Update training parameters for LDP\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#declare-and-run-the-ldp-training","title":"Declare and run the LDP training\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#cdp","title":"CDP\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#dimensioning-the-training-parameters-with-cdp","title":"Dimensioning the training parameters with CDP\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#update-training-parameters-for-cdp","title":"Update training parameters for CDP\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#declare-and-run-the-cdp-training","title":"Declare and run the CDP training\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#testing","title":"Testing\u00b6","text":""},{"location":"tutorials/security/secure-aggregation/","title":"Training with Secure Aggregation","text":"<p>Declare a torch training plan MyTrainingPlan class to send for training on the node</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\n\n# Here we define the model to be used. \n# You can use any class name (here 'Net')\nclass MyTrainingPlan(TorchTrainingPlan):\n    \n    # Defines and return model \n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n    \n    # Defines and return optimizer\n    def init_optimizer(self, optimizer_args):\n        return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n    \n    # Declares and return dependencies\n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\"]\n        return deps\n    \n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            self.fc1 = nn.Linear(9216, 128)\n            self.fc2 = nn.Linear(128, 10)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.dropout2(x)\n            x = self.fc2(x)\n\n\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_data(self):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        train_kwargs = { 'shuffle': True}\n        return DataManager(dataset=dataset1, **train_kwargs)\n    \n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms   # Here we define the model to be used.  # You can use any class name (here 'Net') class MyTrainingPlan(TorchTrainingPlan):          # Defines and return model      def init_model(self, model_args):         return self.Net(model_args = model_args)          # Defines and return optimizer     def init_optimizer(self, optimizer_args):         return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])          # Declares and return dependencies     def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\"]         return deps          class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.conv1 = nn.Conv2d(1, 32, 3, 1)             self.conv2 = nn.Conv2d(32, 64, 3, 1)             self.dropout1 = nn.Dropout(0.25)             self.dropout2 = nn.Dropout(0.5)             self.fc1 = nn.Linear(9216, 128)             self.fc2 = nn.Linear(128, 10)          def forward(self, x):             x = self.conv1(x)             x = F.relu(x)             x = self.conv2(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = self.dropout1(x)             x = torch.flatten(x, 1)             x = self.fc1(x)             x = F.relu(x)             x = self.dropout2(x)             x = self.fc2(x)               output = F.log_softmax(x, dim=1)             return output      def training_data(self):         # Custom torch Dataloader for MNIST data         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         train_kwargs = { 'shuffle': True}         return DataManager(dataset=dataset1, **train_kwargs)          def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss  In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'loader_args': { 'batch_size': 48, }, \n    'optimizer_args': {\n        \"lr\" : 1e-3\n    },\n    'epochs': 1, \n    'dry_run': False,  \n    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n</pre> model_args = {}  training_args = {     'loader_args': { 'batch_size': 48, },      'optimizer_args': {         \"lr\" : 1e-3     },     'epochs': 1,      'dry_run': False,       'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } <p>Fed-BioMed uses Low-Overhead Masking (LOM) as the default secure aggregation scheme. If you followed the configuration steps to use Joye-Libert instead of LOM you can change secure aggregation by declaring the secure scheme as <code>SecaggSchemes.JOYE_LIBERT</code>.</p> <pre>from fedbiomed.researcher.secagg import SecureAggregation, SecureAggregationSchemes\nexp = Experiment(\n        ...\n        secagg = SecureAggregation(scheme=SecaggSchemes.JOYE_LIBERT)\n        ...    \n)\n</pre> <p>The example below will run LOM by default.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\nfrom fedbiomed.researcher.secagg import SecureAggregation, SecureAggregationSchemes\ntags =  ['#MNIST', '#dataset']\nrounds = 2\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None,\n                 secagg=SecureAggregation(), # or secagg=True\n                 # secagg=SecureAggregation(scheme=SecureAggregationSchemes.JOYE_LIBERT),\n                 save_breakpoints=True\n                )\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage from fedbiomed.researcher.secagg import SecureAggregation, SecureAggregationSchemes tags =  ['#MNIST', '#dataset'] rounds = 2  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None,                  secagg=SecureAggregation(), # or secagg=True                  # secagg=SecureAggregation(scheme=SecureAggregationSchemes.JOYE_LIBERT),                  save_breakpoints=True                 ) <p>Please use the attribute <code>secagg</code> to verify secure aggregation is set as active</p> In\u00a0[\u00a0]: Copied! <pre>print(\"Is using secagg: \", exp.secagg.active)\n</pre> print(\"Is using secagg: \", exp.secagg.active) <p>It is also possible to check secure aggregation context using <code>secagg</code> attribute. Since secure aggregation context negotiation will occur during experiment run, context and id should be <code>None</code> at this point.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"Active: \", exp.secagg.active)\nif exp.secagg.scheme == SecureAggregationSchemes.JOYE_LIBERT:\n    print(\"Secagg Servkey \", exp.secagg.servkey)\nelse:\n    print(\"Secagg context\", exp.secagg.dh)\n</pre> print(\"Active: \", exp.secagg.active) if exp.secagg.scheme == SecureAggregationSchemes.JOYE_LIBERT:     print(\"Secagg Servkey \", exp.secagg.servkey) else:     print(\"Secagg context\", exp.secagg.dh) <p>Run the experiment, using secure aggregation. Secure aggregation context will be created before the first training round, and it is going to be updated before each round when new nodes are added or removed to the experiment.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run(increase=True)\n</pre> exp.run(increase=True) <p>Save trained model to file</p> In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().export_model('./trained_model')\n</pre> exp.training_plan().export_model('./trained_model') <p>Display context after running one round of training.</p> <p>Context types</p> <p>In the Joye-Libert scheme, the context refers to the keys that will be used for aggregation. However, in LOM, there is no need for an aggregation key since the sum of masked inputs directly results in the aggregation of the inputs. Therefore, the context in LOM reflects the setup status of each participating node, ensuring that they have successfully created their keying material.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"Active: \", exp.secagg.active)\nif exp.secagg.scheme == SecureAggregationSchemes.JOYE_LIBERT:\n    print(\"Secagg Servkey context: \", exp.secagg.servkey.context)\nelse:\n    print(\"Secagg context\", exp.secagg.dh.context)\n</pre> print(\"Active: \", exp.secagg.active) if exp.secagg.scheme == SecureAggregationSchemes.JOYE_LIBERT:     print(\"Secagg Servkey context: \", exp.secagg.servkey.context) else:     print(\"Secagg context\", exp.secagg.dh.context) In\u00a0[\u00a0]: Copied! <pre># sends new dataset search request\n\nexp.set_training_data(None, True)\n</pre> # sends new dataset search request  exp.set_training_data(None, True)  In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) <p>Setting <code>secagg</code> argument <code>True</code> in <code>Experiment</code> creates a default <code>SecureAggregation</code> instance. Additionally, It is also possible to create <code>SecureAggregation</code> instance and pass it as an argument. Here are the arguments that can be set for the <code>SecureAggregation</code></p> <ul> <li><code>active</code>: <code>True</code> if the round will use secure aggregation. Default is <code>True</code></li> <li><code>clipping_range</code>: Clipping range that is going be use for quantization of model parameters, which means model weights will be bounded in range [-clipping_range, clipping_range]. Default clipping range is <code>3</code>. However, some models can have model weights greater than <code>3</code>. If clipping range is exceeded during the encryption on the nodes, <code>Experiment</code> will log a warning message. In such cases, you can provide a higher clipping range through the argument <code>clipping_range</code>.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.secagg import SecureAggregation\nsecagg = SecureAggregation(\n    active=True, \n    clipping_range=100,\n    # scheme = SecureAggregationSchemes.JOYE_LIBERT, # If secagg scheme Joye-Libert since the beginning of the tutorial\n    \n)\nexp.set_secagg(secagg=secagg)\n</pre> from fedbiomed.researcher.secagg import SecureAggregation secagg = SecureAggregation(     active=True,      clipping_range=100,     # scheme = SecureAggregationSchemes.JOYE_LIBERT, # If secagg scheme Joye-Libert since the beginning of the tutorial      ) exp.set_secagg(secagg=secagg)  In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) In\u00a0[\u00a0]: Copied! <pre>loaded_exp = Experiment.load_breakpoint()\nloaded_exp.info()\n</pre> loaded_exp = Experiment.load_breakpoint() loaded_exp.info() In\u00a0[\u00a0]: Copied! <pre>print(\"Active: \", exp.secagg.active)\nif exp.secagg.scheme == SecureAggregationSchemes.JOYE_LIBERT:\n    print(\"Secagg Servkey context: \", exp.secagg.servkey.context)\nelse:\n    print(\"Secagg context\", exp.secagg.dh.context)\n</pre> print(\"Active: \", exp.secagg.active) if exp.secagg.scheme == SecureAggregationSchemes.JOYE_LIBERT:     print(\"Secagg Servkey context: \", exp.secagg.servkey.context) else:     print(\"Secagg context\", exp.secagg.dh.context) In\u00a0[\u00a0]: Copied! <pre>loaded_exp.run_once(increase=True)\n</pre> loaded_exp.run_once(increase=True) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/security/secure-aggregation/#training-with-secure-aggregation","title":"Training with Secure Aggregation\u00b6","text":"<p>Secure aggregation is one of the security feature that is provided by Fed-BioMed. Please refer to secure aggregation user guide for more information regarding the methods and techniques that are used. This tutorial gives an example of secure aggregation usage in Fed-BioMed.</p>"},{"location":"tutorials/security/secure-aggregation/#setting-up-the-nodes","title":"Setting up the nodes\u00b6","text":"<p>During the tutorial, nodes and researcher will be launched locally using single clone of Fed-BioMed. However, it is also possible to execute notebook cells when the components are configured remotely by respecting following instruction.</p>"},{"location":"tutorials/security/secure-aggregation/#configuringinstalling-element-for-secure-aggregation","title":"Configuring/Installing Element for Secure Aggregation\u00b6","text":"<p>Fed-BioMed provides two secure aggregation schemes: LOM and Joye-Libert. While LOM doesn't require configuration or extra installation. Joye-Libert depends on third-party modules and certificate configuration after a basic installation of Fed-BioMed.</p> <p>You can follow the detailed instructions for configuring Fed-BioMed instance for secure aggregation or apply following shortened instructions for a basic setup for Joye-Libert.</p>"},{"location":"tutorials/security/secure-aggregation/#1-create-node-and-researcher-instances","title":"1. Create node and researcher instances\u00b6","text":""},{"location":"tutorials/security/secure-aggregation/#11-create-nodes","title":"1.1 Create nodes\u00b6","text":"<p>It is mandatory to have at least two nodes for the experiment that requires secure aggregation. Please execute following commands to create two nodes.</p> <p>Node 1:</p> <pre>fedbiomed component create -c node  --path my-node\n</pre> <p>Node 2:</p> <pre>fedbiomed component create -c node  --path my-second-node\n</pre>"},{"location":"tutorials/security/secure-aggregation/#12-create-researcher","title":"1.2 Create researcher\u00b6","text":"<p>Please run the command below to create researcher component.</p> <pre>fedbiomed component create --component researcher\n</pre> <p>Please follow these instructions if you activate Joye-Libert secure aggregation: Joye-Libert configuration requires to know the participating Fed-BioMed components in advance. Therefore, each component that will participate in the training should be created before starting them. Afterwards, participating components can be registered in every other component.</p>"},{"location":"tutorials/security/secure-aggregation/#2-add-dataset-and-start-nodes","title":"2. Add dataset and start nodes\u00b6","text":"<p>The next step will be adding/deploying MNIST dataset in the nodes and starting them. For this step you can follow the instructions for adding dataset into nodes to add MNIST dataset. After the datasets are deployed you can start the nodes and researcher.</p> <p>For MNIST dataets, commands are:</p> <pre>fedbiomed node --path my-node dataset add --mnist\n</pre> <pre>fedbiomed node --path my-second-node dataset add --mnist\n</pre>"},{"location":"tutorials/security/secure-aggregation/#define-an-experiment-model-and-parameters","title":"Define an experiment model and parameters\"\u00b6","text":""},{"location":"tutorials/security/secure-aggregation/#declare-and-run-the-experiment","title":"Declare and run the experiment\u00b6","text":""},{"location":"tutorials/security/secure-aggregation/#access-secure-aggregation-context","title":"Access secure aggregation context\u00b6","text":""},{"location":"tutorials/security/secure-aggregation/#change-in-experiment-triggers-re-creation-of-secure-aggregation-context","title":"Change in experiment triggers re-creation of secure aggregation context\u00b6","text":"<p>The changes like adding new node to the experiment will trigger automatic secure aggregation re-setup for the next round.</p>"},{"location":"tutorials/security/secure-aggregation/#changing-arguments-of-secure-aggregation","title":"Changing arguments of secure aggregation\u00b6","text":""},{"location":"tutorials/security/secure-aggregation/#load-experiment-from-a-breakpoint","title":"Load experiment from a breakpoint\u00b6","text":"<p>Once a breakpoint is loaded if the context is already existing there won't be context setup.</p>"},{"location":"tutorials/security/training-with-approved-training-plans/","title":"Training Process with Training Plan Management","text":"<p>The following model is the model that will be sent to the node for training. Since the model files are processed by the Experiment to configure dependencies, import part of the final file might be different from this one.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\n\n# Here we define the training plan to be used. \nclass MyTrainingPlan(TorchTrainingPlan):\n    \n    # Defines and return model \n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n    \n    # Defines and return optimizer\n    def init_optimizer(self, optimizer_args):\n        return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n    \n    # Declares and return dependencies\n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\"]\n        return deps\n    \n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            self.fc1 = nn.Linear(9216, 128)\n            self.fc2 = nn.Linear(128, 10)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.dropout2(x)\n            x = self.fc2(x)\n\n\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_data(self):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        loader_arguments = { 'shuffle': True}\n        return DataManager(dataset=dataset1, **loader_arguments)\n    \n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms   # Here we define the training plan to be used.  class MyTrainingPlan(TorchTrainingPlan):          # Defines and return model      def init_model(self, model_args):         return self.Net(model_args = model_args)          # Defines and return optimizer     def init_optimizer(self, optimizer_args):         return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])          # Declares and return dependencies     def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\"]         return deps          class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.conv1 = nn.Conv2d(1, 32, 3, 1)             self.conv2 = nn.Conv2d(32, 64, 3, 1)             self.dropout1 = nn.Dropout(0.25)             self.dropout2 = nn.Dropout(0.5)             self.fc1 = nn.Linear(9216, 128)             self.fc2 = nn.Linear(128, 10)          def forward(self, x):             x = self.conv1(x)             x = F.relu(x)             x = self.conv2(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = self.dropout1(x)             x = torch.flatten(x, 1)             x = self.fc1(x)             x = F.relu(x)             x = self.dropout2(x)             x = self.fc2(x)               output = F.log_softmax(x, dim=1)             return output      def training_data(self):         # Custom torch Dataloader for MNIST data         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         loader_arguments = { 'shuffle': True}         return DataManager(dataset=dataset1, **loader_arguments)          def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss  <p>To be able to get/see the final model file we need to initialize the experiment.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MNIST', '#dataset']\nrounds = 2\n\nmodel_args = {}\n\ntraining_args = {\n    'loader_args': { 'batch_size': 48, }, \n    'optimizer_args': {\n        \"lr\" : 1e-3\n    },\n    'epochs': 1, \n    'dry_run': False,  \n    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.federated_workflows import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['#MNIST', '#dataset'] rounds = 2  model_args = {}  training_args = {     'loader_args': { 'batch_size': 48, },      'optimizer_args': {         \"lr\" : 1e-3     },     'epochs': 1,      'dry_run': False,       'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) In\u00a0[\u00a0]: Copied! <pre>exp.training_plan_file(display = True)\n</pre> exp.training_plan_file(display = True) <p>The <code>exp.check_training_plan_status()</code> sends request to the experiment's nodes to check whether the model is approved or not. The nodes that will receive the requests are the nodes that have been found after searching datasets.</p> In\u00a0[\u00a0]: Copied! <pre>status = exp.check_training_plan_status()\n</pre> status = exp.check_training_plan_status() In\u00a0[\u00a0]: Copied! <pre>status\n</pre> status In\u00a0[\u00a0]: Copied! <pre>exp.run_once()\n</pre> exp.run_once() <p>The logs should indicate that the training plan is approved. You can also get status object from the result of the <code>check_training_plan_status()</code>. It returns a list of status objects each for different node. Since we have only launched a single node, it returns only one status object.</p> <ul> <li><code>approval_obligation</code> : Indicates whether the training plan control is enabled in the node.</li> <li><code>status</code>         : Indicates training plan approval status.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\n\n# Here we define the model to be used. \n# You can use any class name (here 'Net')\nclass MyTrainingPlan(TorchTrainingPlan):\n    \n    # Defines and return model \n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n    \n    # Defines and return optimizer\n    def init_optimizer(self, optimizer_args):\n        return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n    \n    # Declares and return dependencies\n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\"]\n        return deps\n    \n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 16, 5, 1, 2)\n            self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)\n            self.fc1 = nn.Linear(32 * 7 * 7, 10)\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_data(self):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        train_kwargs = { 'shuffle': True}\n        return DataManager(dataset=dataset1, **train_kwargs)\n    \n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms   # Here we define the model to be used.  # You can use any class name (here 'Net') class MyTrainingPlan(TorchTrainingPlan):          # Defines and return model      def init_model(self, model_args):         return self.Net(model_args = model_args)          # Defines and return optimizer     def init_optimizer(self, optimizer_args):         return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])          # Declares and return dependencies     def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\"]         return deps          class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.conv1 = nn.Conv2d(1, 16, 5, 1, 2)             self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)             self.fc1 = nn.Linear(32 * 7 * 7, 10)         def forward(self, x):             x = self.conv1(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = self.conv2(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = torch.flatten(x, 1)             x = self.fc1(x)              output = F.log_softmax(x, dim=1)             return output      def training_data(self):         # Custom torch Dataloader for MNIST data         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         train_kwargs = { 'shuffle': True}         return DataManager(dataset=dataset1, **train_kwargs)          def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss <p>In the following cell, we update the training plan class using the setter <code>set_training_plan_class</code>.</p> In\u00a0[\u00a0]: Copied! <pre>exp.set_training_plan_class(MyTrainingPlan, keep_weights=False)\n</pre> exp.set_training_plan_class(MyTrainingPlan, keep_weights=False)  <p>Since we changed the model/network structure (we removed dropouts and one dense layer <code>fc2</code>) in the experiment, the output of the following method should say that the training plan is not approved by the node and <code>is_approved</code> key of the result object should be equal to <code>False</code>.</p> In\u00a0[\u00a0]: Copied! <pre>status = exp.check_training_plan_status()\n</pre> status = exp.check_training_plan_status() In\u00a0[\u00a0]: Copied! <pre>exp.training_plan_file()\n</pre> exp.training_plan_file() In\u00a0[\u00a0]: Copied! <pre>status\n</pre> status <p>Since the training plan is not approved, you won't be able to train your model in the node. The following cell will return an error.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) In\u00a0[\u00a0]: Copied! <pre>exp.training_plan_approve(description=\"my new training plans\")\n</pre> exp.training_plan_approve(description=\"my new training plans\") <p>Once the training plan has been sent, we need to approve it (or reject it) on <code>Node</code> side.</p> <p>Before approving, optionally list models/training plans known to the node with their status (<code>Approved</code>, <code>Pending</code>, <code>Rejected</code>). Your new training plan should appear with <code>Pending</code> status and name <code>my new training plan</code>.</p> <pre>$ fedbiomed node --path ./my-node training-plan list\n</pre> <p>Then approve the training plan, using the following command on a new terminal:</p> <pre>$ fedbiomed node -d my-node training-plan approve\n</pre> <p>Training plans with both <code>Pending</code> or <code>Rejected</code> status will be displayed. Select the training plan you have sent to approve it. You might see a message explaining that training plan has successfully been approved.</p> <p>Optionally list again training plans known to the node with their status. Your training plan should now appear with <code>Approved</code> status.</p> <pre>$ fedbiomed node --path ./my-node training-plan list\n</pre> <p>Back on the <code>Researcher</code> side, let's check it status by running the <code>check_model_status</code> command:</p> In\u00a0[\u00a0]: Copied! <pre>exp.check_training_plan_status()\n</pre> exp.check_training_plan_status() <p>Model's status must have changed from <code>Pending</code> status to <code>Approved</code>, which means model can be trained from now on on the <code>Node</code>. <code>Researcher</code> can now run an <code>Experiment</code> on the <code>Node</code>!</p> In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) In\u00a0[\u00a0]: Copied! <pre>exp.training_plan_file()\n</pre> exp.training_plan_file() <p>The output of the <code>exp.training_plan_file()</code> is a file path that shows where the final training plan is saved. It also prints the content of the training plan file. You can either get the content of training plan from the output cell or the path where it is saved. Anyway, you need to create a new <code>txt</code> file and copy the training plan content in it. You can create new directory in Fed-BioMed called <code>training_plans</code> and inside it, you can create new <code>my-training-plan.txt</code> file and copy the training plan class content into it.</p> <pre>$ mkdir ${FEDBIOMED_DIR}/my_approved_training_plan\n$ cp &lt;training_plan_path_file&gt; ${FEDBIOMED_DIR}/my_approved_training_plan/my-training-plan.txt\n</pre> <p>Where <code>&lt;model_path_file&gt;</code> is the path of the model that is returned by <code>exp.training_plan_file(display=False)</code></p> <p>Afterward, please run following command in other terminal to register training plan file.</p> <pre>$ fedbiomed node --path config-n1.ini training-plan register\n</pre> <p>You should type a unique name for your training plan e.g. 'MyTestTP-1' and a description. The CLI will ask you select training plan file you want to register. Select the file that you saved and continue.</p> <p>Now, you should be able to train your model defined in the training plan.</p> <p>Back on the <code>Researcher</code> side, you should now be able to train your model.</p> In\u00a0[\u00a0]: Copied! <pre>exp.check_training_plan_status()\n</pre> exp.check_training_plan_status() In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) In\u00a0[\u00a0]: Copied! <pre>exp.check_training_plan_status()\n</pre> exp.check_training_plan_status() In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True)"},{"location":"tutorials/security/training-with-approved-training-plans/#training-process-with-training-plan-management","title":"Training Process with Training Plan Management\u00b6","text":""},{"location":"tutorials/security/training-with-approved-training-plans/#introduction","title":"Introduction\u00b6","text":"<p>Fed-BioMed offers a feature to run only the pre-approved training plans on the nodes by default. The nodes which receive your training plan might require approved training plans. Therefore, if the node accepts only the approved training plan, the training plan files that are sent by a researcher with the training request should be approved by the node side in advance. In this workflow, the training plan approval process is done by a real user/person who reviews the code contained in the training plan file/class. The reviewer makes sure the model doesn't contain any code that might cause privacy issues or harm the node.</p> <p>In this tutorial, we will be creating a node with activated training plan control option.</p>"},{"location":"tutorials/security/training-with-approved-training-plans/#setting-up-a-node","title":"Setting Up a Node\u00b6","text":"<p>Enabling training plan control can be done both from config file or Fed-BioMed CLI while starting the node. The process of creating and starting a node with training plan control option is not so different from setting up a normal node. By default, if no option is specified in the CLI when the node is launched for the first time, the node disables training plan control in the security section of the config file. It then looks like the snippet below :</p> <pre>[security]\nhashing_algorithm = SHA256\nallow_default_training_plans = True\ntraining_plan_approval = False\n</pre> <p>It is also possible to manage training plan approval mode using environment variables. <code>FBM_SECURITY_TRAINING_PLAN_APPROVAL=True</code> and <code>FBM_SECURITY_ALLOW_DEFAULT_TRAINING_PLANS=True</code> to activate training plan approval mode. They enable one-time override of the config file options at each launch of the node.</p> <ul> <li><code>FBM_SECURITY_TRAINING_PLAN_APPROVAL=True</code> : This variable enables training plan control for the node. If there isn't a config file for the node while running CLI, it creates a new config file with enabled training plan approval mode <code>training_plan_approval = True</code>.</li> <li><code>FBM_SECURITY_ALLOW_DEFAULT_TRAINING_PLANS=True</code>  : This variable allows default training plans for train requests. These are the training plans that come for Fed-BioMed tutorials. For example, the training plan for MNIST dataset that we will be using for this tutorial. If the default training plans are enabled, node updates/registers training plan files which are located in <code>envs/common/default_training_plans</code> directory during the starting process of the node. This option has no effect if training plan control is not enabled.</li> </ul>"},{"location":"tutorials/security/training-with-approved-training-plans/#adding-mnist-dataset-to-the-node","title":"Adding MNIST Dataset to The Node.\u00b6","text":"<p>In this section we will add MNIST dataset to the node. While adding the dataset through CLI we'll also specify <code>FBM_SECURITY_TRAINING_PLAN_APPROVAL=True</code> and <code>ALLOW_DEFAULt_TRAINING_PLANS=True</code> options. This will create new component in the directory<code>./my-node</code> with following configuration that will be located in the <code>my-node/etc/config.ini</code>.</p> <pre><code>[security]\nhashing_algorithm = SHA256\nallow_default_training_plans = True\ntraining_plan_approval = True\n\n</code></pre> <p>Now, let's run the following command.</p> <pre>$ FBM_SECURITY_TRAINING_PLAN_APPROVAL=True FBM_SECURITY_ALLOW_DEFAULT_TRAINING_PLANS=True fedbiomed node --path ./my-node dataset add\n</pre> <p>The CLI will ask you to select the dataset type. Since we will be working on MNIST dataset, please select <code>2</code> (default) and continue by typing <code>y</code> for the next prompt and select folder that you want to store MNIST dataset. Afterward, if you go to <code>etc</code> directory of fedbiomed, you can see <code>config-n1.ini</code> file.</p>"},{"location":"tutorials/security/training-with-approved-training-plans/#starting-the-node","title":"Starting the Node\u00b6","text":"<p>Now you can start your node by running following command;</p> <pre>$ fedbiomed node --path ./my-node start\n</pre> <p>Since config file has been configured to enable training plan control mode, you do not need to specify any extra parameter while starting the node. But it is also possible to start node with <code>FBM_SECURITY_TRAINING_PLAN_APPROVAL=True</code>, <code>FBM_SECURITY_ALLOW_DEFAULT_TRAINING_PLANS=True</code> or <code>FBM_SECURITY_TRAINING_PLAN_APPROVAL=False</code>, <code>FBM_SECURITY_ALLOW_DEFAULT_TRAINING_PLANS=False</code>. If you start your node with <code>FBM_SECURITY_TRAINING_PLAN_APPROVAL=False</code> it will disable training plan control even it is enabled in the config file.</p>"},{"location":"tutorials/security/training-with-approved-training-plans/#creating-an-experiment","title":"Creating An Experiment\u00b6","text":"<p>In this section we will be using default MNIST model which has been already registered by the node.</p>"},{"location":"tutorials/security/training-with-approved-training-plans/#getting-final-training-plan-file-from-experiment","title":"Getting Final Training Plan File From Experiment\u00b6","text":"<p><code>training_plan_file()</code> displays the training plan file that will be sent to the nodes.</p>"},{"location":"tutorials/security/training-with-approved-training-plans/#changing-training-plan-and-testing-training-plan-approval-status","title":"Changing Training Plan And Testing Training Plan Approval Status\u00b6","text":"<p>Let's change the training plan network codes and test whether it is approved or not. We will be changing the network structure.</p>"},{"location":"tutorials/security/training-with-approved-training-plans/#registering-and-approving-the-training-plan","title":"Registering and Approving the Training Plan\u00b6","text":"<p>To register/approve the training plan that has been created in the previous section, we can use Fed-BioMed CLI. In Fed-Biomed, there are two ways of approving a model:</p> <ol> <li>By sending an <code>ApprovalRequest</code> from the researcher to the <code>Node</code></li> <li>By adding it directly to the <code>Node</code> through model registration facility</li> </ol>"},{"location":"tutorials/security/training-with-approved-training-plans/#1-approving-a-training-plan-through-an-approvalrequest","title":"1. Approving a Training Plan through an <code>ApprovalRequest</code>\u00b6","text":"<p>Fed-BioMed 's <code>Experiment</code> interface provides a method to submit a training plan to the <code>Node</code>, for approval. <code>Node</code> can then review the code and approve the training plan using CLI or GUI.</p> <p>The method of <code>Experiment</code> sending such request is <code>training_plan_approve</code></p>"},{"location":"tutorials/security/training-with-approved-training-plans/#2-registering-a-model-through-node-interface","title":"2. Registering a Model through Node interface\u00b6","text":"<p>Training plan status must have changed from <code>Pending</code> status to <code>Approved</code>, which means model can be trained from now on the <code>Node</code>. <code>Researcher</code> can now run an <code>Experiment</code> on the <code>Node</code>!</p>"},{"location":"tutorials/security/training-with-approved-training-plans/#rejecting-training-plans","title":"Rejecting training plans\u00b6","text":"<p>On <code>Node</code> side, it is possible to reject a Model using cli or GUI. Every type of training plan can be <code>Rejected</code>, even <code>Default</code> models. In Fed-BioMed, <code>Rejected</code> means that training plan cannot be trained/executed on the <code>Node</code> (but training plan is still <code>Registered</code> into the database).</p> <p>Using cli, <code>Node</code> can run:</p> <pre>$ fedbiomed node --path my-node training-plan reject\n</pre> <p>and select the training plan to be <code>Rejected</code>.</p>"},{"location":"user-guide/advanced-optimization/","title":"Advanced Optimization in Fed-BioMed","text":"<p>Advanced Optimization can be done in <code>Fed-BioMed</code> through the use of <code>declearn</code>, a Python package that provides gradient-based <code>Optimizers</code>. <code>declearn</code> is cross-machine learning framework, meaning that it can be used with most machine learning frameworks (scikit-learn, PyTorch, Tensorflow, JAX, ...).</p> <p>The following chapter explores in depth how to use <code>declearn</code> optimization feature in <code>Fed-BioMed</code>. For an example, please refer to the Advanced Optimizer tutorial.</p>"},{"location":"user-guide/advanced-optimization/#1-introduction-to-declearn-based-optimizer-a-cross-framework-optimizer-library","title":"1. Introduction to <code>Declearn</code> based Optimizer: a cross framework <code>Optimizer</code> library","text":""},{"location":"user-guide/advanced-optimization/#11-what-is-declearn-package","title":"1.1. What is <code>declearn</code> package?","text":"<p><code>declearn</code> package is another Federated Learning framework modular and combinable, providing state-of-the-art gradient-based <code>Optimizer</code> algorithms. In <code>Fed-BioMed</code>, we are only using its <code>Optimization</code> facility, leaving aside all other components of <code>declearn</code> that we don't use in <code>Fed-BioMed</code>.</p> <p>References: For further details about <code>declearn</code>, you may visit:</p> <ul> <li> <p><code>declearn</code> repository</p> </li> <li> <p><code>declearn</code> general documentation</p> </li> <li> <p><code>declearn</code> Optimizers documentation</p> </li> </ul>"},{"location":"user-guide/advanced-optimization/#12-declearn-interface-in-fed-biomed-the-optimizer-object","title":"1.2. <code>declearn</code> interface in <code>Fed-BioMed</code>: the <code>Optimizer</code> object","text":"<p>In <code>Fed-BioMed</code>, we provide a <code>Optimizer</code> object, that works as an interface with <code>declearn</code>, and was made in order to use <code>declearn</code>'s Optimizers (see below <code>declearn</code>'s <code>OptiModules</code> and <code>declearn</code>'s <code>Regularizers</code>).</p> <pre><code>from fedbiomed.common.optimizers import Optimizer\n\nOptimizer(lr=.1, decay=.0, modules=[], regularizers=[])\n</code></pre> <p>with the following arguments:</p> <ul> <li> <p><code>lr</code>: the learning rate;</p> </li> <li> <p><code>decay</code>: the weight decay;</p> </li> <li> <p><code>modules</code>: a list of <code>declearn</code> 's <code>OptiModules</code> (or a list of <code>OptiModules' names</code>);</p> </li> <li> <p><code>regularizers</code>: a list of <code>declearn</code> 's <code>Regularizers</code> (or a list of <code>Regularizers' names</code>).</p> </li> </ul> <p></p>"},{"location":"user-guide/advanced-optimization/#13-declearns-optimodules","title":"1.3. <code>declearn</code>'s <code>OptiModules</code>","text":"<p><code>declearn</code> <code>OptiModules</code> are modules that convey <code>Optimizers</code>, which purpose is to optimize a loss function (that can be written using a PyTorch loss function or defined in a scikit learn model) in order to optimize a model. Compatible <code>declearn</code> <code>OptiModules</code> with Fed-BioMed framework are defined in <code>fedbiomed.common.optimizers.declearn</code> module. They should be imported from <code>fedbiomed.common.optimizers.declearn</code>, as shown in the examples below. You can also import them direclty from <code>declearn</code>'s <code>declearn.optimizer.modules</code>, but they will be no guarentees it is compatible with Fed-BioMed. In fact, recommended method is importing modules through <code>fedbiomed.common.optimizers.declearn</code>.</p> <p>Usage:</p> <ul> <li> <p>For basic SGD (Stochastic Gradient Descent), we don't need to specify a <code>declearn</code> <code>OptiModule</code> and/or  a <code>Regularizer</code></p> <pre><code>from fedbiomed.common.optimizers.optimizer import Optimizer\n\nlr = .01\nOptimizer(lr=lr)\n</code></pre> </li> <li> <p>For a specfic Optimizer like Adam, we need to import <code>AdamModule</code> from <code>declearn</code>. Hence, it yields:</p> <pre><code>from fedbiomed.common.optimizers.optimizer import Optimizer\nfrom fedbiomed.common.optimizers.declearn import AdamModule\n\nlr = .01\nOptimizer(lr=lr, modules=[AdamModule()])\n</code></pre> </li> <li> <p>It is possible to chain <code>Optimizer</code> with several <code>OptiModules</code>, meaning to use several <code>Optimizers</code>. Some chains of <code>OptiModule</code> may be non-sensical, so use it at your own risk! Below we showcase the use of Adam with Momentum</p> <pre><code>from fedbiomed.common.optimizers.optimizer import Optimizer\nfrom fedbiomed.common.optimizers.declearn import AdamModule, MomentumModule\n\nlr = .01\nOptimizer(lr=lr, modules=[AdamModule(), MomentumModule()])\n</code></pre> </li> <li> <p>To get all comptible <code>OptiModule</code> in Fed-BioMed, one can run the <code>list_optim_modules</code>.</p> <pre><code>from fedbiomed.common.optimizers.declearn import list_optim_modules\n\nlist_optim_modules()\n</code></pre> </li> </ul> <p>For further information on <code>declearn OptiModule</code>, please visit <code>declearn OptiModule</code> and <code>declearn</code>'s Optimizers documentation.</p> <p>List of available Optimizers provided by <code>declearn</code></p> <p>To get a list of all available Optimizers in declearn, please enter (after having loaded <code>Fed-BioMed</code> conda environment)</p> <pre><code>from declearn.optimizer import list_optim_modules\n\nlist_optim_modules()\n</code></pre> <p></p>"},{"location":"user-guide/advanced-optimization/#14-declearns-regularizers","title":"1.4. <code>declearn</code>'s <code>Regularizers</code>","text":"<p><code>declearn</code>'s <code>Regularizers</code> are objects that enable the use of <code>Regularizer</code>, which add an additional term to the loss function one wants to Optimize through the use of optimizer. It mainly helps to get a more generalizable model, and prevents overfitting.</p> <p>The optimization equation yields to:</p> \\[ \\theta_{t+1} := \\theta_t - \\eta  \\vec{\\nabla} f_{x,y} + \\alpha \\lVert \\theta_t \\rVert \\] <p>with</p> <p>\\(\\theta_t : \\textrm{model weights at update } t\\)</p> <p>\\(\\eta : \\textrm{learning rate}\\)</p> <p>\\(\\alpha : \\textrm{regularization coefficient}\\)</p> <p>\\(f_{x,y}: \\textrm{Loss function used for optimizing the model}\\)</p> <p><code>Regularizers</code> should be used and combined with an Optimizer. For instance, SGD with Ridge regression, or Adam with Lasso regression. <code>FedProx</code> is also considered as a regularization.</p> <p>Optimizer without OptiModules</p> <p>When no <code>OptiModules</code> are specified in the <code>modules</code> argument of <code>Optimizer</code>, plain SGD algorithm is set by default for the optimization.</p> <p>Usage:</p> <ul> <li> <p>for example, SGD with Ridge regression will be written as:</p> <pre><code>from fedbiomed.common.optimizers.optimizer import Optimizer\nfrom fedbiomed.common.optimizers.declearn import RidgeRegularizer\n\nlr = .01\nOptimizer(lr=lr, regularizers=[RidgeRegularizer()])\n</code></pre> </li> <li> <p>Adam with Lasso Regression:</p> <pre><code>from fedbiomed.common.optimizers.optimizer import Optimizer\nfrom fedbiomed.common.optimizers.declearn import AdamModule\nfrom fedbiomed.common.optimizers.declearn import LassoRegularizer\n\nlr = .01\nOptimizer(lr=lr, modules=[AdamModule()], regularizers=[LassoRegularizer()])\n</code></pre> </li> <li> <p>Chaining several Regularizers: an example with Ridge and Lasso regularizers, and Adam with momentum as the Optimizer.</p> <pre><code>from fedbiomed.common.optimizers.optimizer import Optimizer\nfrom fedbiomed.common.optimizers.declearn import LassoRegularizer, RidgeRegularizer\n\nlr = .01\nOptimizer(lr=lr, modules=[AdamModule(), MomentumModule()], regularizers=[LassoRegularizer(), RidgeRegularizer()])\n</code></pre> </li> </ul> <p>For further information on <code>declearn Regularizer</code>, please visit <code>declearn Regularizers</code> documentation webpage </p>"},{"location":"user-guide/advanced-optimization/#15-chaining-optimizers-and-regularizers-with-declearn-modules","title":"1.5. Chaining <code>Optimizers</code> and <code>Regularizers</code> with <code>declearn</code> modules","text":"<p>It is possible in <code>declearn</code> to chain several <code>OptiModules</code> and <code>Regularizers</code> in an Optimizer. Generaly speaking, <code>Optimizer</code> in <code>declearn</code> can be written as:</p> \\[  \\begin{equation}     \\theta_{t+1} := \\theta_t - \\eta\\  \\underbrace{ Opt( \\vec{\\nabla} f_{x,y} - \\overbrace{ Reg(\\theta_t)}^\\textrm{Regularizer})}_\\textrm{OptiModule} - \\tau \\theta_t   \\end{equation}   \\] <p>with</p> <p>\\(Opt : \\textrm{an OptiModule}\\)</p> <p>\\(Reg : \\textrm{a Regularizer}\\)</p> <p>\\(\\theta : \\textrm{model weight}\\)</p> <p>\\(\\eta : \\textrm{learning rate}\\)</p> <p>\\(\\tau : \\textrm{weight decay}\\)</p> <p>\\(f_{x,y}: \\textrm{Loss function used for optimizing the model}\\)</p> <p>The above holds for a single <code>Regularizer</code> and <code>OptiModule</code>. When using (ie chaining) several <code>OptiModules</code> and <code>Regularizers</code> in an <code>Optimizer</code>, the above optimization equation becomes:</p> \\[\\theta_{t+1} := \\theta_t - \\eta\\  \\underbrace{ Opt_{i=1} \\circ Opt_{i=2} \\,\\circ... \\circ \\, Opt_{i=n}( \\vec{\\nabla} f_{x,y} - \\overbrace{ Reg_{i=1}\\circ Reg_{i=2}\\circ... \\circ \\,Reg_{i=m}(\\theta_t)}^\\textrm{ Regularizers})}_\\textrm{OptiModules} - \\tau \\theta_t \\] <p>where</p> <p>\\(Opt_{1\\le i \\le n}: \\textrm{ OptiModules, with }  n   \\textrm{ the total number of OptiModules used}\\)</p> <p>\\(Reg_{1\\le i \\le m}: \\textrm{ Regularizers, with }  m   \\textrm{ the total number of Regularizers used}\\)</p> <p>Example: let's write an <code>Optimizer</code> using <code>RMSProp</code> and <code>Momentum</code> <code>OptiModules</code>, and both Lasso and Ridge Regularizers.</p> <pre><code>from fedbiomed.common.optimizers.optimizer import Optimizer\nfrom fedbiomed.common.optimizers.declearn  import RMSPropModule, MomentumModule, LassoRegularizer, RidgeRegularizer\n\nlr = .01\nOptimizer(lr=lr,\n          modules=[RMSPropModule(), MomentumModule()],\n          regularizers=[LassoRegularizer(), RidgeRegularizer()])\n</code></pre> <p>Using list of strings instead of list of modules</p> <p>In <code>declearn</code>, it is possible to use name of modules instead of loading the actual modules. In the script below, we are rewritting the same <code>Optimizer</code> as the one above but by specifying the module names. A convinient way to get the naming is to use <code>list_optim_modules</code> and  <code>list_optim_regularizers</code> functions, that map module names with their classes respectively.</p> <pre><code>from fedbiomed.common.optimizers.optimizer import Optimizer\n\n\nlr = .01\nOptimizer(lr=lr,\n          modules=['adam', 'momentum'],\n          regularizers=['lasso', 'ridge'])\n</code></pre> <p>To get to know specifcities about all <code>declearn</code>'s modules, please visit <code>declearn</code> webpage.</p>"},{"location":"user-guide/advanced-optimization/#how-to-use-well-known-federated-learning-algorithms-with-declearn-in-fed-biomed","title":"How to use well-known Federated-Learning algorithms with <code>declearn</code> in <code>Fed-BioMed</code>?","text":"<p>Please refer to the following section of this page.</p>"},{"location":"user-guide/advanced-optimization/#2-declearn-optimizer-on-node-side","title":"2. <code>declearn</code> optimizer on Node side","text":"<p>In order to use <code>declearn</code> to optimize <code>Node</code>s local model, you will have to edit <code>init_otpimizer</code> method in the <code>TrainingPlan</code>. Below we showcase how to use it with PyTorch framework (using Adam and Ridge regularizer for the optimization).</p> <pre><code>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.optimizers.optimizer import Optimizer\nfrom fedbiomed.common.optimizers.declearn  import AdamModule, RidgeRegularizer\n...\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    ...\n\n    def init_dependencies(self):\n        deps = [\n                \"from fedbiomed.common.optimizers.optimizer import Optimizer\",\n                \"from fedbiomed.common.optimizers.declearn  import AdamModule, RidgeRegularizer\"\n                ]\n\n        return deps\n\n    def init_optimizer(self):\n        return Optimizer(lr=.01, modules=[AdamModule()], regularizers=[RidgeRegularizer()])\n</code></pre> <p>Important</p> <p>You should specify the <code>OptiModules</code> imported in both the imports at the begining of the <code>Training Plan</code> as well as in the dependencies (in the <code>init_dependencies</code> method within the <code>Training Plan</code>). The same holds for <code>declearn</code>'s <code>Regularizers</code>.</p> <p>Syntax will be the same for scikit-learn as shown below, using the same <code>Optimizer</code>:</p> <pre><code>from fedbiomed.common.training_plans import FedSGDClassifier\nfrom fedbiomed.common.optimizers.optimizer import Optimizer\nfrom fedbiomed.common.optimizers.declearn import AdamModule, RidgeRegularizer\n...\n\nclass MyTrainingPlan(FedSGDClassifier):\n    ...\n\n    def init_dependencies(self):\n        deps = [\n                \"from fedbiomed.common.optimizers.optimizer import Optimizer\",\n                \"from fedbiomed.common.optimizers.declearn  import AdamModule, RidgeRegularizer\"\n                ]\n\n        return deps\n\n    def init_optimizer(self):\n        return Optimizer(lr=.01, modules=[AdamModule()], regularizers=[RidgeRegularizer()])\n</code></pre>"},{"location":"user-guide/advanced-optimization/#3-declearn-optimizer-on-researcher-side-fedopt","title":"3. <code>declearn</code> optimizer on Researcher side (<code>FedOpt</code>)","text":"<p><code>Fed-BioMed</code> provides a way to use  Adaptive Federated Optimization, introduced as <code>FedOpt</code> in this paper. In the paper, authors considered the difference of the global model weights between 2 successive <code>Rounds</code> as a pseudo gradient, paving the way to the possbility to have <code>Optimizers</code> on <code>Researcher</code> side, optimizing the updates of the global model. To do so, <code>fedbiomed.researcher.federated_workflows.Experiment</code> has a method to set the <code>Researcher Optimizer</code>: <code>Experiment.set_agg_optimizer</code></p> <p>Below an example using the <code>set_agg_optimizer</code> with <code>FedYogi</code>:</p> <pre><code>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators import FedAverage\nfrom fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\nfrom fedbiomed.common.optimizers.declearn  import YogiModule as FedYogi\n\ntags = ['#my-data']\n\nexp = Experiment()\nexp.set_training_plan_class(training_plan_class=MyTrainingPlan)\nexp.set_tags(tags = tags)\nexp.set_aggregator(aggregator=FedAverage())\nexp.set_round_limit(2)\nexp.set_training_data(training_data=None, from_tags=True)\nexp.set_strategy(node_selection_strategy=DefaultStrategy())\n\n# here we are adding an Optimizer on Researcher side (FedYogi)\nfed_opt = Optimizer(lr=.8, modules=[FedYogi()])\nexp.set_agg_optimizer(fed_opt)\n\nexp.run(increase=True)\n</code></pre> <p>Important</p> <p>You may have noticed that we are using <code>FedAverage</code> in the <code>Experiment</code> configuration, while using <code>YogiModule</code> as an <code>Optimizer</code>. In fact, <code>FedAverage</code> <code>Aggregator</code> in <code>Fed-BioMed</code> refers to the way model weights are aggregated before optimization, and should not be confused with the whole <code>FedAvg</code> algorithm, which is basically a SGD optimizer performed on <code>Node</code> side using <code>FedAvg</code> <code>Aggregtor</code> on <code>Researcher</code> side.</p> <p>One can also pass directly the <code>agg_optimizer</code> in the <code>Experiment</code> object constructor:</p> <pre><code>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators import FedAverage\nfrom fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\nfrom fedbiomed.common.optimizers.declearn import YogiModule as FedYogi\n\n\ntags = ['#my-data']\nfed_opt = Optimizer(lr=.8, modules=[FedYogi()])\n\n\nexp = Experiment(tags=tags,\n                 training_plan_class=MyTrainingPlan,\n                 round_limit=2,\n                 agg_optimizer=fed_opt,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n\nexp.run(increase=True)\n</code></pre>"},{"location":"user-guide/advanced-optimization/#4-declearn-auxiliary-variables-based-optimizers","title":"4. <code>declearn</code> auxiliary variables based <code>Optimizers</code>","text":"<p>In this subsection, we will take a look at some specific <code>Optimizers</code> that are built around <code>auxiliary variables</code>.</p>"},{"location":"user-guide/advanced-optimization/#41-what-is-an-auxiliary-variable","title":"4.1. What is an auxiliary variable?","text":"<p><code>Auxiliary variable</code> is a parameter that is needed for an <code>Optimizer</code> that requieres to be exchanged between <code>Nodes</code> and <code>Researcher</code>, in addition to model parameters. <code>Scaffold</code> is an example of such <code>Optimizer</code>, because built upon correction states, exchanged from <code>Nodes</code> and <code>Researcher</code>.</p> <p>These <code>Optimizers</code> may come with a specific <code>Researcher</code> version (for <code>Scaffold</code> it is <code>ScaffoldServerModule</code>) and a <code>Node</code> version (resp. <code>ScaffoldClientModule</code>). They may work in a synchronous fashion: <code>Researcher</code> optimizer version may expect auxiliary variables from  <code>Node</code> optimizer, and the other way arround (<code>Node</code> optimizer expecting auxiliary variable input from <code>Reseracher</code> optimizer version).</p> <p>Optimizers using auxiliary variables</p> <p>Currently only <code>Scaffold</code> (ie <code>ScaffoldClientModule</code> and <code>ScaffoldServerModule</code>) uses auxiliary variables.</p>"},{"location":"user-guide/advanced-optimization/#42-an-example-using-optimizer-with-auxiliary-variables-scaffold-with-declearn","title":"4.2. An example using <code>Optimizer</code> with auxiliary variables: <code>Scaffold</code> with <code>declearn</code>","text":"<p>In the last sub-section, we introduced <code>Scaffold</code>. Let's see now how to use it in <code>Fed-BioMed</code> framework.</p> <p>About native Scaffold implementation in Fed-BioMed</p> <p><code>Fed-BioMed</code> provides its own implementation of <code>Scaffold</code> <code>Aggregator</code>, that is only for PyTorch framework. It only works with PyTorch native optimizers <code>torch.optim.Optimizer</code> for the <code>Node Optimizer</code>.</p> <p><code>Training Plan</code> design</p> <p>In the current subsection, we showcase how to edit your <code>Training Plan</code> for PyTorch in order to use <code>Scaffold</code></p> <pre><code>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.optimizers.optimizer import Optimizer\nfrom fedbiomed.common.optimizers.declearn import ScaffoldClientModule\n...\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    ...\n\n    def init_dependencies(self):\n        deps = [\n                \"from fedbiomed.common.optimizers.optimizer import Optimizer\",\n                \"from fedbiomed.common.optimizers.declearn import ScaffoldClientModule\",\n                ]\n\n        return deps\n\n    def init_optimizer(self):\n        return Optimizer(lr=.01, modules=[ScaffoldClientModule()])\n</code></pre> <p><code>Experiment</code> design</p> <p>This is how <code>Experiment</code> can be designed (on the <code>Researcher</code> side)</p> <pre><code>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators import FedAverage\nfrom fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\nfrom declearn.optimizer.modules import ScaffoldServerModule\n\n\ntags = ['#my-data']\nfed_opt = Optimizer(lr=.8, modules=[ScaffoldServerModule()])\n\n\nexp = Experiment(tags=tags,\n                 training_plan_class=MyTrainingPlan,\n                 round_limit=2,\n                 agg_optimizer=fed_opt,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n\nexp.run(increase=True)\n</code></pre> <p>Important</p> <p>You may have noticed that we are using <code>FedAverage</code> in the <code>Experiment</code> configuration, while using <code>ScaffoldServerModule</code> \\ <code>ScaffoldClientModule</code> as an <code>Optimizer</code>. In fact, <code>FedAverage</code> <code>Aggregator</code> in <code>Fed-BioMed</code> refers to the way model weights are aggregated before optimization, and should not be confused with the whole <code>FedAvg</code> algorithm, which is basically a SGD optimizer performed on <code>Node</code> side using <code>FedAvg</code> <code>Aggregtor</code>.</p> <p>Using auxiliary variables with SecAgg</p> <p>In latest releases of <code>Fed-BioMed</code>, <code>declearn</code> optimizers based on auxiliary variables (like <code>Scaffold</code> optimizer) now have thier auxiliary variables encrypted with <code>SecAgg</code>! Please note that this is not the case with native <code>Scaffold</code> algorithm. For security reason, you may choose to use <code>declearn</code> optimizers over the native ones.</p> <p>You can find more examples in Advanced Optimizers tutorial</p>"},{"location":"user-guide/advanced-optimization/#table-to-use-common-federated-learning-algorithm-with-declearn-in-fed-biomed","title":"Table to use common Federated Learning algorithm with <code>declearn</code> in <code>Fed-BioMed</code>","text":"<p>Below we have gathered some of the most well known algorithms in Federated Learning in the following table (as a reminder, <code>Node Optimizer</code> must e defined in the <code>TrainingPlan</code>, whereas <code>Researcher Optimizer</code> in the <code>Experiment</code> object):</p> Federated Learning Algorithm Node Optimizer Researcher Optimizer Aggregator AdaAlter (distributed AdaGrad) AdaGrad  <code>Optimizer(lr=xx, modules=[AdaGradModule()])</code> None <code>FedAverage</code> FedAdagrad SGD <code>Optimizer(lr=xx)</code> AdaGrad <code>Optimizer(lr=xx, modules=[AdaGradModule()])</code> <code>FedAverage</code> FedAdam SGD <code>Optimizer(lr=xx)</code> Adam <code>Optimizer(lr=xx, modules=[AdamModule()])</code> <code>FedAverage</code> FedAvg SGD <code>Optimizer(lr=xx)</code> None <code>FedAverage</code> FedProx SGD  <code>Optimizer(lr=xx, regularizers=[FedProxRegularizer])</code> None <code>FedAverage</code> FedYogi SGD <code>Optimizer(lr=xx)</code> Yogi <code>Optimizer(lr=xx, modules=[YogiModule()])</code> <code>FedAverage</code> Scaffold SGD <code>Optimizer(lr=xx, modules=[ScaffoldClientModule()])</code> SGD <code>Optimizer(lr=xx, modules=[ScaffoldServerModule()])</code> <code>FedAverage</code>"},{"location":"user-guide/advanced-optimization/#5-common-pitfalls-using-declearn-optimizers-in-fed-biomed","title":"5. Common Pitfalls using <code>declearn</code> Optimizers in <code>Fed-BioMed</code>","text":"<p>Below, we are summerizing common pitfalls that may occur when using <code>declearn</code> package in <code>Fed-BioMed</code>:</p> <ul> <li><code>Optimization</code> on <code>Researcher</code> side is only possible through <code>declearn</code> Optimizers (and not through native Optimizers such as PyTorch Optimizers);</li> <li>Some <code>Optimizers</code> may requiere some synchronization: it is the case of <code>Scaffold</code> related modules, ie <code>ScaffoldClientModule</code> and <code>ScaffoldServerModule</code>;</li> <li>For the moment <code>declearn</code> Optimizers that use <code>auxiliary variables</code> (such as <code>Scaffold</code>) cannot be protected yet with Secure Aggregation;</li> <li>For the moment, <code>declearn</code>'s <code>optimizer</code> only comes with a unique learning rate (multiple learning rates <code>Optimizers</code>, for example pytorch optimizers <code>torch.optim.Optimizer</code> can handle a learning rate per model layer );</li> <li>When chaining <code>declearn</code>'s <code>OptiModules</code>, it is only possible to use a unique learning rate, that will be the same for all <code>OptiModules</code>, and that won't change during a <code>Round</code>;</li> <li>check for inconcistent Optimizers! Using a <code>Regularizer</code> on <code>Researcher</code> side may be non-sensical, even if it is doable within <code>declearn</code>;</li> <li><code>Scaffold</code> <code>Fed-BioMed</code> aggregator  must not be used when using both <code>ScaffoldServerModule</code> and <code>ScaffoldClientModule</code>. This <code>aggregator</code> is in fact an alternative to the <code>declearn</code> <code>scaffold</code>, and you have to choose between the <code>Fed-BioMed</code> native version of <code>Scaffold</code> and the <code>declearn</code> 's one. Please note that <code>Fed-BioMed aggregator Scaffold</code> is deprecated, hence, the use of <code>ScaffoldServerModule</code> and <code>ScaffoldClientModule</code> is highly encouraged.</li> </ul>"},{"location":"user-guide/advanced-optimization/#conclusion","title":"Conclusion","text":"<p>We have seen how to use <code>declearn</code> <code>Optimizers</code> in <code>Fed-BioMed</code>. In <code>Fed-BioMed</code>, it is possible to set an <code>Optimizer</code> on both the <code>Node</code> and the <code>Researcher</code> side:</p> <ul> <li> <p>On <code>Node</code> side, such <code>Optimizer</code> is defined in <code>Training Plan</code> and is used to optimize <code>Nodes</code>' local models;</p> </li> <li> <p>On <code>Researcher</code> side, <code>Optimizer</code> is defined in the <code>Experiment</code>, and is made for optimizing global model.</p> </li> </ul> <p>When used with <code>declearn</code> package, <code>Fedd-BioMed</code> <code>Aggregator</code> is used for aggregating weights, before any potential optiization: <code>FedAverage</code> does the weighted sum of all local models sent back by the <code>Nodes</code>.</p> <p><code>declearn</code> comes with the possibility of chaining <code>Optimizers</code>, by passing a list of <code>OptiModule</code> and <code>Regularizers</code>, making possible to try out some more complex optimization process.</p> <p>Check the tutorial related to the use of <code>declearn</code>'s <code>Optimizers</code></p>"},{"location":"user-guide/glossary/","title":"Glossary","text":"<p>Here below the glossary used for Fed-BioMed :</p> <ul> <li> <p>experiment : orchestrates the rounds during the federated learning, on the available nodes</p> <ul> <li>it includes : training plan, model, federated trainer, training parameters, model parameters, set of input data, results</li> <li>an experiment is unique (cannot be replayed) and is over when converged</li> <li>status : running and then done</li> </ul> </li> <li> <p>training : as commonly used in ML, process of feeding a model with data to improve its accuracy on some task.</p> </li> <li>validation : process of giving a heuristic information on the accuracy of a model during training.</li> <li> <p>testing : process of assessing the accuracy of a model after training, on holdout samples different from the one that were used for training. Not implemented yet in Fed-BioMed.</p> </li> <li> <p>job : a pure researcher notion. Interface between the researcher and the nodes of an experiment for one single request (short lived, compared to experiment). It triggers the local work for all sampled nodes for one round.</p> </li> <li>round : everything included in choice of the nodes, perform local work on the nodes, sending back whatever information is required, server performs the aggregation<ul> <li>current Round() class on node corresponds to local work</li> </ul> </li> <li>parameter update : an update of the ML model parameters during the training loop, which usually corresponds to the processing of one batch of data</li> <li> <p>epoch : a number of parameter updates equivalent to processing the entire dataset exactly once</p> </li> <li> <p>researcher (technical) : entity that defines and executes an experiment</p> </li> <li>node (technical) : entity with tagged datasets that replies to researcher queries and performs local work</li> </ul>"},{"location":"user-guide/deployment/deployment-vpn-node2/","title":"Fed-BioMed deployment with VPN/containers and two node instances on the same node machine","text":"<p>Most real-life deployments require protecting node data. Deployment using VPN/containers contributes to this goal by providing isolation of the Fed-BioMed instance from third parties.</p> <p>Deploying two nodes instance on the same node machine is not the normal real life VPN/containers deployment scenario, which consists of one node instance per node machine.</p> <p>Nevertheless, this scenario can be useful for testing purpose. For example, secure aggregation requires at least 2 nodes participating in the experiment. This scenario allows testing secure aggregation with VPN/containers while all components (researcher + 2 nodes) are running on the same machine.</p> <p>Operating a second node instance on the same node machine is mostly equivalent to operating the first node instance. Commands are adapted by replacing any occurrence of:</p> <ul> <li>node by node2</li> <li>gui by gui2</li> <li>NODETAG by NODE2TAG</li> <li><code>https://localhost:8443</code> by <code>https://localhost:8444</code></li> </ul>"},{"location":"user-guide/deployment/deployment-vpn-node2/#deploy-a-second-node-instance-on-the-same-node-machine","title":"Deploy a second node instance on the same node machine","text":"<p>This part of the tutorial is executed once on each node that runs a second node instance, after deploying the server. It covers the initial deployment, including build, configuration and launch of containers.</p> <p>Some commands are executed on the node side, while some commands are executed on the server side (pay attention to the prompt).</p> <p>For each node, choose a unique node tag (eg: NODE2TAG in this example) that represents this specific node instance for server side management commands.</p> <ul> <li> <p>build node-side containers</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn build node2 gui2\n</code></pre> </li> <li> <p>on the server side, generate a configuration for this node (known as NODE2TAG)</p> <pre><code>[user@server $] cd ${FEDBIOMED_DIR}/envs/vpn/docker\n[user@server $] docker compose exec vpnserver bash -ci 'python ./vpn/bin/configure_peer.py genconf node NODE2TAG'\n</code></pre> <p>The configuration file is now available on the server side in path <code>${FEDBIOMED_DIR}/envs/vpn/docker/vpnserver/run_mounts/config/config_peers/node/NODE2TAG/config.env</code> or with command :</p> <pre><code>[user@server $] docker compose exec vpnserver cat /config/config_peers/node/NODE2TAG/config.env\n</code></pre> </li> <li> <p>copy the configuration file from the server side to the node side via a secure channel, to path <code>/tmp/config2.env</code> on the node.</p> <p>In most real life deployments, one shouldn't have access to both server side and node side. Secure channel in an out-of-band secured exchange (outside of Fed-BioMed scope) between the server administrator and the node administrator that provides mutual authentication of the parties, integrity and privacy of the exchanged file.</p> <p>In a test deployment, one may be connected both on server side and node side. In this case, you just need to cut-paste or copy the file to the node.</p> <p>Use the node's copy of the configuration file:</p> <pre><code>[user@node $] cp /tmp/config2.env ./node2/run_mounts/config/config.env\n</code></pre> </li> <li> <p>optionally force the use of secure aggregation by the node (node will refuse to train without the use of secure aggregation):</p> <pre><code>[user@node $] export FBM_SECURITY_FORCE_SECURE_AGGREGATION=True\n</code></pre> </li> <li> <p>start <code>node2</code> container</p> <pre><code>[user@node $] docker compose up -d node2\n</code></pre> </li> <li> <p>retrieve the <code>node2</code>'s publickey</p> <pre><code>[user@node $] docker compose exec node2 wg show wg0 public-key | tr -d '\\r' &gt;/tmp/publickey2-nodeside\n</code></pre> </li> <li> <p>copy the public key from the node side to the server side via a secure channel (see above), to path <code>/tmp/publickey2-serverside</code> on the server.</p> </li> <li> <p>on the server side finalize configuration of the VPN keys for this node (known as NODE2TAG)</p> <pre><code>[user@server $] cd ${FEDBIOMED_DIR}/envs/vpn/docker\n[user@server $] docker compose exec vpnserver bash -ci \"python ./vpn/bin/configure_peer.py add node NODE2TAG $(cat /tmp/publickey2-serverside)\"\n</code></pre> </li> <li> <p>check containers running on the node side</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn status node2\n</code></pre> <p><code>node2</code> container should be up and able to ping the VPN server</p> <pre><code>** Checking docker VPN images &amp; VPN access: node2\n- container node2 is running\n- pinging VPN server from container node2 -&gt; OK\n</code></pre> <p><code>node2</code> container is now ready to be used.</p> </li> </ul> <p>Optionally launch the node GUI :</p> <ul> <li> <p>start <code>gui2</code> container</p> <pre><code>[user@node $] docker compose up -d gui2\n</code></pre> </li> <li> <p>check containers running on the node side</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn status node2 gui2\n</code></pre> <p>Node side containers should be up and able to ping the VPN server</p> <pre><code>** Checking docker VPN images &amp; VPN access: node2 gui2\n- container node2 is running\n- container gui2 is running\n- pinging VPN server from container node2 -&gt; OK\n</code></pre> <p><code>node2</code> and <code>gui2</code> containers are now ready to be used.</p> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn-node2/#use-the-second-node-instance-on-the-same-node-machine","title":"Use the second node instance on the same node machine","text":"<p>This part is executed at least once on each node that runs a second node instance, after deploying the node side containers.</p> <p>Setup the node by sharing datasets and by launching the Fed-BioMed node:</p> <ul> <li> <p>if node GUI is launched, it can be used to share datasets. On the node side machine, connect to <code>http://localhost:8444</code></p> </li> <li> <p>view the node component logs</p> </li> </ul> <pre><code>[user@node $] docker compose logs node2\n</code></pre> <ul> <li> <p>optionally connect to the <code>node2</code> container and launch commands, instead of using the GUI, for example :</p> <ul> <li> <p>connect to the container</p> <p><pre><code>[user@node $] cd ${FEDBIOMED_DIR}/envs/vpn/docker\n[user@node $] docker compose exec -u $(id -u) node2 bash\n</code></pre>     * re-start the Fed-BioMed node, for example in background:</p> <pre><code>[user@node2-container $] kill $(ps auxwwww | grep -E 'python.*fedbiomed node start' | grep -Ev grep | awk '{ print $2}')\n[user@node2-container $] nohup fedbiomed node start $(cat /fbm-node/FBM_NODE_START_OPTIONS) &gt;/fbm-node/fedbiomed_node.out &amp;\n</code></pre> <p>Please note that in that case, the node component output does not appear anymore in <code>docker compose logs node</code></p> </li> <li> <p>share one or more datasets, for example a MNIST dataset or an interactively defined dataset (can also be done via the GUI):</p> <pre><code>[user@node2-container $] fedbiomed node dataset add -m /fbm-node/data\n[user@node2-container $] fedbiomed node dataset add\n</code></pre> </li> </ul> </li> </ul> <p>Example of a few more possible commands:</p> <ul> <li> <p>optionally list shared datasets:</p> <pre><code>[user@node2-container $] fedbiomed node dataset list\n</code></pre> </li> <li> <p>optionally register a new authorized training plan previously copied on the node side in <code>${FEDBIOMED_DIR}/envs/vpn/docker/node2/run_mounts/fbm-node/data/my_training_plan.txt</code></p> <p><pre><code>[user@node2-container $] fedbiomed node training-plan register\n</code></pre> Indicate <code>/fbm-node/data/my_training_plan.txt</code> as path of the training plan file.</p> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/","title":"Fed-BioMed deployment on multiple machines with VPN/containers","text":"<p>Most real-life deployments require protecting node data. Deployment using VPN/containers contributes to this goal by providing isolation of the Fed-BioMed instance from third parties. All communications between the components of a Fed-BioMed instance occur inside WireGuard VPN tunnels with mutual authentication of the VPN endpoints. Using containers can also ease installation on multiple sites.</p> <p>This tutorial details a deployment scenario where:</p> <ul> <li>Fed-BioMed vpnserver and researcher components run on the same machine (\"the server\") in the following <code>docker</code> containers<ul> <li><code>vpnserver</code> / <code>fedbiomed/vpn-vpnserver</code>: WireGuard server</li> <li><code>researcher</code> / <code>fedbiomed/vpn-researcher</code>: a researcher jupyter notebooks</li> </ul> </li> <li>several Fed-BioMed node components run, one node per machine with the following containers<ul> <li><code>node</code> / <code>fedbiomed/vpn-node</code>: a node component</li> <li><code>gui</code> / <code>fedbiomed/vpn-gui</code>: a GUI for managing node component data (optional)</li> </ul> </li> <li>all communications between the components are tunneled through a VPN</li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#requirements","title":"Requirements","text":"<p>Important</p> <p>Fed-BioMed docker images aren't distributed in any platform, and it requires to be rebuilt from source code. This article will guide you building docker images and containers from Fed-BioMed source.</p> <p>Supported operating systems and software requirements</p> <p>Supported operating systems for containers/VPN deployment include Fedora 38, Ubuntu 22.04 LTS. Should also work for most recent Linux, MacOS X 12.6.6, 13 and 14, Windows 11 with WSL2 using Ubuntu-22.04 distribution. Also requires docker and docker compose &gt;= 2.0.</p> <p>Check here for guidelines on <code>docker</code> and <code>docker-compose</code> installation</p> <p>Check here for detailed requirements.</p> <p>Account privileges</p> <p>Components deployment requires an account which can use docker (typically belonging to the <code>docker</code> group). Using a dedicated service account is a good practice. No access to the administrative account is needed, usage of <code>root</code> account for deploying components is discouraged to follow the principle of privilege minimization.</p> <p>Web proxy</p> <p>On sites where web access uses a proxy you need to configure web proxy for docker.</p> <p>User or group ID for containers</p> <p>By default, Fed-BioMed uses the current account's user and group ID for building and running containers.</p> <p>Avoid using low ID for user or group ( &lt; 500 for MacOS, &lt; 1000 for Linux ) inside containers. They often conflict with pre-existing user or group account in the container images. This results in unhandled failures when setting up or starting the containers. Check your account id with <code>id -a</code>.</p> <p>Use the <code>CONTAINER_USER</code>, <code>CONTAINER_UID</code>, <code>CONTAINER_GROUP</code> and <code>CONTAINER_GID</code> variables to use alternate values, eg for MacOS:</p> <p>MacOS commonly uses group <code>staff:20</code> for user accounts, which conflicts with Fed-BioMed VPN/containers mode. So a good configuration choice for MacOS can be:</p> <pre><code>export CONTAINER_GROUP=fedbiomed\nexport CONTAINER_GID=1111\n</code></pre> <p>More options for containers/VPN deployment are not covered in this tutorial but can be found here including:</p> <ul> <li>using GPU in <code>node</code> container</li> <li>building containers (eg: <code>node</code> and <code>gui</code>) on one machine, using this pre-built containers on the nodes</li> <li>using different identity (account) for building and launching a container</li> <li>deploying vpnserver and researcher on distinct machines</li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#notations","title":"Notations","text":"<p>In this tutorial we use the following notations:</p> <ul> <li><code>[user@server $]</code> means the command is launched on the server machine (outside containers)</li> <li><code>[user@node $]</code> means the command is launched on a node machine (outside containers)</li> <li>for commands typed inside containers, <code>[root@vpnserver-container #]</code> means the command is launched inside the <code>vpnserver</code> container as root, <code>[user@node-container $]</code> means the command is launched inside the <code>vpnserver</code> container with same user account as outside the container</li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#deploy-on-the-server-side","title":"Deploy on the server side","text":"<p>This part of the tutorial is executed once on the server side, before deploying the nodes. It covers the initial server deployment, including build, configuration and launch of containers.</p> <ul> <li> <p>download Fed-BioMed software by doing a local clone of the git repository:</p> <pre><code>[user@server $] git clone -b master https://github.com/fedbiomed/fedbiomed.git\n[user@server $] cd fedbiomed\n[user@server $] export FEDBIOMED_DIR=$PWD # use setenv for *csh\n[user@server $] cd envs/vpn/docker\n</code></pre> <p>For the rest of this tutorial <code>${FEDBIOMED_DIR}</code> represents the base directory of the clone.</p> <p><code>docker compose</code> commands need to be launched from <code>${FEDBIOMED_DIR}/envs/vpn/docker directory</code>.</p> </li> <li> <p>optionally choose a unique ID for this instance of Fed-BioMed. This is useful only when multiples instances of Fed-BioMed exist on the same machine. It adds another layer of security by using distinct <code>docker</code> networks for each instance running on the machine. It uses distinct container names for each instance, but does not support yet same containers from different instances running at the same time.</p> <pre><code># example: choose ID manually\n[user@server $] export FBM_CONTAINER_INSTANCE_ID=*my-instance-tag*\n# example: generate ID from docker file directory\n[user@server $] export FBM_CONTAINER_INSTANCE_ID=$(realpath $(pwd)|cksum|cut -d ' ' -f1)\n</code></pre> </li> <li> <p>clean running containers, containers files, temporary files</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean\n</code></pre> </li> <li> <p>optionally clean the container images to force build fresh new images</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean image\n</code></pre> </li> <li> <p>build server-side containers</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn build vpnserver researcher\n</code></pre> </li> <li> <p>configure the VPN keys for containers running on the server side, after starting the <code>vpnserver</code> container</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn configure researcher\n</code></pre> </li> <li> <p>start other server side containers</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn start researcher\n</code></pre> </li> <li> <p>check all containers are running as expected on the server side</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn status vpnserver researcher\n</code></pre> <p>Server side containers should be up and able to ping the VPN server</p> <pre><code>** Checking docker VPN images &amp; VPN access: vpnserver researcher\n- container vpnserver is running\n- container researcher is running\n- pinging VPN server from container vpnserver -&gt; OK\n- pinging VPN server from container researcher -&gt; OK\n</code></pre> <p>Server side containers are now ready for node side deployment.</p> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#deploy-on-the-node-side","title":"Deploy on the node side","text":"<p>This part of the tutorial is executed once on each node, after deploying the server. It covers the initial deployment, including build, configuration and launch of containers.</p> <p>Some commands are executed on the node side, while some commands are executed on the server side (pay attention to the prompt).</p> <p>For each node, choose a unique node tag (eg: NODETAG in this example) that represents this specific node instance for server side management commands.</p> <ul> <li> <p>download Fed-BioMed software by doing a local clone of the git repository:</p> <pre><code>[user@node $] git clone -b master https://github.com/fedbiomed/fedbiomed.git\n[user@node $] cd fedbiomed\n[user@node $] export FEDBIOMED_DIR=$PWD # use setenv for *csh\n[user@node $] cd envs/vpn/docker\n</code></pre> <p>For the rest of this tutorial <code>${FEDBIOMED_DIR}</code> represents the base directory of the clone.</p> <p><code>docker compose</code> commands need to be launched from <code>${FEDBIOMED_DIR}/envs/vpn/docker directory</code>.</p> </li> <li> <p>optionally choose a unique ID for this instance of Fed-BioMed. This is useful only when multiples instances of Fed-BioMed exist on the same machine. It adds another layer of security by using distinct <code>docker</code> networks for each instance running on the machine. It uses distinct container names for each instance, but does not support yet same containers from different instances running at the same time.</p> <pre><code># example: choose ID manually\n[user@node $] export FBM_CONTAINER_INSTANCE_ID=*my-instance-tag*\n# example: generate ID from docker file directory\n[user@node $] export FBM_CONTAINER_INSTANCE_ID=$(realpath $(pwd)|cksum|cut -d ' ' -f1)\n</code></pre> </li> <li> <p>clean running containers, containers files, temporary files (skip that step if node and server run on the same machine)</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean\n</code></pre> </li> <li> <p>optionally clean the container images to force build fresh new images</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean image\n</code></pre> </li> <li> <p>build node-side containers</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn build node gui\n</code></pre> </li> <li> <p>on the server side, generate a configuration for this node (known as NODETAG)</p> <pre><code>[user@server $] cd ${FEDBIOMED_DIR}/envs/vpn/docker\n[user@server $] docker compose exec vpnserver bash -ci 'python ./vpn/bin/configure_peer.py genconf node NODETAG'\n</code></pre> <p>The configuration file is now available on the server side in path <code>${FEDBIOMED_DIR}/envs/vpn/docker/vpnserver/run_mounts/config/config_peers/node/NODETAG/config.env</code> or with command :</p> <pre><code>[user@server $] docker compose exec vpnserver cat /config/config_peers/node/NODETAG/config.env\n</code></pre> </li> <li> <p>copy the configuration file from the server side to the node side via a secure channel, to path <code>/tmp/config.env</code> on the node.</p> <p>In most real life deployments, one shouldn't have access to both server side and node side. Secure channel in an out-of-band secured exchange (outside of Fed-BioMed scope) between the server administrator and the node administrator that provides mutual authentication of the parties, integrity and privacy of the exchanged file.</p> <p>In a test deployment, one may be connected both on server side and node side. In this case, you just need to cut-paste or copy the file to the node.</p> <p>Use the node's copy of the configuration file:</p> <pre><code>[user@node $] cp /tmp/config.env ./node/run_mounts/config/config.env\n</code></pre> </li> <li> <p>optionally force the use of secure aggregation by the node (node will refuse to train without the use of secure aggregation):</p> <pre><code>[user@node $] export FBM_SECURITY_FORCE_SECURE_AGGREGATION=True\n</code></pre> </li> <li> <p>optionally allow all training plans to execute without node side approval (warning: be sure this is coherent with your site security requirements !), or allow the pre-defined training plans to execute without approval:</p> <pre><code>[user@node $] export FBM_SECURITY_ALLOW_DEFAULT_TRAINING_PLANS=True\n[user@node $] export FBM_SECURITY_TRAINING_PLAN_APPROVAL=False\n</code></pre> </li> <li> <p>start <code>node</code> container</p> <pre><code>[user@node $] docker compose up -d node\n</code></pre> </li> <li> <p>retrieve the <code>node</code>'s publickey</p> <pre><code>[user@node $] docker compose exec node wg show wg0 public-key | tr -d '\\r' &gt;/tmp/publickey-nodeside\n</code></pre> </li> <li> <p>copy the public key from the node side to the server side via a secure channel (see above), to path <code>/tmp/publickey-serverside</code> on the server.</p> </li> <li> <p>on the server side finalize configuration of the VPN keys for this node (known as NODETAG)</p> <pre><code>[user@server $] cd ${FEDBIOMED_DIR}/envs/vpn/docker\n[user@server $] docker compose exec vpnserver bash -ci \"python ./vpn/bin/configure_peer.py add node NODETAG $(cat /tmp/publickey-serverside)\"\n</code></pre> </li> <li> <p>check containers running on the node side</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn status node\n</code></pre> <p><code>node</code> container should be up and able to ping the VPN server</p> <pre><code>** Checking docker VPN images &amp; VPN access: node\n- container node is running\n- pinging VPN server from container node -&gt; OK\n</code></pre> <p><code>node</code> container is now ready to be used.</p> </li> </ul> <p>Optionally launch the node GUI :</p> <ul> <li> <p>optionally authorize connection to node GUI from distant machines. By default, only connection from local machine (<code>localhost</code>) is authorized.</p> <pre><code>[user@node $] export GUI_SERVER_IP=0.0.0.0\n</code></pre> <p>To authorize distant connection to only one of the node machine's IP addresses use a command of the form <code>export GUI_SERVER_IP=a.b.c.d</code> where <code>a.b.c.d</code> is one of the IP addresses of the node machine.</p> <p>For security reasons, when authorizing connection from distant machines, it is strongly recommended to use a custom SSL certificate signed by a well-known authority.</p> <p>Custom SSL certificates  for GUI</p> <p>GUI will start serving on port 8443 with self-signed certificates. These certificates will be identified as risky by the browsers, and users will have to approve them. However, it is also possible to set custom trusted SSL certificates by adding <code>crt</code> and <code>key</code> files to the <code>${FEDBIOMED_DIR}/envs/vpn/docker/gui/run_mounts/certs</code> directory before starting the GUI.</p> <p>When adding these files, please ensure that:</p> <ul> <li>the certificate extension is <code>.crt</code> and the key file extension is <code>.key</code></li> <li>there is no more than one file for each certificate and key</li> </ul> </li> <li> <p>optionally restrict the HTTP host names that can be used to connect to the node GUI. By default all the host names (DNS CNAME) of the node machine can be used.</p> <p>For example, if the node machine has two host names <code>my.fqdn.com</code> and <code>other.alias.org</code>, use syntax like <code>export GUI_SERVER_NAME=my.fqdn.com</code> or <code>GUI_SERVER_NAME='*.fqdn.com'</code> (don't forget the enclosing single quotes) to authorize only requests using the first name (eg: <code>https://my.fqdn.com</code>) to reach the node GUI. Use the syntax of Nginx <code>server_name</code>.</p> </li> <li> <p>start <code>gui</code> container</p> <pre><code>[user@node $] docker compose up -d gui\n</code></pre> </li> <li> <p>check containers running on the node side</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn status node gui\n</code></pre> <p>Node side containers should be up and able to ping the VPN server</p> <pre><code>** Checking docker VPN images &amp; VPN access: node gui\n- container node is running\n- container gui is running\n- pinging VPN server from container node -&gt; OK\n</code></pre> <p><code>node</code> and <code>gui</code> containers are now ready to be used.</p> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#optionally-deploy-a-second-node-instance-on-the-same-node","title":"Optionally deploy a second node instance on the same node","text":"<p>Optionally deploy a second node instance on the same node (useful for testing purpose, not a normal deployment scenario):</p> <ul> <li>deploy second node on the same machine</li> </ul> <p>This part of the tutorial is optionally executed on some nodes, after deploying the server.</p>"},{"location":"user-guide/deployment/deployment-vpn/#use-the-node","title":"Use the node","text":"<p>This part is executed at least once on each node, after deploying the node side containers.</p> <p>Setup the node by sharing datasets. The commands below will use default Fed-BioMed node directory which is located <code>/fbm-node</code> inside the container.</p> <ul> <li> <p>if node GUI is launched, it can be used to share datasets. On the node side machine, connect to <code>https://localhost:8443</code> (or <code>https://&lt;host_name_and_domain&gt;:8443</code> if connection from distant machine is authorized)</p> </li> <li> <p>view the node component logs</p> </li> </ul> <pre><code>[user@node $] docker compose logs node\n</code></pre> <ul> <li> <p>optionally connect to the <code>node</code> container and launch commands, instead of using the GUI, for example :</p> <ul> <li> <p>connect to the container</p> <pre><code>[user@node $] cd ${FEDBIOMED_DIR}/envs/vpn/docker\n[user@node $] docker compose exec -u $(id -u) node bash\n</code></pre> </li> <li> <p>re-start the Fed-BioMed node, for example in background:</p> <pre><code>[user@node-container $] kill $(ps auxwwww | grep -E 'python.*fedbiomed node start' | grep -Ev grep | awk '{ print $2}')\n[user@node-container $] nohup fedbiomed node start $(cat /fbm-node/FBM_NODE_START_OPTIONS) &gt;/fbm-node/fedbiomed_node.out &amp;\n</code></pre> <p>Please note that in that case, the node component output does not appear anymore in <code>docker compose logs node</code></p> </li> <li> <p>share one or more datasets, for example a MNIST dataset or an interactively defined dataset (can also be done via the GUI):</p> <pre><code>[user@node-container $] fedbiomed node dataset add -m /fbm-node/data\n[user@node-container $] fedbiomed node dataset add\n</code></pre> </li> </ul> </li> </ul> <p>Example of a few more possible commands:</p> <ul> <li> <p>optionally list shared datasets:</p> <pre><code>[user@node-container $] fedbiomed node dataset list\n</code></pre> </li> <li> <p>optionally register a new authorized training plan previously copied on the node side in <code>${FEDBIOMED_DIR}/envs/vpn/docker/node/run_mounts/fbm-node/data/my_training_plan.txt</code></p> <p><pre><code>[user@node-container $] fedbiomed node training-plan register\n</code></pre> Indicate <code>/fbm-node/data/my_training_plan.txt</code> as path of the training plan file.</p> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#optionally-use-a-second-node-instance-on-the-same-node","title":"Optionally use a second node instance on the same node","text":"<p>This optional part is executed at least once on the nodes where a second node instance is deployed, after deploying the second node side containers:</p> <ul> <li>use second node on the same machine</li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#use-the-server","title":"Use the server","text":"<p>This part is executed at least once on the server after setting up the nodes:</p> <ul> <li> <p>on the server side machine, connect to <code>http://localhost:8888</code>, then choose and run a Jupyter notebook</p> <ul> <li> <p>make more notebooks available from the server side machine (eg: <code>/tmp/my_notebook.ipynb</code>) by copying them to the <code>samples</code> directory</p> <p><pre><code>[user@server $] cp /tmp/my_notebook.ipynb ${FEDBIOMED_DIR}/envs/vpn/docker/researcher/run_mounts/samples/\n</code></pre> The notebook is now available in the Jupyter GUI under the <code>samples</code> subdirectory of the Jupyter notebook interface.</p> </li> </ul> </li> <li> <p>if the notebook uses Tensorboard, it can be viewed</p> <ul> <li>either embedded inside the Jupyter notebook as explained in the Tensorboard documentation</li> <li>or by connecting to <code>http://localhost:6006</code></li> </ul> </li> </ul> <p>Optionally use the researcher container's command line instead of the Jupyter notebooks:</p> <ul> <li> <p>connect to the <code>researcher</code> container</p> <pre><code>[user@server $] cd ${FEDBIOMED_DIR}/envs/vpn/docker\n[user@server $] docker compose exec -u $(id -u) researcher bash\n</code></pre> </li> <li> <p>launch a script, for example a training:</p> <p><pre><code>[user@server-container $] jupyter nbconvert /fbm-researcher/notebooks/101_getting-started.ipynb --output=101_getting-started --to script\n[user@server-container $] python /fbm-researcher/notebooks/101_getting-started.py\n</code></pre> Note: the .py script can be created from the notebooks by a command such as <code>jupyter nbconvert --output=101_getting-started --to script ./notebooks/101_getting-started.ipynb</code>.</p> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#misc-server-management-commands","title":"Misc server management commands","text":"<p>Some possible management commands after initial deployment include:</p> <ul> <li> <p>check all containers running on the server side</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn status vpnserver researcher\n</code></pre> </li> <li> <p>check the VPN peers known from the VPN server</p> <pre><code>[user@server $] ( cd ${FEDBIOMED_DIR}/envs/vpn/docker ; docker compose exec vpnserver bash -ci \"python ./vpn/bin/configure_peer.py list\" )\ntype        id           prefix         peers\n----------  -----------  -------------  ------------------------------------------------\nresearcher  researcher1  10.222.0.2/32  ['1exampleofdummykeyVo+lj/ZfT/wYv+I9ddWYzohC0=']\nnode        NODETAG      10.221.0.2/32  ['1exampleofdummykey/Z1SKEzjsMkSe1qztF0uXglnA=']\n</code></pre> <p>VPN configurations and container files are kept unchanged when restarting containers.</p> </li> <li> <p>clean running containers, container files and temporary files on the server side. Requires to stop containers before.</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn stop vpnserver researcher\n[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean\n</code></pre> <p>Warning: all VPN configurations, researcher configuration files,experiment files and results, etc. are deleted when cleaning.</p> <p>To clean also the container images:</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean image\n</code></pre> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#misc-node-management-commands","title":"Misc node management commands","text":"<p>Some possible management commands after initial deployment include:</p> <ul> <li> <p>check all containers running on the node side</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn status node gui\n</code></pre> </li> <li> <p>restart all containers running on the node side</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn stop node gui\n[user@node $] ( cd ${FEDBIOMED_DIR}/envs/vpn/docker ; docker compose up -d node gui )\n</code></pre> <p>VPN configurations and container files are kept unchanged when restarting containers.</p> </li> <li> <p>clean running containers, container files and temporary files on the node side. Requires to stop containers before.</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn stop node gui\n[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean\n</code></pre> <p>Warning: all VPN configurations, node configuration files, node dataset sharing, etc. are deleted when cleaning.</p> <p>To clean also the container images:</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean image\n</code></pre> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#misc-use-alternate-container-image-version","title":"Misc use alternate container image version","text":"<p>When building or starting a component using <code>fedbiomed_vpn</code> or <code>docker compose</code>, container image version automatically matches the software version of the sources in the current clone.</p> <p>To use a different container image version either:</p> <ul> <li>set the <code>FBM_CONTAINER_VERSION_TAG</code> environment variable (temporary change) or</li> <li>set the <code>FBM_CONTAINER_VERSION_TAG</code>variable in the <code>$FEDBIOMED_DIR/envs/vpn/docker/.env</code> file (permanent change)</li> </ul> <p>Important</p> <p>Using alternate container image version is advanced functionality. It may break the containers configurations of your current clone. Use them only if you know what you are doing. </p> <p>Examples:</p> <ul> <li> <p>clean all containers and images for version <code>my_version_tag</code>, plus containers files and temporary files in current clone of the sources</p> <pre><code>[user@server $] FBM_CONTAINER_VERSION_TAG=my_version_tag ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean image\n</code></pre> </li> <li> <p>build researcher container from the current clone (thus using the sources of current clone's version), then tag the image with version <code>my_version_tag</code></p> <pre><code>[user@server $] FBM_CONTAINER_VERSION_TAG=my_version_tag ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn build researcher\n</code></pre> </li> <li> <p>start researcher container from the current clone (thus using a file tree fitting current clone's version), using an image with version <code>my_version_tag</code></p> <pre><code>[user@server $] FBM_CONTAINER_VERSION_TAG=my_version_tag ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn start researcher\n</code></pre> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/deployment/deployment-vpn/#proxy","title":"Proxy","text":"<p>On a site where access to an Internet web site requires using a proxy, configure web proxy for docker client in <code>~/.docker/config.json</code>.</p> <p>Prefix used by Fed-BioMed's communication inside the VPN (<code>10.220.0.0/14</code>) shall not be proxied. So your proxy configuration may look like (replace <code>mysiteproxy.domain</code> with your site proxy):</p> <pre><code>{\n \"proxies\":\n {\n   \"default\":\n   {\n     \"httpProxy\": \"http://mysiteproxy.domain:3128\",\n     \"httpsProxy\": \"http://mysiteproxy.domain:3128\",\n     \"noProxy\": \"10.220.0.0/14\"\n   }\n }\n}\n</code></pre>"},{"location":"user-guide/deployment/deployment-vpn/#image-build-on-mac-m1m2m3-arm","title":"Image build on Mac M1/M2/M3 ARM","text":"<p>Build may fail on Mac M1/M2/M3 processors for <code>fedbiomed/vpn-base</code> or <code>fedbiomed/vpn-basenode</code> due to arm64 system.</p> <p>By default, docker can use the images that are created for <code>arm64/aarch64</code>. But docker build files of the Fed-BioMed images and the libraries that are installed within are compatible with <code>amd64</code> type of platforms. Therefore, you may get some error while <code>fedbiomed_vpn</code> script builds images. Those errors can be during secure aggregation setup or while installing some of required pip dependencies for Fed-BioMed environment. You can fix this problem by setting environment variable that declares default default docker platform to force docker to use linux/amd64.</p> <pre><code>export DOCKER_DEFAULT_PLATFORM=linux/amd64\n</code></pre> <p>After setting this variable you can execute build command through <code>{FEDBIOMED_DIR}/scripts/fedbiomed_vpn build</code></p>"},{"location":"user-guide/deployment/deployment-vpn/#not-enough-space-on-your-machine-due-to-recursive-build-operations","title":"Not enough space on your machine due to recursive build operations","text":"<p>If you have done too many builds, it may fill up the disk space reserved for Docker containers. In that case, please clear your Docker build cache to free up some space.</p> <pre><code>docker builder prune -a\n</code></pre>"},{"location":"user-guide/deployment/deployment/","title":"Fed-BioMed deployment scenarios","text":"<p>Fed-BioMed can be deployed in different ways:</p> <ul> <li>single-machine (nodes and researcher run on the same machine) or multiple-machine</li> <li>with or without containers (each component runs in its <code>docker</code> container) plus VPN (all communications between components are tunneled in a WireGuard VPN with mutual authentication of the VPN endpoints)</li> </ul> <p>Choose a scenario depending on the context and requirements:</p> <ul> <li> <p>Single-machine without VPN/containers is the basic simple installation scenario described in the introduction tutorials. Use cases include: newcomer testing Fed-BioMed software ; FL researcher designing and testing FL methods with (non sensitive) data on the laptop ; software developer contributing to Fed-BioMed.</p> </li> <li> <p>Single-machine with VPN/containers deployment is a simplified version of Multiple-machine with VPN/containers deployment where all components build and run on the same machine. It can be useful for testing purpose or integration tests.</p> </li> <li> <p>Multiple-machine without VPN/containers deployment is briefly described here. This should only be used when components are connected through a highly secure network.</p> </li> <li> <p>Multiple-machine with VPN/containers deployment is documented here. Most real-life deployments use this scenario to protect node data and communications between components. Typical deployment includes a secure federation server and one node for each data provider site.</p> </li> </ul> <p>Check the security model and network communications to understand which scenario fits your needs.</p>"},{"location":"user-guide/deployment/matrix/","title":"Network communication matrix","text":"<p>This page describes the network communications:</p> <ul> <li>between the Fed-BioMed components (<code>node</code>s, and <code>researcher</code>), aka application internal / backend communications</li> <li>for user access to the Fed-BioMed components GUI</li> <li>for Fed-BioMed software installation and docker image build</li> </ul> <p>Communications between the components depend on the deployment scenario: with or without VPN/containers.</p> <p>Network communications for the setup of host machines and the installation of software requirements are out of the scope of this document. Network communications necessary for the host machines to run (access to DNS, LDAP, etc.) regardless of Fed-BioMed are also out of the scope of this document. They are specific to system configuration and installation processes.</p>"},{"location":"user-guide/deployment/matrix/#introduction","title":"Introduction","text":"<p>Fed-BioMed network communications basic principle is that all communications between components are outbound from one <code>node</code> to the <code>researcher</code>. There is no inbound communications to a <code>node</code>.</p> <p>The exception to this principle are optional direct communications between the components for using the secure aggregation feature (eg. <code>node</code>/<code>researcher</code> to <code>node</code>/<code>researcher</code> communication for cryptographic material negotiation). The communications for crypto material are closed after the negotiation is completed and handle only secagg key negotiation requests.</p> <p>Fed-BioMed provides some optional GUI for the <code>node</code> (node configuration GUI) and the <code>researcher</code> (Jupyter notebook and Tensorboard). By default, these GUI components are not secured (no HTTPS and/or no certificate signed by well known authority). So they are configured by default to accept only communications from the same machine (localhost).</p>"},{"location":"user-guide/deployment/matrix/#software-installation","title":"Software installation","text":"<p>Network communications for software installation cover the Fed-BioMed software installation and setup until the software is ready to be used.</p> <p>They cover all deployment scenarios. If multiple machines are used, each machine needs to authorize these communications.</p> <ul> <li>direction is out (outbound communication from the component) or in (inbound communication to the component)</li> </ul> dir source machine destination machine destination port service out component Internet TCP/80 HTTP out component Internet TCP/443 HTTPS <p></p> <p>\"Component\" is the <code>node</code> or <code>researcher</code>.</p> <p>For destination machine, it is simpler to authorize outbound communications to all Internet addresses for the required ports during installation. Indeed, several packaging systems are used for installation, with no guarantee of stable IP address used by the packaging server:</p> <ul> <li>For all deployment scenarios: conda, pip (all components) and yarn/npm (node GUI component) packages</li> <li>Plus for VPN/containers scenarios: dockerhub images, apt apk and cargo packages, git over https cloning, wget and curl download</li> </ul> <p>Note: when using a VPN/containers scenario, a site with very stringent requirements on <code>node</code>'s communication can avoid authorizing the above communications for installation of the node components (<code>node</code> and <code>gui</code>). For that, it needs to build the components docker image on another machine (with the above filter), save the image, copy it to the node machine, load it on the node machine. This scenario is not fully packaged and documented by Fed-BioMed but you can find some guidelines here.</p>"},{"location":"user-guide/deployment/matrix/#running-without-vpncontainers","title":"Running without VPN/containers","text":"<p>This part describes the communication matrix for running Fed-BioMed without VPN/containers:</p> <ul> <li>The direction is out (outbound communication from the component) or in (inbound communication to the component)</li> <li>Type of communication is either backend (between the application components) or user (user access to a component GUI). Command line user access to component from localhost are not noted here. GUI access are noted though recommended default configuration is to give access only from localhost</li> <li>The status is either mandatory (needed to run Fed-BioMed) or optional (a Fed-BioMed experiment can run without this part)</li> </ul> <p>On the node component (<code>node</code> + <code>gui</code> ):</p> dir source machine destination machine destination port service type status comment out node researcher TCP/50051 gRPC/TLS backend mandatory node-researcher communications in localhost gui TCP/8484 HTTP user optional node GUI <ul> <li><code>node</code> and <code>gui</code> also need a shared filesystem, so they are usually installed on the same machine.</li> </ul> <p></p> <p>On the researcher component (<code>researcher</code>):</p> dir source machine destination machine destination port service type status comment in nodes researcher TCP/50051 gRPC/TLS backend mandatory node-researcher communications in localhost researcher TCP/8888 HTTP user optional Jupyter in localhost researcher TCP/6006 HTTP user optional Tensorboard"},{"location":"user-guide/deployment/matrix/#running-with-vpncontainers","title":"Running with VPN/containers","text":"<p>This part describes the communication matrix for running Fed-BioMed with VPN/containers:</p> <ul> <li>The direction is out (outbound communication from the component) or in (inbound communication to the component)</li> <li>Type of communication is either backend (between the application components) or user (user access to a component GUI). Command line user access to component from localhost are not noted here. GUI access are noted though recommended default configuration is to give access only from localhost</li> <li>The status is either mandatory (needed to run Fed-BioMed) or optional (a Fed-BioMed experiment can run without this part)</li> </ul> <p>On the node component (<code>node</code> + <code>gui</code> ):</p> dir source machine destination machine destination port service type status comment out node vpnserver UDP/51820 WireGuard backend mandatory in localhost gui TCP/8443 HTTPS user optional node GUI <ul> <li><code>node</code> and <code>gui</code> also need a shared filesystem, so they are usually installed on the same machine.</li> </ul> <p></p> <p>On the researcher component (<code>researcher</code>):</p> dir source machine destination machine destination port service type status comment out researcher vpnserver UDP/51820 WireGuard backend mandatory in localhost researcher TCP/8888 HTTP user optional Jupyter in localhost researcher TCP/6006 HTTP user optional Tensorboard"},{"location":"user-guide/deployment/security-model/","title":"Fed-BioMed security model","text":"<p>This page gives an overview of Fed-BioMed security model. A more complete and formal description of the security model underlying Fed-BioMed is currently work in progress.</p>"},{"location":"user-guide/deployment/security-model/#summary","title":"Summary","text":"<p>Fed-BioMed empowers the node sites</p> <p>In a Fed-BioMed instance, nodes have control. There is no notion of trusted party that has full control or full access to the other parties. The researcher can only train a model authorized by the node, on data explicitely shared by the node.</p> <p>Fed-BioMed minimizes firewall filters</p> <p>Fed-BioMed nodes and researcher only need one outbound VPN port to one server for running</p> <p>Fed-BioMed offers high protection against outsiders</p> <p>Fed-BioMed offers high protection against attacks coming from outside of the Fed-BioMed instance by isolating all communications between the components inside a VPN.</p> <p>Fed-BioMed protects from major attacks by insiders</p> <p>Fed-BioMed also identifies possible attacks coming from one of the Fed-BioMed instance components and already offers protection against major attack scenarios.</p>"},{"location":"user-guide/deployment/security-model/#assets-threats-vulnerabilities","title":"Assets, threats, vulnerabilities","text":"<p>Fed-BioMed security assets (as defined in ENISA glossary) and their assessed value are:</p> <ul> <li>node data: the primary goal of Fed-BioMed is to protect the data of the participating nodes </li> <li>host machines: they are an indirect asset (infrastructure asset) of Fed-BioMed as they host the other assets. Thus, the protection of the machines hosting the node and researcher components is at least as important as protecting the assets they host. Moreover, Fed-BioMed software should not be a vector to compromise the host machine or other machines/assets on the host site.</li> <li>experiment inputs (training plan source code, optional custom strategy/optimizer source code, training parameters and model hyper-parameters) and outputs (final trained model, experiment intermediate results, local training updates from nodes): the final trained model parameters are an important asset as they are the main output of the software. Experiment inputs and intermediate results are necessary to compute the final trained model, and the users may value intellectual property on them.</li> </ul> <p>Fed-BioMed identified threats are:</p> <ul> <li>outsiders: they include all the machines/people that do not belong to the Fed-BioMed instance (all but the Fed-BioMed components). They are considered to be the most likely adversaries, conducting active attacks (malicious). They mostly try to breach confidentiality of data, but may attempt any type of impact on assets.</li> <li>insiders: they are the members of the Fed-BioMed instance (nodes, researcher). They are considered to be less likely adversaries. Our current security model addresses in priority the case of honest but curious nodes (parties do not attempt at modifying the protocol for attacks), while the researcher may be malicious. Attacks are primarily aimed at breach data confidentiality, although they may also attempt to other kind of assets.</li> </ul> <p>Fed-BioMed main identified vulnerabilities are:</p> <ul> <li><code>1.</code> federated learning: honest but curious researcher can attempt inference or reconstruction attacks on local model parameters sent by the nodes and try to gain some knowledge about the nodes' data</li> <li><code>2.</code> infrastructure: honest but curious node man in the middle (MITM) can listen to gRPC exchanges between parties. It then learns queries and results of the trainings performed by the nodes. The primary interest is to learn the local model parameters from other nodes and attempt attacks mentioned in <code>1.</code>. It can also learn global model parameters and attempt attacks mentioned in <code>8.</code></li> <li><code>3.</code> federated learning: malicious researcher authorized to train on a node can send malicious training plan code to breach the assets. Typically, it tries to leak data from the nodes.</li> <li><code>4.</code> infrastructure: malicious insider man in the middle (MITM) can spoof the researcher to execute training commands on the node (possibly <code>3.</code>)</li> </ul> <p>Fed-BioMed other vulnerabilities include:</p> <ul> <li><code>5.</code> federated learning: advanced attacks such as model poisoning, free-riding attacks, etc.</li> <li><code>6.</code> infrastructure: outsider may attempt penetration attacks on a VPN endpoint</li> <li><code>7.</code> infrastructure: insider may attempt penetration attacks on another component of the Fed-BioMed instance</li> <li><code>8.</code> federated learning: honest but curious nodes can attempt inference or reconstuction attacks on global model parameters sent by the researcher and try to gain some knowledge about the other nodes' data</li> <li><code>9.</code> inference and reconstruction attacks on the final trained model: a malicious outsider that duly receives a copy of the final trained model for using it may try attacks from <code>1.</code>. This case is considered out of scope of this analysis, as it occurs outside of Fed-BioMed. Same precautions should be taken as for any machine learning model.</li> </ul>"},{"location":"user-guide/deployment/security-model/#addressing-the-vulnerabilities","title":"Addressing the vulnerabilities","text":"<p>Fed-BioMed addresses the above vulnerabilities in the following way:</p> <ul> <li>secure aggregation and differential privacy offer options to remove or reduce the risk coming from <code>1.</code></li> <li>gRPC encryption adresses 2. by encrypting all message and data exchanges between each node and the researcher. </li> <li>exploiting <code>4.</code> would enable a node component to execute same commands (training) or retrieve same information (local training updates from node) as the researcher, but no more. Moreover, inside the VPN the Fed-BioMed instance is star-shaped, which requires active (malicious node) attack. This is why mutual verification of other party identity during gRPC encryption setup (inside the VPN) was not prioritized by Fed-BioMed. Nevertheless, it is in the midterm roadmap.</li> <li>model approval functionality addresses <code>3.</code> by enabling each node site to review and authorize a training plan before it can train on the node.</li> <li>advanced federated learning attacks from <code>5.</code> will be further addressed in future releases with innovative functions. Stay tuned.</li> <li>Fed-BioMed seeks to offer minimal attack surface to penetration attacks from <code>6.</code>: the only network communication between the components is through the WireGuard VPN. Moreover, node sites that hold the data only have outbound connections to further reduce the attack surface on nodes.</li> <li>Fed-BioMed seeks to reduce the attack surface to penetration attacks from <code>7.</code> by design: nodes only have outbound communications to the gRPC server on the researcher. Researchers only accept inbound communications to the gRPC server. Also, the node only accepts a limited set of commands from a legitimate researcher. There is no notion of trusted party that have full control over the nodes or the node data in a Fed-BioMed instance. Finally, our future implementation of secure communication inside the VPN will further reduce this risk.</li> <li>attacks on global updates from <code>8.</code> are considered more complicated than attacks on local updates. Currently, implemented Local and Central Differential Privacy are valid mechanisms to protect against these attacks, and other specific defense strategies will be further addressed in future releases. </li> </ul>"},{"location":"user-guide/deployment/versions/","title":"Versions","text":"<p>Fed-BioMed stores and checks version numbers for several of its components.  The semantics of the versions are as follows:</p> <ul> <li>different major version: incompatibility that results in halting the execution immediately</li> <li>different minor or micro version: backward compatibility, provide a warning message if versions are different</li> </ul> <p>This page tracks the version changes for each component, to provide further information when incompatibilities are detected.</p>"},{"location":"user-guide/deployment/versions/#configuration-files","title":"Configuration files","text":"<ul> <li>researcher configuration file</li> <li>node configuration file</li> <li>node state</li> <li>researcher breakpoint</li> <li>messaging protocol</li> </ul> <p>Note that due to the two-sided nature of the communication, every change to the messaging protocol is equivalent to a major change.</p> <p>Messaging protocol incompatible versions</p> <p>In case of version mismatch, the only solution is to upgrade the software to have the same version on researcher and all nodes.</p>"},{"location":"user-guide/deployment/vpn-dependencies-install/","title":"Fed-BioMed VPN/containers software dependencies","text":"<p>The following packages are required for Fed-BioMed with VPN/containers:</p> <ul> <li><code>docker</code></li> <li><code>docker compose</code> v2: don't confuse it with the obsolete <code>docker-compose</code> v1</li> </ul>"},{"location":"user-guide/deployment/vpn-dependencies-install/#install-docker-and-docker-compose","title":"Install <code>docker</code> and <code>docker compose</code>","text":""},{"location":"user-guide/deployment/vpn-dependencies-install/#linux-fedora","title":"Linux Fedora","text":"<p>Install and start docker engine packages. In simple cases it is enough to run :</p> <pre><code>$ sudo dnf install -y dnf-plugins-core\n$ sudo dnf config-manager \\\n    --add-repo \\\n    https://download.docker.com/linux/fedora/docker-ce.repo\n$ sudo dnf install -y docker-ce docker-ce-cli containerd.io\n$ sudo systemctl start docker\n</code></pre> <p>Allow current account to use docker :</p> <pre><code>$ sudo usermod -aG docker $USER\n</code></pre> <p>Check with the account used to run Fed-BioMed that docker is up and can be used by the current account without error :</p> <pre><code>$ docker run hello-world\n</code></pre> <p>Install <code>docker compose</code>: <pre><code>$ sudo dnf install -y docker-compose-plugin\n</code></pre></p> <p>To make sure you have a docker compose v2, you can run the following: <pre><code>$ docker compose version\n</code></pre></p>"},{"location":"user-guide/deployment/vpn-dependencies-install/#macos","title":"MacOS","text":"<p>Install <code>docker</code> and <code>docker compose</code> choosing one of the available options for example :</p> <ul> <li>official full Docker Desktop installation process, please check product license</li> <li>your favorite third party package manager for example :<ul> <li>macports provides docker port</li> <li>homebrew provides docker formula</li> <li>don't use the <code>docker-compose</code> v1 from macports or homebrew !</li> <li>for <code>docker compose</code> v2, adapt the manual plugin install procedure by picking the proper binary for your hardware</li> </ul> </li> </ul> <p>Check with the account used to run Fed-BioMed docker is up and can be used by the current account without error :</p> <pre><code>$ docker run hello-world\n</code></pre>"},{"location":"user-guide/deployment/vpn-dependencies-install/#other","title":"Other","text":"<p>Connect under an account with administrator privileges, install <code>docker</code>, ensure it is started and give docker privilege for the account used for running Fed-BioMed. Also install <code>docker compose</code> v2.</p> <p>Check with the account used to run Fed-BioMed docker is up and can be used by the current account without error :</p> <pre><code>$ docker run hello-world\n</code></pre>"},{"location":"user-guide/installation/","title":"Installation guide","text":"<ul> <li>Installation guide for Windows</li> </ul>"},{"location":"user-guide/installation/windows-installation/","title":"Specific instructions for Windows installation","text":"<p>Fed-BioMed requires Windows 11 and WSL2. It can run on a physical machine or a virtual machine.</p> <p>This documentation gives the steps for a typical Windows 11 installation, steps may vary depending on your system.</p>"},{"location":"user-guide/installation/windows-installation/#step-0-optional-virtual-machine","title":"Step 0: (optional) virtual machine","text":"<p>Skip this step if running Windows on a physical (native) machine, not a virtual machine.</p> <p>Requirement : choose an hypervisor compatible with other installation</p> <ul> <li>VMware Workstation 15.5.5 or above can be used, it is compatible with HyperV. We successfully ran Fed-BioMed with VMware Workstation 16.</li> <li>VirtualBox 6.1 cannot be used as it conflicts with Hyper-V \"Virtual Machine Platform\" which is needed for WSL2</li> </ul> <p>Tips :</p> <ul> <li>enable virtualization engine for your VM. For VMware Workstation 16, check the boxes in Virtual Machine Settings &gt; Hardware &gt; Processors &gt; Virtualization Engine</li> <li>allocate 8GB RAM or more to the VM, this is needed for conda PyTorch installation. For VMware Workstation 16 this can be done in Virtual Machine Settings &gt; Hardware &gt; Memory</li> </ul>"},{"location":"user-guide/installation/windows-installation/#step-1-windows","title":"Step 1: Windows","text":"<p>Requirement : tested under Windows 11 21H2, though Windows 10 version 2004 and higher (Build 19041 and higher) also support WSL2 and Docker Desktop.</p> <ul> <li>update Windows</li> <li>reboot Windows</li> </ul> <p>You can run Fed-BioMed on any Windows 11 edition including <code>Home</code> edition if you don't use optional containers/dockers functionality.</p> <p>Requirement: if using optional containers/docker, then Windows <code>Enterprise</code>, <code>Pro</code> or <code>Education</code> edition is needed for Hyper-V functionality, which is not present in <code>Home</code> edition. </p> <p>Requirement : Hyper-V \"Virtual Machine Platform\" activation</p> <ul> <li>enable Hyper-V</li> <li>reboot Windows</li> </ul>"},{"location":"user-guide/installation/windows-installation/#step-2-wsl2","title":"Step 2: WSL2","text":"<p>WSL (Windows Subsystem for Linux) is a tool that allows to run Linux within a Windows system. Version 2 of WSL is needed for docker. We successfully tested Fed-BioMed with Ubuntu-24.04 distribution.</p> <p>Requirement : WSL version 2</p> <ul> <li>activate WSL with main menu &gt; enter 'Turn Windows Feature on or off' &gt; and click on 'Windows Subsystem for Linux' checkbox</li> <li>reboot Windows</li> <li>set version 2 in a Windows command tool <pre><code>cmd&gt; wsl --set-default-version 2\n</code></pre></li> <li>reboot Windows</li> <li>Optional: you may need to run  <pre><code>cmd&gt; wsl --install\n</code></pre></li> </ul> <p>Requirement : a WSL distribution, eg Ubuntu</p> <ul> <li>install a distribution in a Windows command tool <pre><code>cmd&gt; wsl --install -d Ubuntu-24.04\n</code></pre></li> <li>if required by install, update the WSL2 kernel: in *main menu &gt; enter 'Windows Update' &gt; select 'Advanced Options' &gt; activate 'Receive updates for other Microsoft products'. Then in Windows Update, install the last version of WSL2.</li> <li>reboot Windows</li> </ul> <p>Check that WSL uses version 2 and Ubuntu is installed in a Windows command tool : <pre><code>cmd&gt; wsl -l -v\n  NAME                   STATE           VERSION\n* Ubuntu                 Running         2\n</code></pre></p> <p>Open a WSL session from a Windows command tool : <pre><code>cmd&gt; wsl\nuser@wsl-ubuntu$\n</code></pre></p>"},{"location":"user-guide/installation/windows-installation/#step-3-docker-optional-required-if-containersvpn-needed","title":"Step 3: docker (optional, required if containers/VPN needed)","text":"<p>Requirement : <code>docker</code> and <code>docker compose</code></p> <p>Install Docker Desktop on the Windows machine :</p> <ul> <li>install Docker Desktop in Windows. Check the product license.</li> <li>reboot Windows</li> </ul> <p>Setup docker in your WSL Ubuntu :</p> <ul> <li> <p>open an administrator session in WSL Ubuntu :</p> <p><pre><code>user@wsl-ubuntu$ sudo bash\nroot@wsl-ubuntu#\n</code></pre>   - install docker engine as admin (root) account in WSL Ubuntu. Please note that <code>docker container run hello-world</code> will not work until we complete the steps below   - if you use an account named <code>USER</code> under Ubuntu, authorize it to use docker by typing under an admin (root) account in WSL Ubuntu :</p> <pre><code>root@wsl-ubuntu# adduser USER docker\n</code></pre> </li> <li> <p>open a new WSL Ubuntu terminal so that it is authorized to use docker</p> </li> </ul> <p>Check that you can use docker with your user account under Ubuntu :</p> <pre><code>user@wsl-ubuntu$ docker container run hello-world\n</code></pre>"},{"location":"user-guide/installation/windows-installation/#step-4-conda","title":"Step 4: conda","text":"<p>It is recommended to run <code>fedbiomed</code> under an environment manager. <code>conda</code> has been tested for Windows WSL2 installation of Fed-BioMed.</p> <p>Requirement : conda installed in Ubuntu and configured for your user account</p> <ul> <li>install Miniconda under Ubuntu, using your user account</li> <li>during installation, answer Yes to question \u201cDo you wish the installer to initialize Anaconda3 by running conda init?\u201d</li> <li> <p>activate conda for your Ubuntu session</p> <pre><code>user@wsl-ubuntu$ source ~/.bashrc\n</code></pre> </li> </ul>"},{"location":"user-guide/installation/windows-installation/#step-5-fed-biomed","title":"Step 5: Fed-BioMed","text":"<p>Follow Fed-BioMed Linux installation tutorial with a <code>pip install fedbiomed[researcher,node,gui]</code> or shrink down to a specific component (<code>node</code>, <code>researcher</code>, <code>gui</code>).</p> <p>When running for the first time, a Windows defender pop up may appear (triggered by docker), choose \"authorize only on private network\".</p> <p>Performance issue</p> <p>To ensure  Fed-BioMed performance in WSL2, be sure to use the native WSL2 Linux filesystem (in <code>/home/login</code>), not the Windows filesystem (in <code>/mnt/c/Users/login</code>), both for cloning the library and storing datasets. We experienced 10-50 time slower execution with native Windows filesystem. This point is documented by Microsoft</p> <p>You may also need to increase the memory available to WSL2. By default, only 50% of the host's RAM is made available.</p>"},{"location":"user-guide/installation/windows-installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/installation/windows-installation/#step-2-troubleshooting","title":"Step 2 troubleshooting","text":""},{"location":"user-guide/installation/windows-installation/#error-1-the-virtual-machine-could-not-be-started-because-a-required-feature-is-not-installed","title":"Error 1: <code>The virtual machine could not be started because a required feature is not installed</code>","text":"<p>When installing Linux, if this error happens:</p> <pre><code>Installing, this may take a few minutes...\nWslRegisterDistribution failed with error: 0x80370102\nError: 0x80370102 The virtual machine could not be started because a required feature is not installed.\n</code></pre> <p>This means either you need to enable virtualisation on the bios of your computer or to enable Hyper-V : for the latter, go in</p> <pre><code>main menu &gt; enter 'Turn Windows Feature on or off' &gt; and click on Hyper-V checkbox\n</code></pre> <p>Then restart Linux distribution installation.</p>"},{"location":"user-guide/installation/windows-installation/#error-2-failed-with-error-0x80004005","title":"Error 2: <code>failed with error: 0x80004005</code>","text":"<p>If this error happen when installing a Linux disribution:</p> <pre><code>WSLRegisterDistribution failed with error: 0x80004005\n</code></pre> <ul> <li>Press <code>Win + R</code>: it will open a window named <code>Run tasks</code></li> <li>Enter <code>REGEDIT</code> and hit <code>OK</code></li> <li>Navigate to : <code>HKEY_LOCAL_MACHINE\\CurrentControlSet\\Services\\LxssManager</code></li> <li>Set the value Data to 2 and exit <code>REGEDIT</code>.</li> <li>reboot the machine and see if it is working</li> </ul>"},{"location":"user-guide/installation/windows-installation/#step-4-troubleshooting","title":"Step 4 troubleshooting:","text":"<ul> <li>If you cannot launch <code>conda</code> after restarting your shell, you may need to activate through <code>source</code>. From your <code>miniconda3</code> installation folder, run: <pre><code>source miniconda3/bin/activate\n</code></pre> <code>conda</code> will then be activated.</li> </ul>"},{"location":"user-guide/installation/windows-installation/#step-5-troubleshooting","title":"Step 5 troubleshooting:","text":"<p>If we encounter 'Operation not permitted' error on cloning git repository, you may follow below steps.</p> <p>Error --&gt; fatal: could not set 'core.filemode' to 'false'</p> <ol> <li> <p>Launch Ubuntu WSL.</p> </li> <li> <p>Create the file /etc/wsl.conf if it doesn't exist.</p> </li> <li> <p>Open the file (nano /etc/wsl.conf) and add the below lines:     [automount]     options = \"metadata\"</p> </li> <li> <p>Save the file and shoutdown WSL</p> </li> <li> <p>Relaunch Ubuntu WSL</p> </li> </ol> <p>If the problem still persists, you may try restarting the machine and then execute git clone command.</p> <p>We detail here two common issues encountered when instlling and running <code>Fed-BioMed</code>.</p> <ul> <li> <p>If launching Fed-BioMed researcher fails with a message mentioning a display error, you may need to use an alternate IP address. There are two ways of fixing this:</p> <ol> <li>run jupyter <code>jupyter notebook --ip $(python3 -c \"import subprocess; subprocess.run(['hostname', \"-I\"], text=True).sdtout\")</code> and connect to the IP address given by jupyter-notebook.</li> <li>or you can just connect to the IP address given by the command <code>ip addr | grep eth0 | grep inet</code> instead of connecting to <code>localhost</code>.</li> </ol> </li> <li> <p>If Fed-BioMed fails with a message mentioning a <code>System.Management.Automation</code> error you may need to give WSL which browser to use. Set the <code>BROWSER</code> environment variable to the path to the browser. For example to use Microsoft Edge the path and command are commonly : <pre><code>user@wsl-ubuntu$ export BROWSER=export BROWSER=/mnt/c/Program\\ Files\\ \\(x86\\)/Microsoft/Edge/Application/msedge.exe\n</code></pre></p> </li> </ul>"},{"location":"user-guide/nodes/","title":"About Nodes specificities and configurations","text":"<ul> <li>Nodes configuration</li> </ul>"},{"location":"user-guide/nodes/configuring-nodes/","title":"Node Configuration","text":"<p>The Fed-BioMed framework consists of two main components: <code>node</code> and <code>researcher</code>. The <code>node</code> stores private datasets and performs training in response to the <code>researcher's</code> training requests. Communication between the <code>node</code> and the <code>researcher</code> component is facilitated through the RPC (Remote Procedure Call) protocol.</p>"},{"location":"user-guide/nodes/configuring-nodes/#basic-node-configuration","title":"Basic <code>node</code> Configuration","text":"<p>A basic configuration for the <code>node</code> component includes the following settings:</p> <ul> <li>A Python environment</li> <li>A unique node ID</li> <li>Security parameters, such as training plan approval mode and hashing algorithms</li> <li>Connection credentials for communicating with the <code>researcher</code> component</li> </ul> <p>Note</p> <p>These basic configurations are generated automatically with default values. While it is possible to manually edit the configuration files, doing so may render certain parameters incompatible. For example, <code>[server]/host</code> and <code>[server]/pem</code> must remain consistent, as the latter depends on the former. To avoid such issues, it is strongly recommended to use the dedicated script for creating configurations: <code>fedbiomed component create</code>, with options described above.</p>"},{"location":"user-guide/nodes/configuring-nodes/#environment-for-nodes","title":"Environment for Nodes","text":"<p>Fed-BioMed can be installed using <code>pip</code>. It can be installed into native host machine or on virtual environment using tools such as <code>pyenv</code>, <code>conda</code> or <code>virtualenv</code>.  You can find more details in installation tutorial.</p>"},{"location":"user-guide/nodes/configuring-nodes/#node-component-installation","title":"Node Component Installation","text":"<p>Fed-BioMed uses the default <code>fbm-node</code> directory to store all component-related files, including temporary files, variables, configuration files, certificates, and other essential documents.</p> <p>When a basic command is executed, Fed-BioMed will automatically create a default <code>fbm-node</code> directory if it does not already exist. If the directory is already present, the system will use the existing one. The component directory is created relative to the directory where the Fed-BioMed commands are executed. For more details, please refer to the generic Component Creation Guide.</p> <p>For example, the following sequence of commands will create a directory named <code>my-workdir</code> in the home directory, navigate to it, and then start the Fed-BioMed node. This operation will automatically instantiate the node component within the <code>my-workdir</code> directory, creating a folder named <code>fbm-node</code>, which serves as the default directory for the Fed-BioMed node component.   Since <code>fbm-node</code> does not yet exist, the CLI will prompt you for permission to initialize the component.</p> <pre><code>mkdir $HOME/my-workdir\ncd $HOME/my-workdir\nfedbiomed node start\n</code></pre> <p>In another terminal, you can inspect the folder structure using the following command:</p> <pre><code>tree $HOME/my-workdir/fbm-node\n</code></pre> <p>The structure will resemble the tree below:</p> <pre><code>fbm-node\n\u251c\u2500\u2500 etc\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 certs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 FBM_certificate.key\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 FBM_certificate.pem\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 config.ini\n\u2514\u2500\u2500 var\n    \u251c\u2500\u2500 cache\n    \u251c\u2500\u2500 db_NODE_992aab29-c485-463f-a84b-6ce055c17c2e.json\n    \u251c\u2500\u2500 tmp\n    \u2514\u2500\u2500 training_plans\n</code></pre> <p>Please see section Using non-default Fed-BioMed node to select different node installations.</p>"},{"location":"user-guide/nodes/configuring-nodes/#configuration-file-configini","title":"Configuration file: config.ini","text":"<p>A configuration file is an <code>ini</code> file that is located in the component directory. Each configuration file is unique to one node.</p> <p>The configuration file is structured following the sections below:</p> <ul> <li> <p>Default Parameters:</p> <ul> <li><code>id</code>: This is the unique ID that identifies the node.</li> <li><code>component</code>: Specifies the component type. It is always <code>NODE</code> for node component</li> <li><code>version</code>: Version of the configuration format to avoid using older configuration files with recent Fed-BioMed versions.</li> <li><code>db</code>: Relative path to the node's database, from this configuration file</li> </ul> </li> <li> <p>Authentication and Identification</p> <ul> <li><code>private_key</code>: Path to private key to use identify the component.</li> <li><code>public_key</code>: Path to public key to share with other parties (nodes and researcher) to verify its identify.</li> </ul> </li> <li> <p>Researcher:</p> <ul> <li><code>ip</code>: The IP address of the researcher component that the node will connect to.</li> <li><code>port</code>: The port of the researcher component.</li> </ul> </li> <li> <p>Security Parameters:</p> <ul> <li><code>hashing_algorithm</code>: The algorithm will be used for hashing training plan scripts to verify if the requested training plan matches the one approved on the node side.</li> <li><code>training_plan_approval</code>: Boolean value to switch training plan approval to verify training plan scripts before the training.</li> <li><code>allow_default_training_plans</code>: Boolean value to enable automatic approval of example training plans provided by Fed-BioMed.</li> <li><code>secure_aggregation</code>: Boolean parameter (True/False) to activate secure aggregation.</li> <li><code>force_secure_aggregation</code>: Boolean parameter (True/False) to force secure aggregation for every action that uses local dataset.</li> <li><code>secagg_insecure_validation</code>: A boolean parameter to activate insecure validation for secure aggregation rounds to verify the correctness of the aggregation on the researcher side. This option is intended for testing and debugging purposes and must be deactivated in production deployments.</li> </ul> </li> </ul> <p>An example for a config file is shown below;</p> <pre><code>[default]\nid = node_73768d5f-6a66-47de-8533-1291c4ef59d1\ncomponent = NODE\nversion = 2\ndb = ../var/db_NODE_94572b0f-f55c-4167-b729-f16247c35a04.json\n\n[security]\nhashing_algorithm = SHA256\nallow_default_training_plans = True\ntraining_plan_approval = False\nsecure_aggregation = True\nforce_secure_aggregation = False\nsecagg_insecure_validation = False\n\n\n[researcher]\nip = localhost\nport = 50051\n</code></pre>"},{"location":"user-guide/nodes/configuring-nodes/#using-non-default-fed-biomed-node","title":"Using non-default Fed-BioMed node","text":"<p>The option <code>--path</code> that comes after specification <code>node</code> in Fed-BioMed command allows to use different node component in different directories than the default one. <code>--path</code> can be relative or absolute. As it is in default command execution, if the given directory for node is existing it will created automatically.</p> <p><pre><code>$ fedbiomed node --path my-node start\n</code></pre> The command above will create a folder called <code>my-node</code> in the directory where the command is executed, or use the one already existing to start a node.</p> <p>The <code>--path</code> flag can be used with any of the <code>node</code> subcommands as in the examples below.</p> <pre><code>$ fedbiomed node --path my-node dataset list\n$ fedbiomed node --path my-node dataset add\n</code></pre>"},{"location":"user-guide/nodes/configuring-nodes/#creating-a-component-without-doing-any-action","title":"Creating a component without doing any action","text":"<p>The <code>component create</code> action allows you to initialize a Fed-BioMed component without needing to start it or add a dataset to a node. This option is ideal for initializing a node component and configuring it later, such as making changes to the configuration file.</p> <pre><code>fedbiomed component create -c NODE --path &lt;/path/for/component&gt;\n</code></pre> <p>The command will raise an error if a component has already been initialized in the specified <code>-r</code> root directory. To prevent this error, you can use the <code>--exist-ok</code> option.</p> <p>Once the component is initialized, you can edit its configuration file located at <code>path/to/component/etc/config.ini</code>.</p>"},{"location":"user-guide/nodes/configuring-nodes/#environment-variables","title":"Environment Variables","text":"<p>Environment variables can be used to parameterize various options for creating the configuration file. The fields that can be controlled, along with their associated environment variables and default values, are described below:</p> <pre><code>[security]:\n- allow_default_training_plans: FBM_SECURITY_ALLOW_DEFAULT_TRAINING_PLANS, True\n- training_plan_approval: FBM_SECURITY_TRAINING_PLAN_APPROVAL, False\n- secure_aggregation: FBM_SECURITY_SECURE_AGGREGATION, True\n- force_secure_aggregation: FBM_SECURITY_FORCE_SECURE_AGGREGATION, False\n- secagg_insecure_validation: FBM_SECURITY_SECAGG_INSECURE_VALIDATION, True\n\n[researcher]:\n- ip: FBM_RESEARCHER_IP, ${IP_ADDRESS}, if not set: localhost\n- port: FBM_RESEARCHER_PORT, 50051\n</code></pre>"},{"location":"user-guide/nodes/configuring-nodes/#examples","title":"Examples","text":"<p>Create the new component with a specified IP and port for the researcher, and disabling secure aggregation. Forcefully delete and recreate the file if it was already existing.</p> <pre><code>export FBM_RESEARCHER_IP=121.203.21.147\nexport FBM_RESEARCHER_PORT=8909\nexport FBM_SECURITY_SECURE_AGGREGATION=0\nfedbiomed component create -c NODE -p my-node\n</code></pre>"},{"location":"user-guide/nodes/deploying-datasets/","title":"Deploying Datasets in Nodes","text":"<p>Deploying datasets on the nodes prepares them for federated training within Fed-BioMed experiments. This process involves providing metadata for each dataset, enabling the system to understand its characteristics. A single node can host multiple datasets. Once deployed, the dataset's metadata is securely stored in the node's database, ensuring persistence even after restarts or shutdowns.</p> <p>Each dataset must include at least the following attributes:</p> <ul> <li>Database Name:  A concise, user-friendly name for the dataset used for display purposes.</li> <li>Description: A detailed explanation providing further context and information about the dataset's content and origin.</li> <li>Tags: Unique identifiers used by the federated training process to distinguish datasets accurately.</li> </ul> <p>Fed-BioMed does not support downloading datasets from remote sources, except for the default <code>MNIST</code> and <code>MedNIST</code> datasets. Therefore, before adding a dataset into a node, please make sure that you already prepared your dataset and saved it on the file system.</p>"},{"location":"user-guide/nodes/deploying-datasets/#adding-a-dataset-using-the-fed-biomed-cli","title":"Adding a dataset using the Fed-BioMed CLI","text":"<p>Use the following command to add a dataset into the node located in the directory <code>./my-node</code></p> <pre><code>$ fedbiomed node --path my-node dataset add\n</code></pre> <p>After given the permission the create component <code>my-node</code>, you will see the following screen in your terminal.</p> <pre><code>Welcome to the Fed-BioMed CLI data manager\nPlease select the data type that you're configuring:\n    1) csv\n    2) default\n    3) mednist\n    4) images\n    5) medical-folder\n    6) flamby\nselect:\n</code></pre> <p>The interface prompts you to select the type of dataset you would like to add. The <code>default</code> and <code>mednist</code> options are preconfigured to automatically download and add the MNIST and MedNIST datasets. To deploy your own data, you can select one of the following options: <code>csv</code>, <code>image</code>, <code>medical-folder</code>, or <code>flamby</code>, based on your requirements. After selecting an option, you will be prompted to provide additional details, covering both common and option-specific attributes.</p> <p>For example, suppose you want to add a CSV dataset. To do this, type <code>1</code> and press Enter. The interface will then ask you to provide the common attributes: dataset name, tags, and description.</p> <p>Flamby</p> <p>To enable option 6 in Fed-BioMed, you'll need to install FLamby as an external dependency. After installing Fed-BioMed, use the following command: <code>pip install git+https://github.com/owkin/FLamby@main</code>.\"</p> <pre><code>Name of the database: My Dataset\nTags (separate them by comma and no spaces): #my-csv-data,#csv-dummy-data\nDescription: Dummy CSV data\n</code></pre> <p>If a graphical interface is available, the next step opens a file browser window and asks you to select your csv file. If a graphical interface is not available, you will be prompted to insert the full path to the file.</p> <p>After selecting the file, you will be shown the details of your dataset.</p> <pre><code>Great! Take a look at your data:\nname        data_type    tags                                 description     shape      path                                   dataset_id                                    dataset_parameters\n----------  -----------  -----------------------------------  --------------  ---------  -----------------  --------------------------------------------  --------------------\nMy Dataset  csv          ['#my-csv-data', '#csv-dummy-data']  Dummy CSV data  [300, 20]  /path/to/your.csv  dataset_&lt;id&gt;\n</code></pre> <p>You can also check the list of deployed datasets by using the following command:</p> <pre><code>$ fedbiomed node --path my-node dataset list\n</code></pre> <p>It will return the datasets saved into the node created in the directory <code>./my-node</code>.</p>"},{"location":"user-guide/nodes/deploying-datasets/#how-to-add-another-dataset-to-the-same-node","title":"How to Add Another Dataset to the Same Node","text":"<p>Nodes can store multiple datasets. You can follow the previous steps as many times as needed to add other datasets.</p> <p>Adding datasets with the same path</p> <p>Using the same files or the same path for multiple datasets is allowed, provided that the tags are unique.</p> <p>Conflicting tags between datasets</p> <p>Tags from one dataset cannot be equal to, or a subset of, the tags of another dataset</p> <p>For example, CLI on a node:</p> <ul> <li>accepts to register dataset1 with tags <code>[ 'tag1', 'tag3' ]</code>, dataset2 with tags <code>[ 'tag1', 'tag2' ]</code> and dataset3 with tags <code>[ 'tag2', 'tag3' ]</code></li> <li>refuses to register dataset1 with tags <code>[ 'tag1', 'tag2' ]</code> if dataset2 with tags <code>[ 'tag1' ]</code> already exists</li> <li>refuses to register dataset1 with tags <code>[ 'tag1', 'tag2' ]</code> if dataset2 with tags <code>[ 'tag1', 'tag2', 'tag3' ]</code> already exists</li> </ul>"},{"location":"user-guide/nodes/node-gui/","title":"Node GUI","text":"<p>Fed-BioMed offers a node user interface that allows users to manage datasets and training plans with ease. This graphical user interface (GUI) serves as an alternative to the command-line interface (CLI). However, since the GUI is currently in its beta stage, it is only accessible locally.</p>"},{"location":"user-guide/nodes/node-gui/#installing-the-node-gui-dependencies","title":"Installing the Node GUI Dependencies","text":"<p>Node GUI dependencies are not installed by default with the standard Fed-BioMed installation. They are provided as an extra module in the Fed-BioMed package. To install them, use <code>pip</code> with the <code>gui</code> option specified, as shown below:</p> <pre><code>pip install fedbiomed[gui]\n</code></pre>"},{"location":"user-guide/nodes/node-gui/#starting-node-gui","title":"Starting Node GUI","text":"<p>The option <code>gui</code> of <code>fedbiomed</code> command is configured for starting Node GUI.</p> <p>Attention!</p> <p>By default <code>fedbiomed node gui start [OPTIONS]</code> starts Flask server accepting access only from <code>localhost</code>. It is not safe to open access from remote host machine since it is not a secured web server yet. We highly recommend to use <code>localhost</code> through SSH Tunnel for remote access.</p>"},{"location":"user-guide/nodes/node-gui/#options-to-start-the-gui","title":"Options to Start The GUI","text":"<p>The Node GUI in Fed-BioMed can be launched using the <code>fedbiomed</code> command with various customizable settings, such as the IP address, port, folder for storing data files, and the configuration specifying the Node the GUI will manage.</p> <p>The following command demonstrates how to start the Node GUI with its default settings. This command uses the default Fed-BioMed Node component directory, which corresponds to the directory where the command is executed. If no Node component exists in that directory, a new one will be automatically created.</p> <p>By default, the Node GUI assumes that data files are stored in the <code>data</code> directory within the Node component folder. For example, if the command is executed in <code>/path/to/workdir</code>, the Node component will be instantiated in <code>/path/to/workdir/fbm-node/</code>, with the default data directory located at <code>/path/to/workdir/fbm-node/data</code>.</p> <pre><code>fedbiomed node gui start\n</code></pre> <p>After running this command the GUI will start listening on <code>localhost</code> on port <code>8484</code>. You can access the GUI through browser <code>http://localhost:8484</code>. This page will redirect you to the login page. The credentials and possible configurations for log-in are explained in the default admin configuration.</p>"},{"location":"user-guide/nodes/node-gui/#using-different-port-and-host","title":"Using different port and host","text":"<p>Custom ports and host IP address can be specified as long as the port in the specified IP isn't already in use.</p> <pre><code>$ fedbiomed node gui start --port &lt;port&gt; --host &lt;ip-address|localhost&gt;\n</code></pre>"},{"location":"user-guide/nodes/node-gui/#specifying-data-folder","title":"Specifying data folder","text":"<p>You might want to store your data files in a different folder. In such cases you can use the option <code>--data-folder</code> to specify which folder is used that includes data files.</p> <pre><code>$ fedbiomed node gui start --data-folder &lt;path/to/data/folder&gt;\n</code></pre> <p>Uploading data files through Fed-BioMed is not allowed.</p> <p>Fed-BioMed assumes that the datasets or the datafiles that will be deployed in the node are already present in the data folder that is specified. Fed-BioMed Node GUI will help you to use these stored datasets in node.</p>"},{"location":"user-guide/nodes/node-gui/#specifying-specific-node-component-whose-gui-will-be-launched","title":"Specifying specific node component whose GUI will be launched","text":"<p>It is possible to specify the node that the user interface will be used for through the option <code>--path</code> or <code>-p</code>.</p> <pre><code>$ fedbiomed node --path &lt;path/to/component/directory&gt; gui start\n````\n\nThanks to this option it is possible to start multiple GUI for multiple nodes on the same machine as long as the ports are different.\n\n\n```shell\n$ fedbiomed node --path ./my-first-node gui start --port 5001\n$ fedbiomed node --path ./my-second-node gui start --port 5002\n$ fedbiomed node --path ./my-third-node gui start --port 5003\n</code></pre> <p>If it is desired they can share the same data folder.</p>"},{"location":"user-guide/nodes/node-gui/#configuration-file","title":"Configuration file","text":"<p>Apart from <code>fedbiomed</code> command, some options can be configured through GUI configuration file and used without specifying each time the node is started. This file is located in node component directory, <code>/path/to/node-component/etc/config_gui.ini</code>.</p>"},{"location":"user-guide/nodes/node-gui/#server-configuration","title":"Server Configuration","text":"<p>You can modify <code>HOST</code>, <code>IP</code> and <code>DATA_PATH</code> (equivalent of <code>--data-folder</code>) in the server section of the configuration.</p> <pre><code>; --------------------------------------------------------------------------------------------\n; Server configuration -----------------------------------------------------------------------\n; --------------------------------------------------------------------------------------------\n[server]\n\nHOST = localhost\nPORT = 8484\nDATA_PATH = data\n</code></pre>"},{"location":"user-guide/nodes/node-gui/#default-admin-configuration","title":"Default Admin Configuration","text":"<p>When the Fed-BioMed GUI is started for the first time it will create a default admin with the credentials declared in the <code>[init_admin]</code> section of the configuration file. By default, the email  will be <code>admin@fedbiomed.gui</code> and the password <code>admin</code>. You can modify the password either in configuration file or in GUI through User Panel but the e-mail can only be modified from the configuration file.</p> <pre><code>;---------------------------------------------------------------------------------------------\n; Initial admin credentials ------------------------------------------------------------------\n; --------------------------------------------------------------------------------------------\n[init_admin]\n\n; --------------------------------------------------------------------------------------------\n; - IMPORTANT!!! Please update initial admin credentials for production ----------------------\n; --------------------------------------------------------------------------------------------\nemail = admin@fedbiomed.gui\npassword = admin\n</code></pre> <p>Admin e-mail</p> <p>Please modify admin e-mail address before starting the node GUI for the first time. Otherwise, it will create an admin with default e-mail address. If the admin is already created it can only be changed manually through database file.</p> <p>e-mail addresses</p> <p>Currently, e-mail addresses are only used a login name by Fed-BioMed GUI. This is neither a user identity existing in the whole Fed-BioMed instance, nor used to send e-mails to the GUI user.</p>"},{"location":"user-guide/nodes/node-gui/#production-mode","title":"Production Mode","text":"<p>By default, <code>fedbiomed node gui</code> launches the Node GUI in production mode, utilizing Gunicorn as the application server. For debugging and development purposes, you can launch the GUI using the <code>--development</code> flag.</p> <pre><code>$ fedbiomed node gui start --development\n</code></pre> <p>Please use a web server</p> <p>Gunicorn is an application server, and it is strongly recommended by Gunicorn to use proxy web server such as Nginx to  forward requests to Gunicorn using reverse proxy.</p>"},{"location":"user-guide/nodes/node-gui/#setting-an-ssl-certificate","title":"Setting an SSL Certificate","text":"<p>Gunicorn allows setting SSL certificate on application server layer. Please use following command to set SSL certificate for the application server.</p> <pre><code>$ fedbiomed node gui start --key-file &lt;path-to-key-file&gt; --cert-file &lt;path-to-cert-file&gt;\n</code></pre> <p>SSL certificate can also be set through proxy server (e.g. Nginx) instead of application server.</p> <p>Nginx proxy server for GUI is provided in VPN/containers deployment mode</p> <p>Fed-BioMed provides a ready-to-deploy Node GUI container in VPN/containers deployment mode, that is configured to use Nginx as a proxy server and Gunicorn as an application server. This also allows for setting custom SSL certificates. Please refer to the  VPN deployment documentation.</p>"},{"location":"user-guide/nodes/training-plan-security-manager/","title":"Security: Training Plan Security Manager","text":"<p>Federated learning in Fed-BioMed is performed by sending training plans to each node and by requesting nodes to train model from the training plan. Hence, the content of training plan files plays an important role in terms of privacy and security. Therefore, Fed-BioMed offers a feature to run only pre-approved training plans. Training plan files that are sent by a researcher with the training request must be previously approved by the node side. This training plan approval process avoids possible attacks that come through the training plan code files. By running into nodes, they may access private data or jeopardize the node. The approval process should be done by a real user/person who reviews the training plan file. The reviewer should make sure the training plan doesn't contain any code that might cause privacy issues or harm the node.</p> <p>We talk about \"Training plan registration\" as it is a whole training plan that is registered, approved and controlled on a node, not only the training plan's model.</p>"},{"location":"user-guide/nodes/training-plan-security-manager/#how-the-training-plans-get-registered-approved-and-controlled","title":"How the Training Plans get Registered, Approved and Controlled","text":"<p>Training Plan registration and approval can be done through the Fed-BioMed node CLI (Command Line Interface) ou GUI (Graphical User Interface) before or after the node is started. Each training plan should have a unique name, a unique file path and a unique hash.</p> <ul> <li>During the registration, the <code>TrainingPlanSecurityManager</code> of the Fed-BioMed hashes the content of the training plan file and saves the hash into the persistent database.</li> <li>During the approval, the training plan is then marked with <code>Approved</code> status in the database (<code>Approved Training Plans</code> in Figure 1).</li> <li>Then when training begins, training plan files that are sent by a researcher are first hashed and compared to the saved hashes of approved training plans: this is training plan control. This process flow is presented in the following figure.</li> </ul> <p> Figure 1 - Controlling training plan file requested by researcher</p> <p>Details of <code>Figure 1</code> workflow:</p> <ol> <li><code>Researcher</code> creates a <code>TrainingPlan</code> to be trained on the <code>Node</code></li> <li><code>Researcher</code> submits a <code>TrainingPlan</code> along with the training request</li> <li><code>Node</code> retrieves <code>TrainingPlan</code> class.</li> <li><code>Node</code> computes hash of the incoming <code>TrainingPlan</code>.</li> <li><code>Node</code> checks if <code>Researcher</code>'s training plan has already been approved by  comparing its hash to the existing pool of approved training plan hashes.</li> <li>If <code>TrainingPlan</code> has been approved by the <code>Node</code>, <code>Researcher</code> will be able to train his/her training plan on the <code>Node</code>.</li> </ol>"},{"location":"user-guide/nodes/training-plan-security-manager/#hashing","title":"Hashing","text":"<p><code>TrainingPlanSecurityManager</code> of Fed-BioMed provides various hashing algorithms. These hashing algorithms are guaranteed by the Python <code>hashlib</code> built-in library. The hashing algorithm can be selected by configuring the configuration file of the node. Hashing algorithms provided by Fed-BioMed are: <code>SHA256</code>, <code>SHA384</code>, <code>SHA512</code>, <code>SHA3_256</code>, <code>SHA3_384</code>, <code>SHA3_512</code>, <code>BLAKE2B</code>, and <code>BLAKE2S</code>. You can have more information about these hashing algorithms in <code>hashlib</code> documentation page.</p>"},{"location":"user-guide/nodes/training-plan-security-manager/#checksum-operation","title":"Checksum Operation","text":"<p>Checksum control operation is done by querying database for hash of the requested training plan. Therefore, the training plan files that are used should be saved and approved into database by using Fed-BioMed CLI ou GUI before the checksum verification operation (train request). This operation controls whether any existing and approved training plan matches the training plan requesting to train on the node.</p> <p><code>TrainingPlanSecurityManager</code> minifies training plan files just before hashing the training plan file. The minification process removes spaces and comments from the training plan file. The purpose of using minified training plans is to avoid errors when the requested training plan file has the same code but more or less comments or empty spaces than the training plan which is approved. Since the spaces and the comments will have no effect when executing training plans, this process will not open a back door for attacks. Therefore, having more spaces or comments than the registered training plan will not affect the checksum result (and thus the hashing).</p>"},{"location":"user-guide/nodes/training-plan-security-manager/#managing-nodes-for-training-plan-control","title":"Managing Nodes for Training Plan Control","text":"<p>Training Plan Control activation on nodes can be managed either through configuration file or Fed-BioMed CLI. The configuration file of the node includes a section named <code>security</code> to control/switch options for selecting hashing algorithm, enabling/disabling training plan control, and accepting default training plans as approved. By default, Fed-BioMed does not enable training plan control. It means when you start or add data to the node for the first time, if the configuration file doesn't exist, it creates a new configuration file with training plan control disabled (<code>training_plan_approval = False</code>).</p>"},{"location":"user-guide/nodes/training-plan-security-manager/#default-training-plans","title":"Default Training Plans","text":"<p>Default training plans are a subset of the training plan files that are created for Fed-BioMed tutorials, i.e. some of the training plans contained in <code>/notebooks</code> folder. These training plans are saved into <code>envs/common/default_training_plans/</code> directory. If the node is configured to allow default training plans for training, it registers default training plans when the node is started. These training plans are saved for testing purposes, and they can be disabled in a production environment.</p> <p>Note</p> <p>The hashes of the default training plans aren't updated while starting the node if the node is configured not to allow default training plans. However, default training plans might be already saved into database previously. Even if there are default training plans in the database, the node does not approve requests for the default training plans as long as this option is disabled.</p>"},{"location":"user-guide/nodes/training-plan-security-manager/#config-files","title":"Config Files","text":"<p>When the new node is created without any specified configuration file or any options, the default configuration file is saved into the <code>etc</code> directory of Fed-BioMed as follows.</p> <p><pre><code>[default]\n# other parameters\n\n[researcher]\n# parameters connecting researcher server\n\n[security]\nhashing_algorithm = SHA256\nallow_default_training_plans = True\ntraining_plan_approval = False\n# other security parameters\n\n[researcher]\n# parameters for grpc\n\n# etc.\n</code></pre> As you can see, by default, training plan control (<code>training_plan_approval</code>) is disabled. For enabling or disabling this feature, you can change its value to <code>True</code> or <code>False</code>. Any values different from <code>True</code> or <code>False</code> will be counted as <code>False</code>. The node should be restarted to apply changes after updating the config file.</p> <p>Attention</p> <p>When the training plan control is <code>False</code>, <code>allow_default_training_plans</code> has no effect because there is no training plan control operation for train requests.</p>"},{"location":"user-guide/nodes/training-plan-security-manager/#changing-hashing-algorithm","title":"Changing Hashing Algorithm","text":"<p>By default, Fed-BioMed uses the <code>SHA256</code> hashing algorithm to hash training plan files both for registering and checking. It can be changed based on provided algorithms by Fed-BioMed. These algorithms are already presented in the \"Hashing\" section of this article. After the hashing algorithm is changed, the node should be restarted. When restarting the node, if the training plan control is enabled, the node updates hashes in the database by recreating them with the chosen hashing algorithm in the config file.</p>"},{"location":"user-guide/nodes/training-plan-security-manager/#using-fed-biomed-cli","title":"Using Fed-BioMed CLI","text":"<p>Fed-BioMed CLI can start nodes with options for tuning training plan management features. It is possible to change the default parameters of config file while starting a node for the first time. For instance, the following command enables training plan control and disables default training plans for the node. Let's assume we are working with a config file called <code>config-n1.ini</code>. If the <code>config-n1.ini</code> file doesn't exist, it creates the <code>config-n1.ini</code> file with the parameters <code>training_plan_approval = True</code> and <code>allow_default_training_plans = False</code>, under <code>[security]</code> sub-section.</p> <pre><code>$ FBM_SECURITY_TRAINING_PLAN_APPROVAL=True  FBM_SECURITY_ALLOW_DEFAULT_TRAINING_PLANS=False fedbiomed node --path ./my-node start\n</code></pre> <p>It is also possible to start a node enabling <code>training_plan_approval</code> mode, even it is disabled in the configuration file. For instance, suppose that the <code>config-n1.ini</code> file is saved as follows,</p> <pre><code>[security]\nhashing_algorithm = SHA256\nallow_default_training_plans = False\ntraining_plan_approval = False\n</code></pre> <p>The command below forces the node to start with training plan control mode enabled and default training plans enabled.</p> <p><pre><code>$ FBM_SECURITY_TRAINING_PLAN_APPROVAL=True FBM_SECURITY_ALLOW_DEFAULT_TRAINING_PLAN=True fedbiomed node --path ./my-node  start\n</code></pre> or the following command enables training plan control while excluding default training plans;</p> <pre><code>$ FBM_SECURITY_TRAINING_PLAN_APPROVAL=True ENABLE_TRAINING_PLAN_APPROVAL=False fedbiomed node -p ./my-node  start\n</code></pre> <p>Note</p> <p>Hashing algorithm should be changed directly from the configuration file.</p>"},{"location":"user-guide/nodes/training-plan-security-manager/#training-plan-registration-and-approval","title":"Training Plan Registration and Approval","text":"<p>The training plan registration and approval process is done by the Fed-BioMed CLI or GUI tool.</p> <p>Fed-BioMed training plans have one of the following types on a node:</p> <ul> <li>requested training plans are sent by the researcher to nodes from inside the application (\"in band\"). This enables the researcher to easily submit a training plan to nodes for approval. This mode is the most commonly used for having an <code>Experiment</code>'s training plan approved on nodes.</li> <li>registered training plans are manually added on this node from a file (\"out of band\"). This enables the node to use a training plan from any source.</li> <li>default training plans are automatically registered and approved at node startup.</li> </ul> <p>Fed-BioMed training plans have one of the following status:</p> <ul> <li>Approved training plans are authorized to train and test on this node.</li> <li>Pending training plans are waiting for review and approval/rejection decision on this node.</li> <li>Rejected training plans are explicitly not authorized to run on this node.</li> </ul> <p>Training Plans are saved in the database with following details:</p> <pre><code>{\n    'name' : '&lt;training-plan-name&gt;',\n    'description' : '&lt;description&gt;',\n    'training_plan_type' : 'registered',\n    'training_plan_path' : '&lt;path/to/training-plan/f\u0131le&gt;',\n    'training_plan_id' : '&lt;Unique id for the training plan&gt;',\n    'researcher_id' : '&lt;The ID of the researcher that sent this training plan or None&gt;',\n    'algorithm' : '&lt;algorithm used for the hash of the training plan file&gt;',\n    'hash' : '&lt;hash of the training plan file&gt;',\n    'date_registered' : '&lt;Registeration date&gt;',\n    'date_created' : '&lt;The date file has been created&gt;',\n    'date_modified' : '&lt;The date file has been modified&gt;',\n    'date_last_action' : '&lt;The date file has been modified or hash recomputed&gt;'\n}\n</code></pre> <p>Note: training plan files are stored in the file system as <code>txt</code> files. Input training plan files used as default or registered training plans must have this format.</p>"},{"location":"user-guide/nodes/training-plan-security-manager/#using-requested-training-plans","title":"Using requested training plans","text":"<p>Requested training plans are training plans which are sent by a researcher to nodes, inside the Fed-BioMed software.</p> <p>After defining an <code>Experiment()</code> named <code>exp</code> in a researcher's notebook, the following command is typed in the notebook to send the requested training plan's training plan to the nodes of <code>exp</code>:</p> <pre><code>#exp = Experiment(...\n#   training_plan_class=MyTrainingPlan,\n#   ...)\n\nexp.training_plan_approve(MyTrainingPlan, description=\"A human readable description of the training plan\")\n</code></pre> <p>When receiving the training plan, <code>exp</code>'s nodes register the training plan in their persistent database with a type <code>registered</code>, a <code>Pending</code> status, and the <code>description</code> sent by the researcher. If a training plan with same hash already exists in the database, nothing is added and an error is returned to the researcher.</p> <p>A human reviewer then checks and decides whether the training plan should be authorized on the node, via the GUI or the CLI.</p> <p>Use the following command to view a training plan on the CLI, optionally indicating your preferred editor with <code>EDITOR</code>. All registered training plans are listed with their name and status, select the one you want to view:</p> <pre><code># EDITOR=emacs fedbiomed node --path ./my-node training-plan view\n\n$ fedbiomed node --path ./my-node training-plan view\n</code></pre> <p>After reviewing the training plan, use one of the following commands to either approve or reject the training plan:</p> <pre><code>$ fedbiomed node --path ./my-node training-plan approve\n\n$ fedbiomed node --path ./my-node training-plan reject\n</code></pre> <p>The command returns the list of training plans that can be approved/rejected, choose the reviewed training plan from the list:</p> <pre><code>Select the training plan to approve:\n1) training_plan_5fa329d7-af27-4461-b7e7-87e5b8b5e7b6    Model ID training_plan_5fa329d7-af27-4461-b7e7-87e5b8b5e7b6  training-plan status Pending  date_last_action None\n2) training_plan_281464db-ab53-494a-bd58-951957eee762    Model ID training_plan_281464db-ab53-494a-bd58-951957eee762  training-plan status Pending  date_last_action None\nSelect: 1\n</code></pre> <p>Training Plan status on the node can later be changed again using the same <code>training-plan approve</code> and <code>training-plan reject</code> commands.</p> <p>When a node doesn't want to keep track of a registered training plan anymore, it can delete it from the database. The command does not delete the file containing the modtraining planel, only the database entry for the training plan.</p> <pre><code>$ fedbiomed node --path ./my-node training-plan delete\n</code></pre> <p>The output of this command lists deletable training plans with their names and id. It asks you to select the training plan file you would like to remove. For example, in the following example, typing 1  removes the MyTrainingPlan from registered/approved list of training plans.</p> <pre><code>Select the training plan to delete:\n1) MyTrainingPlan   Training Plan ID training_plan_98a1e68d-7938-4889-bc46-357e4ce8b6b5\n2) MyTrainingPlan2  Training Plan ID training_plan_18314625-2134-3334-vb35-123f3vbe7fu7\nSelect:\n</code></pre>"},{"location":"user-guide/nodes/training-plan-security-manager/#using-registered-training-plans","title":"Using Registered training plans","text":"<p>Registered training plans are training plans manually added to the node via the GUI or the CLI, from a file containing its training plan.</p> <p>The following command launches Fed-BioMed CLI for selecting a training plan file and entering a name and description for the training plan. The training plan name, its path and its hash should be unique. It means that you can not add the same training plan file multiple times.</p> <p><pre><code>$ fedbiomed node --path ./my-node training-plan register\n</code></pre> After selecting the training plan file, the training plan manager computes a hash for the training plan file and saves it into the persistent database.</p> <p>The training plan type is <code>registered</code>, and status is <code>Approved</code> for the training plans that are saved through Fed-BioMed CLI.</p> <p>Each time that the node is started, training plan manager checks whether the training plan file still exists on the file system. If it is deleted, training plan manager also deletes it from the database. Therefore, please make sure that the training plan file always exists in the path where it is stored.</p> <p>As for requested training plans, registered training plans can later be viewed (<code>training-plan viw</code>), changed status (<code>training-plan approve</code> or <code>training-plan reject</code>) or removed (<code>training-plan delete</code>).</p> <p>It is also possible to update registered training plans with a different file or the same training plan file whose content has changed. This is useful when working on a training plan, and you want it to be updated without having to remove it and restore it in database. The following command launches the CLI to select the training plan that will be updated</p> <pre><code>$ fedbiomed node --path ./my-node training-plan update\n</code></pre> <p>The command lists registered training plans with their names and ids and asks you to select a training plan you would like to update. Afterward, it asks to select a training plan file from file system. You can either select a different or the same training plan file. It computes a hash for the specified training plan file and updates the database.</p> <p>Note</p> <p>You can update hashes only by providing a training plan file. This API does not allow you to update saved hashes directly.</p>"},{"location":"user-guide/nodes/training-plan-security-manager/#using-default-training-plans","title":"Using default training plans","text":"<p>Default training plans are training plans that are pre-authorized by Fed-BioMed, by default.</p> <p>Unlike the registered training plans, the Fed-BioMed GUI and CLI tools don't provide an option for adding new default training plans. Default training plans are already stored in the system shared directory  <code>shared/fedbiomed/envs/common/default_training_plans</code> directory. They are automatically registered when the node is started with training plan type as <code>default</code> and status as <code>Approved</code>.</p> <p>If the default training plans already exists in the database at node start, training plan manager checks whether there is any modification. If any default training plan file is deleted from the filesystem, training plan manager also deletes it from the database. If the training plan file is modified, or the hashing algorithm is changed, training plan manager updates the hashes in the database. This checking/controlling operation is done while starting the node.</p> <p>As for requested training plans, default training plans can later be viewed (<code>training-plan view</code>) or changed status (<code>training-plan approve</code> or <code>training-plan reject</code>).</p> <p>Note</p> <p>Default training plans cannot be removed using Fed-BioMed CLI. They should be removed from the <code>envs/common/default_training_plans</code> directory. After restarting the node, deleted training plan files are also removed from the TrainingPlans table of the node database.</p>"},{"location":"user-guide/nodes/using-gpu/","title":"Using GPU accelerator hardware","text":"<p>Federated learning often implies intensive computation for model training, aggregation or inference. Dedicated accelerator hardware such as Nvidia GPUs can help speed up these steps, with support from the libraries and frameworks.</p> <p>This page explains GPU support in Fed-BioMed.</p>"},{"location":"user-guide/nodes/using-gpu/#support-scope","title":"Support scope","text":"<p>Fed-BioMed supports accelerator hardware with the following requirements and limitations :</p> <ul> <li>Nvidia GPU hardware : basically all models supported by the CUDA interface can be used, but of course the hardware needs to have enough memory for the targeted model.</li> <li>single GPU on each node : if the host machine has multiple GPUs, only one is used by a Fed-BioMed node</li> <li>PyTorch framework using the <code>TorchTrainingPlan</code> Fed-BioMed training plan interface ; other frameworks / training plans (scikit-learn / <code>SGDSkLearnModel</code>) can run on a node with a hardware accelerator but don't use it.</li> <li>only training is accelerated (node side computation), not aggregation (server side computation)</li> <li>no VPN : VPN in Fed-BioMed is based on running each component in a docker container. Node container images provided by Fed-BioMed are not yet GPU-enabled.</li> </ul>"},{"location":"user-guide/nodes/using-gpu/#node-side","title":"Node side","text":"<p>To control GPU usage by Fed-BioMed training from the node side, use these options of the <code>fedbiomed node start</code> command :</p> <ul> <li><code>--gpu</code> : Node offers to use a GPU for training, if a GPU is available on the node and if the researcher requests use of a GPU. If no GPU is available, or the training plan is not supported for GPU (scikit-learn), or the researcher does not request use of a GPU, then training occurs in CPU.</li> <li><code>--gpu-num GPU_NUM</code> : Node chooses the device with number GPU_NUM in CUDA instead of the default device. If this device does not exist, node fallbacks to default CUDA device. This option also implicitely sets <code>--gpu</code>.</li> <li><code>--gpu-only</code> : Node enforces use of a GPU for training, if a GPU is available on the node, even if the researcher doesn't request it. If no GPU is available, or the training plan is not supported for GPU (scikit-learn), then training occurs in CPU.</li> </ul> <p>By default (no options), Fed-BioMed training doesn't use GPU.</p> <p>The reason for not using GPU by default is that even if you have a GPU on a node, it may not have enough memory to train the given model. In this case, the training of a correct model fails with an error message (and you don't want a correct model to fail with default options) :</p> <pre><code>2022-01-13 08:07:28,737 fedbiomed ERROR - Cannot train model in round: CUDA error: out of memory\n</code></pre> <p>Example :</p> <ul> <li>launch a node that enforces use of GPU with CUDA device number 2 (the 3rd GPU on this host machine). If there is no GPU with device number 2, use the default GPU. If there is no GPU available or if not using <code>TorchTrainingPlan</code>, do the training in CPU : <pre><code>$ fedbiomed node start --gpu-only --gpu-num 2\n</code></pre></li> <li>if the researcher didn't request for GPU usage, and there is no GPU numbered 2 in CUDA on the node but another GPU is available, and the training plan supports GPU acceleration, then the following warning messages are emitted : <pre><code>2022-01-21 12:34:31,992 fedbiomed WARNING - Node enforces model training on GPU, though it is not requested by researcher\n2022-01-21 12:34:31,992 fedbiomed WARNING - Bad GPU number 2, using default GPU\n</code></pre></li> </ul>"},{"location":"user-guide/nodes/using-gpu/#researcher-side","title":"Researcher side","text":"<p>To control GPU usage for Fed-BioMed training from the researcher side, set the <code>'use_gpu': True</code> key of the <code>training_args</code> dict passed as argument to <code>Experiment</code>.</p> <p>In this example, the researcher requests the nodes participating in the <code>Experiment</code> to use GPU for training, if they have any GPU available and offer to use it : <pre><code># Researcher notebook or script code\ntraining_args = {\n    'use_gpu': True\n    #......\n}\n\nexp = Experiment(\n    ...\n    training_args=training_args,\n    ... )\n</code></pre></p> <p>Node requirements have precedence over researcher requests. For example, no GPU is used if the node requests it but the nodes does not offer it.</p>"},{"location":"user-guide/nodes/using-gpu/#how-to-enable-gpu-usage","title":"How to enable GPU usage","text":"<p>Fed-BioMed offers a simplified interface for training with GPU, as described above. This hides from the researcher the complexity of the specific resources and requirements of each node's.</p> <p>Warning</p> <p>Fed-BioMed models and training plans should never try to directly access the CUDA resources on the node. For example don't use the <code>pytorch.cuda.is_available()</code>, <code>tensor.to()</code>, <code>tensor.cuda()</code> etc. methods. This is not the supported way of using GPUs in Fed-BioMed.</p>"},{"location":"user-guide/nodes/using-gpu/#option-1-enable-gpu-on-node-and-researcher","title":"Option 1 : enable GPU on node and researcher","text":"<p>In this scenario, the node proposes the use of a GPU and the researcher requests the use of a GPU :</p> <ul> <li> <p>launch node offering GPU usage <pre><code>$ fedbiomed node start --gpu\n</code></pre></p> </li> <li> <p>on the researcher, set the <code>training_args</code> of the notebook or script used for training <pre><code># Researcher notebook or script code\ntraining_args = {\n    'use_gpu': True\n    # .....\n}\n\nexp = Experiment(\n    ...\n    training_args=training_args,\n    ... )\nexp.run()\n</code></pre></p> </li> </ul>"},{"location":"user-guide/nodes/using-gpu/#option-2-force-gpu-usage-on-node","title":"Option 2 : force GPU usage on node","text":"<p>In this scenario, no action is needed on the researcher side, no code modification is needed. The node enforces use of GPU :</p> <ul> <li> <p>launch node enforcing GPU usage <pre><code>$ fedbiomed node start --gpu-only\n</code></pre></p> </li> <li> <p>on the researcher, launch same notebook or script as when using CPU <pre><code># Unmodified researcher notebook or script code\nexp = Experiment( ... )\nexp.run()\n</code></pre></p> </li> </ul>"},{"location":"user-guide/nodes/using-gpu/#remarks","title":"Remarks","text":""},{"location":"user-guide/nodes/using-gpu/#heterogeneous-nodes","title":"Heterogeneous nodes","text":"<p>When using multiple nodes, they can have different GPU support and requirements. For example an <code>Experiment</code> can use 4 nodes with :</p> <ul> <li>1 node has no GPU available</li> <li>1 node has GPU available but does not offer GPU</li> <li>1 node has GPU available and offers GPU</li> <li>1 node has GPU available and enforces GPU usage</li> <li>etc.</li> </ul>"},{"location":"user-guide/nodes/using-gpu/#multiples-nodes-on-the-same-host","title":"Multiples nodes on the same host","text":"<p>When running multiples nodes on a same host machine that has multiple GPUs available, each node can use a different GPU. For example on a node with 2 GPUs numbered 0 and 1 : <pre><code># Launch node (in background) offering use of GPU 0\n$ fedbiomed node start --gpu-num 0 &amp;\n# Launch node offering use of GPU 1\n$ fedbiomed node start --gpu-num 1\n</code></pre></p>"},{"location":"user-guide/nodes/using-gpu/#security","title":"Security","text":"<p>warning</p> <p>Warning: from a security perspective, a malicious researcher can write a training plan that directly accesses the node's GPU (eg: with <code>tensor.to()</code>) even if not offered by the node. This should be addressed by using training plan approval and conducting proper training plan review.</p>"},{"location":"user-guide/researcher/","title":"Researcher component user guide","text":"<ul> <li>Training Plan</li> <li>Training Data</li> <li>Experiment</li> <li>Aggregation</li> <li>Client selection strategies</li> <li>Listing datasets and selecting nodes</li> <li>Training plan validation</li> <li>Tensorboard</li> </ul>"},{"location":"user-guide/researcher/aggregation/","title":"Parameter Aggregation in Fed-BioMed","text":"<p>Aggregation of model parameters plays an important role in federated learning, where we naturally deal with data heterogeneity. Unlike the distributed learning datasets, model parameters are saved as same-sized data blocks for each node training the same model. The number of samples, the quality of the samples cand their data distribution can vary in every <code>Node</code>. In Fed-BioMed, we currently work on providing various solutions for this heterogeneity. Up to now, we support <code>FedAverage</code> which performs the standard aggregation scheme in federated learning: federated averaging. We also provide <code>FedProx</code> and <code>SCAFFOLD</code> aggregation methods.</p> <p>Important</p> <p>The following <code>Aggregators</code> can also be used with <code>declearn Optimizers</code>, providing advanced gradient-based optimization modules. These <code>Optimizers</code> are cross-framework, meaning it is possible to use it with all machine learning framework provided by Fed-BioMed. Please visit the webpage dedicated to advanced optimization.</p>"},{"location":"user-guide/researcher/aggregation/#fed-biomed-aggregators","title":"Fed-BioMed <code>Aggregators</code>:","text":"<p>Fed-BioMed <code>Aggregators</code> are showcased in the following tutorial.</p>"},{"location":"user-guide/researcher/aggregation/#federated-averaging-fedaveraging","title":"Federated Averaging (FedAveraging)","text":"<p><code>FedAveraging</code> is the default <code>Aggregator</code> in Fed-BioMed, introduced by McMahan et al.. It performs a weighted mean of local model parameters based on the size of node specific datasets. This operation occurs after each round of training in the <code>Nodes</code>.</p> \\[w_{t+1} := \\sum_{k=1}^K\\frac{n_k}{n}w_{t+1}^k\\] <p>where \\( w_{t} \\) are the weights at round \\(t\\), \\(K\\) is the number of <code>Nodes</code> participating at round \\(t\\), and \\( n_k, n \\) are the number of samples of the \\(k\\)-th node and of the total federation respectively.</p>"},{"location":"user-guide/researcher/aggregation/#fedprox","title":"FedProx","text":"<p>Similar to <code>FedAveraging</code>, <code>FedProx</code> performs a weighted sum of local model parameters. <code>FedProx</code> however introduces a regularization operation, using \\(\\mathcal{L}_2\\) norm, in order to tackle statistical heterogeneity. Basically, it reformulates the loss function by:</p> \\[F_k(w) + \\frac{\\mu}{2}|| w - w^t ||^2_2\\] <p>using the same notation as above, with \\(\\mu\\) the regularization parameter (we obtain <code>FedAveraging</code> by setting \\(\\mu=0\\)) and \\(F_k\\) the objective function.</p> <p>To use <code>FedProx</code>, use <code>FedAverage</code> from <code>fedbiomed.researcher.aggregators</code> and specify a value for \\(\\mu\\) in the training arguments <code>training_args</code> using the argument name <code>fedprox_mu</code>.</p> <p>FedProx generalizes FedAveraging</p> <p><code>FedProx</code> is similar to <code>FedAVeraging</code> if <code>fedprox_mu=0</code>.</p>"},{"location":"user-guide/researcher/aggregation/#scaffold","title":"SCAFFOLD","text":"<p><code>SCAFFOLD</code> stands for Stochastic Controlled Averaging for Federated Learning. It introduces a correction state parameter in order to tackle the client drift, depicting the fact that when data across each <code>Node</code> are heterogeneous, each of the <code>Nodes</code> pushes the model in a different direction in the optimization space and the global model does not converge towards the true optima. In Fed-BioMed, only option 2 of the <code>SCAFFOLD</code> paper has been implemented. Additional details about the implementation can be found in the developer API reference.</p> <p>The corrected loss function used to update the model is computed as follows:</p> \\[F_k(w) + c \\cdot w - c_k \\cdot w\\] <p>where \\(c_k\\) is the <code>Node</code> correction term,  \\(c = \\frac{1}{K}\\sum_{k=1}^K{c_k}\\) is the server's correction term, and \\(K\\) is the total number of participating <code>Nodes</code> as above.</p> <p>On the <code>Researcher</code> side, the global model is updated by performing gradient descent.</p> <p>Additional parameters are needed when working with <code>SCAFFOLD</code>:</p> <ul> <li><code>server_lr</code>: <code>Researcher</code>'s learning rate for performing a gradient step</li> <li><code>num_updates</code>: the number of updates (ie gradient descent optimizer steps) to be performed on each <code>Node</code>. Relying only on <code>epochs</code> could lead to some inconsistencies in the computation of the correction term: thus, in Fed-BioMed, <code>SCAFFOLD</code> aggregator cannot be used with <code>epochs</code>.</li> </ul> <p>Please note that:</p> <ul> <li><code>SCAFFOLD</code> should be used only with <code>SGD</code> optimizer. Using other <code>Optimizers</code> in Fed-BioMed is possible, but without any convergence guarantees.</li> <li><code>SCAFFOLD</code> can only be used with the <code>PyTorch</code> framework at the moment, and correction terms are not encrypted when exchanged (even when using SecAgg).</li> <li><code>SCAFFOLD</code> requires using the <code>num_updates</code> training argument to control the number of training iterations. Using only <code>epochs</code> will raise an error.</li> <li><code>SCAFFOLD</code> also exists as an <code>declearn</code> cross framework optimizer. Using <code>SCAFFOLD</code> implementation in <code>declearn</code> enables the use of other machine learning frameworks such as <code>scikit-learn</code>.</li> </ul> <p>Unsecure SCAFFOLD</p> <p>This version of SCAFFOLD should not be used in a highly secure setting, especially if the use of  Secure Aggregation is considered. While model parameters will be encrypted with Secure Aggregation, the SCAFFOLD correction terms won't. For more security, it is recommended to use <code>declearn</code>'s <code>Scaffold</code> optimizers with <code>FedAveraging</code> instead.</p>"},{"location":"user-guide/researcher/aggregation/#how-to-create-your-custom-aggregator","title":"How to Create Your Custom Aggregator","text":""},{"location":"user-guide/researcher/aggregation/#designing-your-own-aggregator-class-the-aggregation-method","title":"Designing your own <code>Aggregator</code> class: the <code>aggregation</code> method","text":"<p>Before designing your custom aggregation algorithm we recommend you to see default <code>FedAverage</code> aggregate method</p> <p><code>aggregate</code> method is expecting at least <code>model_params</code> and <code>weights</code> arguments. Additional argument can be passed through <code>*args</code> and <code>kwargs</code> depending, on the values needed for your <code>Aggregator</code>.</p> <p>It is possible to create your custom aggregator by creating a new class which inherits from the Aggregator class defined in <code>fedbiomed.researcher.aggregators.Aggregator</code>.</p> <pre><code>class Aggregator:\n    \"\"\"\n    Defines methods for aggregating strategy\n    (eg FedAvg, FedProx, SCAFFOLD, ...).\n    \"\"\"\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def normalize_weights(weights) -&gt; list:\n        # Load list of weights assigned to each node and\n        # normalize these weights so they sum up to 1\n        norm = [w/sum(weights) for w in weights]\n        return norm\n\n    def aggregate(self,  model_params: list, weights: list, *args, **kwargs) -&gt; Dict: # pragma: no cover\n        \"\"\"Strategy to aggregate models\"\"\"\n        pass\n</code></pre> <p>Your child class should extend the method <code>aggregate</code> that gets model parameters and weights as arguments. The model parameters are those which have been locally updated in each node during the last round. The weights represent the ratio of the number of samples in each node and the total number of samples. Your custom aggregator class should return aggregated parameters.</p> <p>You should also pay attention to the way the parameters are loaded. For example, it may be a dictionary that contains tensor data types or just an array. As you can see from the following example, the aggregator first checks the data type of the parameters, and then it does the averaging.</p> <pre><code>    if t == 'tensor':\n        for model, weight in zip(model_params, proportions):\n            for key in avg_params.keys():\n                avg_params[key] += weight * model[key]\n\n    if t == 'array':\n        for key in avg_params.keys():\n            matr = np.array([ d[key] for d in model_params ])\n            avg_params[key] = np.average(matr,weights=np.array(weights),axis=0)\n</code></pre>"},{"location":"user-guide/researcher/aggregation/#desinging-your-own-aggregator-class-the-create_aggregator_args-method","title":"Desinging your own <code>Aggregator</code> class: the <code>create_aggregator_args</code> method","text":"<p>For some advanced <code>Aggregators</code>, you may need to send some argument to <code>Nodes</code> in order to update the local model. For instance, <code>SCAFFOLD</code> <code>Aggregator</code> sends specific correction terms for each of the <code>Nodes</code> involved in the training.</p> <p>The method that has this responsability is <code>create_aggregator_args</code>, and is designed as follow (in the <code>fedbiomed.researcher.aggregators.Aggregator</code> class):</p> <pre><code>def create_aggregator_args(self, *args, **kwargs) -&gt; Tuple[dict, dict]:\n    \"\"\"Returns aggregator arguments that are expecting by the nodes\n\n    Returns:\n    dict: contains `Aggregator` parameters that will be sent to nodes\n    dict: contains parameters that will be sent through file exchange message.\n            Both dictionaries are mapping node_id to `Aggregator` parameters specific\n            to each Node.\n\n    \"\"\"\n    return self._aggregator_args or {}, {}\n</code></pre>"},{"location":"user-guide/researcher/aggregation/#how-to-use-your-aggregator","title":"How to use your aggregator?","text":"<p>Once your aggregator has been created, you can pass it to your <code>Experiment</code> instance by calling the <code>set_aggregator</code> method.</p> <pre><code>from fedbiomed.researcher.federated_workflow import Experiment\n\nexp = Experiment()\nexp.set_aggregator(MyAgregator())\n</code></pre>"},{"location":"user-guide/researcher/aggregation/#conclusions","title":"Conclusions","text":"<p>In this article, the aggregation process has been explained. Currently, Fed-BioMed only supports the vanilla federated averaging scheme for the aggregation operation called <code>FedAverage</code>, as well as <code>FedProx</code> and <code>SCAFFOLD</code>. However, Fed-BioMed also allows you to create your custom aggregator using the <code>Aggregator</code> parent class. It means that you can define your custom aggregator based on your use case(s). You can define it in your notebook or python script and passed into the Experiment as an argument.</p>"},{"location":"user-guide/researcher/client-selection-strategies/","title":"Node Selection Strategies in Fed-BioMed","text":"<p>Node selection might play an important role to increase the performance of the model. Currently, in the development,  Fed-BioMed doesn't provide any node selection strategy. This means that, by default, all nodes disposing of the  dataset that is required for training are selected.   </p>"},{"location":"user-guide/researcher/configuration/","title":"Configuring and Using the Researcher Component in Fed-BioMed","text":"<p>The Researcher component in Fed-BioMed serves as the central orchestrator for collaborative learning experiments. It allows users to define training plans, configure experiments, and manage communication with multiple nodes. This guide explains how to configure and use the Researcher component in detail.</p>"},{"location":"user-guide/researcher/configuration/#creating-the-researcher-component","title":"Creating the Researcher Component","text":"<p>To begin, create a Researcher component in your working directory. The following command initializes a new Researcher component:</p> <pre><code>fedbiomed component create -c researcher\n</code></pre> <p>This command creates a folder named <code>fbm-researcher</code> in your current working directory. This folder contains all the necessary assets for the Researcher component, including configuration files, experiment assets and logs.</p> <p>It is also possible to create the Researcher component in a custom directory using the <code>--path</code> option:</p> <pre><code>fedbiomed component create -c researcher --path /path/to/custom-directory\n</code></pre>"},{"location":"user-guide/researcher/configuration/#starting-the-researcher-component","title":"Starting the Researcher Component","text":"<p>Executing the following command will start the Researcher component.</p> <pre><code>fedbiomed researcher start\n</code></pre> <p>This command launches a Jupyter Notebook server preconfigured to use the Researcher component created in your working directory. If the Researcher component is located in a different directory, the path should be specified using the <code>--path</code> option:</p> <pre><code>fedbiomed researcher --path /path/to/fbm-researcher start\n</code></pre> <p>After the Jupyter Notebook is launched, it serves as an interface for defining training plans and experiments. Preconfigured notebooks are available in the tutorials to assist with these tasks.</p>"},{"location":"user-guide/researcher/configuration/#configuring-the-researcher-component","title":"Configuring the Researcher Component","text":"<p>The Researcher component\u2019s configuration file is located in the <code>etc</code> folder of the <code>fbm-researcher</code> directory. For example:</p> <pre><code>/path/to/fbm-researcher/etc/config.ini\n</code></pre> <p>Here is an example configuration file: <pre><code>[default]\nid = RESEARCHER_2ba562cc-6943-4430-8f79-cb3877b2ea79\ncomponent = RESEARCHER\nversion = 3\ndb = ../var/db_RESEARCHER_2ba562cc-6943-4430-8f79-cb3877b2ea79.json\n\n[server]\nhost = localhost\nport = 50051\n\n[certificate]\nprivate_key = certs/server_certificate.key\npublic_key = certs/server_certificate.pem\n\n[security]\nsecagg_insecure_validation = True\n</code></pre></p> <p>This file contains settings for connecting to a server, managing security certificates, and configuring the database. Here's a breakdown of each section:</p>"},{"location":"user-guide/researcher/configuration/#default","title":"<code>[default]</code>","text":"<ul> <li>id: This is a unique identifier for the researcher.</li> <li>component: Refers to the component or role being used, here it's <code>RESEARCHER</code>, indicating that the configuration is for a researcher component.</li> <li>version: The version of the configuration.</li> <li>db: Specifies the location of the database used by this researcher component, pointing to a JSON file. This file likely stores data related to the researcher\u2019s activities or state.</li> </ul>"},{"location":"user-guide/researcher/configuration/#server","title":"<code>[server]</code>","text":"<ul> <li>host: Defines the server host address to connect to.</li> <li>port: Specifies the port number <code>50051</code> for the server connection.</li> </ul>"},{"location":"user-guide/researcher/configuration/#certificate","title":"<code>[certificate]</code>","text":"<ul> <li>private_key: The path to the server's private key, which is used for secure communication.</li> <li>public_key: The path to the server's public key, stored at <code>certs/server_certificate.pem</code>, which is used for encryption in secure communications.</li> </ul>"},{"location":"user-guide/researcher/configuration/#security","title":"<code>[security]</code>","text":"<ul> <li>secagg_insecure_validation: Set to <code>True</code>, this indicates that the system is using insecure validation for secure aggregation. This is used for development purposes, but should be switched to <code>False</code> for production to ensure proper security.</li> </ul> <p>To modify the configuration, open the file in a text editor and make the necessary changes. Ensure that the component is restarted after editing the configuration file to apply the changes. This restart can be done by relaunching researcher component, restarting Jupyter Notebook or re-executing a python script that defines an experiment.</p>"},{"location":"user-guide/researcher/configuration/#using-plain-python-scripts-without-jupyter-notebook","title":"Using Plain Python Scripts Without Jupyter Notebook","text":"<p>If you prefer to use plain Python scripts instead of the Jupyter Notebook interface, ensure that the environment variable <code>FBM_RESEARCHER_COMPONENT_ROOT</code> points to the correct Researcher component directory:</p>"},{"location":"user-guide/researcher/configuration/#option-1-set-the-environment-variable-in-your-script","title":"Option 1: Set the Environment Variable in Your Script","text":"<pre><code>import os\nos.environ['FBM_RESEARCHER_COMPONENT_ROOT'] = '/path/to/fbm-researcher'\n\n# Import and use Fed-BioMed components\n</code></pre>"},{"location":"user-guide/researcher/configuration/#option-2-export-the-variable-in-the-shell","title":"Option 2: Export the Variable in the Shell","text":"<pre><code>export FBM_RESEARCHER_COMPONENT_ROOT=/path/to/fbm-researcher\npython my_script.py\n</code></pre>"},{"location":"user-guide/researcher/configuration/#option-3-inline-environment-variable","title":"Option 3: Inline Environment Variable","text":"<pre><code>FBM_RESEARCHER_COMPONENT_ROOT=/path/to/fbm-researcher python my_script.py\n</code></pre>"},{"location":"user-guide/researcher/configuration/#managing-multiple-researcher-configurations","title":"Managing Multiple Researcher Configurations","text":"<p>While the default Researcher component is named <code>fbm-researcher</code>, it is always possible to create multiple Researcher components for different configurations. To switch between configurations, specify the path to the desired Researcher component when starting or using it:</p> <pre><code>fedbiomed researcher --path /path/to/another-researcher start\n</code></pre>"},{"location":"user-guide/researcher/configuration/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Component Not Found or Duplicated: Ensure that the <code>FBM_RESEARCHER_COMPONENT_ROOT</code> environment variable or the <code>--path</code> option points to the correct directory. If researcher environment is going to be initialized for the for time the component directory should be empty.</li> <li>Communication Errors: Verify the IP address, port, and certificates in the configuration file.</li> <li>Permission Denied: Check that you have write permissions for the working directory.</li> </ol>"},{"location":"user-guide/researcher/experiment/","title":"Experiment Class of Fed-BioMed","text":""},{"location":"user-guide/researcher/experiment/#introduction","title":"Introduction","text":"<p>The <code>Experiment</code> class in Fed-BioMed is in charge of orchestrating the federated learning process on available nodes. Specifically, it takes care of:</p> <ul> <li>Searching the datasets on active nodes, based on specific tags given by a researcher and used by the nodes to identify the dataset.</li> <li>Sending model, training plan and training arguments to the nodes.</li> <li>Tracking the training process on the nodes during all training rounds.</li> <li>Checking the nodes' responses to handle possible failures.</li> <li>Receiving the local model parameters after every round of training.</li> <li>Aggregating the local model parameters based on the specified federated approach.</li> <li>Sending the aggregated parameters to the selected nodes for the next round.</li> <li>Optimizing the global model (i.e. the aggregated model).</li> </ul> <p></p>"},{"location":"user-guide/researcher/experiment/#defining-an-experiment","title":"Defining an experiment","text":"<p>You may configure an <code>Experiment</code> by providing arguments to its constructor, as shown below.</p> <pre><code>from fedbiomed.researcher.federated_workflows import Experiment\n\nexp = Experiment(tags=tags,\n                 nodes=None,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</code></pre> <p>It is also possible to define an empty experiment and set the arguments afterwards, using the setters of the experiment object. Please visit the tutorial In depth experiment configuration to find out more about declaring an experiment step by step</p> <p>Under the hood, the <code>Experiment</code> class takes care of a lot of heavy lifting for you. For example, when you initialize an experiment with the <code>tags</code> argument, it uses them to automatically create a <code>FederatedDataSet</code> by querying the federation. Afterwards, <code>Experiment</code> initializes several internal variables to manage federated training on all participating nodes. Finally, it also creates the strategy to select the nodes for each training round. When the <code>node_selection_strategy</code> is set to <code>None</code>, the experiment uses the default strategy which is <code>DefaultStrategy</code>.</p>"},{"location":"user-guide/researcher/experiment/#setting-the-training-data","title":"Setting the training data","text":""},{"location":"user-guide/researcher/experiment/#setting-the-training-data-by-setting-the-tags","title":"Setting the training data by setting the tags","text":"<p>Each dataset deployed on the nodes is identified by tags. Tags allow researchers to select the same dataset registered under a given tag (or list of tags) on each node for the training.</p> <p>The argument <code>tags</code> of the experiment is used for dataset search request. It can be a list of tags which are of type <code>string</code>, or single tag of type <code>string</code>.</p> <pre><code>exp = Experiment()\nexp.set_tags(tags=['#MNIST', '#dataset'])\n#or\nexp.set_tags(tags='#MNIST')\n</code></pre> <p>Setting tags also sets the <code>Experiment</code>'s training data</p> <p>Whenever the <code>set_tags</code> method is called, a query is always issued to identify all nodes in the federation that have datasets with matching tags. Consequently, the training data of <code>Experiment</code> is changed to match the results from the query.</p> <p>You can check your tags in your experiment as follows:</p> <pre><code>tags = exp.tags()\nprint(tags)\n# &gt; OUTPUT:\n# &gt; ['#MNIST', '#dataset']\n</code></pre> <p>Tags matching multiple datasets</p> <p>An <code>Experiment</code> object must have one unique dataset per node. Object creation fails if this is not the case when trying to instantiate the <code>FederatedDataSet</code> object. This is done to ensure that training for an <code>Experiment</code> uses only a single dataset for each node.</p> <p>As a consequence, <code>tags</code> specified for an <code>Experiment</code> should not be ambiguous, which means they cannot match multiple datasets on one node.</p> <p>For example if you instantiate <code>Experiment(tags='#dataset')</code> and a node has registered one dataset with tags <code>['#dataset', '#MNIST']</code> and another dataset with tags <code>['#dataset', '#foo']</code> then experiment creation fails.</p>"},{"location":"user-guide/researcher/experiment/#setting-the-training-data-by-providing-the-metadata-directly","title":"Setting the training data by providing the metadata directly","text":"<p>The dataset metadata can be provided directly using the <code>set_training_data</code> method. The metadata can be a <code>FederatedDataSet</code> object or a nested <code>dict</code> with format <code>{node_id: {metadata_key: metadata_value}}</code>.</p> <p>When you provide a metadata object directly, the <code>Experiment</code>'s tags attribute is set to <code>None</code>.</p>"},{"location":"user-guide/researcher/experiment/#under-the-hood-consistency-with-all-members-of-experiment","title":"Under-the-hood consistency with all members of <code>Experiment</code>","text":"<p>When you change the training data (either through <code>set_tags</code> or <code>set_training_data</code>), the <code>Experiment</code> class performs a lot of operations to ensure that consistency is maintained for all of its attributes that use the training data. In particular, the <code>aggregator</code> and <code>node_state_agent</code> classes are updated with the new training data.</p>"},{"location":"user-guide/researcher/experiment/#selecting-specific-nodes-for-the-training","title":"Selecting specific Nodes for the training","text":"<p>The argument <code>nodes</code> stands for filtering the nodes that are going to be used for federated training. It is useful when there are too many nodes on the network, and you want to perform federated training on specific ones. <code>nodes</code> argument is a list that contains node ids. When it is set, the experiment queries only the nodes that are in the list for a dataset matching tags. You can visit listing datasets and selecting nodes documentation to get more information about this feature.</p> <pre><code>nodes = ['node-id-1', 'node-id-2']\nexp.set_nodes(nodes=nodes)\n</code></pre> <p>By default, <code>nodes</code> argument is <code>None</code> which means that each node that has a registered dataset matching all the tags will be part of the federated training.</p> <pre><code>exp.set_nodes(nodes=None)\n</code></pre> <p>Node filtering happens at training time</p> <p>Setting nodes doesn't mean sending another dataset search request to the nodes. Node filtering happens dynamically each time a training request is sent to nodes. In other words, if you search again for datasets after setting <code>nodes</code> by running <code>exp.set_training_data(training_data=None, from_tags=True)</code> you select in your <code>FederatedDataset</code> the same nodes as with <code>nodes=None</code>.</p>"},{"location":"user-guide/researcher/experiment/#load-your-training-plan-training-plan-class","title":"Load your Training Plan: Training Plan Class","text":"<p>The <code>training_plan_class</code> is the class where the model, training data and training step are defined. Although not required, optimizers and dependencies can also be defined in this class. The experiment will extract source of your training plan, save as a python module (script), and send the source code to the nodes every round of training. Thanks to that, each node can construct the model and perform the training.</p> <pre><code>class MyTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args):\n        # Builds the model and returns it\n        return model\n\n    def init_optimizer(self, optimizer_args):\n        # Builds the optimizer and returns it\n        return optimizer\n\n    def training_step(self):\n        # Training step at each iteration of training\n        return loss\n\n    def training_data(self):\n        # Loads the dat and returns Fed-BioMed DataManager\n        return DataManager()\n\nexp.set_training_plan_class(training_plan_class=MyTrainingPlan)\n\n# Retrieving training plan class from experiment object\ntraining_plan_class = exp.training_plan_class()\n</code></pre>"},{"location":"user-guide/researcher/experiment/#model-arguments","title":"Model Arguments","text":"<p>The <code>model_args</code> is a dictionary with the arguments related to the model (e.g. number of layers, layer arguments and dimensions, etc.). This will be passed to the <code>init_model</code> method during model setup. An example for passing the number of input adn output features for a model is shown below.</p> <pre><code>model_args = {\n    \"in_features\"   : 15,\n    \"out_features\"  : 1\n}\nexp.set_model_args(model_args=model_args)\n</code></pre> <p>Incompatible <code>model_args</code></p> <p>If you try to set new <code>model_args</code> that are incompatible with the current model weights, the function will raise an exception and the <code>Experiment</code> class will be left in an inconsistent state. To rectify this, immediately re-execute <code>set_model_args</code> with additional keyword argument <code>keep_weights=False</code> as in the example below: <pre><code>exp.set_model_args(model_args, keep_weights=False)\n</code></pre></p> <p>Model arguments can then be used within a <code>TrainingPlan</code> as in the example below,</p> <pre><code>class MyTrainingPlan(TorchTrainingPlan):\n\n    def init_model(self, model_args):\n        model = self.Net(model_args)\n        return model\n\n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.in_features = model_args['in_features']\n            self.out_features = model_args['out_features']\n            self.fc1 = nn.Linear(self.in_features, 5)\n            self.fc2 = nn.Linear(5, self.out_features)\n</code></pre> <p>Special model arguments for scikit-learn experiments</p> <p>In scikit-learn experiments, you are required to provide additional special arguments in the <code>model_args</code> dictionary. For classification tasks, you must provide both a <code>n_features</code> and an <code>n_classes</code> field, while for regression tasks you are only required to provide a <code>n_features</code> field.</p> <ul> <li><code>n_features</code>: an integer indicating the number of input features for your model</li> <li><code>n_classes</code>: an integer indicating the number of classes in your task</li> </ul> <p>Note that, as an additional requirement for classification tasks, classes must be identified by integers in the range <code>0..n_classes</code></p>"},{"location":"user-guide/researcher/experiment/#training-arguments","title":"Training Arguments","text":"<p><code>training_args</code> is a dictionary, containing the arguments for the training on the node side (e.g. data loader arguments, optimizer arguments, epochs, etc.). These arguments are dynamic, in the sense that you may change them between two rounds of the same experiment, and the updated changes will be taken into account (provided you also update the experiment using the <code>set_training_args</code> method).</p> <p>A list of valid arguments is given in the TrainingArgs.default_scheme documentation.</p> <p>To set the training arguments you may either pass them to the <code>Experiment</code> constructor, or set them on an instance with the <code>set_training_arguments</code> method:</p> <p><pre><code>exp.set_training_arguments(training_args=training_args)\n</code></pre> To get the current training arguments that are used for the experiment, you can write:</p> <pre><code>exp.training_args()\n</code></pre>"},{"location":"user-guide/researcher/experiment/#controlling-the-number-of-training-loop-iterations","title":"Controlling the number of training loop iterations","text":"<p>The preferred way is to set the <code>num_updates</code> training argument. This argument is equal to the number of iterations to be performed in the training loop. In mini-batch based scenarios, this corresponds to the number of updates to the model parameters, hence the name. In PyTorch notation, this is equivalent to the number of calls to <code>optimizer.step</code>.</p> <p>Another way to determine the number of training loop iterations is to set <code>epochs</code> in the training arguments. In this case, you may optionally set a <code>batch_maxnum</code> argument, in order to exit the training loop before a full epoch is completed. If <code>batch_maxnum</code> is set and is greater than 0, then only <code>batch_maxnum</code> iterations will be performed per epoch.</p> <p><code>num_updates</code> is the same for all nodes</p> <p>If you specify <code>num_updates</code> in your <code>training_args</code>, then every node will perform the same number of updates. Conversely, if you specify <code>epochs</code>, then each node may perform a different number of iterations if their local dataset sizes differ.</p> <p>Note that if you set both <code>num_updates</code> and <code>epochs</code> by mistake, the value of <code>num_updates</code> takes precedence, and <code>epochs</code> will effectively be ignored.</p> <p>Why num updates?</p> <p>In a federated scenario, different nodes may have datasets with very different sizes. By performing the same number of epochs on each node, we would be biasing the global model towards the larger nodes, because we would be performing more gradient updates based on their data. Instead, we fix the number of gradient updates regardless of the dataset size with the <code>num_updates</code> parameter.</p>"},{"location":"user-guide/researcher/experiment/#compatibility","title":"Compatibility","text":"<p>Not all configurations are compatible with all types of aggregators and experiments. We list here the known constraints:</p> <ul> <li>the Scaffold aggregator requires using <code>num_updates</code></li> </ul>"},{"location":"user-guide/researcher/experiment/#batch-size-and-other-data-loader-arguments","title":"Batch size and other data loader arguments","text":"<p>Dataloader arguments are automatically injected through the <code>DataManager</code> class</p> <p>It is strongly recommended to always provide a <code>loader_args</code> key in your training arguments, with a dictionary as value containing at least the <code>batch_size</code> key.</p> <p>Example of minimal loader arguments: <pre><code>training_args = {\n    'loader_args': {\n        'batch_size': 1,\n    },\n}\n</code></pre></p> <p>Note that the <code>loader_arguments</code>, as well as any additional keyword arguments that you will specify in your <code>DataManager</code> constructor, will be automatically injected in the definition of the data loader. Please refer to the <code>training_data</code> method documentation for more details.</p>"},{"location":"user-guide/researcher/experiment/#setting-a-random-seed-for-reproducibility","title":"Setting a random seed for reproducibility","text":"<p>The <code>random_seed</code> argument allows to set a random seed at the beginning of each round.</p> <p><code>random_seed</code> is set both on the node and the researcher</p> <p>The <code>random_seed</code> is set whenever the <code>TrainingPlan</code> is instantiated: both on the researcher side before sending the train command for a new round, and on the node side at the beginning of the configuration of the new training round.</p> <p>Setting the <code>random_seed</code> affects:</p> <ul> <li>the random initialization of model parameters at the beginning of the experiment</li> <li>the random shuffling of data in the <code>DataLoader</code></li> <li>any other random effect on the node and researcher side.</li> </ul> <p>The same seed is used for the built-in <code>random</code> module, <code>numpy.random</code> and <code>torch.random</code>, effectively equivalent to: <pre><code>import random\nimport numpy as np\nimport torch\n\nrandom.seed(training_args['random_seed'])\nnp.random.seed(training_args['random_seed'])\ntorch.manual_seed(training_args['random_seed'])\n</code></pre></p> <p><code>random_seed</code> is reset at every round</p> <p>The random_seed argument will be reset to the specified value at the beginning of every round.</p> <p>Because of this, the same sequence will be used for random effects like shuffling the dataset for all the rounds within an <code>exp.run()</code> execution. This is not what the user typically wants. A simple workaround is to manually change the seed at every round and use <code>exp.run_once</code> instead:</p> <pre><code>for i in range(num_rounds):\n    training_args['random_seed'] = 42 + i\n    exp.set_training_args(training_args)\n    exp.run_once()\n</code></pre>"},{"location":"user-guide/researcher/experiment/#sub-arguments-for-optimizer-and-differential-privacy","title":"Sub-arguments for optimizer and differential privacy","text":"<p>In Pytorch experiments, you may include sub arguments such as <code>optimizer_args</code> and <code>dp_args</code>. Optimizer arguments represents the arguments that are going to be passed to <code>def init_optimizer(self, o_args)</code> method as dictionary and (<code>dp_args</code>) represents the arguments of differential privacy.</p> <p>         Optimizer arguments and differential privacy arguments are valid only in PyTorch base training plan.     </p> <pre><code>training_args = {\n    'loader_args': {\n        'batch_size': 20,\n    },\n    'num_updates': 100,\n    'optimizer_args': {\n        'lr': 1e-3,\n    },\n    'dp_args': {\n        'type': 'local',\n        'sigma': 0.4,\n        'clip': 0.005\n    },\n    'dry_run': False,\n}\n</code></pre>"},{"location":"user-guide/researcher/experiment/#sharing-persistent-buffers","title":"Sharing persistent buffers","text":"<p>In Pytorch experiments, you may include the argument <code>share_persistent_buffers</code>. When set to <code>True</code> (default), nodes will share the full <code>state_dict()</code> of the Pytorch module, which contains both the learnable parameters and the persistent buffers (defined as invariant in the network, like batchnorm\u2019s <code>running_mean</code> and <code>running_var</code>). When set to <code>False</code>, nodes will only share learnable parameters.</p> <p>This argument will be ignored for scikit-learn experiments, as the notion of persistent buffers is specific to Pytorch.</p>"},{"location":"user-guide/researcher/experiment/#aggregator","title":"Aggregator","text":"<p>An aggregator is one of the required arguments for the experiment. It is used on the researcher for aggregating model parameters that are received from the nodes after every round. By default, when the experiment is initialized without passing any aggregator, it will automatically use the default <code>FedAverage</code> aggregator class. However, it is also possible to set a different aggregation algorithm with the method <code>set_aggregator</code>. Currently, Fed-BioMed has only <code>FedAverage</code> and <code>Scaffold</code> classes, but it is possible to create a custom aggregator class. You can see the current aggregator by running <code>exp.aggregator()</code>. It will return the aggregator object that will be used for aggregation.</p> <p>When you pass the aggregator argument as <code>None</code> it will use <code>FedAverage</code> aggregator (performing a Federated Averaging aggregation) by default.</p> <pre><code>exp.set_aggregator(aggregator=None)\n</code></pre> <p>or you can directly pass an aggregator instance</p> <pre><code>from fedbiomed.researcher.aggregators.fedavg import FedAverage\nexp.set_aggregator(aggregator=FedAverage())\n</code></pre> <p>Custom aggregator classes should inherit from the base class <code>Aggregator</code> of Fed-BioMed. Please visit user guide for  aggregators for more information.</p> <p>About Scaffold Aggregator</p> <p><code>FedAverage</code> reflects only how local models sent back by <code>Nodes</code> are aggregated, whereas <code>Scaffold</code> also implement additional elements such as the <code>Optimizer</code> on <code>Researcher</code> side. Please note that currently only <code>FedAverage</code> is compatible with <code>declearn</code>'s <code>Optimizers</code>.</p>"},{"location":"user-guide/researcher/experiment/#node-selection-strategy","title":"Node Selection Strategy","text":"<p>Node selection Strategy is also one of the required arguments for the experiment. It is used for selecting nodes before each round of training. Since the strategy will be used for selecting nodes, thus, training data should be already set before setting any strategies. Then, strategy will be able to select among training nodes that are currently available regarding their dataset.</p> <p>By default, <code>set_strategy(node_selection_strategy=None)</code> will use the default <code>DefaultStrategy</code> strategy. It is the default strategy in Fed-BioMed that selects for the training all the nodes available regardless their datasets. However, it is also possible to set different strategies. Currently, Fed-BioMed only provides <code>DefaultStrategy</code> but you can create your custom strategy classes.</p>"},{"location":"user-guide/researcher/experiment/#round-limit","title":"Round Limit","text":"<p>The experiment should have a round limit that specifies the max number of training round. By default, it is <code>None</code>, and it needs to be created either declaring/building experiment class or using setter method for round limit. Setting round limit doesn't mean that it is going to be permanent. It can be changed after running the experiment once or more.</p> <pre><code>exp.set_round_limit(round_limit=4)\n</code></pre> <p>To see current round limit of the experiment:</p> <pre><code>exp.round_limit()\n</code></pre> <p>You might also wonder how many rounds have been completed in the experiment. The method <code>round_current()</code> will return the last round that has been completed.</p> <pre><code>exp.round_currrent()\n</code></pre>"},{"location":"user-guide/researcher/experiment/#displaying-training-loss-values-through-tensorboard","title":"Displaying training loss values through Tensorboard","text":"<p>The argument <code>tensorboard</code> is of type boolean, and it is used for activating tensorboard during the training. When it is <code>True</code> the loss values received from each node will be written into tensorboard event files in order to display training loss function on the tensorboard interface.</p> <p>Tensorboard events are controlled by the class called <code>Monitor</code>. To enable tensorboard after the experiment has already been initialized, you can use the method <code>set_monitor()</code> of the experiment object.</p> <pre><code>exp.set_monitor(tensorboard=True)\n</code></pre> <p>You can visit tensorboard documentation page to get more information about how to use tensorboard with Fed-BioMed</p>"},{"location":"user-guide/researcher/experiment/#saving-breakpoints","title":"Saving Breakpoints","text":"<p>Breakpoint is a researcher side function that saves an intermediate status and training results of an experiment to disk files. The argument <code>save_breakpoints</code> is of type boolean, and it indicates whether breakpoints of the experiment should be saved after each round of training or not. <code>save_breakpoints</code> can be declared while creating the experiment or after using its setter method.</p> <pre><code>exp.set_save_breakpoints(True)\n</code></pre> <p>Info</p> <p>Setting <code>save_breakpoints</code> to <code>True</code> after the experiment has performed several rounds of training will only save the breakpoints for remaining rounds.</p> <p>Please visit the tutorial \"Breakpoints (experiment saving facility)\" to find out more about breakpoints.</p>"},{"location":"user-guide/researcher/experiment/#experimentation-folder","title":"Experimentation Folder","text":"<p>Experimentation folder indicates the name of the folder in which all the experiment results will be stored/saved. By default, it will be <code>Experiment_XXXX</code>, and <code>XXXX</code> part stands for the auto increment (hence, first folder will be named <code>Experiment_0001</code>, the second one <code>Experiment_0002</code> and so on). However, you can also define your custom experimentation folder name.</p> <p>Passing experimentation folder while creating the experiment; <pre><code>exp = Experiment(\n    #....\n    experimentation_folder='MyExperiment'\n    #...\n)\n</code></pre></p> <p>Setting experimentation folder using setter; <pre><code>exp.set_experimentation_folder(experimentation_folder='MyExperiment')\n</code></pre></p> <p>Using custom folder name for your experimentation might be useful for identifying different types of experiment. Experiment folders will be located at <code>${FEDBIOMED_DIR}/var/experiments</code>. However, you can always get exact path to your experiment folder using the getter method <code>experimentation_path()</code>. Below is presented a way to retrieve all the files from the folder using <code>os</code> builtin package.</p> <pre><code>import os\n\nexp_path = exp.experimentation_path()\nos.listdir(exp_path)\n</code></pre>"},{"location":"user-guide/researcher/experiment/#running-an-experiment","title":"Running an Experiment","text":""},{"location":"user-guide/researcher/experiment/#train_request-and-train_reply-messages","title":"<code>train_request</code> and <code>train_reply</code> messages","text":"<p>Running an experiment means starting the training process by sending train request to nodes. It creates training request that are subscribed by each live node that has the dataset. After sending training commands it waits for the responses that will be sent by the nodes. The following code snippet represents an example of train request.</p> <pre><code>{\n  \"researcher_id\": \"researcher id that sends training command\",\n  \"experiment_id\": \"created experiment id by experiment\",\n  \"state_id\": \"state id for this round this experiment on this node\",\n  \"training_args\": {\n    \"loader_args\": {\n      \"batch_size\": 32\n    },\n    \"optimizer_args\": {\n      \"lr\": 0.001\n    },\n    \"epochs\": 1,\n    \"dry_run\": false,\n    \"batch_maxnum\": 100\n  },\n  \"dataset_id\": \"id of the used dataset on this node\",\n  \"training\": True,\n  \"model_args\": &lt;args&gt;,\n  \"params\": &lt;model weights&gt;,\n  \"training_plan\": \"&lt;training plan code&gt;\",\n  \"training_plan_class\": \"MyTrainingPlan\",\n  \"command\": \"train\",\n  \"round\": &lt;round_number&gt;,\n  \"aggregator_args\": &lt;args&gt;,\n  \"aux_vars\": [list of auxiliary variables],\n  \"secagg_arguments\": {\n    \"secagg_servkey_id\": \"secure aggregation server key id\",\n    \"secagg_random\": &lt;random number&gt;,\n    \"secagg_clipping_range\": 3\n      ]\n  }\n  }\n}\n</code></pre> <p>After sending train requests, Experiment waits for the replies that are going to be published by each node once every round of training is completed. These replies are called training replies, and they include information about the training and the URL from which to download model parameters that have been uploaded by the nodes to the file repository. The following code snippet shows an example of <code>training_reply</code> from a node.</p> <p><pre><code>{\n   \"researcher_id\":\"researcher id that sends the training command\",\n   \"experiment_id\":\"experiment id that creates training request\",\n   \"success\":True,\n   \"node_id\":\"ID of the node that completes the training \",\n   \"dataset_id\":\"dataset_dcf88a68-7f66-4b60-9b65-db09c6d970ee\",\n   \"timing\":{\n      \"rtime_training\":87.74385611899197,\n      \"ptime_training\":330.388954968\n   },\n   \"msg\":\"\",\n   \"command\":\"train\",\n  \"state_id\": \"state id for new round this experiment on this node\",\n  \"params\": &lt;model weights&gt;,\n  ...\n}\n</code></pre> <code>training_reply</code> always results of a <code>training_request</code> sent by the <code>Researcher</code> to the <code>Node</code>.</p> <p>To complete one round of training, the experiment waits until receiving each reply from nodes. At the end of the round, it downloads the model parameters that are indicated in the training replies. It aggregates the model parameters based on a given aggregation class/algorithm. This process is repeated until every round is completed. Please see Figure 1 to understand how federated training is performed between the nodes and the Researcher (<code>Experiment</code>) component.</p> <p> Figure 2 - Federated training workflow among the components of Fed-BioMed. It illustrates the messages exchanged between <code>Researcher</code> and 2 <code>Nodes</code> during a Federated Training</p>"},{"location":"user-guide/researcher/experiment/#the-methods-runand-run_once","title":"The Methods <code>run()</code>and <code>run_once()</code>","text":"<p>In order to provide more control over the training rounds, <code>Experiment</code> class has two methods as <code>run</code> and <code>run_once</code> to run training rounds.</p> <ul> <li><code>run()</code> runs the experiment rounds from current round to round limit. If the round limit is reached it will indicate that the round limit has been reached. However, the method <code>run</code> takes 2 arguments as <code>rounds</code> and <code>increase</code>.<ul> <li><code>rounds</code> is an integer that indicates number of rounds that are going to be run. If the experiment is at round <code>0</code>,  the round limit is <code>4</code>, and if you pass <code>rounds</code> as 3, it will run the experiment only for <code>3</code> rounds.</li> <li><code>increase</code> is a boolean that indicates whether round limit should be increased if the given <code>rounds</code> pass over the  round limit. For example, if the current round is <code>3</code>, the round limit is <code>4</code>, and the <code>rounds</code> argument is <code>2</code>, the experiment will increase round limit to <code>5</code></li> </ul> </li> <li><code>run_once()</code> runs the experiment for single round of training. If the round limit is reached it will indicate that the round limit has been reached. This command is the same as <code>run(rounds=1, increase=False)</code>. However, if <code>run_once</code> is executed as <code>run_once(increase=True)</code>, then, when the round limit is reached, it increases the round limit for one extra round.</li> </ul> <p>To run your experiment until the round limit;</p> <pre><code>exp.run()\n</code></pre> <p>To run your experiment for given number of rounds (while not passing the round limit):</p> <pre><code>exp.run(rounds=2)\n</code></pre> <p>To run your experiment for given number of rounds and increase the round limit accordingly if needed:</p> <pre><code>exp.run(rounds=2, increase=True)\n</code></pre> <p>To run your experiment only once (while not passing the round limit):</p> <pre><code>exp.run_once()\n</code></pre> <p>To run your experiment only once even round limit is reached (and increase the round limit accordingly if needed):</p> <pre><code>exp.run_once(increase=True)\n</code></pre> <p>Running experiment with both <code>run(rounds=rounds, increase=True)</code> and <code>run_once(increase=True)</code> will automatically increase/update round limit if it is exceeded.</p>"},{"location":"user-guide/researcher/listing-datasets-and-selecting-nodes/","title":"Listing Datasets and Selecting Nodes","text":"<p>In this article, you will learn how to list datasets that are deployed in nodes and select specific nodes to conduct your experiment.</p>"},{"location":"user-guide/researcher/listing-datasets-and-selecting-nodes/#listing-datasets","title":"Listing Datasets","text":"<p>The <code>list()</code> method of the <code>Requests</code> class has been created for listing datasets on the active nodes. It sends <code>list</code> request to the nodes and waits for the reply. It gets two arguments as <code>nodes</code> and <code>verbose</code>;</p> <ul> <li><code>verbose</code>: If it is <code>True</code>, it will print the dataset lists in table format for each node. Default is <code>False</code></li> <li><code>nodes</code>: It is a list that includes the node ids to send list requests. Default is <code>None</code> and it means that it sends list requests to all activate nodes.</li> </ul> <p>It returns a python <code>dict</code> that includes datasets for each node.</p> <pre><code>from fedbiomed.researcher.requests import Requests\nfrom fedbiomed.researcher.config import config\n# Optional: To select specific researcher configuration\n# config.load(root=&lt;fedbiomed-root&gt;)\nreq = Requests(config)\nreq.list(verbose=True)\n</code></pre> <p>If you set <code>verbose=True</code> you will get the following output that shows datasets on nodes up and running.</p> <pre><code> Node: node_481d9ec3-79e5-49d1-96a2-9f4928d3ecf4 | Number of Datasets: 1\n+--------+-------------+--------+---------------+---------+\n| name   | data_type   | tags   | description   | shape   |\n+========+=============+========+===============+=========+\n| sk     | csv         | ['sk'] | sk            | [20, 6] |\n+--------+-------------+--------+---------------+---------+\n\n2021-10-19 16:51:59,699 fedbiomed INFO -\n Node: node_e289dfdc-4635-4c3c-938a-9548dbb85c92 | Number of Datasets: 2\n+--------+-------------+------------------------+----------------+--------------------+\n| name   | data_type   | tags                   | description    | shape              |\n+========+=============+========================+================+====================+\n| MNIST  | default     | ['#MNIST', '#dataset'] | MNIST database | [60000, 1, 28, 28] |\n+--------+-------------+------------------------+----------------+--------------------+\n| sk     | csv         | ['sk']                 | sk             | [20, 6]            |\n+--------+-------------+------------------------+----------------+--------------------+\n</code></pre> <p>Listing datasets technically lists active nodes in the network. When the <code>verbose</code> argument is <code>True</code> it also prints nodes that don't have any dataset and indicates that the node has no dataset.</p> <p>You can also list datasets in specific nodes;</p> <pre><code>req.list(nodes=['node_e289dfdc-4635-4c3c-938a-9548dbb85c92'], verbose=True)\n</code></pre> <p>It will return the datasets deployed only in the node: <code>node_e289dfdc-4635-4c3c-938a-9548dbb85c92</code></p> <pre><code> Node: node_e289dfdc-4635-4c3c-938a-9548dbb85c92 | Number of Datasets: 2\n+--------+-------------+------------------------+----------------+--------------------+\n| name   | data_type   | tags                   | description    | shape              |\n+========+=============+========================+================+====================+\n| MNIST  | default     | ['#MNIST', '#dataset'] | MNIST database | [60000, 1, 28, 28] |\n+--------+-------------+------------------------+----------------+--------------------+\n| sk     | csv         | ['sk']                 | sk             | [20, 6]            |\n+--------+-------------+------------------------+----------------+--------------------+\n</code></pre>"},{"location":"user-guide/researcher/listing-datasets-and-selecting-nodes/#selecting-nodes-for-the-experiment","title":"Selecting Nodes for the Experiment","text":"<p>The experiment class has <code>nodes</code> arguments to optionally select specific nodes on which the federated training will be performed. If you pass a non-empty list of node ids, then only the nodes that have a matching dataset and belong to the <code>nodes</code> list are selected.</p> <p>Let's assume that you want to perform training only in the node  <code>node_e289dfdc-4635-4c3c-938a-9548dbb85c92</code></p> <pre><code>nodes = ['node_e289dfdc-4635-4c3c-938a-9548dbb85c92']\n</code></pre> <p>Afterwards, you need to pass the <code>nodes</code> list while you are initializing the experiment class. The experiment will send a search request to the nodes in the <code>nodes</code> list for datasets deployed with the given tags.</p> <pre><code>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MNIST', '#dataset']\nrounds = 2\n\nexp = Experiment(tags=tags,\n                 nodes=nodes,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</code></pre> <p>The output of the initialization will be similar to the following output.</p> <pre><code>2021-10-19 17:06:16,599 fedbiomed INFO - Searching dataset with data tags: ['#MNIST', '#dataset'] on specified nodes: ['node_e289dfdc-4635-4c3c-938a-9548dbb85c92']\n2021-10-19 17:06:16,631 fedbiomed INFO - log from: node_e289dfdc-4635-4c3c-938a-9548dbb85c92 - DEBUG Message received: {'researcher_id': 'researcher_1c4fc722-02c8-41b2-b9ed-b85d97968ba9', 'tags': ['#MNIST', '#dataset'], 'command': 'search'}\n2021-10-19 17:06:26,612 fedbiomed INFO - Node selected for training -&gt; node_e289dfdc-4635-4c3c-938a-9548dbb85c92\n</code></pre>"},{"location":"user-guide/researcher/model-testing-during-federated-training/","title":"Model Validation During Federated Training on the Nodes","text":"<p>Model validation is critical to discover how the model performs during the training rounds, when no dedicated holdout dataset is available for testing. In federated training, models are refined on the different nodes with different datasets. Therefore, model validation should be implemented on each node separately to compare model performances after its parameters are updated. Fed-BioMed provides a validation routine on the datasets that are randomly split at each round of training.</p> <p>In the federated learning concept, two validation types can be applied at each round of training:</p> <ul> <li>Validation on globally updated parameters (<code>test_on_global_updates</code>): It is the validation applied on the aggregated parameters before performing the training for the current round on a node.</li> <li>Validation on locally updated parameters (<code>test_on_local_updates</code>): It is the validation applied after the local training for the current round is performed on a node, and model parameters have been locally updated.</li> </ul> <p>These two validations allow the users to compare how training on the node has improved the model performance.</p> <p>Validation and training dataset are kept seperated when testing is enabled</p> <p>Newest <code>Fed-BioMed</code> releases (&gt;=6.1) allows <code>Researcher</code>s to keep distincts validation (for validating model performance) and training dataset when running the <code>Experiment</code>. It is now the default behaviour.  In previous versions of <code>Fed-BioMed</code>, <code>Node</code>s did not provide  completely separated datasets for validating model performance. Since the samples for validation and training were picked randomly at each round of training, the same samples could be used for training in one round and for validation in another round. To revert back to this old behaviour, you can set in the <code>Experiment.set_training_args</code> <code>shuffle_testing_dataset=True</code> (defaults to <code>False</code>). Setting <code>shuffle_testing_dataset=True</code> could make sense when dataset are very small, and you may want to use the whole dataset to train your model, validation samples included.</p> <p>Validation dataset are re-shuffled if <code>test_ratio</code> changes or if <code>Node's</code> dataset change from one <code>Round</code> to another</p> <p>Changing the value of <code>test_ratio</code> from one <code>Round</code> to another automatically re-shuffles the validation and testing dataset. It is equivalent to setting <code>shuffle_testing_dataset=True</code>. Similary, if changes in the <code>Node</code> dataset happens form one <code>Round</code> to another, dataset training and validation may change as well. </p> <p>Figure 1 illustrates the phases of validation and training during 2 rounds of federated training. As it can be seen in the figure, after the last round of training, one last validation on global updates is performed on the last aggregated parameters by each node. Therefore, the number of validation on globally updated parameters, if it is activated, will be equal to the number of rounds + 1</p> <p> Figure 1 - Validation on global and local updates</p>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#default-metrics-provided-by-fed-biomed","title":"Default Metrics Provided by Fed-BioMed","text":"<p>Fed-BioMed provides several test metrics to perform validation that can be used without defining a validation function in the training plan. It allows the user to launch an experiment by providing as less code as possible. You can display all the metric provided by Fed-BioMed as shown in the following code snippet.</p> <pre><code>from fedbiomed.common.metrics import MetricTypes\nMetricTypes.get_all_metrics()\n\n# &gt; Output:\n# &gt; ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL',\n# &gt; 'MEAN_SQUARE_ERROR', 'MEAN_ABSOLUTE_ERROR',\n# &gt; 'EXPLAINED_VARIANCE']\n</code></pre> <p>Note</p> <p>By default, <code>ACCURACY</code> metric is used as a test metric if there isn't a metric defined by the researcher. Therefore, please pay attention to whether <code>ACCURACY</code> is relevant for the model that is going to be trained. Otherwise, metric results might be inconsistent.</p>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#validation-arguments","title":"Validation Arguments","text":"<p>Validation during the training is an optional process, and validation arguments should be configured in order to activate it. Here is the list of validation arguments that can be configured.</p> <ul> <li><code>test_ratio</code>: Ratio of the validation partition of the dataset.  The remaining samples will be used for training. By                  default, it is <code>0.0</code>.</li> <li><code>test_on_global_updates</code>: Boolean value that indicates whether validation will be applied to globally updated (aggregated) parameters (see Figure 1). Default is <code>False</code></li> <li><code>test_on_local_updates</code>: Boolean value that indicates whether validation will be applied to locally updated (trained) parameters (see Figure 1). Default is <code>False</code></li> <li><code>test_metric</code>: One of <code>MetricTypes</code> that indicates which metric will be used for validation. It can be <code>str</code> or     an instance of <code>MetricTypes</code> (e.g. <code>MetricTypes.RECALL</code> or <code>RECALL</code> ). If it is <code>None</code> and there isn't <code>testing_step</code>     defined in the training plan (see section: Define Custom Validation Step) default metric will be     <code>ACCURACY</code>.</li> <li><code>test_metric_args</code>: A dictionary that contains the arguments that will be used for the metric function.</li> <li><code>test_batch_size</code>: A value used to compute metrics using batches instead of  loading the full testing dataset (specified by <code>test_ratio</code>). Setting <code>test_batch_size</code> can avoid having <code>MemoryError</code> errors due to large and /or heavy datasets. You should select wisely the batch size and the metric, for some metrics can be meaningless if computed over several small batches of data (e.g. explained variance). <code>test_batch_size</code> should be greater or equal than <code>1</code> (enabled) or equal to <code>0</code> or <code>None</code> (disabled).</li> <li><code>shuffle_testing_dataset</code>: This argument will perform shuffling of the dataset. If it is switched from <code>False</code> too <code>True</code> it will reinitialize testing dataset that will be different than the one used in the previous rounds.</li> </ul> <p>Info</p> <p>Validation functions for each default metric executes functions from scikit-learn framework. Therefore, <code>test_metric_args</code> should be coherent with the arguments of \"scikit-learn\" metrics functions. Please visit here to see API documentation of scikit-learn metrics.</p> <p>To activate validation on the node side, the arguments <code>test_ratio</code> and at least one of <code>test_on_local_updates</code> or <code>test_on_global_updates</code> should be set to <code>True</code>. Since the default values of <code>test_on_local_updates</code> and <code>test_on_global_updates</code> are <code>False</code>, setting <code>test_ratio</code> will only split dataset as validation and train sets but won't perform validation.</p>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#setting-validation-arguments-in-training-arguments","title":"Setting Validation Arguments in Training Arguments","text":"<p>Validation arguments are considered a part of the training on the node side. Therefore, it is possible to define validation arguments in the training arguments and pass them to the experiment.</p> <pre><code>from fedbiomed.common.metrics import MetricTypes\nfrom fedbiomed.researcher.federated_workflows import Experiment\ntraining_args = {\n    #....\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 2,\n    'batch_maxnum': 100,\n    #...\n    'test_ratio' : 0.25,\n    'test_metric': MetricTypes.F1_SCORE,\n    'test_on_global_updates': True,\n    'test_on_local_updates': True,\n    'test_batch_size': 0,\n    'test_metric_args': {'average': 'macro'}\n}\n\nexp = Experiment(# ....\n                 training_args=training_args)\n</code></pre>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#setting-validation-arguments-using-setters-of-the-experiment-class","title":"Setting Validation Arguments using Setters of the Experiment Class","text":"<p>Each validation argument has its own setter method in the experiment class where federated training is managed. Therefore, validation arguments can be set, modified, or reset using the setters. To enable setters for validation arguments, the experiment should be created in advance.</p> <pre><code>from fedbiomed.common.metrics import MetricTypes\nfrom fedbiomed.researcher.federated_workflows import Experiment\n\ntraining_args = {\n    'optimizer_args': {\n        'lr': 1e-3,\n    },\n    'epochs': 2,\n    'batch_maxnum': 100,\n    'test_batch_size': 0\n}\nexp = Experiment(training_args=training_args)\n\nexp.set_test_ratio(0.25)\nexp.set_test_on_local_updates(True)\nexp.set_test_on_global_updates(True)\nexp.set_test_metric(MetricTypes.F1_SCORE) # or exp.set_test_metric('F1_SCORE')\nexp.set_test_metric_args({'average': 'macro'})\n</code></pre> <p>Setters allow updating validation arguments from one round to others.</p> <pre><code>exp.run(rounds=2, increase=True)\nexp.set_test_ratio(0.35)\nexp.set_set_test_metric(MetricTypes.ACCURACY)\nexp.run(rounds=2, increase=True)\n</code></pre>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#using-validation-facility-with-heavy-datasets","title":"Using Validation facility with heavy datasets","text":"<p>In some specific cases, you may have some very huge datain your datasets, such as 3D images, or you want a huge validation set, that you can load on <code>Node</code>s only through batches, in order to avoid <code>Nodes</code> to crash due to lack of Memory. <code>Fed-BioMed</code> provides an option to compute validation metrics through batches, in the <code>TrainingArgument</code>: <code>test_batch_size</code>.</p> <p>Validation will be computed over batches instead of computing metric using the whole dataset (which is the default behaviour) if <code>test_batch_size</code> is set. For now, <code>Fed-BioMed</code> does not provide a way to reconcile all these metrics under one metric (such as computing the mean of Accuracy for instance). If you want to do so, please consider defining your own <code>Validation step</code> by writing your own <code>testing_step</code> method in the <code>TrainingPlan</code> (see below for further details).</p> <pre><code>training_args = {\n    'optimizer_args': {\n        'lr': 1e-3,\n    },\n    'epochs': 2,\n    'test_ratio': .4,\n    'test_batch_size': 64  # validation metrics will be done by considering batches of size 64 each\n}\nexp = Experiment(training_args=training_args)\n</code></pre>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#define-custom-validation-step","title":"Define Custom Validation Step","text":"<p>Fed-BioMed training plans allow defining custom validation steps for model evaluation on the node side. The name of the method that should be defined in the training plan is <code>testing_step</code>. It should take two input arguments as data/inputs and target/actual that are provided on the node side. The validation step can calculate and return multiple testing metrics as long as the return value of the method is supported. The method should return:</p> <ul> <li>Single <code>float</code> or <code>int</code> value that represents a single validation result. The name of the metric will be displayed as <code>Custom</code>.</li> </ul> <pre><code>def testing_step(self, data, target):\n    # Validation actions ...\n    value = 1.001\n\n    return value\n</code></pre> <ul> <li>List of multiple validation results. Metrics names will be displayed as <code>Custom_1</code>, <code>Custom_2</code>, <code>Custom_n</code> .</li> </ul> <pre><code>def testing_step(self, data, target):\n    # Validation actions ...\n    value_1 = 1.001\n    value_2 = 1.002\n    return [value_1, value_2]\n</code></pre> <ul> <li>Dictionary of multiple metric results as <code>int</code> or <code>float</code>. Metrics names will be displayed as the keys of dictionary.</li> </ul> <pre><code>def testing_step(self, data, target):\n    # Validation actions ...\n    result = {'metric-1' : 0.01, 'metric-2': 0.02}\n    return result\n</code></pre> <p>Info</p> <p><code>testing_step</code> has a higher priority than default test metrics. It means that if both <code>testing_step</code> in training plan and <code>test_metric</code><code>argument in the validation arguments are defined, node will only execute the method</code>testing_step`</p> <p>The modules, functions, and methods that are going to be used in the validation method should be added as dependencies in the training plan (see PyTorch and Sklearn). Please also make sure that the modules whose functions will be used in the validation step do exist in the Fed-BioMed node environment.</p>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#pytorch","title":"PyTorch","text":"<p>The validation method in PyTorch-based training plans takes two arguments respectively for input (<code>X</code>) and target (<code>y</code>). These arguments are instances of <code>torch.Tensor</code>. The validation mode of PyTorch will be already activated on the node side before running <code>testing_step</code> with <code>self.eval()</code>. Therefore, there is no need to configure it again in the validation step method.</p> <p>The following code snippet shows an example <code>testing_step</code> that calculates negative log-likelihood, cross-entropy and accuracy.</p> <pre><code>import torch\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\n\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    # Other necessary methods e.g. `def init_model`\n    # .......\n\n    def testing_step(self, data, target):\n\n        pred = self.model().forward(data)\n        nll   = torch.nn.functional.nll_loss(pred, target)  # negative log likelihood loss\n        ce = torch.nn.functional.cross_entropy(pred,target) # cross entropy\n\n\n        _, predicted = torch.max(pred.data,1)\n        acc = torch.sum(predicted==target)\n        accuracy = acc/len(target)  # accuracy\n\n        return { 'NLL': nll, 'CE': ce, 'ACCURACY': accuracy}\n</code></pre> <p>Info</p> <p>Datasets for validation (<code>data</code> and <code>target</code>) are not a batch iterator. They contain all the samples in one block. However, it is possible to define batch iterator in the validation method as long as the method returns a single value for each metric that is calculated.</p>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#sklearn","title":"SkLearn","text":"<p>The validation method in scikit-learn based training plans also takes two arguments respectively for input/data (<code>X</code>) and target (<code>y</code>). These arguments are instances of <code>np.ndarray</code>.</p> <p>The following code snippet shows an example <code>testing_step</code> that calculates hinge loss and accuracy.</p> <pre><code>from fedbiomed.common.training_plans import FedPerceptron\nfrom sklearn.metrics import hinge_loss\nimport numpy as np\n\n\nclass SkLearnClassifierTrainingPlan(FedPerceptron):\n    def init_dependencies(self):\n        return ['import torch',\n                \"from sklearn.linear_model import Perceptron\",\n                \"from torchvision import datasets, transforms\",\n                \"from torch.utils.data import DataLoader\",\n                \"from sklearn.metrics import hinge_loss\"]\n\n\n    def compute_accuracy_for_specific_digit(self, data, target, digit: int):\n        idx_data_equal_to_digit = target == digit\n\n        predicted = self.model().predict(data[idx_data_equal_to_digit])\n        well_predicted_label = np.sum(predicted == digit) / np.sum(idx_data_equal_to_digit)\n        return well_predicted_label\n\n    def testing_step(self, data, target):\n        # hinge loss\n        distance_from_hyperplan = self.model().decision_function(data)\n        loss = hinge_loss(target, distance_from_hyperplan)\n\n        # get the accuracy only on images representing digit 1\n        well_predicted_label_1 = self.compute_accuracy_for_specific_digit(data, target, 1)\n\n        # Returning results as dict\n        return {'Hinge Loss': loss, 'Well Predicted Label 1' : well_predicted_label_1}\n</code></pre>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#conclusion","title":"Conclusion","text":"<p>The validation part in FL plays an important role in evaluating model performance that is trained on different nodes with different datasets. Applying a validation on the node side for each training round allows comparing the impacts of particular nodes on the trained model. Understanding and comparing different impacts will be clearer thanks to two types of validations: validation on aggregated parameters and validation on locally trained parameters.</p>"},{"location":"user-guide/researcher/tensorboard/","title":"Displaying Loss Values on Tensorboard","text":"<p>Tensorboard is one of the most useful tools to display various metrics during the training. Fed-BioMed offers an option to display loss values on the tensorboard interface. This article focuses on how to use tensorboard on Fed-BioMed to display loss values during the training rounds in each node. This section is presented as follows:</p> <ul> <li>Running experiment with tensorboard option</li> <li>Launching tensorboard</li> <li>Using tensorboard</li> </ul> <p>The tensorboard logs of an experiment is saved in a directory, by default <code>TENSORBOARD_RESULTS_DIR</code>. Thus, if you re-use the same directory for another experiment, the previous experiment's tensorboard logs are cleared. See below to learn how to specify per-experiment directory.</p> <p>Breakpoints currently do not save tensorboard monitoring status and tensorboard logs. If you continue from a breakpoint, tensorboard monitoring is not restarted and logs from pre-breakpoint run are not restored.</p>"},{"location":"user-guide/researcher/tensorboard/#running-experiment-with-tensorboard-option","title":"Running Experiment with Tensorboard Option","text":"<p>During the training of each round, scalar values are sent by each node through the <code>monitoring</code> channel. The experiment does not write scalar values to the event file as long as it has been specified. To do that you need to set <code>tensorboard=True</code> while you are initializing an experiment (see below).  Afterward, the <code>Monitor</code> will be activated, and it will write the loss values coming from each node into a new log file. Thanks to that, it is possible to display and compare on the tensorboard loss evolution (and model performances) trained on each node. By default, losses are saved in files under  the <code>runs</code> directory.</p> <pre><code>from fedbiomed.researcher.federated_workflows import Experiment\n\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None,\n                 tensorboard=True\n                )\n</code></pre> <p>Validation facility</p> <p>Tensorboard displays the results of the validation/testing steps. During each Round, values will be exported into Tensorboard in real time. If test_batch_size is set to a specific value, each computed value will be reported (depending on the size of test_batch_size set).</p>"},{"location":"user-guide/researcher/tensorboard/#launching-tensorboard","title":"Launching Tensorboard","text":""},{"location":"user-guide/researcher/tensorboard/#1-from-terminal","title":"1. From Terminal","text":"<p>Tensorboard comes with the <code>fedbiomed-researcher</code> conda environment. Therefore, please make sure that you have activated the conda <code>fedbiomed-researcher</code> environment in your new terminal window before launching the tensorboard. You can either activate the conda environment using <code>conda activate fedbiomed-researcher</code> or <code>${FEDBIOMED_DIR}/scripts/fedbiomed-environment researcher</code>.</p> <p>You can launch the tensorboard while your experiment is running or not. If you launch the tensorboard before running your experiment, it won't show any result at first. After running the experiment, it will save tensorboard event logs into the <code>runs</code> directory during the training. Afterward, you can refresh tensorboard page to see the current results.</p> <p>While you are launching the tensorboard, you need to pass the correct logs directory with <code>--logdir</code> parameter. You can either change your directory to Fed-BioMed's base directory or use the <code>FEDBIOMED_DIR</code> environment variable if you set it while you were installing Fed-BioMed.</p> <p>Option 1: <pre><code>$ cd path/to/fedbiomed\n$ tensorboard --logdir runs\n</code></pre></p> <p>Option 2: <pre><code>$ tensorboard --logdir $FEDBIOMED_DIR/runs\n</code></pre></p>"},{"location":"user-guide/researcher/tensorboard/#2-from-jupyter-notebook","title":"2. From Jupyter Notebook","text":"<p>It is also possible to launch a tensorboard inside the Jupyter notebook with the tensorboard extension.  Therefore, before launching the tensorboard from the notebook you need to load the tensorboard extension. It is important to launch the tensorboard before running the experiment in the notebook. Otherwise, you will have to wait until the experiment is done to be able to launch the tensorboard because the notebook kernel will be busy running the model training.</p> <p>First, please run the following command in another cell of your notebook to load the tensorboard extension.</p> <pre><code>%load_ext tensorboard\n</code></pre> <p>Afterward, you will be able to start the tensorboard. It is important to pass the correct path to the <code>runs</code> directory. You can use <code>ROOT_DIR</code> to set the correct logs directory. This is the base directory of the Fed-BioMed that <code>runs</code> directory is located.</p> <p>First please import the <code>TENSORBOARD_RESULTS_DIR</code> global variable in a different cell.</p> <pre><code>from fedbiomed.researcher.config import config\ntensorboard_dir = config.vars['TENSORBOARD_RESULTS_DIR']\n</code></pre> <p>Then, you can pass <code>TENSORBOARD_RESULTS_DIR</code> to <code>--logdir</code> parameter of the <code>tensorboard</code> command. Please create a new cell and run the following command to start the tensorboard.</p> <pre><code>tensorboard --logdir \"$tensorboard_dir\"\n</code></pre> <p>Afterward, the tensorboard interface will be displayed inside the notebook cell.</p>"},{"location":"user-guide/researcher/tensorboard/#using-tensorboard","title":"Using Tensorboard","text":"<p>After launching the tensorboard it will open a interface. If you haven't run your experiment yet, tensorboard will say <code>No dashboard is active for the current data set.</code> as seen in the following image. It is because there is no logs file that has been written in the <code>runs</code> directory.</p> <p></p> <p>After running your experiment, it will start to write loss values into tensorboard log files. You can refresh the tensorboard by clicking the refresh button located at the right top menu of the tensorboard view.</p> <p></p> <p>By default, the tensorboard doesn't set a time period to refresh the scalar values on the interface. You can click the gear button at the top right corner to set reload period to update the interface. The minimum reload period is 30 seconds. It means that the tensorboard interface will refresh itself every 30 seconds.</p> <p></p>"},{"location":"user-guide/researcher/tensorboard/#conclusions","title":"Conclusions","text":"<p>You can visit tensorboard documentation page to have more information about using tensorboard. Tensorboard can be used for all training plans provided by Fed-BioMed (including Pytorch an scikit-Learn training plans). Currently, in Fed-BioMed, tensorboard has been configured to display only loss values during training in each node. In the future, there might be extra indicators / statistics such as accuracy. Stay tuned!</p>"},{"location":"user-guide/researcher/training-data/","title":"Loading Dataset for Training","text":"<p>Datasets in the nodes are saved on the disk. Therefore, before the training, each node should load these datasets from  the file system. Since the type of datasets (image, tabular, etc.) and the way of loading might vary from one to  another, the user (researcher) should define a method called <code>training_data</code>. The method <code>training_data</code> is mandatory  for each training plan (<code>TrochTrainingPlan</code> and <code>SkLearnSGDModel</code>). If it is not defined nodes will return an error at  the very beginning of the first round. </p>"},{"location":"user-guide/researcher/training-data/#defining-the-method-training-data","title":"Defining The Method Training Data","text":"<p>The method <code>training_data</code> defines the logic related to loading data on the node. In particular,  it defines:</p> <ul> <li>the type of data and the <code>Dataset</code> class</li> <li>any preprocessing (either data transforms, imputation, or augmentation)</li> <li>also implicitly defines the <code>DataLoader</code> for iterating over the data </li> </ul> <p>This method takes no inputs and returns a <code>DataManager</code>,  therefore its signature is:</p> <pre><code>def training_data(self) -&gt; fedbiomed.common.data.DataManager\n</code></pre> <p>The <code>training_data</code> method is always part of the training plan, as follows:</p> <pre><code>from fedbiomed.common.training_plans import TorchTrainingPlan\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def __init__(self):\n        pass\n        # ....\n    def training_data(self):\n        pass\n</code></pre> <p>For details on how arguments are passed to the data loader, please refer to the section below  Passing arguments to data loaders.</p>"},{"location":"user-guide/researcher/training-data/#the-datamanager-return-type","title":"The <code>DataManager</code> return type","text":"<p>The method <code>training_data</code> should always return <code>DataManager</code> of Fed-BioMed defined in the module  <code>fedbiomed.common.data.DataManager</code>. <code>DataManager</code> has been designed for managing different types of data objects for  different types of training plans. It is also responsible for splitting a given dataset into training and validation if  model validation is activated in the experiment. </p> <p>What is a <code>DataManager</code>?</p> <p>A <code>DataManager</code> is a Fed-BioMed concept that makes the link between a <code>Dataset</code> and the corresponding <code>DataLoader</code>. It has a generic interface that is framework-agnostic (Pytorch, sklearn, etc...)</p> <p><code>DataManager</code> takes two main input arguments as <code>dataset</code> and <code>target</code>. <code>dataset</code> should be an instance of one of PyTorch <code>Dataset</code>, Numpy <code>ndarray</code>, <code>pd.DataFrame</code> or <code>pd.Series</code>. The argument <code>target</code> should be an instance of one of Numpy <code>ndarray</code>,  <code>pd.DataFrame</code> or <code>pd.Series</code>. By default, the argument <code>target</code> is <code>None</code>. If <code>target</code> is <code>None</code> the data manager  considers that the <code>dataset</code> is an object that includes both input and target variables. This is the case where  the dataset is an instance of the PyTorch dataset. If <code>dataset</code> is an instance of Numpy <code>Array</code> or Pandas <code>DataFrame</code>,  it is mandatory to provide the <code>target</code> variable. </p> <p>As it is mentioned, <code>DataManager</code> is capable of managing/configuring datasets/data-loaders based on the training plans  that are going to be used for training. This configuration is necessary since each training plan requires different  types of data loader/batch iterator. </p>"},{"location":"user-guide/researcher/training-data/#defining-training-data-in-different-training-plans","title":"Defining Training Data in Different Training Plans","text":""},{"location":"user-guide/researcher/training-data/#defining-training-data-for-pytorch-based-training-plans","title":"Defining Training Data for PyTorch Based Training Plans","text":"<p>In the following code snippet, <code>training_data</code> of PyTorch-based training plan returns a <code>DataManager</code> object instantiated  with <code>dataset</code> and <code>target</code> as <code>pd.Series</code>. Since PyTorch-based training requires a PyTorch <code>DataLoader</code>, <code>DataManager</code>  converts <code>pd.Series</code> to a proper <code>torch.utils.data.Dataset</code> object and create a PyTorch <code>DataLoader</code> to pass it to the  training loop on the node side. </p> <pre><code>import pandas as pd\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_model(self):\n        # ....\n    def init_dependencies(self):\n        # ....\n    def init_optimizer(self):\n        # ....\n\n    def training_data(self):\n        feature_cols = self.model_args()[\"feature_cols\"]\n        dataset = pd.read_csv(self.dataset_path, header=None, delimiter=',')\n        X = dataset.iloc[:,0:feature_cols].values\n        y = dataset.iloc[:,feature_cols]\n        return DataManager(dataset=X, target=y.values, )\n</code></pre> <p>It is also possible to define a custom PyTorch <code>Dataset</code> and use it in the <code>DataManager</code> without declaring the argument <code>target</code>. </p> <p><pre><code>import pandas as pd\nfrom torch.utils.data import Dataset\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\n\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    class CSVDataset(Dataset):\n        \"\"\" Cusotm PyTorch Dataset \"\"\"\n        def __init__(self, dataset_path, features):\n            self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)\n            x_train = self.input_file.iloc[:,0:features].values\n            y_train = self.input_file.iloc[:,features].values\n            self.X_train = torch.from_numpy(x_train).float()\n            self.Y_train = torch.from_numpy(y_train).float()\n\n        def __len__(self):            \n            return len(self.Y_train)\n\n        def __getitem__(self, idx):\n            return self.X_train[idx], self.Y_train[idx]\n\n    def training_data(self): \n        feature_cols = self.model_args()[\"feature_cols\"]    \n        dataset = self.CSVDataset(self.dataset_path, feature_cols)\n        loader_kwargs = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset=dataset, **loader_kwargs)\n</code></pre> In the code snippet above, <code>loader_kwargs</code> contains the arguments that are going to be used while creating a  PyTorch <code>DataLoader</code>.</p>"},{"location":"user-guide/researcher/training-data/#defining-training-data-for-sklearn-based-training-plans","title":"Defining Training Data for SkLearn Based Training Plans","text":"<p>The operations in the <code>training_data</code> for SkLearn based training plans are not much different than<code>TorchTrainingPlan</code>.  Currently, SkLearn based training plans do not require a data loader for training. This means that all samples will be  used for fitting the model. That's why passing <code>**loader_args</code> does not make sense for SkLearn based training plans. These arguments will be ignored even if they are set. </p> <pre><code>import pandas as pd\nfrom fedbiomed.common.training_plans import FedPerceptron\nfrom fedbiomed.common.data import DataManager\n\nclass SGDRegressorTrainingPlan(FedPerceptron):\n\n    def training_data(self):\n        num_cols = self.model_args()[\"number_cols\"]\n        dataset = pd.read_csv(self.dataset_path, header=None, delimiter=',')\n        X = dataset.iloc[:,0:num_cols].values\n        y = dataset.iloc[:,num_cols]\n        return DataManager(dataset=X, target=y.values, batch_size)\n</code></pre>"},{"location":"user-guide/researcher/training-data/#preprocessing-for-data","title":"Preprocessing for Data","text":"<p>Since the method <code>training_data</code> is defined by the user, it is possible to do preprocessing before creating the  <code>DataManager</code> object. In the code snippet below, a preprocess for normalization is shown for the dataset MNIST.</p> <pre><code>def training_data(self):\n    # Custom torch Dataloader for MNIST data\n    transform = transforms.Compose([transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))])\n    dataset_mnist = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n    train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n    return DataManager(dataset=dataset_mnist, **train_kwargs)\n</code></pre> <p>Training and validation partitions are created on the node side using returned <code>DataManager</code> object. Therefore,     preprocessing in <code>training_data</code> will be applied for both validation and training data.</p>"},{"location":"user-guide/researcher/training-data/#data-loaders","title":"Data Loaders","text":"<p>A <code>DataLoader</code> is a class that takes care of handling the logic of iterating over a certain dataset.  Thus, while a <code>Dataset</code> is concerned with loading and preprocessing samples one-by-one, the <code>DataLoader</code> is responsible for:</p> <ul> <li>calling the dataset's <code>__getitem__</code> method when needed</li> <li>collating samples in a batch</li> <li>shuffling the data at every epoch</li> <li>in general, managing the actions related to iterating over a certain dataset</li> </ul>"},{"location":"user-guide/researcher/training-data/#passing-arguments-to-data-loaders","title":"Passing arguments to Data Loaders","text":"<p>All of the key-value pairs contained in the <code>loader_args</code> sub-dictionary of <code>training_args</code> are passed as keyword arguments to the data loader. Additionally, any keyword arguments passed to the <code>DataManager</code> class inside the <code>training_data</code> function are also passed as keyword arguments to the data loader.</p> <p>For example, the following setup:</p> <pre><code>class MyTrainingPlan(TorchTrainingPlan):\n    # ....\n    def training_data(self):\n        dataset = MyDataset()\n        return DataManager(dataset, shuffle=True)\n\ntraining_args = {\n    'loader_args': {\n        'batch_size': 5,\n        'drop_last': True\n    }\n}\n</code></pre> <p>Leads to the following data loader definition:</p> <pre><code>loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)\n</code></pre> <p>Double-specified loader arguments</p> <p>Keyword arguments passed to the <code>DataManager</code> class take precedence over arguments with the same name provided in the <code>loader_args</code> dictionary.</p> <p>For PyTorch and scikit-learn experiments, the <code>DataLoaders</code> have been heavily inspired by the <code>torch.utils.data.DataLoader</code>  class, so please refer to that documentation for the meaning of the supported keyword arguments.</p>"},{"location":"user-guide/researcher/training-data/#conclusion","title":"Conclusion","text":"<p><code>training_data</code> should be provided in each training plan. The way it is defined is almost the same for each framework's  training plan as long as the structure of the datasets is simple. Since the method is defined by users, it provides  flexibility to load and pre-process complex datasets distributed on the nodes. However, this method will be executed  on the node side. Therefore, typos and lack of arguments may cause errors in the nodes even if it does not create any  errors on the researcher side.</p>"},{"location":"user-guide/researcher/training-plan/","title":"The Training Plan","text":"<p>A training plan is a class that defines the four main components of federated model training: the data, the model, the loss and the optimizer. It is responsible for providing custom methods allowing every node to perform the training.  In Fed-BioMed, you will be required to define a training plan class before submitting a federated training experiment.  You will do so by sub-classing one of the base training plan classes provided by the library, and overriding certain methods to suit your needs as explained below. The code of the whole training plan class is shipped to the nodes, meaning that you may define custom classes and functions inside it, and re-use them within the training routine.</p> <p>Training Plans</p> <p>A Training Plan contains the recipe for executing the training loop on the nodes. It defines: the data, the model, the loss function, and the optimizer. The code in the training plan is shipped in its entirety to the nodes, where its different parts are executed at different times during the training loop.</p>"},{"location":"user-guide/researcher/training-plan/#the-trainingplan-class","title":"The <code>TrainingPlan</code> class","text":"<p>Fed-BioMed provides a base training plan class for two commonly-used ML frameworks: PyTorch (<code>fedbiomed.common.training_plans.TorchTrainingPlan</code>) and scikit-learn (<code>fedbiomed.common.training_plans.SKLearnTrainingPlan</code>). Therefore, the first step of the definition of your federated training experiment will be to define a new training plan class that inherits from one of these.</p>"},{"location":"user-guide/researcher/training-plan/#pytorch-training-plan","title":"Pytorch Training Plan","text":"<p>The interfaces for the two frameworks differ quite a bit, so let's start by taking the example of PyTorch:</p> <pre><code>from fedbiomed.common.training_plans import TorchTrainingPlan\n\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    pass\n</code></pre> <p>The above example will not lead to a meaningful experiment, because we need to provide at least the following information to complete our training plan:</p> <ul> <li>a model instance</li> <li>an optimizer instance</li> <li>a list of dependencies (i.e. modules to be imported before instantiating the model and optimizer)</li> <li>how to load the training data (and potential preprocessing)</li> <li>a loss function</li> </ul> <p>Following the PyTorch example, here is what the prototype of your training plan would look like: <pre><code>from fedbiomed.common.training_plans import TorchTrainingPlan\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args):\n        # defines and returns a model\n        pass\n\n    def init_optimizer(self, optimizer_args):\n        # defines and returns an optimizer\n        pass\n\n    def init_dependencies(self):\n        # returns a list of dependencies\n        pass\n\n    def training_data(self):\n        # returns a Fed-BioMed DataManager object\n        pass\n\n    def training_step(self, data, target):\n        # returns the loss\n        pass\n</code></pre></p>"},{"location":"user-guide/researcher/training-plan/#scikit-learn-training-plan","title":"Scikit-learn Training Plan","text":"<p>In the case of scikit-learn, Fed-BioMed already does a lot of the heavy lifting for you by providing the <code>FedPerceptron</code>, <code>FedSGDClassifier</code> and <code>FedSGDRegressor</code> classes as training plans. These classes already take care of the model and loss functions for you, so you only need to define how the data will be loaded, how to optimize the model and the dependencies. For example, in the case of <code>FedSGDClassifier</code>:</p> <pre><code>from fedbiomed.common.training_plans import FedSGDClassifier\n\nclass MyTrainingPlan(FedSGDClassifier):\n    def training_data(self):\n        # returns a Fed-BioMed DataManager object\n        pass\n\n    def init_optimizer(self, optimizer_args):\n        # defines and returns an optimizer: only declearn optimizer are permitted here\n        pass\n\n    def init_dependencies(self):\n        # returns a list of dependencies\n        pass\n</code></pre>"},{"location":"user-guide/researcher/training-plan/#_1","title":"The Training Plan of Fed-BioMed","text":"<p>Definition of <code>__init__</code> is discouraged for all training plans</p> <p>As you may have noticed, none of the examples defined an <code>__init__</code> function for the training plan. This is on purpose! Overriding <code>__init__</code> is not required, and is actually discouraged, as it is reserved for the library's internal use. If you decide to override it, you do it at your own risk!</p>"},{"location":"user-guide/researcher/training-plan/#accessing-the-training-plan-attributes","title":"Accessing the Training Plan attributes","text":"<p>Fed-BioMed provides the following getter functions to access Training Plan attributes:</p> attribute function TorchTrainingPlan SKLearnTrainingPlan notes model <code>model()</code> you may not dynamically reassign a model. The instance of the model is created at initialization by storing the output of the <code>init_model</code> function. optimizer <code>optimizer()</code> you may not dynamically reassign an optimizer. The instance of the optimizer is created at initialization by storing the output of the <code>init_optimizer</code> function. model arguments <code>model_args()</code> training arguments <code>training_args()</code> optimizer arguments <code>optimizer_args()</code>"},{"location":"user-guide/researcher/training-plan/#_2","title":"The Training Plan of Fed-BioMed","text":"<p>Lifecycle of Training Plan Attributes</p> <p>The attributes in the table above will not be available during the <code>init_model</code>, <code>init_optimizer</code> and <code>init_dependencies</code> functions, as they are set just after initialization. You may however use them in the definition of <code>training_data</code>, <code>training_step</code> or <code>training_routine</code>.</p>"},{"location":"user-guide/researcher/training-plan/#defining-the-training-data","title":"Defining the training data","text":"<p>The method <code>training_data</code> defines how datasets should be loaded in nodes to make them ready for training. In both PyTorch and scikit-learn training plans, you are required to define a <code>training_data</code> method with the following specs:</p> <ol> <li>takes as input a <code>batch_size</code> parameter</li> <li>returns a <code>fedbiomed.common.data.DataManager</code> object</li> <li>inside the method, a dataset is instantiated according to the data type that you wish to use (one of <code>torch.Dataset</code>,    <code>numpy.ndarray</code> or a <code>*Dataset</code> class from the <code>fedbiomed.common.data</code> module)</li> <li>the dataset is used to initialize a <code>DataManager</code> class to be returned</li> </ol> <p>The signature of the <code>training_data</code> function is then: <pre><code>def training_data(self) -&gt; DataManager:\n</code></pre></p> <p>You can read the documentation for training data to learn more about the <code>DataManager</code> class and various use cases.</p>"},{"location":"user-guide/researcher/training-plan/#initializing-the-model","title":"Initializing the model","text":"<p>In Pytorch training plans, you must also define a <code>init_model</code> function with the following signature: <pre><code>def init_model(self, model_args: Dict[str, Any]) -&gt; torch.nn.Module:\n</code></pre></p> <p>The purpose of <code>init_model</code> is to return an instance of a trainable PyTorch model. Since the definition of such models can be quite large, a common pattern is to define the neural network class inside the training plan namespace, and simply instantiate it within <code>init_model</code>. This also allows to minimize the amount of adjustments needed to go from local PyTorch code to its federated version. Remember that only the code defined inside the training plan namespace will be shipped to the nodes for execution, so you may not use classes that are defined outside of it.</p> <p>The Pytorch neural network class that you define must satisfy the following constraints: 1. it should inherit from <code>torch.nn.Module</code> 2. it should implement a <code>forward</code> method that takes a <code>torch.Tensor</code> as input and returns a <code>torch.Tensor</code> Note that inheriting from <code>torch.nn.Sequential</code> and using the default <code>forward</code> method would also respect the conditions above.</p> <p>The <code>model_args</code> argument is a dictionary of model arguments that you may provide to the <code>Experiment</code> class and that will be automatically passed to the <code>init_model</code> function internally. If you followed the suggested pattern of defining the model class within the training plan namespace, you can easily adapt the model's constructor to make use of any model arguments that you wish to define.</p> <p>The example below, adapted from our getting started notebook, shows the suggested pattern, the use of <code>init_model</code>, and the use of <code>model_args</code>.</p> <pre><code>import torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\n\n\n# Here we define the model to be used.\n# You can use any class name (here 'Net')\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    # Defines and return model\n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n\n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n\n            fc_hidden_layer_size = model_args.get('fc_hidden_size', 128)\n\n            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            self.fc1 = nn.Linear(9216, fc_hidden_layer_size)\n            self.fc2 = nn.Linear(fc_hidden_layer_size, 10)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.dropout2(x)\n            x = self.fc2(x)\n\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_data(self):\n        pass\n\n    def training_step(self, data, target):\n        pass\n\n    def init_optimizer(self, optimizer_args):\n        pass\n\n    def init_dependencies(self):\n        pass\n</code></pre>"},{"location":"user-guide/researcher/training-plan/#defining-the-optimizer","title":"Defining the optimizer","text":""},{"location":"user-guide/researcher/training-plan/#optimizer-in-pytorch-training-plans","title":"Optimizer in PyTorch Training Plans","text":"<p>For Pytorch training plans, you must also define a <code>init_optimizer</code> function with the following signature:</p> <pre><code>def init_optimizer(self, optimizer_args: Dict[str, Any]) -&gt; Union[torch.optim.Optimizer, fedbiomed.common.optimizer.Optimizer]:\n</code></pre> <p>The purpose of <code>init_optimizer</code> is to return an instance of a PyTorch optimizer or a <code>Fed-BioMed</code> optimizer powered with <code>declearn</code> optimizzation modules. You may instantiate a \"vanilla\" optimizer directly from <code>torch.optim</code>, or follow a similar pattern to <code>init_model</code> by defining a custom optimizer class within the training plan namespace.</p>"},{"location":"user-guide/researcher/training-plan/#_3","title":"The Training Plan of Fed-BioMed","text":"<p>The output of <code>init_optimizer</code> must be either a <code>torch.optim</code> type or a <code>fedbiomed.common.optimizer.Optimizer</code></p> <p>The output of <code>init_optimizer</code> must be either a vanilla optimizer provided by the <code>torch.optim</code> module, or a class that inherits from <code>torch.optim.Optimizer</code>, or a <code>fedbiomed.common.optimizer.Optimizer</code>, populated with <code>declearn</code>'s <code>OptiModules</code> and <code>Regularizers</code>.</p> <p>About declearn</p> <p><code>declearn</code> provides a cross framework optimizers that can be used regardless of the machine learning framework. It also provides well known federated learning algorithms such as <code>Scaffold</code>. For further details on <code>declearn</code>'s <code>Optimizer</code>, please visit the following webpage.</p> <p>Similarly, the <code>optimizer_args</code> follow the same pattern as <code>model_args</code> described above. Note that the learning rate will always be included in the optimizer arguments with the key <code>lr</code>.</p> <p>A pretty straightforward example can be again found in the getting started notebook</p> <pre><code>def init_optimizer(self, optimizer_args):\n    return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n</code></pre>"},{"location":"user-guide/researcher/training-plan/#optimizer-in-scikit-learn-training-plans","title":"Optimizer in scikit-learn Training Plans","text":"<p>In Scikit-Learn <code>Training Plans</code>, only <code>fedbimed.common.optimizer.Optimizer</code> optimizers can be defined in the <code>init_optimizer</code> method. Hence, its signature is:</p> <pre><code>def init_optimizer(self, optimizer_args: Dict[str, Any]) -&gt; fedbiomed.common.optimizer.Optimizer:\n</code></pre>"},{"location":"user-guide/researcher/training-plan/#defining-the-loss-function","title":"Defining the loss function","text":"<p>The PyTorch training plan requires you to define the loss function via the <code>training_step</code> method, with the following signature:</p> <pre><code>def training_step(self, data, target) -&gt; float:\n</code></pre> <p>The <code>training_step</code> method of the training class defines how the cost is computed by forwarding input values through the network and using the loss function. It should return the loss value. By default, it is not defined in the parent <code>TrainingPlan</code> class: it should be defined by the researcher in his/her model class, same as the <code>forward</code> method. An example of training step for PyTorch is shown below.</p> <pre><code>    def training_step(self, data, target):\n        output = self.forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</code></pre>"},{"location":"user-guide/researcher/training-plan/#type-of-data-and-target","title":"Type of <code>data</code> and <code>target</code>","text":"<p>The <code>training_step</code> function takes as input two arguments, <code>data</code> and <code>target</code>, which are obtained by cycling through the dataset defined in the <code>training_data</code> function. There is some flexibility concerning what type of variables they might be.</p> <p>In a Pytorch training plan, the following data types are supported:</p> <ul> <li>a <code>torch.Tensor</code></li> <li>a collection (a <code>dict</code>, <code>tuple</code> or <code>list</code>) of <code>torch.Tensor</code></li> <li>a recursive collection of collections, arbitrarily nested, that ultimately contain <code>torch.Tensor</code> objects</li> </ul> <p>Be aware of the data types in your dataset</p> <p>It is ultimately your responsibility to write the code for <code>training_step</code> that correctly handles the data types returned by the <code>__getitem__</code> function of the dataset you are targeting. Be aware of the specifics of your dataset when writing this function.</p>"},{"location":"user-guide/researcher/training-plan/#adding-dependencies","title":"Adding Dependencies","text":"<p>By dependencies we mean here the python modules that are necessary to build all the various elements of your training plan on the node side. The method <code>init_dependencies</code> allows you to indicate modules that are needed by your model class, with the following signature:</p> <pre><code>def init_dependencies(self) -&gt; List[str]:\n</code></pre> <p>Each dependency should be defined as valid import statement in a string, for example <code>from torch.optim import Adam</code> or <code>import torch</code>, or <code>from declearn.optimizer.modules import AdamModule</code> (for its <code>declearn</code> alternative). You must specify dependencies for any python module that you wish to use, regardless of whether it is for the data, optimizer, model, etc...</p>"},{"location":"user-guide/researcher/training-plan/#training_routine","title":"<code>training_routine</code>","text":"<p>The training routine is the heart of the training plan. This method performs the model training loop, based on given model and training arguments. For example, if the model is a neural network based on the PyTorch framework, the training routine is in charge of performing the training part over looping epochs and batches. If the model is a Scikit-Learn model, it fits the model by the given ML method and Scikit-Learn does the rest. The training routine is executed by the nodes after they have received a train request from the researcher and downloaded the training plan file.</p> <p>Overriding <code>training_routine</code> is discouraged</p> <p>Both PyTorch and scikit-learn training plans already implement a <code>training_routine</code>, that internally uses the <code>training_step</code> provided by you to compute the loss function (only in the PyTorch case). Overriding this default routine is strongly discouraged, and you may do so only at your own risk.</p> <p>As you can see from the following code snippet, the training routine requires some training arguments such as <code>epochs</code>, <code>lr</code>, <code>batch_size</code> etc. Since the <code>training_routine</code> is already defined by Fed-BioMed, you are only allowed to control the training process by changing these arguments. Modifying the training routine from the training plan class might raise unexpected errors.</p> <pre><code> def training_routine(self,\n                         epochs: int = 2,\n                         log_interval: int = 10,\n                         lr: Union[int, float] = 1e-3,\n                         batch_size: int = 48,\n                         batch_maxnum: int = 0,\n                         dry_run: bool = False,\n                         ... ):\n\n        # You can see details from `fedbiomed.common.torchnn`\n        # .....\n\n        for epoch in range(1, epochs + 1):\n            training_data = self.training_data()\n            for batch_idx, (data, target) in enumerate(training_data):\n                self.train()\n                data, target = data.to(self.device), target.to(self.device)\n                self.optimizer.zero_grad()\n                res = self.training_step(data, target)\n                res.backward()\n                self.optimizer.step()\n\n                #.....\n</code></pre>"},{"location":"user-guide/researcher/training-plan/#exporting-and-importing-model","title":"Exporting and importing model","text":"<p>Each training plan provides export and import functionality.</p> <ul> <li>Export facility is used for saving model parameters to a file after training the model in Fed-BioMed, so it can be used in another software (eg for inference).</li> <li>Import facility is used for loading model parameters from a file, for example to specialize with Fed-BioMed a model pre-trained with another software (transfer learning) or a previous Fed-BioMed run.</li> </ul> <p>Exports and imports are handled through the <code>Experiment</code> interface. <code>Experiment</code> interface will initialize the model for you, by calling internally <code>Training Plan</code> methods <code>init_method</code> and <code>post_init</code>. See example below for an instantiated <code>Experiment</code> object named <code>exp</code>.</p> <p>To save model to file <code>/path/to/file</code> use:</p> <pre><code>exp.training_plan().export_model('/path/to_file')\n</code></pre> <p>To load model from file <code>/path/to/file</code> use:</p> <pre><code>exp.training_plan().import_model('/path/to_file')\n</code></pre> <p>Of course, loaded model needs to be identical to the training plan's model.</p> <p><code>export_model()</code> and <code>import_model()</code> actions depends on framework</p> <p>With PyTorch, these methods save and load the model parameters (<code>model.state_dict()</code>) with <code>torch.save()</code>/<code>torch.load()</code> as it is a common practice</p> <p>With scikit-learn, these methods save and load the whole model with <code>joblib.dump()</code>/<code>joblib.load()</code> as it is also a common practice</p> <p>Security notice</p> <p>Only use <code>import_model()</code> with a trusted model file (trained by a trusted source, transmitted via secure channel).</p> <p>In both PyTorch and scikit-learn, the model saving and loading facility are based on pickle. While it is the recommended way of saving models in these frameworks, a malicious pickle model can execute arbitrary code on your machine when loaded. Thus make sure you are loading a model from a reliable source.</p> <p>Usage through <code>Experiment</code></p> <p>Both exports and imports must be used through Experiment interface. Indeed, <code>Experiment</code> class has methods to load Training Plans and for initializing Model. Once the Model is initialized, you can use both <code>export_model</code> and <code>import_model</code> for saving model into a file and respectively load it from a file.</p>"},{"location":"user-guide/secagg/certificate-registration/","title":"DEPRECATED: Joye-Libert Registration of Certificate and Network Parameters of FL Parties","text":"<p>If Joye-Libert secagg scheme is activated, Fed-BioMed uses the MP-SPDZ library for conducting multi-party computation and also uses the MP-SPDZ network infrastructure for MPC, where each party runs an MP-SPDZ instance listening on an IP and port. In order to proceed with Multi-Party Computation (MPC) in federated experiments, each participating party is required to register the network parameters of all the other participating parties. The registration must be done before the experiment.</p> <p>Attention</p> <p>Certificate registration and IP configuration are necessary only for Joye-Libert secure aggregation scheme.</p>"},{"location":"user-guide/secagg/certificate-registration/#registration-through-cli","title":"Registration Through CLI","text":"<p>Fed-BioMed CLI provide options to manage network parameters registration of other parties easily. Registration process involves two steps as:</p> <ol> <li>Getting the certificate and network details of current component and send it to other parties.</li> <li>Registering certificates and network details received from other parties.</li> </ol>"},{"location":"user-guide/secagg/certificate-registration/#retrieve-certificate-and-registration-instructions","title":"Retrieve certificate and registration instructions","text":"<p>The option <code>registration-instructions</code> facilitates the certificate registration process by providing the necessary details and commands that must be executed by other parties to register the component.</p> <p>The command below generates registration instructions to assist in the process of registering the researcher component in the participating parties. This command must be executed by the researcher component, and the instructions sent to other parties for registration.</p> <pre><code>fedbiomed researcher certificate registration-instructions\n</code></pre> <p>the output is:</p> <pre><code>Hi There!\n\n\nPlease find following certificate to register\n\n-----BEGIN CERTIFICATE-----\nMIIDBzCCAe+gAwIBAgIDAPQiMA0GCSqGSIb3DQEBCwUAMEYxCjAIBgNVBAMMASox\nODA2BgNVBAoML3Jlc2VhcmNoZXJfZTFjNWMxMDEtMGM3OS00M2IxLThiZjEtZDcw\nYjA2YjkxODMwMB4XDTIzMDQwNDEwMTcxN1oXDTI4MDQwMjEwMTcxN1owRjEKMAgG\nA1UEAwwBKjE4MDYGA1UECgwvcmVzZWFyY2hlcl9lMWM1YzEwMS0wYzc5LTQzYjEt\nOGJmMS1kNzBiMDZiOTE4MzAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIB\nAQC+NQU1HzoNJFWguQY8W97oNWWpkZOtXQE/C63JStZoepbos5nsHpMTZ67Qihfu\nBdCe7XNBaZwzTxO3xjKByWocnw+UaadSvNK5zZZNqGiAj3P9F2q1duaWXMldtK/Y\nl9bRAW6pp4ri/lnAU5gADDcV7M79pVxhfhMI3xKFP03CA0OqQnXABgZheMCWmtll\nx8DVEsKj4jCZSaUqMUHDpxX3l1eUPeDryG3kpcWT28dElBSAynRQznq3StTNghC8\nNPMWUQR8uU5HG13n9Xv8+TBZ33b4iXE5Ei24IleFeTJG0PjtRGY6KvEkFKxGvqYs\noKAwpc7u5v0QeDjDeNDrSUhJAgMBAAEwDQYJKoZIhvcNAQELBQADggEBAB5WoUo2\nq4VSExJoIpIDEwCimcEKz/pHX9IYBgLGluzGUPfFfN+cjUsmKjzXtIqTRau+LtVO\nV/TZ7jRbhTZ7A3FZDrmsE/FOENjUQjFeHIW1Ombqso8BmBfgmn84UF/i1q9rieqZ\njMd+0WppJGp0JNV33mV+veuVbZFaFadRznQ/yUflBcYp0Hfji9/ZU74ivaTdl6vF\nLlSIEKmPyHGx+dHub4uzUyfAHlCTxsaOaZzhc8BCR+qbJ499WvKIO5x02r5+mwqN\nIe5FpFt8M14gC+YEfE/KRSOsRlhKHE+wThdNqEC9UpePpkHdS1/9vNs3ql+PojI8\nojZqtVij//Fp8S4=\n-----END CERTIFICATE-----\n\nPlease follow the instructions below to register this certificate:\n\n\n 1- Copy certificate content into a file e.g 'Hospital1.pem'\n 2- Change your directory to 'fedbiomed' root\n 2- Run: \"fedbiomed [node | researcher] certificate register -pk [PATH WHERE CERTIFICATE IS SAVED] -pi researcher_e1c5c101-0c79-43b1-8bf1-d70b06b91830  --ip 193.0.0.1 --port 14002\"\n    Examples commands to use for VPN/docker mode:\n      fedbiomed node certificate register -pk ./etc/cert-secagg -pi researcher_e1c5c101-0c79-43b1-8bf1-d70b06b91830 --ip 193.0.0.1 --port 14002\n      fedbiomed researcher certificate register -pk ./etc/cert-secagg -pi researcher_e1c5c101-0c79-43b1-8bf1-d70b06b91830 --ip 193.0.0.1 --port 14002\n</code></pre> <p>The aforementioned instructions provide essential details for registering a party among the other participants in a federated experiment. The information includes the certificate, component identification number (<code>-pi</code>), IP address (<code>--ip</code>), and port (<code>--port</code>).</p> <p>Certificates should be shared outside Fed-BioMed through a trusted channel.</p> <p>Fed-BioMed does not provide a way to exchange certificate and network parameters internally. Therefore, parameters should be shared using third party trusted channels such as e-mail or other messaging channels.</p>"},{"location":"user-guide/secagg/certificate-registration/#registering-the-certificate","title":"Registering the certificate","text":"<p>Certificates of other parties should be registered with their component ID, IP and port information. Certificates must be copied and saved in a file. Then, the file path is given with the option <code>-pk</code>.</p> <pre><code>fedbiomed [node | researcher] certificate register -pk &lt;certificate-file-path&gt; -pi &lt;component-id&gt; --ip  &lt;IP&gt; --port &lt;PORT&gt;\"\n</code></pre> <p>One of <code>[node | researcher]</code> must be chosen according to component type that registers the certificate.</p>"},{"location":"user-guide/secagg/certificate-registration/#registration-through-gui","title":"Registration Through GUI","text":"<p>Currently, certificate registration is not supported through GUI.</p>"},{"location":"user-guide/secagg/certificate-registration/#certificate-registration-in-developmenttesting-mode","title":"Certificate registration in development/testing mode","text":"<p>Certificate registration is a lengthy procedure, as every network parameter must be registered by every other participating component. This process can be time-consuming when components are launched locally for testing or development purposes.</p> <p>However, Fed-BioMed CLI provides a magic script when all components run in development mode in the same clone. The script parses every configuration file created in the <code>etc</code> directory and registers all available parties automatically in every component.</p> <p>After all the components are created, please run the following command to complete certificate registration for development environment.</p> <pre><code>fedbiomed certificate-dev-setup\n</code></pre> <p>Important</p> <p>Secure aggregation setup requires at least 2 nodes and 1 researcher in the FL experiment.</p>"},{"location":"user-guide/secagg/configuration/","title":"Secure Aggregation Configuration","text":"<p>Secure aggregation is implemented in Fed-BioMed and can be activated or deactivated as an option through the configuration. Even if secure aggregation is not configured during the initial installation, Fed-BioMed still works as long as the researcher or node components don't use it.</p>"},{"location":"user-guide/secagg/configuration/#activating-deactivating-and-forcing-secure-aggregation","title":"Activating, Deactivating and Forcing Secure Aggregation","text":"<p>Nodes have the privilege of activating, deactivating, and enforcing secure aggregation. This means that model parameters can either be encrypted (required), optionally encrypted, or unencrypted. If a node requires model encryption and a training request from a researcher does not include secure aggregation context, the node will refuse training. If secure aggregation is allowed but not forced by the node, end-users are able to send training requests with or without secure aggregation.</p> <p>In a federated setup, if one of the nodes requires secure aggregation but the researcher does not activate it, the FL round fails. Please refer to the researcher secure aggregation interface for more details.</p> <p>Researcher</p> <p>Researcher configuration file does not have parameter regarding secure aggregation activation. However, secure aggregation context is managed through Experiment interface (class).</p> <p>Example: security section of the configuration file with secure aggregation optional.</p> <pre><code>[security]\nhashing_algorithm = SHA256\nallow_default_training_plans = True\ntraining_plan_approval = False\nsecure_aggregation = True\nforce_secure_aggregation = False\n</code></pre>"},{"location":"user-guide/secagg/configuration/#low-overhead-masking-lom","title":"Low-Overhead Masking (LOM)","text":"<p>LOM scheme is secure aggregation relies on pair-wise key setup using Deffie-Hellman algorithm. It is possible to directly activate or force secure aggregation on the node side as it is explained in the section  Activating, Deactivating and Forcing Secure Aggregation. Pair-wise keys will be setup by Fed-BioMed automatically honest-but-curious security model.</p> <p>Example: security section of the configuration file with secure aggregation mandatory.</p> <pre><code>[security]\nhashing_algorithm = SHA256\nallow_default_training_plans = True\ntraining_plan_approval = False\nsecure_aggregation = True\nforce_secure_aggregation = True\n</code></pre>"},{"location":"user-guide/secagg/configuration/#joye-libert-scheme-jls","title":"Joye-Libert Scheme (JLS)","text":"<p>JLS is an HE-based secure aggregation scheme that relies on generating keys for encryption and decryption. The keying material is automatically generated by Fed-BioMed in a secure manner using the Additive Secret Sharing algorithm, under the honest-but-curious researcher/server assumption. This secret sharing is conducted through encrypted node-to-node communication, ensuring that the server cannot decrypt the transmitted information.</p>"},{"location":"user-guide/secagg/introduction/","title":"Secure Aggregation","text":"<p>Fed-BioMed offers a secure aggregation framework where local model parameters of each node are encrypted before sending them to the researcher/aggregator for aggregation. Decryption of the parameters is tied to the execution of fixed computations. This guarantees that the local model parameters will remain private on aggregator level, and researcher (component) or/and end-user will only have the final decrypted aggregated parameters.</p>"},{"location":"user-guide/secagg/introduction/#available-secure-aggregation-schemes","title":"Available Secure Aggregation Schemes","text":"<p>Fed-BioMed supports two different secure aggregation schemes: Joye-Libert and Low-Overhead Masking (LOM). While Joye-Libert is an additive homomorphic encryption scheme, LOM is based on a masking approach.</p>"},{"location":"user-guide/secagg/introduction/#low-overhead-masking-lom","title":"Low-Overhead Masking (LOM)","text":"<p>LOM is a secure aggregation scheme based on masking. It protects user inputs by applying masks using pairwise keys agreed upon by participating nodes. These pairwise keys are applied to private inputs in such a way that when all user inputs are summed at the aggregator level, the masks cancel out. One advantage of this scheme is that it does not require a setup phase for each training round, leading to faster training rounds.</p>"},{"location":"user-guide/secagg/introduction/#process-flow","title":"Process flow","text":"<p>LOM consists of three phases: setup, protect, and aggregate. The setup phase is triggered by the first training request, the protect phase is applied on each node after each training, and aggregation is performed on the researcher's side over protected/encrypted model weights or other inputs, such as auxiliary variables, depending on the type of training, optimizer, etc.</p> <p>During the setup phase, all participating nodes agree on pairwise secrets using the Diffie-Hellman key exchange. These pairwise key agreements are established once per experiment, meaning that the agreed-upon keys can be used for multiple rounds within the same experiment. If a new node joins the experiment, all other participating nodes perform a key agreement with the new node. All these operations are coordinated by the researcher component.</p> <p>Before the first training round, the researcher checks if the secure aggregation context (pairwise keys) is set up on the nodes. If not, the researcher sends secure aggregation setup request to the nodes. With this request, nodes receive a list of all nodes that will participate in the training, as well as the scheme that will be used for secure aggregation. Depending on the secure aggregation scheme (e.g., LOM), each node sends key agreement requests to the other participating nodes to create the pairwise secrets that will be used for the training.</p> <p>Once all pairwise secrets are established, the researcher sends a train request to start the training. After each round of training, model weights are masked/encrypted using the pairwise keys (LOM scheme), so that summing all encrypted model weights of all the nodes will result in the unmasked aggregated model weights.</p>"},{"location":"user-guide/secagg/introduction/#joye-libert-secure-aggregation-scheme","title":"Joye-Libert Secure Aggregation Scheme","text":"<p>Secure aggregation in Fed-BioMed is achieved through the use of a mix of the Joye-Libert (JL) aggregation scheme and the Shamir multi-party computation (MPC) protocol. JL is an additively homomorphic encryption (AHE) technique that encrypts model parameters locally using private and unique 2048-bit keys. The sum of encrypted model parameters can only be decrypted using the sum of private keys from each node that participate in the federated learning (FL) experiment. However, the encryption key used on each node is private and not shared with other parties or the central aggregator. Therefore, server key is calculated using MPC without revealing user (node) keys (server-key shares).</p>"},{"location":"user-guide/secagg/introduction/#additive-secret-sharing-protocol","title":"Additive Secret Sharing Protocol","text":"<p>Additive secret sharing is a protocol used to compute the server key, which is equal to the negative sum of the nodes' keys and is used for decryption on the researcher component. This secret-sharing-based MPC algorithm computes the sum of the nodes' keys without revealing the nodes' private keys to the aggregator.</p>"},{"location":"user-guide/secagg/introduction/#technologies","title":"Technologies","text":""},{"location":"user-guide/secagg/introduction/#fault-tolerant-secure-aggregation","title":"Fault Tolerant Secure Aggregation","text":"<p>Fed-BioMed uses a modified version of  Joye-Libert scheme implementation from repository fault-tolerant-secure-agg.</p>"},{"location":"user-guide/secagg/introduction/#process-flow_1","title":"Process-flow","text":"<p>Since FL experiments are launched through researcher component, activating secure aggregation and setting up necessary context is done through <code>Experiment</code> class of researcher component. However, the status of the secure aggregation can be managed by node as well: node owner can disable, enable or force secure aggregation (see secure aggregation node configuration for more details).</p>"},{"location":"user-guide/secagg/introduction/#1-public-parameter-biprime","title":"1. Public Parameter Biprime","text":"<p><code>Biprime</code> is multiplication of two prime numbers. Prime number is public while prime shares are private and used for <code>Biprime</code> calculation. Fed-BioMed uses securely generated default static biprime which is located in <code>envs/common/default_primes/biprime0.json</code>.</p>"},{"location":"user-guide/secagg/introduction/#2-generating-random-key-that-are-double-the-length-of-biprime","title":"2. Generating random key that are double the length of biprime","text":"<p>Researcher sends a request for generating private key of each node and the corresponding server key for researcher component. Each node generates random private keys.</p> <p>Key-size</p> <p>Key size depends on biprime number that is used for secure aggregation. Maximum key-size should be less or equal the double of biprime key-size.</p>"},{"location":"user-guide/secagg/introduction/#3-execute-additive-secret-sharing","title":"3. Execute Additive Secret Sharing","text":"<p>The nodes that receives secure aggregation setup request to generate keys to protect model parameters (or any other private value that can be aggregated) launches Additive Secret Sharing algorithm to calculate server key that is going to be used for aggregating private protected data.</p>"},{"location":"user-guide/secagg/introduction/#4-encrypting-model-parameters","title":"4. Encrypting model parameters","text":"<p>If secure aggregation is activated for the <code>Experiment</code>, the training request contains information about which secure aggregation context will be used for encryption. Once training is completed, model parameters are encrypted using biprime and the user (node)-key.</p>"},{"location":"user-guide/secagg/introduction/#5-decrypting-sum-of-encrypted-model-parameters","title":"5. Decrypting sum of encrypted model parameters","text":"<p>After the encryption is done and the encrypted model parameters are received by the researcher, all model parameters are aggregated using JL scheme. Aggregated parameters are clear text. Aggregation and decryption can not be performed independently. It is an atomic operation.</p> <p>Important</p> <p>Secure aggregation requires at least 3 parties in FL experiment with one researcher and 2 nodes.</p>"},{"location":"user-guide/secagg/introduction/#conclusions","title":"Conclusions","text":"<p>Joye-Libert was the first secure aggregation algorithm implemented in Fed-BioMed. Later, the LOM scheme was introduced to simplify certain operations, such as the pre-setup phase and speed up encryption processing. While there are similarities between the two, there are also key differences.</p> <p>In LOM, unlike Joye-Libert, secure aggregation does not require parties to perform certificate registration. Communication among the nodes is managed by the researcher using an honest-but-curious security model. This approach eliminates the need for the complicated and time-consuming pre-setup of Fed-BioMed nodes, where each party manually registers the certificates of other parties.</p> <p>Another difference between the two schemes is that Joye-Libert requires the server/aggregator to possess a key to aggregate encrypted model inputs. In contrast, LOM does not require the aggregator to have an encryption key; the sum of the encrypted inputs directly results in the sum of the inputs. This makes Joye-Libert preferable in scenarios where it is necessary to explicitly identify a party that is allowed to perform the aggregation. In LOM, any party with access to all the masked inputs can obtain the aggregated inputs. This is not a concern in setups where all parties have equal rights to access the aggregated inputs.</p> <p>Neither algorithm is tolerant to dropouts. However, in LOM, if a new node joins the next round of training, all other nodes perform pairwise key setup with the new node. If one or more nodes drop out, there is no need to re-establish pairwise keys. In Joye-Libert, regardless of whether a new node joins or some nodes drop out, all keys must be regenerated.</p> <p>In terms of encryption and aggregation processing time, LOM is significantly faster in most cross-silo federated learning setups, typically with increasing model parameters size and number of nodes no more than a few dozens.</p> <p>The security model implemented in Fed-BioMed's secure aggregation primarily targets the honest-but-curious parties' scenario, which applies to both algorithms.</p>"},{"location":"user-guide/secagg/introduction/#next-steps","title":"Next steps","text":"<ul> <li> <p>Joye-Libert Scheme (JLS) Configuration documentation for more detail about configuring Fed-BioMed   instances for secure aggregation.</p> </li> <li> <p>Activating secure aggregation for the training through researcher component.</p> </li> </ul>"},{"location":"user-guide/secagg/researcher-interface/","title":"Managing Secure Aggregation on Researcher Side","text":"<p>Researcher component is responsible for managing secure aggregation context setup that prepares necessary elements to apply secure aggregation over encrypted model parameters. Some nodes might require secure aggregation while some of them don't, and some others don't support secure aggregation. Therefore, end-user (researcher) should activate secure aggregation depending on all participating nodes configuration.</p>"},{"location":"user-guide/secagg/researcher-interface/#managing-secure-aggregation-through-experiment","title":"Managing secure aggregation through Experiment","text":""},{"location":"user-guide/secagg/researcher-interface/#activation","title":"Activation","text":"<p>By default, secure aggregation is deactivated in <code>Experiment</code> class. It can be activated by setting the <code>secagg</code> as <code>True</code>, and the default secure aggregation scheme is LOM.</p> <pre><code>from fedbiomed.researcher.federated_workflows import Experiment\nExperiment(\n    secagg=True\n)\n</code></pre> <p>Setting secagg <code>True</code> instantiates a <code>SecureAggregation</code> with default arguments as <code>timeout</code> and <code>clipping_range</code>.  However, it is also possible to create a secure aggregation instance by providing desired argument values.</p> <pre><code>from fedbiomed.researcher.federated_workflows import Experiment\nfrom fedbiomed.researcher.secagg import SecureAggregation\nExperiment(\n    #...\n    secagg=SecureAggregation(clipping_range=30),\n    #....\n)\n</code></pre> <p>Federated averaging</p> <p>Once the secure aggregation is activated, experiment doesn't use the <code>aggregator</code> parameter of the <code>Experiment</code> (eg <code>FedAverage</code>) for aggregation. Secure aggregation aggregates model parameters with its own federated average, but without weighting them. Therefore, using <code>num_updates</code> instead of <code>epochs</code> in <code>training_args</code> is strongly recommended for secure aggregation.</p> <p>The argument <code>scheme</code> of <code>SecureAggregation</code> allows to select secure aggregation scheme that is going to be used. However, schemes may require different pre or post configuration on the node side and researcher side. Therefore,  please carefully read the configuration guide before changing secure aggregation scheme.</p> <pre><code>from fedbiomed.researcher.secagg import SecureAggregation, SecureAggregationSchemes\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None,\n                 secagg=SecureAggregation(scheme=SecureAggregationSchemes.JOYE_LIBERT),\n                 # or custom SecureAggregation(active=&lt;bool&gt;, clipping_range=&lt;int&gt;)\n                 save_breakpoints=True)\n</code></pre>"},{"location":"user-guide/secagg/researcher-interface/#timeout","title":"Timeout","text":"<p>Secure aggregation setup starts specific processing in each Fed-BioMed component that participates in the federated training. However, these processes and communication delay might be longer or shorter than expected depending on number of nodes and communication bandwidth. Default timeouts cannot currently be configured through the user API, it is needed to edit the <code>researcher.secagg.SecaggContext</code> in the library for each component accordingly.</p>"},{"location":"user-guide/secagg/researcher-interface/#clipping-range","title":"Clipping Range","text":"<p>Encryption on the node-side is performed after the quantization of model weights/parameters. However, the maximum and minimum values of model parameters may vary depending on the technique used. Therefore, the clipping range of quantization depends on the model, data, or technique. The clipping range should always be greater than or equal to the maximum model weight value, but kept reasonably low.</p> <p>By default, the clipping range is set to 3. If the clipping range is exceeded while encrypting model parameters, a warning is raised instead of failing. Therefore, the end-user is aware that the clipping range should be increased for the next rounds.</p> <p>Setting clipping range</p> <p>The optimal clipping range depends on the specific scenario and the models being used. In some cases, using too high of a clipping range can result in a loss of information and lead to decreased performance. Therefore, it is important to carefully choose the appropriate clipping range based on the specific situation and the characteristics of the models being used.</p>"},{"location":"user-guide/secagg/researcher-interface/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/secagg/researcher-interface/#can-not-set-secure-aggregation-context-on-the-researcher-side","title":"Can not set secure aggregation context on the researcher side","text":"<p>This may be because of the timeout on the researcher side. If you have low bandwidth, connection latency or many nodes, please try to increase timeout.</p>"},{"location":"user-guide/secagg/researcher-interface/#model-encryption-takes-too-much-time","title":"Model encryption takes too much time","text":"<p>The time of encryption depends on model size. If the model is larger, it is normal that the encryption takes longer.</p>"},{"location":"user-guide/secagg/researcher-interface/#i-want-to-set-secure-aggregation-context-without-re-running-a-round","title":"I want to set secure aggregation context without re-running a round.","text":"<p>It is possible to access the secagg instance through the experiment object in order to reset the secure aggregation context by providing a list of parties and the experiment <code>experiment_id</code>. This step works for all secure aggregation schemes.</p> <p><pre><code>from fedbiomed.researcher.federated_workflows import Experiment\n\nexp = Experiment(secagg=True,\n                 #....\n                 )\n\nexp.secagg.setup(\n    parties= parties=[exp.researcher_id] + exp.filtered_federation_nodes(),\n    experiment_id=exp.id,\n    researcher_id=exp.researcher_id\n)\n</code></pre> If a context has already been set, you can use the force argument to forcefully recreate the context. <pre><code>exp.secagg.setup(\n    parties= parties=[exp.researcher_id] + exp.filtered_federation_nodes(),\n    experiment_id=exp.id,\n    researcher_id=exp.researcher_id # or config.get('default', 'id')\n    force=True\n)\n</code></pre></p> <p>The outcome of the setup action can vary depending on the secure aggregation scheme used. For example, in the Joye-Libert scheme, the setup action generates <code>servkey</code>, and attaches a default biprime number into its context. In contrast, the LOM scheme only tracks the secure aggregation setup status of the participating nodes. This ensures that all participating nodes have created their own context/elements for training before the system sends the train request.</p>"}]}