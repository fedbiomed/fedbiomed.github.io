{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Federated Learning in Healthcare","text":""},{"location":"#open-transparent-and-trusted-federated-learning-for-real-world-healthcare-applications","title":"Open, Transparent and Trusted Federated Learning for Real-world Healthcare Applications","text":"<p>What is Fed-BioMed?</p> <p>Fed-BioMed is an open-source research and development initiative aiming at translating federated learning (FL) into real-world medical research applications.</p> <p>Fed-BioMed provides:</p> <ul> <li>A demonstrated framework for deploying federated learning in hospital networks,</li> <li>Easy deployment of state-of-the art federated learning methods, </li> <li>User-friendly tools for data managment and client participation to federated learning,</li> <li>A framework-agnostic environment for easily deploying machine learning methods, </li> <li>Clear solutions compliant with data providers' privacy, and nodes governance requirements.</li> </ul> <p>Fed-BioMed is an ongoing initiative, and the code is available on GitHub.</p>"},{"location":"#licensing","title":"Licensing","text":"<p>Fed-BioMed is released under the Apache License 2.0. For further details please see the LICENSE file in the Fed-BioMed source code repository.</p>"},{"location":"#join-us","title":"Join us!","text":"<ul> <li>If you want to be part of Fed-BioMed contact <code>fedbiomed _at_ inria _dot_ fr</code></li> </ul>"},{"location":"developer/","title":"Developer documentation","text":""},{"location":"developer/#developer-guidelines","title":"Developer guidelines","text":"<ul> <li>Usages and tools</li> <li>Continuous integration</li> </ul>"},{"location":"developer/ci/","title":"Developer info on continuous integration","text":"<p>Continuous integration uses GitHub Actions. </p>"},{"location":"developer/ci/#events-that-trigger-ci-tests","title":"Events that trigger CI tests","text":"<p>CI tests are triggered automatically by GitHub on a:</p> <ul> <li>pull request to <code>develop</code> or <code>master</code> branch</li> <li>push in <code>develop</code>, <code>master</code>, <code>feature/test_ci</code> branches (eg: after a merge, pushing a fix directly to this branch)</li> </ul> <p>The pull request can not be completed before CI pipeline succeeds</p> <ul> <li>pushing a fix to the branch with the open pull request re-triggers the CI test</li> <li>CI test can also be manually triggered form <code>Pull Requests</code> &gt; <code>Check</code> &gt; <code>Re-run all checks</code> or directly from <code>Action</code> tab. </li> </ul> <p>CI pipeline currently contains :</p> <ul> <li> <p>running unit tests</p> <ul> <li>update conda envs for <code>network</code> and <code>researcher</code></li> <li>launch nework</li> <li>run unit tests</li> </ul> </li> <li> <p>running a simplenet + federated average training, on a few batches of a MNIST dataset, with 1 node. For that, CI launches <code>./scripts/run_test_mnist</code> (an also be launched on localhost)</p> <ul> <li>update conda env for <code>node</code> (rely on unit tests for others)</li> <li>activate conda and environments, launch network and node.</li> <li>choose an existing git branch for running the test for each of the repos, by decreasing preference order : source branch of the PR, target branch of the PR, <code>develop</code></li> <li>launch the <code>fedbiomed</code> script <code>./notebooks/101_getting-started.py</code></li> <li>succeed if the script completes without failure.</li> </ul> </li> <li> <p>running test build process for documentation </p> </li> </ul> <p>!!! \"note\" Execution exceptions      CI build tests are run if a file related to the build is changed. For example, if the changes (difference between base and feature branch) in a pull request are only made in the gui directory or docs, the CI action for unit tests will be skipped. Please see the exceptions in <code>.gihub/workflows/*.yml</code></p>"},{"location":"developer/ci/#displaying-outputs-and-results","title":"Displaying Outputs and Results","text":"<p>To view CI test output and logs:</p> <ul> <li>view the pull request in gitlab (select <code>Pull requests</code> in top bar, then select your pull request).</li> <li>click on the <code>Checks</code> at the top bar of the pull request and select the <code>Check</code> that you want to display.</li> <li>Click on the jobs to see its console output. </li> </ul>"},{"location":"developer/ci/#unit-tests-coverage","title":"Unit tests coverage","text":"<p>Unit tests coverage reports are published on Codecov platform for each branch/pull request. The report contains overall test coverage for the branch and detailed coverage rates file by file. </p> <ul> <li>Once a GitHub workflow/pipeline is executed for unit-test Codecov with automatically add a comment to the pull request that shows:<ul> <li>Overall test coverage</li> <li>The difference code coverage between base and feature branch </li> </ul> </li> </ul> <p>To access reports on Codecov please go Fed-BioMed Codecov dashboard or go to your pull request,click on <code>Checks</code> at the top of the pull request view and click on <code>View this Pull Request on Codecov</code></p>"},{"location":"developer/ci/#ci-and-github-actions-configuration","title":"CI and GitHub Actions Configuration","text":"<p>GitHub actions are configured using <code>yml</code> files for each workflow. Workflow files can contain multiple jobs and multiple steps for each job. Please go <code>.github/workflow</code> directory to display all workflows for CI. </p> <p>The <code>name</code> value in each <code>yml</code> file corresponds to the name of the workflows that are displayed in <code>Actions</code> page of the Fed-BioMed repository. The <code>name</code> value under each <code>job</code> corresponds to each <code>Checks</code> in pull requests.</p> <p>Please see GitHub actions documentation for more information. </p>"},{"location":"developer/ci/#ci-slaves","title":"CI slaves","text":"<p>CI slaves are located on <code>ci.inria.fr</code>. To be able to add extra configuration and installation you have to connect with your account on <code>ci.inria.fr</code>. You need to be approved by one member of the Fed-BioMed CI project or to be a member of Inria to be able get an account on <code>ci.inria.fr</code>. You can request the Fed-BioMed team to become a member of the Fed-BioMed CI project.</p>"},{"location":"developer/ci/#testing","title":"Testing","text":"<p>Using branch <code>feature/test_ci</code> can be useful when testing/debugging the CI setup (triggers CI on every push, not only on pull request).</p> <p>More integration tests run on a nightly basis. They need a conda environment <code>fedbiomed-ci.yaml</code> which can be found in <code>./envs/ci/conda</code></p>"},{"location":"developer/definition-of-done/","title":"Definition of Done for Fed-BioMed","text":"<p>v1.1 - 2023-05-31</p> <p>The Definition of Done is a set of items that must be completed and quality measures that must be met, before a task or a user story can be considered complete. The DoD gives the team a shared understanding of the work that was completed. </p>"},{"location":"developer/definition-of-done/#validate-ci","title":"Validate CI","text":"<ul> <li>Pass CI build tests </li> <li>Make sure documentation test build process is passed. Changes in docstring and documentation impacts documentation build process.  </li> </ul>"},{"location":"developer/definition-of-done/#review-of-the-code","title":"Review of the code","text":"<p>The reviewer can question any aspect of the increment in coherence with Usage and Tools, exchange with the developer (good practice : leave a gitlab trace of the exchanges), approve it or not.</p> <ul> <li>Be specific in the pull request about what to review (critical code or properties of the code).</li> <li>Coding style: inspire from PEP-8.</li> <li>Understand the code and try to detect bugs.</li> <li>Remove detected bugs.</li> </ul>"},{"location":"developer/definition-of-done/#documentation","title":"Documentation","text":""},{"location":"developer/definition-of-done/#code","title":"Code","text":"<ul> <li>Comment critical or difficult points in the code; obvious lines (eg: tests) are excluded from comments.</li> <li>Add <code>FIXME</code> or <code>TODO</code> tags for any detected bugs or improvements that are technically beyond the scope of the pull request.</li> <li>Write minimal comments in the code (docstring) for a function or a class: parameters and typing, return, purpose of the class or the function.</li> </ul>"},{"location":"developer/definition-of-done/#userdeveloper-docs","title":"User/Developer Docs","text":"<ul> <li>Add API reference in <code>docs/developer/api</code> if there is a new module introduced.</li> <li>Write minimal documentation for scripts (separate README file) or notebooks (inside the notebook).</li> <li>Update/Add documentation in <code>docs</code> if there is a new feature or change in API that impacts the content or examples in documentation.</li> </ul>"},{"location":"developer/definition-of-done/#write-unit-test-for-the-code","title":"Write unit-test for the code","text":"<ul> <li>Be clever : put reasonable effort on writing tests. Current target of unit tests is to reach 100% coverage of code, with reasonably clever functional coverage.</li> <li>Add unit test when correcting a bug.</li> </ul>"},{"location":"developer/definition-of-done/#post-merge-actions","title":"Post-merge actions","text":"<p>After merging:</p> <ul> <li>close the pull request</li> <li>update the issue</li> <li>if the pull request terminates the issue, mark the issue as <code>done</code> and close it.</li> </ul>"},{"location":"developer/usage_and_tools/","title":"Developer usages and tools","text":""},{"location":"developer/usage_and_tools/#introduction","title":"Introduction","text":"<p>The purpose of this guide is to explicit the coding rules and conventions used for this project and explain the use of some of our tools.</p> <p>This guide is dedicated to all Fed-BioMed developers: Contributors, Reviewers, Team Developers, Core Developers.</p> <p>Some aspects of this guide may change in the future, stay alert for such changes.</p>"},{"location":"developer/usage_and_tools/#code","title":"Code","text":""},{"location":"developer/usage_and_tools/#coding-environment","title":"Coding environment","text":"<p>Except for some bash tools and scripts, the python language is used for most parts of the code.</p> <p>conda is used to ease the installation of python and the necessary packages.</p>"},{"location":"developer/usage_and_tools/#coding-style","title":"Coding style","text":"<p>We try to stick as close as possible to python coding style as described here</p> <p>We do not enforce coding style validation at each commit. In the future, we may implement some of the tools described here</p>"},{"location":"developer/usage_and_tools/#coding-rules","title":"Coding rules","text":"<p>Project specific coding rules come in addition to general coding style. Their goal is to favour code homogeneity within the project. They are meant to evolve during the project when needed.</p>"},{"location":"developer/usage_and_tools/#license","title":"License","text":"<p>Project code files should begin with these comment lines to help trace their origin: <pre><code># This file is originally part of Fed-BioMed\n# SPDX-License-Identifier: Apache-2.0\n</code></pre></p> <p>Code files can be reused from another project with a compatible non-contaminating license. They shall retain the original license and copyright mentions. The <code>CREDIT.md</code> file and <code>credit/</code> directory shall be completed and updated accordingly.</p>"},{"location":"developer/usage_and_tools/#authors","title":"Authors","text":"<p>Project does not mention authors in the code files. Developers can add themselves to <code>AUTHORS.md</code>.</p>"},{"location":"developer/usage_and_tools/#repositories","title":"Repositories","text":""},{"location":"developer/usage_and_tools/#framework","title":"Framework","text":"<p>The framework is contained in one git repository with 3 functional parts:</p> <ul> <li> <p>network: a top layer which contains network layers (http server, message server) and a set of scripts to start the services and the components of fedbiomed.</p> </li> <li> <p>node: the library and tools to run on each node</p> </li> <li> <p>researcher: the library and tools to run on researcher's side</p> </li> </ul>"},{"location":"developer/usage_and_tools/#documentation","title":"Documentation","text":"<p>The documentation is contained in the repository under <code>docs</code> directory that is used for building the web site. The static files that are obtained after building documentation are kept in the repository <code>fedbiomed/fedbiomed.github.io</code> to serve for web.</p> <p>Fed-BioMed documentation page is configured to be built and published once there is new version tag released.  Publish process is launched as GitHub workflow job where the documentation is built and pushed to public repository <code>fedbiomed/fedbiomed.github.io</code>.</p>"},{"location":"developer/usage_and_tools/#events-for-documentation-build","title":"Events for documentation build","text":"<p>There are two events that trigger documentation publishing:</p> <ul> <li> <p><code>Publish MASTER fedbiomed/fedbiomed.github.io</code> when pushing a new commit to master</p> <p>The documentation website contains static pages such as the home page, about us, and support (main pages). These pages are separate from the documentation versioning process since they can be updated without requiring a new version to be published. As a result, whenever a new commit is pushed to the master branch, the GitHub workflow named <code>Publish MASTER fedbiomed/fedbiomed.github.io</code> is triggered. This workflow, located at <code>.github/workflows/doc-github-io-main-build.yml</code>, is responsible for publishing the changes made to the main pages.</p> </li> <li> <p><code>Publish NEW TAG in fedbiomed/fedbiomed.github.io</code> when pushing a new version tag</p> <p>The documentation-related pages located in the directories <code>getting-started</code>, <code>developer</code>, <code>tutorials</code>, and <code>user-guide</code> are built whenever a new version tag is pushed. The name of the workflow is <code>Publish NEW TAG in fedbiomed/fedbiomed.github.io</code> and the workflow file is located at <code>.github/workflows/doc-github-io-version-build.yml</code>.</p> </li> </ul>"},{"location":"developer/usage_and_tools/#process-flow-for-documentation-deployment","title":"Process flow for documentation deployment","text":"<ul> <li>The workflow file checks out the pushed commit or tag.</li> <li>It clones the <code>fedbiomed/fedbiomed.github.io</code> repository, which stores all the web static files.</li> <li>The documentation is built, and the artifacts are copied into the cloned folder of <code>fedbiomed/fedbiomed.github.io</code>.</li> <li>Changes are committed and pushed to <code>fedbiomed/fedbiomed.github.io</code>.</li> <li>The push event triggers the deployment job in the <code>fedbiomed/fedbiomed.github.io</code> repository.</li> </ul>"},{"location":"developer/usage_and_tools/#roles-and-accesses","title":"Roles and accesses","text":"<p>Current roles in Fed-BioMed development process are:</p> <ul> <li>Fed-BioMed Users: people using Fed-BioMed for research and/or deployment in federated learning applications, and reporting issues.</li> <li>Fed-BioMed Contributors: developers proposing their changes to the Fed-BioMed code and documentation via pull requests.</li> <li>Fed-BioMed Reviewers: developers reviewing the pull requests.<ul> <li>Reviewers can be Contributors, Team Developers or Core Developers.</li> </ul> </li> <li>Fed-BioMed Team Developers: developers recurrently proposing changes to the Fed-BioMed code and documentation via pull requests, and working in coordinated manner with other Team Developers<ul> <li>Currently, Team Developers are chosen by the existing Team Developers among the volunteer Contributors.</li> </ul> </li> <li>Fed-BioMed Core Developers: developers coordinating the coding of components and documentation of Fed-BioMed, design of extensions and modifications the API.<ul> <li>Currently, Core Developers also give final approval and merge the pull requests</li> <li>and new Core Developers are chosen by the existing Core Developers among the Team Developers.</li> </ul> </li> </ul> <p>In terms of mapping to accounts and roles on GitHub Fed-BioMed repository and organization:</p> <ul> <li>Users and Contributors have no specific access to the Fed-BioMed repository, they are not member of the Fed-BioMed GitHub organization</li> <li>Reviewers and Team Developers receive the github repository write access.</li> <li>Core Developers receive the github repository maintain access.</li> </ul> <p>Fed-BioMed developers/users access are personal and shall not be shared with someone else.</p> <p>Reviewers, Team Developers and Core Developers receive:</p> <ul> <li>registration as members of the GitHub Fed-BioMed organization, and membership in the Developers team (plus CoreDevelopers team for Core Cevelopers) of the organization</li> <li>invitation to Fed-BioMed developer Discord server</li> <li>registration in Fed-BioMed developer mailing lists (     discussion list <code>fedbiomed-developers _at_ inria _dot_ fr</code>,     development notifications list <code>fedbiomed-notifications _at_ inria _dot_ fr</code>)</li> <li>invitation to Fed-BioMed technical team shared files zone on <code>mybox.inria.fr</code></li> </ul> <p>Current list of Core Developers listed by alphabetical order:</p> <ul> <li>Yannick Bouillard</li> <li>Sergen Cansiz</li> <li>Francesco Cremonesi</li> <li>Marco Lorenzi</li> <li>Riccardo Taiello</li> <li>Marc Vesin</li> </ul>"},{"location":"developer/usage_and_tools/#lifecycle","title":"Lifecycle","text":""},{"location":"developer/usage_and_tools/#gitflow-paradigm","title":"Gitflow paradigm","text":"<p>The gitflow paradigm must be followed when creating new development branches and for code release ( see here or here)</p>"},{"location":"developer/usage_and_tools/#release-next-release","title":"Release, next release","text":"<p>Creating a release or integrating a feature to the next release is the responsibility of Core Developers.</p> <p>As we use the gitflow paradigm, the <code>master</code> branch of each repository contains the releases. Next release is integrated under <code>develop</code>.</p> <p>In other words, the <code>master</code> and <code>develop</code> branches are protected and only writable by Core Developers.</p>"},{"location":"developer/usage_and_tools/#pull-request","title":"Pull request","text":"<p>New features are developed in a <code>feature</code> branch (refer to gitflow paradigm).</p> <p>Branch name for developing new features should start with <code>feature/</code> and make them easily linkable with the corresponding issue. For example if the branch is related to issue 123, name it <code>feature/123-my-short-explanation</code>.</p> <p>When the feature is ready, the Developer creates a pull request (PR) via GitHub. Be sure to request merging to the <code>develop</code> branch.</p> <p>The Core Developers team then assigns the pull request one Core Developer (Assignee PR field in GitHub) and one Reviewer (Reviewer PR field in GitHub). The Assignee and the Reviewer can be the same physical person, but they both shall be different people from the developer of the feature.</p> <p>The Reviewer then does a technical review of the pull request evaluating:</p> <ul> <li>the functional correctness of the feature (eg match with implemented algorithm or formula)</li> <li>the maturity of the feature implementation including conformance to the definition of done (DoD).</li> <li>the absence of technical regression introduced by the feature</li> <li>the technical coherence of the implementation of the feature with the existing code base</li> </ul> <p>The Reviewer marks the PR as Approved in GitHub once it is technically ready to be merged.</p> <p>The Assignee assesses:</p> <ul> <li>the interest of the feature in relation with the project goal and roadmap</li> <li>the absence of functional conflict introduced by the feature</li> <li>the valid timeline for merging the feature (if any dependency with other features)</li> </ul> <p>The Assignee merges the PR if it meets these requirements and it is Approved. If the merging needs to be delayed for some reason, the Assignee gives the final approval for merging with its condition/timeline as a comment of the PR.</p> <p>Once a branch is merged (or stalled , abandoned) it is usually deleted.  If there is some reason to keep it, it should then be renamed to something starting with <code>attic/</code> (eg <code>attic/short-description-of-branch</code>).</p>"},{"location":"developer/usage_and_tools/#organization-and-scrum","title":"Organization and Scrum","text":"<ul> <li> <p>The development team is in charge of implementing Fed-BioMed's project goal and roadmap. It carries the bulk of the development effort, coordinating the work of Reviewers, Team Developers, Core Developers.</p> <p>It works as an agile team inspiring from Scrum and loosely implementing it. Development team's work is usually organized in sprints.</p> <p>Reviewers, Team Developers and Core Developers are welcome to the team meetings (daily meeting, sprint review, sprint retrospective) in the developer Discord lounge.</p> <p>Core Developers are invited to sprint planning meetings. Reviewers and Team Developers can be invited to sprint planning meetings depending on their involvement in current actions.</p> <p>Participating to the meetings is recommended in periods when one is actively taking part in a major development action where interaction is needed with other team members.</p> </li> <li> <p>External developers are autonomous developers (Contributors) working at their own pace. This typically fits primarily for punctual contribution, work on some specific function, PoC, etc. External developers are encouraged to interact with the development team to ensure coherence of their planned contributions with the rest of the development activity.</p> </li> </ul>"},{"location":"developer/usage_and_tools/#product-backlog","title":"Product backlog","text":"<p>Product backlog is a Scrum artifact composed of the product goal and product backlog entries. Each product backlog entry can contain a functional requirement, a user story, a task, etc.</p> <p>The current product goal content is:</p> <ol> <li>priority 1 : translating Federated Learning to real world healthcare applications</li> <li>priority 2 : as an open source software initiative, developing of the community, welcoming other contributions </li> <li>priority 3 : supporting initiatives that use Fed-BioMed</li> <li>priority 4 : experimenting new research and technologies</li> </ol> <p>Product backlog entries are:</p> <ul> <li>all milestones except those with a [PoC] mark starting their title</li> <li>issues with a product backlog label</li> </ul> <p>Product backlog is modified by the product owner only or with explicit validation of the product owner.</p> <p>Modifications of the product backlog include:</p> <ul> <li>adding new entries (issues/milestones) to the product backlog</li> <li>during sprint planning, moving issues from the product backlog to the new sprint's sprint backlog (they are selected for next sprint)</li> <li>during sprint planning, moving back uncomplete issues from the previous sprint's sprint backlog to the product backlog (they won't be continued during next sprint)</li> <li>moving product backlog issues to attic (they are now considered obsolete)</li> <li>closing product backlog milestones</li> </ul> <p>Note: product backlog entries and sprint backlog entries can mention \"priority 1\", etc. in their description to explicitely link to a product goal priority.</p>"},{"location":"developer/usage_and_tools/#sprint-backlog","title":"Sprint backlog","text":"<p>Sprint backlog is a Scrum artifact composed of a sprint goal (why) and product backlog elements selected for the sprint.</p> <p>Sprint backlog is a plan by and for the developers in order to achieve the sprint goal.</p> <p>Sprint backlog entries are:</p> <ul> <li>issues with a sprint backlog label</li> </ul> <p>Sprint backlog is created by the development team during the sprint planning. It can be updated and refined during the sprint (new issues, tasks and functional requirements rewriting) in accordance with the sprint goal.</p> <p>During the sprint planning, all uncomplete entries remaining from the previous sprint's sprint backlog can be:</p> <ul> <li>kept in the sprint backlog (they will be continued during next sprint)</li> <li>moved back to the product backlog (they won't be continued during next sprint)</li> <li>moved to the attic (they are now considered obsolete) and closed</li> </ul> <p>During the sprint planning, all complete entries from the previous sprint's sprint backlog:</p> <ul> <li>should already be closed (if not, close them) and marked with done label</li> <li>are removed from the sprint backlog</li> </ul>"},{"location":"developer/usage_and_tools/#proof-of-concepts","title":"Proof of concepts","text":"<p>Proof of concept (PoC) are used to experiment new scientific or technical explorations in Fed-BioMed: PoCs are not bound in time or attached to a sprint. They are closed when they complete or after being stalled for several months.</p> <p>Proof of concepts are:</p> <ul> <li>all milestones with a [PoC] mark starting their title</li> </ul> <p>PoCs code is not integrated to the next release (no merge). PoCs are not committed to code quality practices (eg: meeting the DoD). When a PoC completes, it may be decided that the PoC functionality:</p> <ul> <li>will not be implemented: close the PoC milestone</li> <li>will be implemented: convert the PoC milestone to a product backlog milestone</li> </ul> <p>PoC use branches starting with <code>poc/</code> eg <code>poc/my-short-poc-description</code>.</p> <p>PoC is not a Scrum notion.</p>"},{"location":"developer/usage_and_tools/#milestones-and-issues","title":"Milestones and issues","text":"<p>GitHub milestones and issues are used to keep track of product backlog, sprint backlog and other product items (bugs, proposals, user requests).</p>"},{"location":"developer/usage_and_tools/#milestones","title":"Milestones","text":"<p>Milestones are used to describe mid-term (eg: multi-months) major goals of the project (tasks, functional requirements, user stories).</p> <p>A milestone is:</p> <ul> <li>either a proof of concept (PoC)</li> <li>or a product backlog entry</li> </ul>"},{"location":"developer/usage_and_tools/#issues","title":"Issues","text":"<p>Issues are used to describe smaller goals of the project (tasks, functional requirements, user stories)</p> <p>An open issue has exactly one type amongst:</p> <ul> <li>needs-triage for new user issues</li> <li>a candidate </li> <li>a product backlog entry</li> <li>a sprint backlog entry</li> </ul> <p>An issue:</p> <ul> <li>can be created by a user. It must then be labelled as needs-triage</li> <li>can be created by an individual developer. It must then label as a candidate.</li> <li>can be moved from needs-triage to candidate by a team developer. The team developer ensures it contains necessary information and is explicit enough. Team developer can also add misc labels.</li> <li>can be moved to the product backlog by the product owner or with explicit validation of the product owner</li> <li>can be moved to the sprint backlog during sprint planning by the team developers</li> <li>is closed and marked done when it is completed. If it belongs to the sprint backlog, it should keep this label until the end of the current sprint.</li> </ul> <p>A closed issue has exactly one type amongst:</p> <ul> <li>done (and not anymore in the sprint backlog)</li> <li>attic</li> </ul> <p>An issue can be labelled as attic and closed when it is considered obsolete or not relevant. It then loses its open issue type label (needs-triage, candidate, product backlog, sprint backlog).</p>"},{"location":"developer/usage_and_tools/#labels","title":"Labels","text":"<p>Zero or more labels are associated to an issue.  We sort labels in several categories:</p>"},{"location":"developer/usage_and_tools/#type-labels","title":"type labels:","text":"<ul> <li>needs-triage : a user submits a work request to the team (extension proposal, bug, other request)</li> <li>candidate : an individual developer submits a work request to the team (extension proposal, bug, other request)</li> <li>product backlog : the product owner adds an entry to the product backlog</li> <li>sprint backlog : the development team adds an entry to the sprint backlog</li> <li>attic : the entry is not completed, but is now considered obsolete and closed</li> <li>done: the entry is completed, closed, and not anymore in the sprint backlog</li> </ul>"},{"location":"developer/usage_and_tools/#status-labels","title":"status labels:","text":"<p>All sprint backlog issues have one status label. Other issues only have a status label when they are active (eg: a developer not participating to a sprint, a developer working during intersprint).</p> <ul> <li>todo : issue not started yet (but intention to start soon)</li> <li>doing : issue implementation in progress</li> <li>in review : issue implementation is finished, a pull request open and is ready for review (or under review)</li> <li>done : issue is completed, it meets the DoD and was merged to the next release integration branch, but it still belongs to the sprint backlog</li> </ul>"},{"location":"developer/usage_and_tools/#misc-labels","title":"misc labels","text":"<p>These are optional label that give additional information on an issue.</p> <ul> <li>bug : this issue is about reporting and resolving a suspected bug</li> <li>documentation : documentation related issue</li> <li>good first issue : nice to pick for a new contributor</li> </ul> <p>Note: some previously existing tags are now removed - postponed, feature, improvement, intersprint</p>"},{"location":"developer/usage_and_tools/#example","title":"Example","text":"<ul> <li> <p>an issue with labels sprint backlog + todo + bug means that this issue is in the current sprint's backlog, that it is not yet started, and that it solves a bug.</p> </li> <li> <p>summary :</p> </li> </ul> <p></p>"},{"location":"developer/usage_and_tools/#other-tools","title":"Other tools","text":"<ul> <li>project file repository (Mybox Fed-BioMed-tech)</li> </ul>"},{"location":"developer/api/common/constants/","title":"Constants","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants","title":"fedbiomed.common.constants","text":"Module: <code>fedbiomed.common.constants</code> <p>Fed-BioMed constants/enums</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.CACHE_FOLDER_NAME","title":"CACHE_FOLDER_NAME     <code>module-attribute</code>","text":"<pre><code>CACHE_FOLDER_NAME = 'cache'\n</code></pre> <p>Directory/folder name where cache files are saved</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.CONFIG_FOLDER_NAME","title":"CONFIG_FOLDER_NAME     <code>module-attribute</code>","text":"<pre><code>CONFIG_FOLDER_NAME = 'etc'\n</code></pre> <p>Directory/folder name where configurations are saved</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DB_FOLDER_NAME","title":"DB_FOLDER_NAME     <code>module-attribute</code>","text":"<pre><code>DB_FOLDER_NAME = VAR_FOLDER_NAME\n</code></pre> <p>Directory/folder name where DB files are saved</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DB_PREFIX","title":"DB_PREFIX     <code>module-attribute</code>","text":"<pre><code>DB_PREFIX = 'db_'\n</code></pre> <p>Prefix for database files name</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.MPSPDZ_certificate_prefix","title":"MPSPDZ_certificate_prefix     <code>module-attribute</code>","text":"<pre><code>MPSPDZ_certificate_prefix = 'MPSPDZ_certificate'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.NODE_PREFIX","title":"NODE_PREFIX     <code>module-attribute</code>","text":"<pre><code>NODE_PREFIX = 'node_'\n</code></pre> <p>Prefix for node ID</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TENSORBOARD_FOLDER_NAME","title":"TENSORBOARD_FOLDER_NAME     <code>module-attribute</code>","text":"<pre><code>TENSORBOARD_FOLDER_NAME = 'runs'\n</code></pre> <p>Directory/folder name where tensorboard logs are saved</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TMP_FOLDER_NAME","title":"TMP_FOLDER_NAME     <code>module-attribute</code>","text":"<pre><code>TMP_FOLDER_NAME = 'tmp'\n</code></pre> <p>Directory/folder name where temporary files are saved</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.VAR_FOLDER_NAME","title":"VAR_FOLDER_NAME     <code>module-attribute</code>","text":"<pre><code>VAR_FOLDER_NAME = 'var'\n</code></pre> <p>Directory/folder name where variable files are saved</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants-classes","title":"Classes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.BiprimeType","title":"BiprimeType","text":"<p>           Bases: <code>_BaseEnum</code></p> <p>Constant values for secure aggregation biprime type that will be saved into db</p> <p>Attributes:</p> Name Type Description <code>DYNAMIC</code> <p>means biprime dynamically added after negoti</p> <code>DEFAULT</code> <p>means biprime is a default one provided by Fed-BioMed</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.BiprimeType-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.BiprimeType.DEFAULT","title":"DEFAULT     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = 'default'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.BiprimeType.DYNAMIC","title":"DYNAMIC     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DYNAMIC = 'dynamic'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ComponentType","title":"ComponentType","text":"<p>           Bases: <code>_BaseEnum</code></p> <p>Enumeration class, used to characterize the type of component of the fedbiomed architecture</p> <p>Attributes:</p> Name Type Description <code>RESEARCHER</code> <code>int</code> <p>Researcher component</p> <code>NODE</code> <code>int</code> <p>Node component</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ComponentType-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ComponentType.NODE","title":"NODE     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NODE: int = 2\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ComponentType.RESEARCHER","title":"RESEARCHER     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RESEARCHER: int = 1\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DataLoadingBlockTypes","title":"DataLoadingBlockTypes","text":"CLASS  <pre><code>DataLoadingBlockTypes(args)\n</code></pre> <p>           Bases: <code>_BaseEnum</code></p> <p>Base class for typing purposes.</p> <p>Concrete enumeration types should be defined within the scope of their implementation or application. To define a concrete enumeration type, one must subclass this class as follows: <pre><code>class MyLoadingBlockTypes(DataLoadingBlockTypes, Enum):\n    MY_KEY: str 'myKey'\n    MY_OTHER_KEY: str 'myOtherKey'\n</code></pre></p> <p>Subclasses must respect the following conditions: - All fields must be str; - All field values must be unique.</p> <p>Warning</p> <p>This class must always be empty as it is not allowed to contain any fields!</p> Source code in <code>fedbiomed/common/constants.py</code> <pre><code>def __init__(self, *args):\ncls = self.__class__\nif not isinstance(self.value, str):\nraise ValueError(\"all fields of DataLoadingBlockTypes subclasses\"\n\" must be of str type\")\nif any(self.value == e.value for e in cls):\na = self.name\ne = cls(self.value).name\nraise ValueError(\nf\"duplicate values not allowed in DataLoadingBlockTypes and \"\nf\"its subclasses: {a} --&gt; {e}\")\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes","title":"DatasetTypes","text":"<p>           Bases: <code>_BaseEnum</code></p> <p>Types of Datasets implemented in Fed-BioMed</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.DEFAULT","title":"DEFAULT     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = 'default'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.FLAMBY","title":"FLAMBY     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FLAMBY = 'flamby'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.IMAGES","title":"IMAGES     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGES = 'images'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.MEDICAL_FOLDER","title":"MEDICAL_FOLDER     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEDICAL_FOLDER = 'medical-folder'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.MEDNIST","title":"MEDNIST     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEDNIST = 'mednist'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.NONE","title":"NONE     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = 'none'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.TABULAR","title":"TABULAR     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TABULAR = 'csv'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.DatasetTypes.TEST","title":"TEST     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TEST = 'test'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers","title":"ErrorNumbers","text":"<p>           Bases: <code>_BaseEnum</code></p> <p>List of all error messages types</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB100","title":"FB100     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB100 = 'FB100: undetermined messaging server error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB101","title":"FB101     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB101 = 'FB101: cannot connect to the messaging server'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB102","title":"FB102     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB102 = 'FB102: messaging server does not answer in dedicated time'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB103","title":"FB103     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB103 = 'FB103: messaging call error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB104","title":"FB104     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB104 = 'FB104: message exchange error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB200","title":"FB200     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB200 = 'FB200: undetermined repository server error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB201","title":"FB201     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB201 = 'FB201: server not reachable'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB202","title":"FB202     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB202 = 'FB202: server returns 404 error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB203","title":"FB203     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB203 = 'FB203: server returns other 4xx or 500 error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB300","title":"FB300     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB300 = 'FB300: undetermined node error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB301","title":"FB301     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB301 = 'FB301: Protocol error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB302","title":"FB302     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB302 = 'FB302: TrainingPlan class does not load'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB303","title":"FB303     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB303 = 'FB303: TrainingPlan class does not contain expected methods'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB304","title":"FB304     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB304 = 'FB304: TrainingPlan method crashes'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB305","title":"FB305     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB305 = 'FB305: TrainingPlan loops indefinitely'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB306","title":"FB306     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB306 = 'FB306: bad URL for TrainingPlan (.py)'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB307","title":"FB307     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB307 = 'FB307: bad URL for training params (.mpk)'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB308","title":"FB308     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB308 = 'FB308: bad training request (.json)'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB309","title":"FB309     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB309 = 'FB309: bad model params (.mpk)'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB310","title":"FB310     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB310 = 'FB310: bad data format'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB311","title":"FB311     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB311 = 'FB311: receiving a new computation request during a running computation'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB312","title":"FB312     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB312 = 'FB312: Node stopped in SIGTERM signal handler'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB313","title":"FB313     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB313 = 'FB313: no dataset matching request'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB314","title":"FB314     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB314 = 'FB314: Node round error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB315","title":"FB315     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB315 = 'FB315: Error while loading the data '\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB316","title":"FB316     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB316 = 'FB316: Data loading plan error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB317","title":"FB317     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB317 = 'FB317: FLamby package import error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB318","title":"FB318     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB318 = 'FB318: Secure aggregation setup error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB319","title":"FB319     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB319 = 'FB319: Command not found error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB320","title":"FB320     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB320 = 'FB320: bad model type'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB321","title":"FB321     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB321 = 'FB321: Secure aggregation delete error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB322","title":"FB322     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB322 = 'FB322: Dataset registration error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB400","title":"FB400     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB400 = 'FB400: undetermined application error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB401","title":"FB401     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB401 = 'FB401: aggregation crashes or returns an error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB402","title":"FB402     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB402 = 'FB402: strategy method crashes or sends an error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB403","title":"FB403     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB403 = 'FB403: bad URL (.pt) for model param'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB404","title":"FB404     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB404 = 'FB404: bad model param (.pt) format for TrainingPlan'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB405","title":"FB405     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB405 = 'FB405: received delayed answer for previous computation round'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB406","title":"FB406     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB406 = 'FB406: list of nodes is empty at data lookup phase'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB407","title":"FB407     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB407 = 'FB407: list of nodes became empty when training (all nodes failed training or did not answer)'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB408","title":"FB408     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB408 = 'FB408: training failed on node or node did not answer during training'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB409","title":"FB409     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB409 = 'FB409: node sent Status=Error during training'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB410","title":"FB410     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB410 = 'FB410: bad type or value for experiment argument'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB411","title":"FB411     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB411 = 'FB411: cannot train an experiment that is not fully defined'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB412","title":"FB412     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB412 = 'FB412: cannot do model checking for experiment'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB413","title":"FB413     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB413 = 'FB413: cannot save or load breakpoint for experiment'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB414","title":"FB414     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB414 = 'FB414: bad type or value for training arguments'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB415","title":"FB415     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB415 = 'FB415: secure aggregation handling error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB416","title":"FB416     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB416 = 'FB416: federated dataset error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB417","title":"FB417     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB417 = 'FB417: Secure aggregation error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB500","title":"FB500     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB500 = 'FB500: undetermined node error, detected by server'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB501","title":"FB501     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB501 = 'FB501: node not reachable'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB600","title":"FB600     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB600 = 'FB600: environ error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB601","title":"FB601     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB601 = 'FB601: message error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB602","title":"FB602     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB602 = 'FB602: logger error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB603","title":"FB603     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB603 = 'FB603: task queue error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB604","title":"FB604     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB604 = 'FB604: repository error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB605","title":"FB605     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB605 = 'FB605: training plan error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB606","title":"FB606     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB606 = 'FB606: model manager error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB607","title":"FB607     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB607 = 'FB607: data manager error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB608","title":"FB608     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB608 = 'FB608: torch data manager error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB609","title":"FB609     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB609 = 'FB609: scikit-learn data manager error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB610","title":"FB610     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB610 = 'FB610: Torch based tabular dataset creation error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB611","title":"FB611     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB611 = 'FB611: Error while trying to evaluate using the specified metric'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB612","title":"FB612     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB612 = 'FB612: Torch based NIFTI dataset error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB613","title":"FB613     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB613 = 'FB613: Medical Folder dataset error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB614","title":"FB614     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB614 = 'FB614: data loading block error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB615","title":"FB615     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB615 = 'FB615: data loading plan error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB616","title":"FB616     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB616 = 'FB616: differential privacy controller error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB617","title":"FB617     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB617 = 'FB617: FLamby dataset error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB618","title":"FB618     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB618 = 'FB618: FLamby data transformation error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB619","title":"FB619     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB619 = 'FB619: Certificate error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB620","title":"FB620     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB620 = 'FB620: MPC protocol error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB621","title":"FB621     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB621 = 'FB621: declearn optimizer error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB622","title":"FB622     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB622 = 'FB622: Model error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB623","title":"FB623     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB623 = 'FB623: Secure aggregation database error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB624","title":"FB624     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB624 = 'FB624: Secure aggregation crypter error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB625","title":"FB625     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB625 = 'FB625: Component version error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB626","title":"FB626     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB626 = 'FB626: Fed-BioMed optimizer error'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ErrorNumbers.FB999","title":"FB999     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FB999 = 'FB999: unknown error code sent by the node'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms","title":"HashingAlgorithms","text":"<p>           Bases: <code>_BaseEnum</code></p> <p>Enumeration class, used to characterize the hashing algorithms</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.BLAKE2B","title":"BLAKE2B     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BLAKE2B = 'BLAKE2B'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.BLAKE2S","title":"BLAKE2S     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BLAKE2S = 'BLAKE2S'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.SHA256","title":"SHA256     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA256 = 'SHA256'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.SHA384","title":"SHA384     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA384 = 'SHA384'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.SHA3_256","title":"SHA3_256     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA3_256 = 'SHA3_256'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.SHA3_384","title":"SHA3_384     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA3_384 = 'SHA3_384'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.SHA3_512","title":"SHA3_512     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA3_512 = 'SHA3_512'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.HashingAlgorithms.SHA512","title":"SHA512     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA512 = 'SHA512'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ProcessTypes","title":"ProcessTypes","text":"<p>           Bases: <code>_BaseEnum</code></p> <p>Enumeration class for Preprocess types</p> <p>Attributes:</p> Name Type Description <code>DATA_LOADER</code> <p>Preprocess for DataLoader</p> <code>PARAMS</code> <p>Preprocess for model parameters</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ProcessTypes-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ProcessTypes.DATA_LOADER","title":"DATA_LOADER     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DATA_LOADER = 0\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.ProcessTypes.PARAMS","title":"PARAMS     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PARAMS = 1\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecaggElementTypes","title":"SecaggElementTypes","text":"<p>           Bases: <code>_BaseEnum</code></p> <p>Enumeration class for secure aggregation element types</p> <p>Attributes:</p> Name Type Description <code>SERVER_KEY</code> <code>int</code> <p>server key split between the parties</p> <code>BIPRIME</code> <code>int</code> <p>biprime shared between the parties</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecaggElementTypes-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecaggElementTypes.BIPRIME","title":"BIPRIME     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BIPRIME: int = 1\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.SecaggElementTypes.SERVER_KEY","title":"SERVER_KEY     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SERVER_KEY: int = 0\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanApprovalStatus","title":"TrainingPlanApprovalStatus","text":"<p>           Bases: <code>_BaseEnum</code></p> <p>Enumeration class for training plan approval status of a training plan on a node when training plan approval is active.</p> <p>Attributes:</p> Name Type Description <code>APPROVED</code> <p>training plan was accepted for this node, can be executed now</p> <code>REJECTED</code> <p>training plan was disapproved for this node, cannot be executed</p> <code>PENDING</code> <p>training plan is waiting for review and approval, cannot be executed yet</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanApprovalStatus-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanApprovalStatus.APPROVED","title":"APPROVED     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>APPROVED = 'Approved'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanApprovalStatus.PENDING","title":"PENDING     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PENDING = 'Pending'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanApprovalStatus.REJECTED","title":"REJECTED     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REJECTED = 'Rejected'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanApprovalStatus-functions","title":"Functions","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanApprovalStatus.str2enum","title":"<pre><code>str2enum(name)\n</code></pre>","text":"Source code in <code>fedbiomed/common/constants.py</code> <pre><code>def str2enum(name: str):\nfor e in TrainingPlanApprovalStatus:\nif e.value == name:\nreturn e\nreturn None\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanStatus","title":"TrainingPlanStatus","text":"<p>           Bases: <code>_BaseEnum</code></p> <p>Constant values for training plan type that will be saved into db</p> <p>Attributes:</p> Name Type Description <code>REQUESTED</code> <p>means training plan submitted in-application by the researcher</p> <code>REGISTERED</code> <p>means training plan added by a hospital/node</p> <code>DEFAULT</code> <p>means training plan is default training plan provided by Fed-BioMed</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanStatus-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanStatus.DEFAULT","title":"DEFAULT     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = 'default'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanStatus.REGISTERED","title":"REGISTERED     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REGISTERED = 'registered'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlanStatus.REQUESTED","title":"REQUESTED     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REQUESTED = 'requested'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlans","title":"TrainingPlans","text":"<p>           Bases: <code>_BaseEnum</code></p> <p>Enumeration class for Training plans</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlans-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlans.SkLearnTrainingPlan","title":"SkLearnTrainingPlan     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SkLearnTrainingPlan = 'SkLearnTrainingPlan'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.TrainingPlans.TorchTrainingPlan","title":"TorchTrainingPlan     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TorchTrainingPlan = 'TorchTrainingPlan'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRequestStatus","title":"UserRequestStatus","text":"<p>           Bases: <code>str</code>, <code>_BaseEnum</code></p> <p>Enumeration class, used to characterize the status for user registration requests</p> <p>Attributes:</p> Name Type Description <code>NEW</code> <p>New user registration</p> <code>REJECTED</code> <p>Rejected status</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRequestStatus-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRequestStatus.NEW","title":"NEW     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NEW = 'NEW'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRequestStatus.REJECTED","title":"REJECTED     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REJECTED = 'REJECTED'\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRoleType","title":"UserRoleType","text":"<p>           Bases: <code>int</code>, <code>_BaseEnum</code></p> <p>Enumeration class, used to characterize the type of component of the fedbiomed architecture</p> <p>Attributes:</p> Name Type Description <code>ADMIN</code> <p>User with Admin role</p> <code>USER</code> <p>Simple user</p>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRoleType-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRoleType.ADMIN","title":"ADMIN     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ADMIN = 1\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.UserRoleType.USER","title":"USER     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>USER = 2\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.VEParameters","title":"VEParameters","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.VEParameters-attributes","title":"Attributes","text":""},{"location":"developer/api/common/constants/#fedbiomed.common.constants.VEParameters.CLIPPING_RANGE","title":"CLIPPING_RANGE     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CLIPPING_RANGE: int = 3\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.VEParameters.KEY_SIZE","title":"KEY_SIZE     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KEY_SIZE: int = 2048\n</code></pre>"},{"location":"developer/api/common/constants/#fedbiomed.common.constants.VEParameters.TARGET_RANGE","title":"TARGET_RANGE     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TARGET_RANGE: int = 10000\n</code></pre>"},{"location":"developer/api/common/data/","title":"Data","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data","title":"fedbiomed.common.data","text":"Module: <code>fedbiomed.common.data</code> <p>Classes that simplify imports from fedbiomed.common.data</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data-classes","title":"Classes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingBlock","title":"DataLoadingBlock","text":"CLASS  <pre><code>DataLoadingBlock()\n</code></pre> <p>           Bases: <code>ABC</code></p> <p>The building blocks of a DataLoadingPlan.</p> <p>A DataLoadingBlock describes an intermediary layer between the researcher and the node's filesystem. It allows the node to specify a customization in the way data is \"perceived\" by the data loaders during training.</p> <p>A DataLoadingBlock is identified by its type_id attribute. Thus, this attribute should be unique among all DataLoadingBlockTypes in the same DataLoadingPlan. Moreover, we may test equality between a DataLoadingBlock and a string by checking its type_id, as a means of easily testing whether a DataLoadingBlock is contained in a collection.</p> <p>Correct usage of this class requires creating ad-hoc subclasses. The DataLoadingBlock class is not intended to be instantiated directly.</p> <p>Subclasses of DataLoadingBlock must respect the following conditions:</p> <ol> <li>implement a default constructor</li> <li>the implemented constructor must call <code>super().__init__()</code></li> <li>extend the serialize(self) and the deserialize(self, load_from: dict) functions</li> <li>both serialize and deserialize must call super's serialize and deserialize respectively</li> <li>the deserialize function must always return self</li> <li>the serialize function must update the dict returned by super's serialize</li> <li>implement an apply function that takes arbitrary arguments and applies         the logic of the loading_block</li> <li>update the _validation_scheme to define rules for all new fields returned by the serialize function</li> </ol> <p>Attributes:</p> Name Type Description <code>__serialization_id</code> <p>(str) identifies one serialized instance of the DataLoadingBlock</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def __init__(self):\nself.__serialization_id = 'serialized_dlb_' + str(uuid.uuid4())\nself._serialization_validator = SerializationValidation()\nself._serialization_validator.update_validation_scheme(SerializationValidation.dlb_default_scheme())\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingBlock-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingBlock.apply","title":"<pre><code>apply(args, kwargs)\n</code></pre>  <code>abstractmethod</code>","text":"<p>Abstract method representing an application of the DataLoadingBlock</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>@abstractmethod\ndef apply(self, *args, **kwargs):\n\"\"\"Abstract method representing an application of the DataLoadingBlock\n    \"\"\"\npass\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingBlock.deserialize","title":"<pre><code>deserialize(load_from)\n</code></pre>","text":"<p>Reconstruct the DataLoadingBlock from a serialized version.</p> <p>Parameters:</p> Name Type Description Default <code>load_from</code> <code>dict</code> <p>a dictionary as obtained by the serialize function.</p> required <p>Returns:</p> Type Description <code>TDataLoadingBlock</code> <p>the self instance</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def deserialize(self, load_from: dict) -&gt; TDataLoadingBlock:\n\"\"\"Reconstruct the DataLoadingBlock from a serialized version.\n    Args:\n        load_from (dict): a dictionary as obtained by the serialize function.\n    Returns:\n        the self instance\n    \"\"\"\nself._serialization_validator.validate(load_from, FedbiomedLoadingBlockValueError)\nself.__serialization_id = load_from['dlb_id']\nreturn self\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingBlock.get_serialization_id","title":"<pre><code>get_serialization_id()\n</code></pre>","text":"<p>Expose serialization id as read-only</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def get_serialization_id(self):\n\"\"\"Expose serialization id as read-only\"\"\"\nreturn self.__serialization_id\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingBlock.instantiate_class","title":"<pre><code>instantiate_class(loading_block)\n</code></pre>  <code>staticmethod</code>","text":"<p>Instantiate one DataLoadingBlock object of the type defined in the arguments.</p> <p>Uses the <code>loading_block_module</code> and <code>loading_block_class</code> fields of the loading_block argument to identify the type of DataLoadingBlock to be instantiated, then calls its default constructor. Note that this function does not call deserialize.</p> <p>Parameters:</p> Name Type Description Default <code>loading_block</code> <code>dict</code> <p>DataLoadingBlock metadata in the format returned by the serialize function.</p> required <p>Returns:</p> Type Description <code>TDataLoadingBlock</code> <p>A default-constructed instance of a DataLoadingBlock of the type defined in the metadata.</p> <p>Raises:</p> Type Description <code>FedbiomedLoadingBlockError</code> <p>if the instantiation process raised any exception.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>@staticmethod\ndef instantiate_class(loading_block: dict) -&gt; TDataLoadingBlock:\n\"\"\"Instantiate one [DataLoadingBlock][fedbiomed.common.data._data_loading_plan.DataLoadingBlock]\n    object of the type defined in the arguments.\n    Uses the `loading_block_module` and `loading_block_class` fields of the loading_block argument to\n    identify the type of [DataLoadingBlock][fedbiomed.common.data._data_loading_plan.DataLoadingBlock]\n    to be instantiated, then calls its default constructor.\n    Note that this function **does not call deserialize**.\n    Args:\n        loading_block (dict): [DataLoadingBlock][fedbiomed.common.data._data_loading_plan.DataLoadingBlock]\n            metadata in the format returned by the serialize function.\n    Returns:\n        A default-constructed instance of a\n            [DataLoadingBlock][fedbiomed.common.data._data_loading_plan.DataLoadingBlock]\n            of the type defined in the metadata.\n    Raises:\n       FedbiomedLoadingBlockError: if the instantiation process raised any exception.\n    \"\"\"\ntry:\ndlb_module = import_module(loading_block['loading_block_module'])\ndlb = eval(f\"dlb_module.{loading_block['loading_block_class']}()\")\nexcept Exception as e:\nmsg = f\"{ErrorNumbers.FB614.value}: could not instantiate DataLoadingBlock from the following metadata: \" +\\\n              f\"{loading_block} because of {type(e).__name__}: {e}\"\nlogger.debug(msg)\nraise FedbiomedLoadingBlockError(msg)\nreturn dlb\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingBlock.instantiate_key","title":"<pre><code>instantiate_key(key_module, key_classname, loading_block_key_str)\n</code></pre>  <code>staticmethod</code>","text":"<p>Imports and loads DataLoadingBlockTypes regarding the passed arguments</p> <p>Parameters:</p> Name Type Description Default <code>key_module</code> <code>str</code> <p>description</p> required <code>key_classname</code> <code>str</code> <p>description</p> required <code>loading_block_key_str</code> <code>str</code> <p>description</p> required <p>Raises:</p> Type Description <code>FedbiomedDataLoadingPlanError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>DataLoadingBlockTypes</code> <code>DataLoadingBlockTypes</code> <p>description</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>@staticmethod\ndef instantiate_key(key_module: str, key_classname: str, loading_block_key_str: str) -&gt; DataLoadingBlockTypes:\n\"\"\"Imports and loads [DataLoadingBlockTypes][fedbiomed.common.constants.DataLoadingBlockTypes]\n    regarding the passed arguments\n    Args:\n        key_module (str): _description_\n        key_classname (str): _description_\n        loading_block_key_str (str): _description_\n    Raises:\n        FedbiomedDataLoadingPlanError: _description_\n    Returns:\n        DataLoadingBlockTypes: _description_\n    \"\"\"\ntry:\nkeys = import_module(key_module)\nloading_block_key = eval(f\"keys.{key_classname}('{loading_block_key_str}')\")\nexcept Exception as e:\nmsg = f\"{ErrorNumbers.FB615.value} Error deserializing loading block key \" + \\\n              f\"{loading_block_key_str} with path {key_module}.{key_classname} \" + \\\n              f\"because of {type(e).__name__}: {e}\"\nlogger.debug(msg)\nraise FedbiomedDataLoadingPlanError(msg)\nreturn loading_block_key\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingBlock.serialize","title":"<pre><code>serialize()\n</code></pre>","text":"<p>Serializes the class in a format similar to json.</p> <p>Returns:</p> Type Description <code>dict</code> <p>a dictionary of key-value pairs sufficient for reconstructing</p> <code>dict</code> <p>the DataLoadingBlock.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def serialize(self) -&gt; dict:\n\"\"\"Serializes the class in a format similar to json.\n    Returns:\n        a dictionary of key-value pairs sufficient for reconstructing\n        the DataLoadingBlock.\n    \"\"\"\nreturn dict(\nloading_block_class=self.__class__.__qualname__,\nloading_block_module=self.__module__,\ndlb_id=self.__serialization_id\n)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlan","title":"DataLoadingPlan","text":"CLASS  <pre><code>DataLoadingPlan(args, kwargs)\n</code></pre> <p>           Bases: <code>Dict[DataLoadingBlockTypes, DataLoadingBlock]</code></p> <p>Customizations to the way the data is loaded and presented for training.</p> <p>A DataLoadingPlan is a dictionary of {name: DataLoadingBlock} pairs. Each DataLoadingBlock represents a customization to the way data is loaded and presented to the researcher. These customizations are defined by the node, but they operate on a Dataset class, which is defined by the library and instantiated by the researcher.</p> <p>To exploit this functionality, a Dataset must be modified to accept the customizations provided by the DataLoadingPlan. To simplify this process, we provide the DataLoadingPlanMixin class below.</p> <p>The DataLoadingPlan class should be instantiated directly, no subclassing is needed. The DataLoadingPlan is a dict, and exposes the same interface as a dict.</p> <p>Attributes:</p> Name Type Description <code>dlp_id</code> <p>str representing a unique plan id (auto-generated)</p> <code>desc</code> <p>str representing an optional user-friendly short description</p> <code>target_dataset_type</code> <p>a DatasetTypes enum representing the type of dataset targeted by this DataLoadingPlan</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def __init__(self, *args, **kwargs):\nsuper(DataLoadingPlan, self).__init__(*args, **kwargs)\nself.dlp_id = 'dlp_' + str(uuid.uuid4())\nself.desc = \"\"\nself.target_dataset_type = DatasetTypes.NONE\nself._serialization_validation = SerializationValidation()\nself._serialization_validation.update_validation_scheme(SerializationValidation.dlp_default_scheme())\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlan-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingPlan.desc","title":"desc     <code>instance-attribute</code>","text":"<pre><code>desc = ''\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingPlan.dlp_id","title":"dlp_id     <code>instance-attribute</code>","text":"<pre><code>dlp_id = 'dlp_' + str(uuid.uuid4())\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingPlan.target_dataset_type","title":"target_dataset_type     <code>instance-attribute</code>","text":"<pre><code>target_dataset_type = DatasetTypes.NONE\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlan-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingPlan.deserialize","title":"<pre><code>deserialize(serialized_dlp, serialized_loading_blocks)\n</code></pre>","text":"<p>Reconstruct the DataLoadingPlan][fedbiomed.common.data._data_loading_plan.DataLoadingPlan] from a serialized version.</p> <p>Calling this function will clear the contained [DataLoadingBlockTypes].</p> <p>This function may not be used to \"update\" nor to \"append to\" a DataLoadingPlan.</p> <p>Parameters:</p> Name Type Description Default <code>serialized_dlp</code> <code>dict</code> <p>a dictionary of data loading plan metadata, as obtained from the first output of the serialize function</p> required <code>serialized_loading_blocks</code> <code>List[dict]</code> <p>a list of dictionaries of loading_block metadata, as obtained from the second output of the serialize function</p> required <p>Returns:</p> Type Description <code>TDataLoadingPlan</code> <p>the self instance</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def deserialize(self, serialized_dlp: dict, serialized_loading_blocks: List[dict]) -&gt; TDataLoadingPlan:\n\"\"\"Reconstruct the DataLoadingPlan][fedbiomed.common.data._data_loading_plan.DataLoadingPlan] from a serialized version.\n    !!! warning \"Calling this function will *clear* the contained [DataLoadingBlockTypes].\"\n        This function may not be used to \"update\" nor to \"append to\"\n        a [DataLoadingPlan][fedbiomed.common.data._data_loading_plan.DataLoadingPlan].\n    Args:\n        serialized_dlp: a dictionary of data loading plan metadata, as obtained from the first output of the\n            serialize function\n        serialized_loading_blocks: a list of dictionaries of loading_block metadata, as obtained from the\n            second output of the serialize function\n    Returns:\n        the self instance\n    \"\"\"\nself._serialization_validation.validate(serialized_dlp, FedbiomedDataLoadingPlanValueError)\nself.clear()\nself.dlp_id = serialized_dlp['dlp_id']\nself.desc = serialized_dlp['dlp_name']\nself.target_dataset_type = DatasetTypes(serialized_dlp['target_dataset_type'])\nfor loading_block_key_str, dlb_id in serialized_dlp['loading_blocks'].items():\nkey_module, key_classname = serialized_dlp['key_paths'][loading_block_key_str]\nloading_block_key = DataLoadingBlock.instantiate_key(key_module, key_classname, loading_block_key_str)\nloading_block = next(filter(lambda x: x['dlb_id'] == dlb_id,\nserialized_loading_blocks))\ndlb = DataLoadingBlock.instantiate_class(loading_block)\nself[loading_block_key] = dlb.deserialize(loading_block)\nreturn self\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingPlan.infer_dataset_type","title":"<pre><code>infer_dataset_type(dataset)\n</code></pre>  <code>staticmethod</code>","text":"<p>Infer the type of a given dataset.</p> <p>This function provides the mapping between a dataset's class and the DatasetTypes enum. If the dataset exposes the correct interface (i.e. the get_dataset_type method) then it directly calls that, otherwise it tries to apply some heuristics to guess the type of dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Any</code> <p>the dataset whose type we want to infer.</p> required <p>Returns:</p> Type Description <code>DatasetTypes</code> <p>a DatasetTypes enum element which identifies the type of the dataset.</p> <p>Raises:</p> Type Description <code>FedbiomedDataLoadingPlanValueError</code> <p>if the dataset does not have a <code>get_dataset_type</code> method and moreover the type could not be guessed.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>@staticmethod\ndef infer_dataset_type(dataset: Any) -&gt; DatasetTypes:\n\"\"\"Infer the type of a given dataset.\n    This function provides the mapping between a dataset's class and the DatasetTypes enum. If the dataset exposes\n    the correct interface (i.e. the get_dataset_type method) then it directly calls that, otherwise it tries to\n    apply some heuristics to guess the type of dataset.\n    Args:\n        dataset: the dataset whose type we want to infer.\n    Returns:\n        a DatasetTypes enum element which identifies the type of the dataset.\n    Raises:\n        FedbiomedDataLoadingPlanValueError: if the dataset does not have a `get_dataset_type` method and moreover\n            the type could not be guessed.\n    \"\"\"\nif hasattr(dataset, 'get_dataset_type'):\nreturn dataset.get_dataset_type()\nelif dataset.__class__.__name__ == 'ImageFolder':\n# ImageFolder could be both an images type or mednist. Try to identify mednist with some heuristic.\nif hasattr(dataset, 'classes') and \\\n                all([x in dataset.classes for x in ['AbdomenCT', 'BreastMRI', 'CXR', 'ChestCT', 'Hand', 'HeadCT']]):\nreturn DatasetTypes.MEDNIST\nelse:\nreturn DatasetTypes.IMAGES\nelif dataset.__class__.__name__ == 'MNIST':\nreturn DatasetTypes.DEFAULT\nmsg = f\"{ErrorNumbers.FB615.value} Trying to infer dataset type of {dataset} is not supported \" + \\\n        f\"for datasets of type {dataset.__class__.__qualname__}\"\nlogger.debug(msg)\nraise FedbiomedDataLoadingPlanValueError(msg)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingPlan.serialize","title":"<pre><code>serialize()\n</code></pre>","text":"<p>Serializes the class in a format similar to json.</p> <p>Returns:</p> Type Description <code>Tuple[dict, List]</code> <p>a tuple sufficient for reconstructing the DataLoading plan. It includes: - a dictionary of key-value pairs with the DataLoadingPlan parameters. - a list of dict containing the data for reconstruction all the DataLoadingBlock     of the DataLoadingPlan</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def serialize(self) -&gt; Tuple[dict, List]:\n\"\"\"Serializes the class in a format similar to json.\n    Returns:\n        a tuple sufficient for reconstructing the DataLoading plan. It includes:\n            - a dictionary of key-value pairs with the\n            [DataLoadingPlan][fedbiomed.common.data._data_loading_plan.DataLoadingPlan] parameters.\n            - a list of dict containing the data for reconstruction all the DataLoadingBlock\n                of the [DataLoadingPlan][fedbiomed.common.data._data_loading_plan.DataLoadingPlan] \n    \"\"\"\nreturn dict(\ndlp_id=self.dlp_id,\ndlp_name=self.desc,\ntarget_dataset_type=self.target_dataset_type.value,\nloading_blocks={key.value: dlb.get_serialization_id() for key, dlb in self.items()},\nkey_paths={key.value: (f\"{key.__module__}\", f\"{key.__class__.__qualname__}\") for key in self.keys()}\n), [dlb.serialize() for dlb in self.values()]\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlanMixin","title":"DataLoadingPlanMixin","text":"CLASS  <pre><code>DataLoadingPlanMixin()\n</code></pre> <p>Utility class to enable DLP functionality in a dataset.</p> <p>Any Dataset class that inherits from [DataLoadingPlanMixin] will have the basic tools necessary to support a DataLoadingPlan. Typically, the logic of each specific DataLoadingBlock in the DataLoadingPlan will be implemented in the form of hooks that are called within the Dataset's implementation using the helper function apply_dlb defined below.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def __init__(self):\nself._dlp = None\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataLoadingPlanMixin-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingPlanMixin.apply_dlb","title":"<pre><code>apply_dlb(default_ret_value, dlb_key, args, kwargs)\n</code></pre>","text":"<p>Apply one DataLoadingBlock identified by its key.</p> <p>Note that we want to easily support the case where the DataLoadingPlan is not activated, or the requested loading block is not contained in the DataLoadingPlan. This is achieved by providing a default return value to be returned when the above conditions are met. Hence, most of the calls to apply_dlb will look like this: <pre><code>value = self.apply_dlb(value, 'my-loading-block', my_apply_args)\n</code></pre> This will ensure that value is not changed if the DataLoadingPlan is not active.</p> <p>Parameters:</p> Name Type Description Default <code>default_ret_value</code> <code>Any</code> <p>the value to be returned in case that the dlp functionality is not required</p> required <code>dlb_key</code> <code>DataLoadingBlockTypes</code> <p>the key of the DataLoadingBlock to be applied</p> required <code>*args</code> <code>Optional[Any]</code> <p>forwarded to the DataLoadingBlock's apply function</p> <code>()</code> <code>**kwargs</code> <code>Optional[Any]</code> <p>forwarded to the DataLoadingBlock's apply function</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>the output of the DataLoadingBlock's apply function, or the default_ret_value when dlp is None or it does not contain the requested loading block</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def apply_dlb(self, default_ret_value: Any, dlb_key: DataLoadingBlockTypes,\n*args: Optional[Any], **kwargs: Optional[Any]) -&gt; Any:\n\"\"\"Apply one DataLoadingBlock identified by its key.\n    Note that we want to easily support the case where the DataLoadingPlan\n    is not activated, or the requested loading block is not contained in the\n    DataLoadingPlan. This is achieved by providing a default return value\n    to be returned when the above conditions are met. Hence, most of the\n    calls to apply_dlb will look like this:\n    ```\n    value = self.apply_dlb(value, 'my-loading-block', my_apply_args)\n    ```\n    This will ensure that value is not changed if the DataLoadingPlan is\n    not active.\n    Args:\n        default_ret_value: the value to be returned in case that the dlp\n            functionality is not required\n        dlb_key: the key of the DataLoadingBlock to be applied\n        *args: forwarded to the DataLoadingBlock's apply function\n        **kwargs: forwarded to the DataLoadingBlock's apply function\n    Returns:\n        the output of the DataLoadingBlock's apply function, or\n            the default_ret_value when dlp is None or it does not contain\n            the requested loading block\n    \"\"\"\nif not isinstance(dlb_key, DataLoadingBlockTypes):\nraise FedbiomedDataLoadingPlanValueError(f\"Key {dlb_key} is not of enum type DataLoadingBlockTypes\"\nf\" in DataLoadingPlanMixin.apply_dlb\")\nif self._dlp is not None and dlb_key in self._dlp:\nreturn self._dlp[dlb_key].apply(*args, **kwargs)\nelse:\nreturn default_ret_value\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingPlanMixin.clear_dlp","title":"<pre><code>clear_dlp()\n</code></pre>","text":"Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def clear_dlp(self):\nself._dlp = None\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.DataLoadingPlanMixin.set_dlp","title":"<pre><code>set_dlp(dlp)\n</code></pre>","text":"<p>Sets the dlp if the target dataset type is appropriate</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def set_dlp(self, dlp: DataLoadingPlan):\n\"\"\"Sets the dlp if the target dataset type is appropriate\"\"\"\nif not isinstance(dlp, DataLoadingPlan):\nmsg = f\"{ErrorNumbers.FB615.value} Trying to set a DataLoadingPlan but the argument is of type \" + \\\n              f\"{type(dlp).__name__}\"\nlogger.debug(msg)\nraise FedbiomedDataLoadingPlanValueError(msg)\ndataset_type = DataLoadingPlan.infer_dataset_type(self)  # `self` here will refer to the Dataset instance\nif dlp.target_dataset_type != DatasetTypes.NONE and dataset_type != dlp.target_dataset_type:\nraise FedbiomedDataLoadingPlanValueError(f\"Trying to set {dlp} on dataset of type {dataset_type.value} but \"\nf\"the target type is {dlp.target_dataset_type}\")\nelif dlp.target_dataset_type == DatasetTypes.NONE:\ndlp.target_dataset_type = dataset_type\nself._dlp = dlp\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataManager","title":"DataManager","text":"CLASS  <pre><code>DataManager(dataset, target=None, kwargs)\n</code></pre> <p>           Bases: <code>object</code></p> <p>Factory class that build different data loader/datasets based on the type of <code>dataset</code>. The argument <code>dataset</code> should be provided as <code>torch.utils.data.Dataset</code> object for to be used in PyTorch training.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[np.ndarray, pd.DataFrame, pd.Series, Dataset]</code> <p>Dataset object. It can be an instance, PyTorch Dataset or Tuple.</p> required <code>target</code> <code>Union[np.ndarray, pd.DataFrame, pd.Series]</code> <p>Target variable or variables.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional parameters that are going to be used for data loader</p> <code>{}</code> Source code in <code>fedbiomed/common/data/_data_manager.py</code> <pre><code>def __init__(self,\ndataset: Union[np.ndarray, pd.DataFrame, pd.Series, Dataset],\ntarget: Union[np.ndarray, pd.DataFrame, pd.Series] = None,\n**kwargs: dict) -&gt; None:\n\"\"\"Constructor of DataManager,\n    Args:\n        dataset: Dataset object. It can be an instance, PyTorch Dataset or Tuple.\n        target: Target variable or variables.\n        **kwargs: Additional parameters that are going to be used for data loader\n    \"\"\"\n# TODO: Improve datamanager for auto loading by given dataset_path and other information\n# such as inputs variable indexes and target variables indexes\nself._dataset = dataset\nself._target = target\nself._loader_arguments = kwargs\nself._data_manager_instance = None\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.DataManager-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._data_manager.DataManager.load","title":"<pre><code>load(tp_type)\n</code></pre>","text":"<p>Loads proper DataManager based on given TrainingPlan and <code>dataset</code>, <code>target</code> attributes.</p> <p>Parameters:</p> Name Type Description Default <code>tp_type</code> <code>TrainingPlans</code> <p>Enumeration instance of TrainingPlans that stands for type of training plan.</p> required <p>Raises:</p> Type Description <code>FedbiomedDataManagerError</code> <p>If requested DataManager does not match with given arguments.</p> Source code in <code>fedbiomed/common/data/_data_manager.py</code> <pre><code>def load(self, tp_type: TrainingPlans):\n\"\"\"Loads proper DataManager based on given TrainingPlan and\n    `dataset`, `target` attributes.\n    Args:\n        tp_type: Enumeration instance of TrainingPlans that stands for type of training plan.\n    Raises:\n        FedbiomedDataManagerError: If requested DataManager does not match with given arguments.\n    \"\"\"\n# Training plan is type of TorcTrainingPlan\nif tp_type == TrainingPlans.TorchTrainingPlan:\nif self._target is None and isinstance(self._dataset, Dataset):\n# Create Dataset for pytorch\nself._data_manager_instance = TorchDataManager(dataset=self._dataset, **self._loader_arguments)\nelif isinstance(self._dataset, (pd.DataFrame, pd.Series, np.ndarray)) and \\\n                isinstance(self._target, (pd.DataFrame, pd.Series, np.ndarray)):\n# If `dataset` and `target` attributes are array-like object\n# create TabularDataset object to instantiate a TorchDataManager\ntorch_dataset = TabularDataset(inputs=self._dataset, target=self._target)\nself._data_manager_instance = TorchDataManager(dataset=torch_dataset, **self._loader_arguments)\nelse:\nraise FedbiomedDataManagerError(f\"{ErrorNumbers.FB607.value}: Invalid arguments for torch based \"\nf\"training plan, either provide the argument  `dataset` as PyTorch \"\nf\"Dataset instance, or provide `dataset` and `target` arguments as \"\nf\"an instance one of pd.DataFrame, pd.Series or np.ndarray \")\nelif tp_type == TrainingPlans.SkLearnTrainingPlan:\n# Try to convert `torch.utils.Data.Dataset` to SkLearnBased dataset/datamanager\nif self._target is None and isinstance(self._dataset, Dataset):\ntorch_data_manager = TorchDataManager(dataset=self._dataset)\ntry:\nself._data_manager_instance = torch_data_manager.to_sklearn()\nexcept Exception as e:\nraise FedbiomedDataManagerError(f\"{ErrorNumbers.FB607.value}: PyTorch based `Dataset` object \"\n\"has been instantiated with DataManager. An error occurred while\"\n\"trying to convert torch.utils.data.Dataset to numpy based \"\nf\"dataset: {str(e)}\")\n# For scikit-learn based training plans, the arguments `dataset` and `target` should be an instance\n# one of `pd.DataFrame`, `pd.Series`, `np.ndarray`\nelif isinstance(self._dataset, (pd.DataFrame, pd.Series, np.ndarray)) and \\\n                isinstance(self._target, (pd.DataFrame, pd.Series, np.ndarray)):\n# Create Dataset for SkLearn training plans\nself._data_manager_instance = SkLearnDataManager(inputs=self._dataset, target=self._target,\n**self._loader_arguments)\nelse:\nraise FedbiomedDataManagerError(f\"{ErrorNumbers.FB607.value}: The argument `dataset` and `target` \"\nf\"should be instance of pd.DataFrame, pd.Series or np.ndarray \")\nelse:\nraise FedbiomedDataManagerError(f\"{ErrorNumbers.FB607.value}: Undefined training plan\")\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.FlambyDataset","title":"FlambyDataset","text":"CLASS  <pre><code>FlambyDataset()\n</code></pre> <p>           Bases: <code>DataLoadingPlanMixin</code>, <code>Dataset</code></p> <p>A federated Flamby dataset.</p> <p>A FlambyDataset is a wrapper around a flamby FedClass instance, adding functionalities and interfaces that are specific to Fed-BioMed.</p> <p>A FlambyDataset is always created in an empty state, and it requires a DataLoadingPlan to be finalized to a correct state. The DataLoadingPlan must contain at least the following DataLoadinBlock key-value pair: - FlambyLoadingBlockTypes.FLAMBY_DATASET_METADATA : FlambyDatasetMetadataBlock</p> <p>The lifecycle of the DataLoadingPlan and the wrapped FedClass are tightly interlinked: when the DataLoadingPlan is set, the wrapped FedClass is initialized and instantiated. When the DataLoadingPlan is cleared, the wrapped FedClass is also cleared. Hence, an invariant of this class is that the self._dlp and self.__flamby_fed_class should always be either both None, or both set to some value.</p> <p>Attributes:</p> Name Type Description <code>_transform</code> <p>a transform function of type MonaiTransform or TorchTransform that will be applied to every sample when data is loaded.</p> <code>__flamby_fed_class</code> <p>a private instance of the wrapped Flamby FedClass</p> Source code in <code>fedbiomed/common/data/_flamby_dataset.py</code> <pre><code>def __init__(self):\nsuper().__init__()\nself.__flamby_fed_class = None\nself._transform = None\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.FlambyDataset-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._flamby_dataset.FlambyDataset.clear_dlp","title":"<pre><code>clear_dlp()\n</code></pre>","text":"<p>Clears dlp and automatically clears the FedClass</p> <p>Tries to guarantee some semblance of integrity by also clearing the FedClass, since setting the dlp initializes it.</p> Source code in <code>fedbiomed/common/data/_flamby_dataset.py</code> <pre><code>def clear_dlp(self):\n\"\"\"Clears dlp and automatically clears the FedClass\n    Tries to guarantee some semblance of integrity by also clearing the FedClass, since setting the dlp\n    initializes it.\n    \"\"\"\nsuper().clear_dlp()\nself._clear()\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._flamby_dataset.FlambyDataset.get_center_id","title":"<pre><code>get_center_id()\n</code></pre>","text":"<p>Returns the center id. Requires that the DataLoadingPlan has already been set.</p> <p>Returns:</p> Type Description <code>int</code> <p>the center id (int).</p> <p>Raises:</p> Type Description <code>FedbiomedDatasetError</code> <p>in one of the two scenarios below - if the data loading plan is not set or is malformed. - if the wrapped FedClass is not initialized but the dlp exists</p> Source code in <code>fedbiomed/common/data/_flamby_dataset.py</code> <pre><code>@_check_fed_class_initialization_status(require_initialized=True,\nrequire_uninitialized=False,\nmessage=\"Flamby dataset is in an inconsistent state: a Data Loading Plan \"\n\"is set but the wrapped FedClass was not initialized.\")\n@_requires_dlp\ndef get_center_id(self) -&gt; int:\n\"\"\"Returns the center id. Requires that the DataLoadingPlan has already been set.\n    Returns:\n        the center id (int).\n    Raises:\n        FedbiomedDatasetError: in one of the two scenarios below\n            - if the data loading plan is not set or is malformed.\n            - if the wrapped FedClass is not initialized but the dlp exists\n    \"\"\"\nreturn self.apply_dlb(None, FlambyLoadingBlockTypes.FLAMBY_DATASET_METADATA)['flamby_center_id']\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._flamby_dataset.FlambyDataset.get_dataset_type","title":"<pre><code>get_dataset_type()\n</code></pre>  <code>staticmethod</code>","text":"<p>Returns the Flamby DatasetType</p> Source code in <code>fedbiomed/common/data/_flamby_dataset.py</code> <pre><code>@staticmethod\ndef get_dataset_type() -&gt; DatasetTypes:\n\"\"\"Returns the Flamby DatasetType\"\"\"\nreturn DatasetTypes.FLAMBY\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._flamby_dataset.FlambyDataset.get_flamby_fed_class","title":"<pre><code>get_flamby_fed_class()\n</code></pre>","text":"<p>Returns the instance of the wrapped Flamby FedClass</p> Source code in <code>fedbiomed/common/data/_flamby_dataset.py</code> <pre><code>def get_flamby_fed_class(self):\n\"\"\"Returns the instance of the wrapped Flamby FedClass\"\"\"\nreturn self.__flamby_fed_class\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._flamby_dataset.FlambyDataset.get_transform","title":"<pre><code>get_transform()\n</code></pre>","text":"<p>Gets the transform attribute</p> Source code in <code>fedbiomed/common/data/_flamby_dataset.py</code> <pre><code>def get_transform(self):\n\"\"\"Gets the transform attribute\"\"\"\nreturn self._transform\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._flamby_dataset.FlambyDataset.init_transform","title":"<pre><code>init_transform(transform)\n</code></pre>","text":"<p>Initializes the transform attribute. Must be called before initialization of the wrapped FedClass.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Union[MonaiCompose, TorchCompose]</code> <p>a composed transform of type torchvision.transforms.Compose or monai.transforms.Compose</p> required <p>Raises:</p> Type Description <code>FedbiomedDatasetError</code> <p>if the wrapped FedClass was already initialized.</p> <code>FedbiomedDatasetValueError</code> <p>if the input is not of the correct type.</p> Source code in <code>fedbiomed/common/data/_flamby_dataset.py</code> <pre><code>@_check_fed_class_initialization_status(require_initialized=False,\nrequire_uninitialized=True,\nmessage=\"Calling init_transform is not allowed if the wrapped FedClass \"\n\"has already been initialized. At your own risk, you may call \"\n\"clear_dlp to reset the full FlambyDataset\")\ndef init_transform(self, transform: Union[MonaiCompose, TorchCompose]) -&gt; Union[MonaiCompose, TorchCompose]:\n\"\"\"Initializes the transform attribute. Must be called before initialization of the wrapped FedClass.\n    Arguments:\n        transform: a composed transform of type torchvision.transforms.Compose or monai.transforms.Compose\n    Raises:\n        FedbiomedDatasetError: if the wrapped FedClass was already initialized.\n        FedbiomedDatasetValueError: if the input is not of the correct type.\n    \"\"\"\nif not isinstance(transform, (MonaiCompose, TorchCompose)):\nmsg = f\"{ErrorNumbers.FB618.value}. FlambyDataset transform must be of type \" \\\n              f\"torchvision.transforms.Compose or monai.transforms.Compose\"\nlogger.critical(msg)\nraise FedbiomedDatasetValueError(msg)\nself._transform = transform\nreturn self._transform\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._flamby_dataset.FlambyDataset.set_dlp","title":"<pre><code>set_dlp(dlp)\n</code></pre>","text":"<p>Sets the Data Loading Plan and ensures that the flamby_fed_class is initialized</p> <p>Overrides the set_dlp function from the DataLoadingPlanMixin to make sure that self._init_flamby_fed_class is also called immediately after.</p> Source code in <code>fedbiomed/common/data/_flamby_dataset.py</code> <pre><code>def set_dlp(self, dlp):\n\"\"\"Sets the Data Loading Plan and ensures that the flamby_fed_class is initialized\n    Overrides the set_dlp function from the DataLoadingPlanMixin to make sure that self._init_flamby_fed_class\n    is also called immediately after.\n    \"\"\"\nsuper().set_dlp(dlp)\ntry:\nself._init_flamby_fed_class()\nexcept FedbiomedDatasetError as e:\n# clean up\nsuper().clear_dlp()\nraise FedbiomedDatasetError from e\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._flamby_dataset.FlambyDataset.shape","title":"<pre><code>shape()\n</code></pre>","text":"<p>Returns the shape of the flamby_fed_class</p> Source code in <code>fedbiomed/common/data/_flamby_dataset.py</code> <pre><code>@_check_fed_class_initialization_status(require_initialized=True,\nrequire_uninitialized=False,\nmessage=\"Cannot compute shape because FedClass was not initialized.\")\ndef shape(self) -&gt; List[int]:\n\"\"\"Returns the shape of the flamby_fed_class\"\"\"\nreturn [len(self)] + list(self.__getitem__(0)[0].shape)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.FlambyDatasetMetadataBlock","title":"FlambyDatasetMetadataBlock","text":"CLASS  <pre><code>FlambyDatasetMetadataBlock()\n</code></pre> <p>           Bases: <code>DataLoadingBlock</code></p> <p>Metadata about a Flamby Dataset.</p> <p>Includes information on: - identity of the type of flamby dataset (e.g. fed_ixi, fed_heart, etc...) - the ID of the center of the flamby dataset</p> Source code in <code>fedbiomed/common/data/_flamby_dataset.py</code> <pre><code>def __init__(self):\nsuper().__init__()\nself.metadata = {\n\"flamby_dataset_name\": None,\n\"flamby_center_id\": None\n}\nself._serialization_validator.update_validation_scheme(\nFlambyDatasetMetadataBlock._extra_validation_scheme())\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.FlambyDatasetMetadataBlock-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._flamby_dataset.FlambyDatasetMetadataBlock.metadata","title":"metadata     <code>instance-attribute</code>","text":"<pre><code>metadata = {'flamby_dataset_name': None, 'flamby_center_id': None}\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.FlambyDatasetMetadataBlock-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._flamby_dataset.FlambyDatasetMetadataBlock.apply","title":"<pre><code>apply()\n</code></pre>","text":"<p>Returns a dictionary of dataset metadata.</p> <p>The metadata dictionary contains: - flamby_dataset_name: (str) the name of the selected flamby dataset. - flamby_center_id: (int) the center id selected at dataset add time.</p> <p>Note that the flamby_dataset_name will be the same as the module name required to instantiate the FedClass. However, it will not contain the full module path, hence to properly import this module it must be prepended with <code>flamby.datasets</code>, for example <code>import flamby.datasets.flamby_dataset_name</code></p> <p>Returns:</p> Type Description <code>dict</code> <p>this data loading block's metadata</p> Source code in <code>fedbiomed/common/data/_flamby_dataset.py</code> <pre><code>def apply(self) -&gt; dict:\n\"\"\"Returns a dictionary of dataset metadata.\n    The metadata dictionary contains:\n    - flamby_dataset_name: (str) the name of the selected flamby dataset.\n    - flamby_center_id: (int) the center id selected at dataset add time.\n    Note that the flamby_dataset_name will be the same as the module name required to instantiate the FedClass.\n    However, it will not contain the full module path, hence to properly import this module it must be\n    prepended with `flamby.datasets`, for example `import flamby.datasets.flamby_dataset_name`\n    Returns:\n        this data loading block's metadata\n    \"\"\"\nif any([v is None for v in self.metadata.values()]):\nmsg = f\"{ErrorNumbers.FB316}. Attempting to read Flamby dataset metadata, but \" \\\n              f\"the {[k for k,v in self.metadata.items() if v is None]} keys were not previously set.\"\nlogger.critical(msg)\nraise FedbiomedLoadingBlockError(msg)\nreturn self.metadata\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._flamby_dataset.FlambyDatasetMetadataBlock.deserialize","title":"<pre><code>deserialize(load_from)\n</code></pre>","text":"<p>Reconstruct the DataLoadingBlock from a serialized version.</p> <p>Parameters:</p> Name Type Description Default <code>load_from</code> <code>dict</code> <p>a dictionary as obtained by the serialize function.</p> required <p>Returns:</p> Type Description <code>DataLoadingBlock</code> <p>the self instance</p> Source code in <code>fedbiomed/common/data/_flamby_dataset.py</code> <pre><code>def deserialize(self, load_from: dict) -&gt; DataLoadingBlock:\n\"\"\"Reconstruct the DataLoadingBlock from a serialized version.\n    Args:\n        load_from: a dictionary as obtained by the serialize function.\n    Returns:\n        the self instance\n    \"\"\"\nsuper().deserialize(load_from)\nself.metadata['flamby_dataset_name'] = load_from['flamby_dataset_name']\nself.metadata['flamby_center_id'] = load_from['flamby_center_id']\nreturn self\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._flamby_dataset.FlambyDatasetMetadataBlock.serialize","title":"<pre><code>serialize()\n</code></pre>","text":"<p>Serializes the class in a format similar to json.</p> <p>Returns:</p> Type Description <code>dict</code> <p>a dictionary of key-value pairs sufficient for reconstructing</p> <code>dict</code> <p>the DataLoadingBlock.</p> Source code in <code>fedbiomed/common/data/_flamby_dataset.py</code> <pre><code>def serialize(self) -&gt; dict:\n\"\"\"Serializes the class in a format similar to json.\n    Returns:\n         a dictionary of key-value pairs sufficient for reconstructing\n         the DataLoadingBlock.\n    \"\"\"\nret = super().serialize()\nret.update({'flamby_dataset_name': self.metadata['flamby_dataset_name'],\n'flamby_center_id': self.metadata['flamby_center_id']\n})\nreturn ret\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.FlambyLoadingBlockTypes","title":"FlambyLoadingBlockTypes","text":"<p>           Bases: <code>DataLoadingBlockTypes</code>, <code>Enum</code></p> <p>Additional DataLoadingBlockTypes specific to Flamby data</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data.FlambyLoadingBlockTypes-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._flamby_dataset.FlambyLoadingBlockTypes.FLAMBY_DATASET_METADATA","title":"FLAMBY_DATASET_METADATA     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FLAMBY_DATASET_METADATA: str = 'flamby_dataset_metadata'\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MapperBlock","title":"MapperBlock","text":"CLASS  <pre><code>MapperBlock()\n</code></pre> <p>           Bases: <code>DataLoadingBlock</code></p> <p>A DataLoadingBlock for mapping values.</p> <p>This DataLoadingBlock can be used whenever an \"indirect mapping\" is needed. For example, it can be used to implement a correspondence between a set of \"logical\" abstract names and a set of folder names on the filesystem.</p> <p>The apply function of this DataLoadingBlock takes a \"key\" as input (a str) and returns the mapped value corresponding to map[key]. Note that while the constructor of this class sets a value for type_id, developers are recommended to set a more meaningful value that better speaks to their application.</p> <p>Multiple instances of this loading_block may be used in the same DataLoadingPlan, provided that they are given different type_id via the constructor.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def __init__(self):\nsuper(MapperBlock, self).__init__()\nself.map = {}\nself._serialization_validator.update_validation_scheme(MapperBlock._extra_validation_scheme())\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MapperBlock-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.MapperBlock.map","title":"map     <code>instance-attribute</code>","text":"<pre><code>map = {}\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MapperBlock-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.MapperBlock.apply","title":"<pre><code>apply(key)\n</code></pre>","text":"<p>Returns the value mapped to the key, if it exists.</p> <p>Raises:</p> Type Description <code>FedbiomedLoadingBlockError</code> <p>if map is not a dict or the key does not exist.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def apply(self, key):\n\"\"\"Returns the value mapped to the key, if it exists.\n    Raises:\n        FedbiomedLoadingBlockError: if map is not a dict or the key does not exist.\n    \"\"\"\nif not isinstance(self.map, dict) or key not in self.map:\nmsg = f\"{ErrorNumbers.FB614.value} Mapper block error: no key '{key}' in mapping dictionary\"\nlogger.debug(msg)\nraise FedbiomedLoadingBlockError(msg)\nreturn self.map[key]\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.MapperBlock.deserialize","title":"<pre><code>deserialize(load_from)\n</code></pre>","text":"<p>Reconstruct the DataLoadingBlock from a serialized version.</p> <p>Parameters:</p> Name Type Description Default <code>load_from</code> <code>dict</code> <p>a dictionary as obtained by the serialize function.</p> required <p>Returns:</p> Type Description <code>DataLoadingBlock</code> <p>the self instance</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def deserialize(self, load_from: dict) -&gt; DataLoadingBlock:\n\"\"\"Reconstruct the [DataLoadingBlock][fedbiomed.common.data._data_loading_plan.DataLoadingBlock]\n    from a serialized version.\n    Args:\n        load_from (dict): a dictionary as obtained by the serialize function.\n    Returns:\n        the self instance\n    \"\"\"\nsuper(MapperBlock, self).deserialize(load_from)\nself.map = load_from['map']\nreturn self\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.MapperBlock.serialize","title":"<pre><code>serialize()\n</code></pre>","text":"<p>Serializes the class in a format similar to json.</p> <p>Returns:</p> Type Description <code>dict</code> <p>a dictionary of key-value pairs sufficient for reconstructing</p> <code>dict</code> <p>the DataLoadingBlock.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def serialize(self) -&gt; dict:\n\"\"\"Serializes the class in a format similar to json.\n    Returns:\n        a dictionary of key-value pairs sufficient for reconstructing\n        the [DataLoadingBlock][fedbiomed.common.data._data_loading_plan.DataLoadingBlock].\n    \"\"\"\nret = super(MapperBlock, self).serialize()\nret.update({'map': self.map})\nreturn ret\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase","title":"MedicalFolderBase","text":"CLASS  <pre><code>MedicalFolderBase(root=None)\n</code></pre> <p>           Bases: <code>DataLoadingPlanMixin</code></p> <p>Controller class for Medical Folder dataset.</p> <p>Contains methods to validate the MedicalFolder folder hierarchy and extract folder-base metadata information such as modalities, number of subject etc.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Union[str, Path, None]</code> <p>path to Medical Folder root folder.</p> <code>None</code> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def __init__(self, root: Union[str, Path, None] = None):\n\"\"\"Constructs MedicalFolderBase\n    Args:\n        root: path to Medical Folder root folder.\n    \"\"\"\nsuper(MedicalFolderBase, self).__init__()\nif root is not None:\nroot = self.validate_MedicalFolder_root_folder(root)\nself._root = root\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderBase.default_modality_names","title":"default_modality_names     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_modality_names = ['T1', 'T2', 'label']\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderBase.root","title":"root     <code>property</code> <code>writable</code>","text":"<pre><code>root\n</code></pre> <p>Root property of MedicalFolderController</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderBase-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderBase.available_subjects","title":"<pre><code>available_subjects(subjects_from_index, subjects_from_folder=None)\n</code></pre>","text":"<p>Checks missing subject folders and missing entries in demographics</p> <p>Parameters:</p> Name Type Description Default <code>subjects_from_index</code> <code>Union[list, pd.Series]</code> <p>Given subject folder names in demographics</p> required <code>subjects_from_folder</code> <code>list</code> <p>List of subject folder names to get intersection of given subject_from_index</p> <code>None</code> <p>Returns:</p> Name Type Description <code>available_subjects</code> <code>list[str]</code> <p>subjects that have an imaging data folder and are also present in the demographics file</p> <code>missing_subject_folders</code> <code>list[str]</code> <p>subjects that are in the demographics file but do not have an imaging data folder</p> <code>missing_entries</code> <code>list[str]</code> <p>subjects that have an imaging data folder but are not present in the demographics file</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def available_subjects(self,\nsubjects_from_index: Union[list, pd.Series],\nsubjects_from_folder: list = None) -&gt; tuple[list[str], list[str], list[str]]:\n\"\"\"Checks missing subject folders and missing entries in demographics\n    Args:\n        subjects_from_index: Given subject folder names in demographics\n        subjects_from_folder: List of subject folder names to get intersection of given subject_from_index\n    Returns:\n        available_subjects: subjects that have an imaging data folder and are also present in the demographics file\n        missing_subject_folders: subjects that are in the demographics file but do not have an imaging data folder\n        missing_entries: subjects that have an imaging data folder but are not present in the demographics file\n    \"\"\"\n# Select all subject folders if it is not given\nif subjects_from_folder is None:\nsubjects_from_folder = self.subjects_with_imaging_data_folders()\n# Missing subject that will cause warnings\nmissing_subject_folders = list(set(subjects_from_index) - set(subjects_from_folder))\n# Missing entries that will cause errors\nmissing_entries = list(set(subjects_from_folder) - set(subjects_from_index))\n# Intersection\navailable_subjects = list(set(subjects_from_index).intersection(set(subjects_from_folder)))\nreturn available_subjects, missing_subject_folders, missing_entries\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderBase.complete_subjects","title":"<pre><code>complete_subjects(subjects, modalities)\n</code></pre>","text":"<p>Retrieves subjects that have given all the modalities.</p> <p>Parameters:</p> Name Type Description Default <code>subjects</code> <code>List[str]</code> <p>List of subject folder names</p> required <code>modalities</code> <code>List[str]</code> <p>List of required modalities</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of subject folder names that have required modalities</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def complete_subjects(self, subjects: List[str], modalities: List[str]) -&gt; List[str]:\n\"\"\"Retrieves subjects that have given all the modalities.\n    Args:\n        subjects: List of subject folder names\n        modalities: List of required modalities\n    Returns:\n        List of subject folder names that have required modalities\n    \"\"\"\nreturn [subject for subject in subjects if all(self.is_modalities_existing(subject, modalities))]\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderBase.demographics_column_names","title":"<pre><code>demographics_column_names(path)\n</code></pre>  <code>staticmethod</code>","text":"Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>@staticmethod\ndef demographics_column_names(path: Union[str, Path]):\nreturn MedicalFolderBase.read_demographics(path).columns.values\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderBase.get_dataset_type","title":"<pre><code>get_dataset_type()\n</code></pre>  <code>staticmethod</code>","text":"Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>@staticmethod\ndef get_dataset_type() -&gt; DatasetTypes:\nreturn DatasetTypes.MEDICAL_FOLDER\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderBase.is_modalities_existing","title":"<pre><code>is_modalities_existing(subject, modalities)\n</code></pre>","text":"<p>Checks whether given modalities exists in the subject directory</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>str</code> <p>Subject ID or subject folder name</p> required <code>modalities</code> <code>List[str]</code> <p>List of modalities to check</p> required <p>Returns:</p> Type Description <code>List[bool]</code> <p>List of <code>bool</code> that represents whether modality is existing respectively for each of modality.</p> <p>Raises:</p> Type Description <code>FedbiomedDatasetError</code> <p>bad argument type</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def is_modalities_existing(self, subject: str, modalities: List[str]) -&gt; List[bool]:\n\"\"\"Checks whether given modalities exists in the subject directory\n    Args:\n        subject: Subject ID or subject folder name\n        modalities: List of modalities to check\n    Returns:\n        List of `bool` that represents whether modality is existing respectively for each of modality.\n    Raises:\n        FedbiomedDatasetError: bad argument type\n    \"\"\"\nif not isinstance(subject, str):\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Expected string for subject folder/ID, \"\nf\"but got {type(subject)}\")\nif not isinstance(modalities, list):\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Expected a list for modalities, \"\nf\"but got {type(modalities)}\")\nif not all([type(m) is str for m in modalities]):\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Expected a list of string for modalities, \"\nf\"but some modalities are \"\nf\"{' '.join([ str(type(m) for m in modalities if type(m) != str)])}\")\nare_modalities_existing = list()\nfor modality in modalities:\nmodality_folder = self._subject_modality_folder(subject, modality)\nare_modalities_existing.append(bool(modality_folder) and\nself._root.joinpath(subject, modality_folder).is_dir())\nreturn are_modalities_existing\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderBase.modalities","title":"<pre><code>modalities()\n</code></pre>","text":"<p>Gets all modalities based either on all possible candidates or those provided by the DataLoadingPlan.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of unique available modalities</p> <code>list</code> <p>List of all encountered modality folders in each subject folder, appearing once per folder</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def modalities(self) -&gt; Tuple[list, list]:\n\"\"\"Gets all modalities based either on all possible candidates or those provided by the DataLoadingPlan.\n    Returns:\n         List of unique available modalities\n         List of all encountered modality folders in each subject folder, appearing once per folder\n    \"\"\"\nmodality_candidates, modality_folders_list = self.modalities_candidates_from_subfolders()\nif self._dlp is not None and MedicalFolderLoadingBlockTypes.MODALITIES_TO_FOLDERS in self._dlp:\nmodalities = list(self._dlp[MedicalFolderLoadingBlockTypes.MODALITIES_TO_FOLDERS].map.keys())\nreturn modalities, modality_folders_list\nelse:\nreturn modality_candidates, modality_folders_list\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderBase.modalities_candidates_from_subfolders","title":"<pre><code>modalities_candidates_from_subfolders()\n</code></pre>","text":"<p>Gets all possible modality folders under root directory</p> <p>Returns:</p> Type Description <code>list</code> <p>List of unique available modality folders appearing at least once</p> <code>list</code> <p>List of all encountered modality folders in each subject folder, appearing once per folder</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def modalities_candidates_from_subfolders(self) -&gt; Tuple[list, list]:\n\"\"\" Gets all possible modality folders under root directory\n    Returns:\n         List of unique available modality folders appearing at least once\n         List of all encountered modality folders in each subject folder, appearing once per folder\n    \"\"\"\n# Accept only folders that don't start with \".\" and \"_\"\nmodalities = [f.name for f in self._root.glob(\"*/*\") if f.is_dir() and not f.name.startswith((\".\", \"_\"))]\nreturn sorted(list(set(modalities))), modalities\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderBase.read_demographics","title":"<pre><code>read_demographics(path, index_col=None)\n</code></pre>  <code>staticmethod</code>","text":"<p>Read demographics tabular file for Medical Folder dataset</p> <p>Raises:</p> Type Description <code>FedbiomedDatasetError</code> <p>bad file format</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>@staticmethod\ndef read_demographics(path: Union[str, Path], index_col: Optional[int] = None):\n\"\"\" Read demographics tabular file for Medical Folder dataset\n    Raises:\n        FedbiomedDatasetError: bad file format\n    \"\"\"\npath = Path(path)\nif not path.is_file() or path.suffix.lower() not in [\".csv\", \".tsv\"]:\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Demographics should be CSV or TSV files\")\nreturn pd.read_csv(path, index_col=index_col, engine='python')\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderBase.subjects_with_imaging_data_folders","title":"<pre><code>subjects_with_imaging_data_folders()\n</code></pre>","text":"<p>Retrieves subject folder names under Medical Folder root directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>subject folder names under Medical Folder root directory.</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def subjects_with_imaging_data_folders(self) -&gt; List[str]:\n\"\"\"Retrieves subject folder names under Medical Folder root directory.\n    Returns:\n        subject folder names under Medical Folder root directory.\n    \"\"\"\nreturn [f.name for f in self._root.iterdir() if f.is_dir() and not f.name.startswith(\".\")]\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderBase.validate_MedicalFolder_root_folder","title":"<pre><code>validate_MedicalFolder_root_folder(path)\n</code></pre>  <code>staticmethod</code>","text":"<p>Validates Medical Folder root directory by checking folder structure</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>path to root directory</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to root folder of Medical Folder dataset</p> <p>Raises:</p> Type Description <code>FedbiomedDatasetError</code> <ul> <li>If path is not an instance of <code>str</code> or <code>pathlib.Path</code>                    - If path is not a directory</li> </ul> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>@staticmethod\ndef validate_MedicalFolder_root_folder(path: Union[str, Path]) -&gt; Path:\n\"\"\" Validates Medical Folder root directory by checking folder structure\n    Args:\n        path: path to root directory\n    Returns:\n        Path to root folder of Medical Folder dataset\n    Raises:\n        FedbiomedDatasetError: - If path is not an instance of `str` or `pathlib.Path`\n                               - If path is not a directory\n    \"\"\"\nif not isinstance(path, (Path, str)):\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: The argument root should an instance of \"\nf\"`Path` or `str`, but got {type(path)}\")\nif not isinstance(path, Path):\npath = Path(path)\npath = Path(path).expanduser().resolve()\nif not path.exists():\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Folder or file {path} not found on system\")\nif not path.is_dir():\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Root for Medical Folder dataset \"\nf\"should be a directory.\")\ndirectories = [f for f in path.iterdir() if f.is_dir()]\nif len(directories) == 0:\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Root folder of Medical Folder should \"\nf\"contain subject folders, but no sub folder has been found. \")\nmodalities = [f for f in path.glob(\"*/*\") if f.is_dir()]\nif len(modalities) == 0:\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value} Subject folders for Medical Folder should \"\nf\"contain modalities as folders. Folder structure should be \"\nf\"root/&lt;subjects&gt;/&lt;modalities&gt;\")\nreturn path\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderController","title":"MedicalFolderController","text":"CLASS  <pre><code>MedicalFolderController(root=None)\n</code></pre> <p>           Bases: <code>MedicalFolderBase</code></p> <p>Utility class to construct and verify Medical Folder datasets without knowledge of the experiment.</p> <p>The purpose of this class is to enable key functionalities related to the MedicalFolderDataset at the time of dataset deployment, i.e. when the data is being added to the node's database.</p> <p>Specifically, the MedicalFolderController class can be used to: - construct a MedicalFolderDataset with all available data modalities, without knowing which ones will be used as     targets or features during an experiment - validate that the proper folder structure has been respected by the data managers preparing the data - identify which subjects have which modalities</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Folder path to dataset. Defaults to None.</p> <code>None</code> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def __init__(self, root: str = None):\n\"\"\"Constructs MedicalFolderController\n    Args:\n        root: Folder path to dataset. Defaults to None.\n    \"\"\"\nsuper(MedicalFolderController, self).__init__(root=root)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderController-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderController.load_MedicalFolder","title":"<pre><code>load_MedicalFolder(tabular_file=None, index_col=None)\n</code></pre>","text":"<p>Load Medical Folder dataset with given tabular_file and index_col</p> <p>Parameters:</p> Name Type Description Default <code>tabular_file</code> <code>Union[str, Path]</code> <p>File path to demographics data set</p> <code>None</code> <code>index_col</code> <code>Union[str, int]</code> <p>Column index that represents subject folder names</p> <code>None</code> <p>Returns:</p> Type Description <code>MedicalFolderDataset</code> <p>MedicalFolderDataset object</p> <p>Raises:</p> Type Description <code>FedbiomedDatasetError</code> <p>If Medical Folder dataset is not successfully loaded</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def load_MedicalFolder(self,\ntabular_file: Union[str, Path] = None,\nindex_col: Union[str, int] = None) -&gt; MedicalFolderDataset:\n\"\"\" Load Medical Folder dataset with given tabular_file and index_col\n    Args:\n        tabular_file: File path to demographics data set\n        index_col: Column index that represents subject folder names\n    Returns:\n        MedicalFolderDataset object\n    Raises:\n        FedbiomedDatasetError: If Medical Folder dataset is not successfully loaded\n    \"\"\"\nif self._root is None:\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Can not load Medical Folder dataset without \"\nf\"declaring root directory. Please set root or build MedicalFolderController \"\nf\"with by providing `root` argument use\")\nmodalities, _ = self.modalities()\ntry:\ndataset = MedicalFolderDataset(root=self._root,\ntabular_file=tabular_file,\nindex_col=index_col,\ndata_modalities=modalities,\ntarget_modalities=modalities)\nexcept FedbiomedError as e:\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Can not create Medical Folder dataset. {e}\")\nif self._dlp is not None:\ndataset.set_dlp(self._dlp)\nreturn dataset\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderController.subject_modality_status","title":"<pre><code>subject_modality_status(index=None)\n</code></pre>","text":"<p>Scans subjects and checks which modalities are existing for each subject</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[List, pd.Series]</code> <p>Array-like index that comes from reference csv file of Medical Folder dataset. It represents subject folder names. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Modality status for each subject that indicates which modalities are available</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def subject_modality_status(self, index: Union[List, pd.Series] = None) -&gt; Dict:\n\"\"\"Scans subjects and checks which modalities are existing for each subject\n    Args:\n        index: Array-like index that comes from reference csv file of Medical Folder dataset. It represents subject\n            folder names. Defaults to None.\n    Returns:\n        Modality status for each subject that indicates which modalities are available\n    \"\"\"\nmodalities, _ = self.modalities()\nsubjects = self.subjects_with_imaging_data_folders()\nmodality_status = {\"columns\": [*modalities], \"data\": [], \"index\": []}\nif index is not None:\n_, missing_subjects, missing_entries = self.available_subjects(subjects_from_index=index)\nmodality_status[\"columns\"].extend([\"in_folder\", \"in_index\"])\nfor subject in subjects:\nmodality_report = self.is_modalities_existing(subject, modalities)\nstatus_list = [status for status in modality_report]\nif index is not None:\nstatus_list.append(False if subject in missing_subjects else True)\nstatus_list.append(False if subject in missing_entries else True)\nmodality_status[\"data\"].append(status_list)\nmodality_status[\"index\"].append(subject)\nreturn modality_status\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset","title":"MedicalFolderDataset","text":"CLASS  <pre><code>MedicalFolderDataset(root, data_modalities='T1', transform=None, target_modalities='label', target_transform=None, demographics_transform=None, tabular_file=None, index_col=None)\n</code></pre> <p>           Bases: <code>Dataset</code>, <code>MedicalFolderBase</code></p> <p>Torch dataset following the Medical Folder Structure.</p> <p>The Medical Folder structure is loosely inspired by the BIDS standard [1]. It should respect the following pattern: <pre><code>\u2514\u2500 MedicalFolder_root/\n    \u2514\u2500 demographics.csv\n    \u2514\u2500 sub-01/\n        \u251c\u2500 T1/\n        \u2502  \u2514\u2500 sub-01_xxx.nii.gz\n        \u2514\u2500 T2/\n            \u251c\u2500 sub-01_xxx.nii.gz\n</code></pre> where the first-level subfolders or the root correspond to the subjects, and each subject's folder contains subfolders for each imaging modality. Images should be in Nifti format, with either the .nii or .nii.gz extensions. Finally, within the root folder there should also be a demographics file containing at least one index column with the names of the subject folders. This column will be used to explore the data and load the images. The demographics file may contain additional information about each subject and will be loaded alongside the images by our framework.</p> <p>[1] https://bids.neuroimaging.io/</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Union[str, PathLike, Path]</code> <p>Root folder containing all the subject directories.</p> required <code>data_modalities</code> <code>(str, Iterable)</code> <p>Modality or modalities to be used as data sources.</p> <code>'T1'</code> <code>transform</code> <code>Union[Callable, Dict[str, Callable]]</code> <p>A function or dict of function transform(s) that preprocess each data source.</p> <code>None</code> <code>target_modalities</code> <code>Optional[Union[str, Iterable[str]]]</code> <p>(str, Iterable): Modality or modalities to be used as target sources.</p> <code>'label'</code> <code>target_transform</code> <code>Union[Callable, Dict[str, Callable]]</code> <p>A function or dict of function transform(s) that preprocess each target source.</p> <code>None</code> <code>demographics_transform</code> <code>Optional[Callable]</code> <p>TODO</p> <code>None</code> <code>tabular_file</code> <code>Union[str, PathLike, Path, None]</code> <p>Path to a CSV or Excel file containing the demographic information from the patients.</p> <code>None</code> <code>index_col</code> <code>Union[int, str, None]</code> <p>Column name in the tabular file containing the subject ids which mush match the folder names.</p> <code>None</code> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def __init__(self,\nroot: Union[str, PathLike, Path],\ndata_modalities: Optional[Union[str, Iterable[str]]] = 'T1',\ntransform: Union[Callable, Dict[str, Callable]] = None,\ntarget_modalities: Optional[Union[str, Iterable[str]]] = 'label',\ntarget_transform: Union[Callable, Dict[str, Callable]] = None,\ndemographics_transform: Optional[Callable] = None,\ntabular_file: Union[str, PathLike, Path, None] = None,\nindex_col: Union[int, str, None] = None,\n):\n\"\"\"Constructor for class `MedicalFolderDataset`.\n    Args:\n        root: Root folder containing all the subject directories.\n        data_modalities (str, Iterable): Modality or modalities to be used as data sources.\n        transform: A function or dict of function transform(s) that preprocess each data source.\n        target_modalities: (str, Iterable): Modality or modalities to be used as target sources.\n        target_transform: A function or dict of function transform(s) that preprocess each target source.\n        demographics_transform: TODO\n        tabular_file: Path to a CSV or Excel file containing the demographic information from the patients.\n        index_col: Column name in the tabular file containing the subject ids which mush match the folder names.\n    \"\"\"\nsuper(MedicalFolderDataset, self).__init__(root=root)\nself._tabular_file = tabular_file\nself._index_col = index_col\nself._data_modalities = [data_modalities] if isinstance(data_modalities, str) else data_modalities\nself._target_modalities = [target_modalities] if isinstance(target_modalities, str) else target_modalities\nself._transform = self._check_and_reformat_transforms(transform, data_modalities)\nself._target_transform = self._check_and_reformat_transforms(target_transform, target_modalities)\nself._demographics_transform = demographics_transform if demographics_transform is not None else lambda x: {}\n# Image loader\nself._reader = Compose([\nLoadImage(ITKReader(), image_only=True),\nToTensor()\n])\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderDataset.ALLOWED_EXTENSIONS","title":"ALLOWED_EXTENSIONS     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ALLOWED_EXTENSIONS = ['.nii', '.nii.gz']\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderDataset.demographics","title":"demographics     <code>property</code> <code>cached</code>","text":"<pre><code>demographics: pd.DataFrame\n</code></pre> <p>Loads tabular data file (supports excel, csv, tsv and colon separated value files).</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderDataset.index_col","title":"index_col     <code>property</code> <code>writable</code>","text":"<pre><code>index_col\n</code></pre> <p>Getter/setter of the column containing folder's name (in the tabular file)</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderDataset.subjects_has_all_modalities","title":"subjects_has_all_modalities     <code>property</code>","text":"<pre><code>subjects_has_all_modalities\n</code></pre> <p>Gets only the subjects that have all required modalities</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderDataset.subjects_registered_in_demographics","title":"subjects_registered_in_demographics     <code>property</code> <code>cached</code>","text":"<pre><code>subjects_registered_in_demographics\n</code></pre> <p>Gets the subject only those who are present in the demographics file.</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderDataset.tabular_file","title":"tabular_file     <code>property</code> <code>writable</code>","text":"<pre><code>tabular_file\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderDataset-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderDataset.get_nontransformed_item","title":"<pre><code>get_nontransformed_item(item)\n</code></pre>","text":"Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def get_nontransformed_item(self, item):\n# For the first item retrieve complete subject folders\nsubjects = self.subject_folders()\nif not subjects:\n# case where subjects is an empty list (subject folders have not been found)\nraise FedbiomedDatasetError(\nf\"{ErrorNumbers.FB613.value}: Cannot find complete subject folders with all the modalities\")\n# Get subject folder\nsubject_folder = subjects[item]\n# Load data modalities\ndata = self.load_images(subject_folder, modalities=self._data_modalities)\n# Load target modalities\ntargets = self.load_images(subject_folder, modalities=self._target_modalities)\n# Demographics\ndemographics = self._get_from_demographics(subject_id=subject_folder.name)\nreturn (data, demographics), targets\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderDataset.load_images","title":"<pre><code>load_images(subject_folder, modalities)\n</code></pre>","text":"<p>Loads modality images in given subject folder</p> <p>Parameters:</p> Name Type Description Default <code>subject_folder</code> <code>Path</code> <p>Subject folder where modalities are stored</p> required <code>modalities</code> <code>list</code> <p>List of available modalities</p> required <p>Returns:</p> Type Description <code>Dict[str, torch.Tensor]</code> <p>Subject image data as victories where keys represent each modality.</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def load_images(self, subject_folder: Path, modalities: list) -&gt; Dict[str, torch.Tensor]:\n\"\"\"Loads modality images in given subject folder\n    Args:\n        subject_folder: Subject folder where modalities are stored\n        modalities: List of available modalities\n    Returns:\n        Subject image data as victories where keys represent each modality.\n    \"\"\"\nsubject_data = {}\nfor modality in modalities:\nmodality_folder = self._subject_modality_folder(subject_folder, modality)\nimage_folder = subject_folder.joinpath(modality_folder)\nnii_files = [p.resolve() for p in image_folder.glob(\"**/*\")\nif ''.join(p.suffixes) in self.ALLOWED_EXTENSIONS]\n# Load the first, we assume there is going to be a single image per modality for now.\nimg_path = nii_files[0]\nimg = self._reader(img_path)\nsubject_data[modality] = img\nreturn subject_data\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderDataset.set_dataset_parameters","title":"<pre><code>set_dataset_parameters(parameters)\n</code></pre>","text":"<p>Sets dataset parameters.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>dict</code> <p>Parameters to initialize</p> required <p>Raises:</p> Type Description <code>FedbiomedDatasetError</code> <p>If given parameters are not of <code>dict</code> type</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def set_dataset_parameters(self, parameters: dict):\n\"\"\"Sets dataset parameters.\n    Args:\n        parameters: Parameters to initialize\n    Raises:\n        FedbiomedDatasetError: If given parameters are not of `dict` type\n    \"\"\"\nif not isinstance(parameters, dict):\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Expected type for `parameters` is `dict, \"\nf\"but got {type(parameters)}`\")\nfor key, value in parameters.items():\nif hasattr(self, key):\nsetattr(self, key, value)\nelse:\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB613.value}: Trying to set non existing attribute '{key}'\")\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderDataset.shape","title":"<pre><code>shape()\n</code></pre>","text":"<p>Retrieves shape information for modalities and demographics csv</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def shape(self) -&gt; dict:\n\"\"\"Retrieves shape information for modalities and demographics csv\"\"\"\n# Get all modalities\ndata_modalities = list(set(self._data_modalities))\ntarget_modalities = list(set(self._target_modalities))\nmodalities = list(set(self._data_modalities + self._target_modalities))\n(image, _), targets = self.get_nontransformed_item(0)\nresult = {modality: list(image[modality].shape) for modality in data_modalities}\nresult.update({modality: list(targets[modality].shape) for modality in target_modalities})\nnum_modalities = len(modalities)\ndemographics_shape = self.demographics.shape if self.demographics is not None else None\nresult.update({\"demographics\": demographics_shape, \"num_modalities\": num_modalities})\nreturn result\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderDataset.subject_folders","title":"<pre><code>subject_folders()\n</code></pre>","text":"<p>Retrieves subject folder names of only those who have their complete modalities</p> <p>Returns:</p> Type Description <code>List[Path]</code> <p>List of subject directories that has all requested modalities</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def subject_folders(self) -&gt; List[Path]:\n\"\"\"Retrieves subject folder names of only those who have their complete modalities\n    Returns:\n        List of subject directories that has all requested modalities\n    \"\"\"\n# If demographics are present\nif self._tabular_file and self._index_col is not None:\ncomplete_subject_folders = self.subjects_registered_in_demographics\nelse:\ncomplete_subject_folders = self.subjects_has_all_modalities\nreturn [self._root.joinpath(folder) for folder in complete_subject_folders]\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderLoadingBlockTypes","title":"MedicalFolderLoadingBlockTypes","text":"<p>           Bases: <code>DataLoadingBlockTypes</code>, <code>Enum</code></p>"},{"location":"developer/api/common/data/#fedbiomed.common.data.MedicalFolderLoadingBlockTypes-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.MedicalFolderLoadingBlockTypes.MODALITIES_TO_FOLDERS","title":"MODALITIES_TO_FOLDERS     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MODALITIES_TO_FOLDERS: str = 'modalities_to_folders'\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NIFTIFolderDataset","title":"NIFTIFolderDataset","text":"CLASS  <pre><code>NIFTIFolderDataset(root, transform=None, target_transform=None)\n</code></pre> <p>           Bases: <code>Dataset</code></p> <p>A Generic class for loading NIFTI Images using the folder structure as the target classes' labels.</p> <p>Supported formats: - NIFTI and compressed NIFTI files: <code>.nii</code>, <code>.nii.gz</code></p> <p>This is a Dataset useful in classification tasks. Its usage is quite simple, quite similar to <code>torchvision.datasets.ImageFolder</code>. Images must be contained in first level sub-folders (level 2+ sub-folders are ignored) that describe the target class they belong to (target class label is the name of the folder).</p> <pre><code>nifti_dataset_root_folder\n\u251c\u2500\u2500 control_group\n\u2502   \u251c\u2500\u2500 subject_1.nii\n\u2502   \u2514\u2500\u2500 subject_2.nii\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 disease_group\n    \u251c\u2500\u2500 subject_3.nii\n    \u2514\u2500\u2500 subject_4.nii\n    \u2514\u2500\u2500 ...\n</code></pre> <p>In this example, there are 4 samples (one from each *.nii file), 2 target class, with labels <code>control_group</code> and <code>disease_group</code>. <code>subject_1.nii</code> has class label <code>control_group</code>, <code>subject_3.nii</code> has class label <code>disease_group</code>,etc.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Union[str, PathLike, Path]</code> <p>folder where the data is located.</p> required <code>transform</code> <code>Union[Callable, None]</code> <p>transforms to be applied on data.</p> <code>None</code> <code>target_transform</code> <code>Union[Callable, None]</code> <p>transforms to be applied on target indexes.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedDatasetError</code> <p>bad argument type</p> <code>FedbiomedDatasetError</code> <p>bad root path</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def __init__(self, root: Union[str, PathLike, Path],\ntransform: Union[Callable, None] = None,\ntarget_transform: Union[Callable, None] = None\n):\n\"\"\"Constructor of the class\n    Args:\n        root: folder where the data is located.\n        transform: transforms to be applied on data.\n        target_transform: transforms to be applied on target indexes.\n    Raises:\n        FedbiomedDatasetError: bad argument type\n        FedbiomedDatasetError: bad root path\n    \"\"\"\n# check parameters type\nfor tr, trname in ((transform, 'transform'), (target_transform, 'target_transform')):\nif not callable(tr) and tr is not None:\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB612.value}: Parameter {trname} has incorrect \"\nf\"type {type(tr)}, cannot create dataset.\")\nif not isinstance(root, str) and not isinstance(root, PathLike) and not isinstance(root, Path):\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB612.value}: Parameter `root` has incorrect type \"\nf\"{type(root)}, cannot create dataset.\")\n# initialize object variables\nself._files = []\nself._class_labels = []\nself._targets = []\ntry:\nself._root_dir = Path(root).expanduser()\nexcept RuntimeError as e:\nraise FedbiomedDatasetError(\nf\"{ErrorNumbers.FB612.value}: Cannot expand path {root}, error message is: {e}\")\nself._transform = transform\nself._target_transform = target_transform\nself._reader = Compose([\nLoadImage(ITKReader(), image_only=True),\nToTensor()\n])\nself._explore_root_folder()\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NIFTIFolderDataset-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.NIFTIFolderDataset.files","title":"<pre><code>files()\n</code></pre>","text":"<p>Retrieves the paths to the sample images.</p> <p>Gives sames order as when retrieving the sample images (eg <code>self.files[0]</code> is the path to <code>self.__getitem__[0]</code>)</p> <p>Returns:</p> Type Description <code>List[Path]</code> <p>List of the absolute paths to the sample images</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def files(self) -&gt; List[Path]:\n\"\"\"Retrieves the paths to the sample images.\n    Gives sames order as when retrieving the sample images (eg `self.files[0]`\n    is the path to `self.__getitem__[0]`)\n    Returns:\n        List of the absolute paths to the sample images\n    \"\"\"\nreturn self._files\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._medical_datasets.NIFTIFolderDataset.labels","title":"<pre><code>labels()\n</code></pre>","text":"<p>Retrieves the labels of the target classes.</p> <p>Target label index is the index of the corresponding label in this list.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of the labels of the target classes.</p> Source code in <code>fedbiomed/common/data/_medical_datasets.py</code> <pre><code>def labels(self) -&gt; List[str]:\n\"\"\"Retrieves the labels of the target classes.\n    Target label index is the index of the corresponding label in this list.\n    Returns:\n        List of the labels of the target classes.\n    \"\"\"\nreturn self._class_labels\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NPDataLoader","title":"NPDataLoader","text":"CLASS  <pre><code>NPDataLoader(dataset, target, batch_size=1, shuffle=False, random_seed=None, drop_last=False)\n</code></pre> <p>DataLoader for a Numpy dataset.</p> <p>This data loader encapsulates a dataset composed of numpy arrays and presents an Iterable interface. One design principle was to try to make the interface as similar as possible to a torch.DataLoader.</p> <p>Attributes:</p> Name Type Description <code>_dataset</code> <p>(np.ndarray) a 2d array of features</p> <code>_target</code> <p>(np.ndarray) an optional array of target values</p> <code>_batch_size</code> <p>(int) the number of elements in one batch</p> <code>_shuffle</code> <p>(bool) if True, shuffle the data at the beginning of every epoch</p> <code>_drop_last</code> <p>(bool) if True, drop the last batch if it does not contain batch_size elements</p> <code>_rng</code> <p>(np.random.Generator) the random number generator for shuffling</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>np.ndarray</code> <p>2D Numpy array</p> required <code>target</code> <code>np.ndarray</code> <p>Numpy array of target values</p> required <code>batch_size</code> <code>int</code> <p>batch size for each iteration</p> <code>1</code> <code>shuffle</code> <code>bool</code> <p>shuffle before iteration</p> <code>False</code> <code>random_seed</code> <code>Optional[int]</code> <p>an optional integer to set the numpy random seed for shuffling. If it equals None, then no attempt will be made to set the random seed.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>whether to drop the last batch in case it does not fill the whole batch size</p> <code>False</code> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def __init__(self,\ndataset: np.ndarray,\ntarget: np.ndarray,\nbatch_size: int = 1,\nshuffle: bool = False,\nrandom_seed: Optional[int] = None,\ndrop_last: bool = False):\n\"\"\"Construct numpy data loader\n    Args:\n        dataset: 2D Numpy array\n        target: Numpy array of target values\n        batch_size: batch size for each iteration\n        shuffle: shuffle before iteration\n        random_seed: an optional integer to set the numpy random seed for shuffling. If it equals\n            None, then no attempt will be made to set the random seed.\n        drop_last: whether to drop the last batch in case it does not fill the whole batch size\n    \"\"\"\nif not isinstance(dataset, np.ndarray) or not isinstance(target, np.ndarray):\nmsg = f\"{ErrorNumbers.FB609.value}. Wrong input type for `dataset` or `target` in NPDataLoader. \" \\\n              f\"Expected type np.ndarray for both, instead got {type(dataset)} and\" \\\n              f\"{type(target)} respectively.\"\nlogger.error(msg)\nraise FedbiomedTypeError(msg)\n# If the researcher gave a 1-dimensional dataset, we expand it to 2 dimensions\nif dataset.ndim == 1:\nlogger.info(f\"NPDataLoader expanding 1-dimensional dataset to become 2-dimensional.\")\ndataset = dataset[:, np.newaxis]\n# If the researcher gave a 1-dimensional target, we expand it to 2 dimensions\nif target.ndim == 1:\nlogger.info(f\"NPDataLoader expanding 1-dimensional target to become 2-dimensional.\")\ntarget = target[:, np.newaxis]\nif dataset.ndim != 2 or target.ndim != 2:\nmsg = f\"{ErrorNumbers.FB609.value}. Wrong shape for `dataset` or `target` in NPDataLoader. \" \\\n              f\"Expected 2-dimensional arrays, instead got {dataset.ndim}-dimensional \" \\\n              f\"and {target.ndim}-dimensional arrays respectively.\"\nlogger.error(msg)\nraise FedbiomedValueError(msg)\nif len(dataset) != len(target):\nmsg = f\"{ErrorNumbers.FB609.value}. Inconsistent length for `dataset` and `target` in NPDataLoader. \" \\\n              f\"Expected same length, instead got len(dataset)={len(dataset)}, len(target)={len(target)}\"\nlogger.error(msg)\nraise FedbiomedValueError(msg)\nif not isinstance(batch_size, int):\nmsg = f\"{ErrorNumbers.FB609.value}. Wrong type for `batch_size` parameter of NPDataLoader. Expected a \" \\\n              f\"non-zero positive integer, instead got type {type(batch_size)}.\"\nlogger.error(msg)\nraise FedbiomedTypeError(msg)\nif batch_size &lt;= 0:\nmsg = f\"{ErrorNumbers.FB609.value}. Wrong value for `batch_size` parameter of NPDataLoader. Expected a \" \\\n              f\"non-zero positive integer, instead got value {batch_size}.\"\nlogger.error(msg)\nraise FedbiomedValueError(msg)\nif not isinstance(shuffle, bool):\nmsg = f\"{ErrorNumbers.FB609.value}. Wrong type for `shuffle` parameter of NPDataLoader. Expected `bool`, \" \\\n              f\"instead got {type(shuffle)}.\"\nlogger.error(msg)\nraise FedbiomedTypeError(msg)\nif not isinstance(drop_last, bool):\nmsg = f\"{ErrorNumbers.FB609.value}. Wrong type for `drop_last` parameter of NPDataLoader. \" \\\n              f\"Expected `bool`, instead got {type(drop_last)}.\"\nlogger.error(msg)\nraise FedbiomedTypeError(msg)\nif random_seed is not None and not isinstance(random_seed, int):\nmsg = f\"{ErrorNumbers.FB609.value}. Wrong type for `random_seed` parameter of NPDataLoader. \" \\\n              f\"Expected int or None, instead got {type(random_seed)}.\"\nlogger.error(msg)\nraise FedbiomedTypeError(msg)\nself._dataset = dataset\nself._target = target\nself._batch_size = batch_size\nself._shuffle = shuffle\nself._drop_last = drop_last\nself._rng = np.random.default_rng(random_seed)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NPDataLoader-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._sklearn_data_manager.NPDataLoader.dataset","title":"dataset     <code>property</code>","text":"<pre><code>dataset: np.ndarray\n</code></pre> <p>Returns the encapsulated dataset</p> <p>This needs to be a property to harmonize the API with torch.DataLoader, enabling us to write generic code for both DataLoaders.</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data._sklearn_data_manager.NPDataLoader.target","title":"target     <code>property</code>","text":"<pre><code>target: np.ndarray\n</code></pre> <p>Returns the array of target values</p> <p>This has been made a property to have a homogeneous interface with the dataset property above.</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data.NPDataLoader-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._sklearn_data_manager.NPDataLoader.batch_size","title":"<pre><code>batch_size()\n</code></pre>","text":"<p>Returns the batch size</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def batch_size(self) -&gt; int:\n\"\"\"Returns the batch size\"\"\"\nreturn self._batch_size\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._sklearn_data_manager.NPDataLoader.drop_last","title":"<pre><code>drop_last()\n</code></pre>","text":"<p>Returns the boolean drop_last attribute</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def drop_last(self) -&gt; bool:\n\"\"\"Returns the boolean drop_last attribute\"\"\"\nreturn self._drop_last\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._sklearn_data_manager.NPDataLoader.n_remainder_samples","title":"<pre><code>n_remainder_samples()\n</code></pre>","text":"<p>Returns the remainder of the division between dataset length and batch size.</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def n_remainder_samples(self) -&gt; int:\n\"\"\"Returns the remainder of the division between dataset length and batch size.\"\"\"\nreturn len(self._dataset) % self._batch_size\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._sklearn_data_manager.NPDataLoader.rng","title":"<pre><code>rng()\n</code></pre>","text":"<p>Returns the random number generator</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def rng(self) -&gt; np.random.Generator:\n\"\"\"Returns the random number generator\"\"\"\nreturn self._rng\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._sklearn_data_manager.NPDataLoader.shuffle","title":"<pre><code>shuffle()\n</code></pre>","text":"<p>Returns the boolean shuffle attribute</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def shuffle(self) -&gt; bool:\n\"\"\"Returns the boolean shuffle attribute\"\"\"\nreturn self._shuffle\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SerializationValidation","title":"SerializationValidation","text":"CLASS  <pre><code>SerializationValidation()\n</code></pre> <p>Provide Validation capabilities for serializing/deserializing a [DataLoadingBlock] or [DataLoadingPlan].</p> <p>When a developer inherits from [DataLoadingBlock] to define a custom loading block, they are required to call the <code>_serialization_validator.update_validation_scheme</code> function with a dictionary argument containing the rules to validate all the additional fields that will be used in the serialization of their loading block.</p> <p>These rules must follow the syntax explained in the SchemeValidator class.</p> <p>For example <pre><code>    class MyLoadingBlock(DataLoadingBlock):\n        def __init__(self):\n            self.my_custom_data = {}\n            self._serialization_validator.update_validation_scheme({\n                'custom_data': {\n                    'rules': [dict, ...any other rules],\n                    'required': True\n                }\n            })\n        def serialize(self):\n            serialized = super().serialize()\n            serialized.update({'custom_data': self.my_custom_data})\n            return serialized\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>_validation_scheme</code> <p>(dict) an extensible set of rules to validate the DataLoadingBlock metadata.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def __init__(self):\nself._validation_scheme = {}\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SerializationValidation-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.SerializationValidation.dlb_default_scheme","title":"<pre><code>dlb_default_scheme()\n</code></pre>  <code>classmethod</code>","text":"<p>The dictionary of default validation rules for a serialized [DataLoadingBlock].</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>@classmethod\ndef dlb_default_scheme(cls) -&gt; Dict:\n\"\"\"The dictionary of default validation rules for a serialized [DataLoadingBlock].\"\"\"\nreturn {\n'loading_block_class': {\n'rules': [str, cls._identifier_validation_hook],\n'required': True,\n},\n'loading_block_module': {\n'rules': [str, cls._identifier_validation_hook],\n'required': True,\n},\n'dlb_id': {\n'rules': [str, cls._serial_id_validation_hook],\n'required': True,\n},\n}\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.SerializationValidation.dlp_default_scheme","title":"<pre><code>dlp_default_scheme()\n</code></pre>  <code>classmethod</code>","text":"<p>The dictionary of default validation rules for a serialized [DataLoadingPlan].</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>@classmethod\ndef dlp_default_scheme(cls) -&gt; Dict:\n\"\"\"The dictionary of default validation rules for a serialized [DataLoadingPlan].\"\"\"\nreturn {\n'dlp_id': {\n'rules': [str],\n'required': True,\n},\n'dlp_name': {\n'rules': [str],\n'required': True,\n},\n'target_dataset_type': {\n'rules': [str, cls._target_dataset_type_validator],\n'required': True,\n},\n'loading_blocks': {\n'rules': [dict, cls._loading_blocks_types_validator],\n'required': True\n},\n'key_paths': {\n'rules': [dict, cls._key_paths_validator],\n'required': True\n}\n}\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.SerializationValidation.update_validation_scheme","title":"<pre><code>update_validation_scheme(new_scheme)\n</code></pre>","text":"<p>Updates the validation scheme.</p> <p>Parameters:</p> Name Type Description Default <code>new_scheme</code> <code>dict</code> <p>(dict) new dict of rules</p> required Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def update_validation_scheme(self, new_scheme: dict) -&gt; None:\n\"\"\"Updates the validation scheme.\n    Args:\n        new_scheme: (dict) new dict of rules\n    \"\"\"\nself._validation_scheme.update(new_scheme)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._data_loading_plan.SerializationValidation.validate","title":"<pre><code>validate(dlb_metadata, exception_type, only_required=True)\n</code></pre>","text":"<p>Validate a dict of dlb_metadata according to the _validation_scheme.</p> <p>Parameters:</p> Name Type Description Default <code>dlb_metadata</code> <code>dict) </code> <p>the [DataLoadingBlock] metadata, as returned by serialize or as loaded from the node database.</p> required <code>exception_type</code> <code>Type[FedbiomedError]</code> <p>the type of the exception to be raised when validation fails.</p> required <code>only_required</code> <code>bool) </code> <p>see SchemeValidator.populate_with_defaults</p> <code>True</code> <p>Raises:</p> Type Description <code>exception_type</code> <p>if the validation fails.</p> Source code in <code>fedbiomed/common/data/_data_loading_plan.py</code> <pre><code>def validate(self,\ndlb_metadata: Dict,\nexception_type: Type[FedbiomedError],\nonly_required: bool = True) -&gt; None:\n\"\"\"Validate a dict of dlb_metadata according to the _validation_scheme.\n    Args:\n        dlb_metadata (dict) : the [DataLoadingBlock] metadata, as returned by serialize or as loaded from the\n            node database.\n        exception_type (Type[FedbiomedError]): the type of the exception to be raised when validation fails.\n        only_required (bool) : see SchemeValidator.populate_with_defaults\n    Raises:\n        exception_type: if the validation fails.\n    \"\"\"\ntry:\nsc = SchemeValidator(self._validation_scheme)\nexcept RuleError as e:\nmsg = ErrorNumbers.FB614.value + f\": {e}\"\nlogger.critical(msg)\nraise exception_type(msg)\ntry:\ndlb_metadata = sc.populate_with_defaults(dlb_metadata,\nonly_required=only_required)\nexcept ValidatorError as e:\nmsg = ErrorNumbers.FB614.value + f\": {e}\"\nlogger.critical(msg)\nraise exception_type(msg)\ntry:\nsc.validate(dlb_metadata)\nexcept ValidateError as e:\nmsg = ErrorNumbers.FB614.value + f\": {e}\"\nlogger.critical(msg)\nraise exception_type(msg)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SkLearnDataManager","title":"SkLearnDataManager","text":"CLASS  <pre><code>SkLearnDataManager(inputs, target, kwargs)\n</code></pre> <p>           Bases: <code>object</code></p> <p>Wrapper for <code>pd.DataFrame</code>, <code>pd.Series</code> and <code>np.ndarray</code> datasets.</p> <p>Manages datasets for scikit-learn based model training. Responsible for managing inputs, and target variables that have been provided in <code>training_data</code> of scikit-learn based training plans.</p> <p>The loader arguments will be passed to the [fedbiomed.common.data.NPDataLoader] classes instantiated when split is called. They may include batch_size, shuffle, drop_last, and others. Please see the [fedbiomed.common.data.NPDataLoader] class for more details.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[np.ndarray, pd.DataFrame, pd.Series]</code> <p>Independent variables (inputs, features) for model training</p> required <code>target</code> <code>Union[np.ndarray, pd.DataFrame, pd.Series]</code> <p>Dependent variable/s (target) for model training and validation</p> required <code>**kwargs</code> <code>dict</code> <p>Loader arguments</p> <code>{}</code> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def __init__(self,\ninputs: Union[np.ndarray, pd.DataFrame, pd.Series],\ntarget: Union[np.ndarray, pd.DataFrame, pd.Series],\n**kwargs: dict):\n\"\"\" Construct a SkLearnDataManager from an array of inputs and an array of targets.\n    The loader arguments will be passed to the [fedbiomed.common.data.NPDataLoader] classes instantiated\n    when split is called. They may include batch_size, shuffle, drop_last, and others. Please see the\n    [fedbiomed.common.data.NPDataLoader] class for more details.\n    Args:\n        inputs: Independent variables (inputs, features) for model training\n        target: Dependent variable/s (target) for model training and validation\n        **kwargs: Loader arguments\n    \"\"\"\nif not isinstance(inputs, (np.ndarray, pd.DataFrame, pd.Series)) or \\\n            not isinstance(target, (np.ndarray, pd.DataFrame, pd.Series)):\nmsg = f\"{ErrorNumbers.FB609.value}. Parameters `inputs` and `target` for \" \\\n              f\"initialization of {self.__class__.__name__} should be one of np.ndarray, pd.DataFrame, pd.Series\"\nlogger.error(msg)\nraise FedbiomedTypeError(msg)\n# Convert pd.DataFrame or pd.Series to np.ndarray for `inputs`\nif isinstance(inputs, (pd.DataFrame, pd.Series)):\nself._inputs = inputs.to_numpy()\nelse:\nself._inputs = inputs\n# Convert pd.DataFrame or pd.Series to np.ndarray for `target`\nif isinstance(target, (pd.DataFrame, pd.Series)):\nself._target = target.to_numpy()\nelse:\nself._target = target\n# Additional loader arguments\nself._loader_arguments = kwargs\n# Subset None means that train/validation split has not been performed\nself._subset_test: Union[Tuple[np.ndarray, np.ndarray], None] = None\nself._subset_train: Union[Tuple[np.ndarray, np.ndarray], None] = None\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.SkLearnDataManager-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._sklearn_data_manager.SkLearnDataManager.dataset","title":"<pre><code>dataset()\n</code></pre>","text":"<p>Gets the entire registered dataset.</p> <p>This method returns whole dataset as it is without any split.</p> <p>Returns:</p> Name Type Description <code>inputs</code> <code>np.ndarray</code> <p>Input variables for model training</p> <code>targets</code> <code>np.ndarray</code> <p>Target variable for model training</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def dataset(self) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Gets the entire registered dataset.\n    This method returns whole dataset as it is without any split.\n    Returns:\n         inputs: Input variables for model training\n         targets: Target variable for model training\n    \"\"\"\nreturn self._inputs, self._target\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._sklearn_data_manager.SkLearnDataManager.split","title":"<pre><code>split(test_ratio)\n</code></pre>","text":"<p>Splits <code>np.ndarray</code> dataset into train and validation.</p> <p>Parameters:</p> Name Type Description Default <code>test_ratio</code> <code>float</code> <p>Ratio for validation set partition. Rest of the samples will be used for training</p> required <p>Raises:</p> Type Description <code>FedbiomedSkLearnDataManagerError</code> <p>If the <code>test_ratio</code> is not between 0 and 1</p> <p>Returns:</p> Name Type Description <code>train_loader</code> <code>NPDataLoader</code> <p>NPDataLoader of input variables for model training</p> <code>test_loader</code> <code>NPDataLoader</code> <p>NPDataLoader of target variable for model training</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def split(self, test_ratio: float) -&gt; Tuple[NPDataLoader, NPDataLoader]:\n\"\"\"Splits `np.ndarray` dataset into train and validation.\n    Args:\n         test_ratio: Ratio for validation set partition. Rest of the samples will be used for training\n    Raises:\n        FedbiomedSkLearnDataManagerError: If the `test_ratio` is not between 0 and 1\n    Returns:\n         train_loader: NPDataLoader of input variables for model training\n         test_loader: NPDataLoader of target variable for model training\n    \"\"\"\nif not isinstance(test_ratio, float):\nmsg = f'{ErrorNumbers.FB609.value}: The argument `ratio` should be type `float` not {type(test_ratio)}'\nlogger.error(msg)\nraise FedbiomedTypeError(msg)\nif test_ratio &lt; 0. or test_ratio &gt; 1.:\nmsg = f'{ErrorNumbers.FB609.value}: The argument `ratio` should be equal or between 0 and 1, ' \\\n             f'not {test_ratio}'\nlogger.error(msg)\nraise FedbiomedTypeError(msg)\nempty_subset = (np.array([]), np.array([]))\nif test_ratio &lt;= 0.:\nself._subset_train = (self._inputs, self._target)\nself._subset_test = empty_subset\nelif test_ratio &gt;= 1.:\nself._subset_train = empty_subset\nself._subset_test = (self._inputs, self._target)\nelse:\nx_train, x_test, y_train, y_test = train_test_split(self._inputs, self._target, test_size=test_ratio)\nself._subset_test = (x_test, y_test)\nself._subset_train = (x_train, y_train)\ntest_batch_size = max(1, len(self._subset_test[0]))\nreturn self._subset_loader(self._subset_train, **self._loader_arguments), \\\n        self._subset_loader(self._subset_test, batch_size=test_batch_size)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._sklearn_data_manager.SkLearnDataManager.subset_test","title":"<pre><code>subset_test()\n</code></pre>","text":"<p>Gets Subset of dataset for validation partition.</p> <p>Returns:</p> Name Type Description <code>test_inputs</code> <code>np.ndarray</code> <p>Input variables of validation subset for model validation</p> <code>test_target</code> <code>np.ndarray</code> <p>Target variable of validation subset for model validation</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def subset_test(self) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Gets Subset of dataset for validation partition.\n    Returns:\n        test_inputs: Input variables of validation subset for model validation\n        test_target: Target variable of validation subset for model validation\n    \"\"\"\nreturn self._subset_test\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._sklearn_data_manager.SkLearnDataManager.subset_train","title":"<pre><code>subset_train()\n</code></pre>","text":"<p>Gets Subset for train partition.</p> <p>Returns:</p> Name Type Description <code>test_inputs</code> <code>np.ndarray</code> <p>Input variables of training subset for model training</p> <code>test_target</code> <code>np.ndarray</code> <p>Target variable of training subset for model training</p> Source code in <code>fedbiomed/common/data/_sklearn_data_manager.py</code> <pre><code>def subset_train(self) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Gets Subset for train partition.\n    Returns:\n        test_inputs: Input variables of training subset for model training\n        test_target: Target variable of training subset for model training\n    \"\"\"\nreturn self._subset_train\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TabularDataset","title":"TabularDataset","text":"CLASS  <pre><code>TabularDataset(inputs, target)\n</code></pre> <p>           Bases: <code>Dataset</code></p> <p>Torch based Dataset object to create torch Dataset from given numpy or dataframe type of input and target variables</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[np.ndarray, pd.DataFrame, pd.Series]</code> <p>Input variables that will be passed to network</p> required <code>target</code> <code>Union[np.ndarray, pd.DataFrame, pd.Series]</code> <p>Target variable for output layer</p> required <p>Raises:</p> Type Description <code>FedbiomedTorchDatasetError</code> <p>If input variables and target variable does not have equal length/size</p> Source code in <code>fedbiomed/common/data/_tabular_dataset.py</code> <pre><code>def __init__(self,\ninputs: Union[np.ndarray, pd.DataFrame, pd.Series],\ntarget: Union[np.ndarray, pd.DataFrame, pd.Series]):\n\"\"\"Constructs PyTorch dataset object\n    Args:\n        inputs: Input variables that will be passed to network\n        target: Target variable for output layer\n    Raises:\n        FedbiomedTorchDatasetError: If input variables and target variable does not have\n            equal length/size\n    \"\"\"\n# Inputs and target variable should be converted to the torch tensors\n# PyTorch provides `from_numpy` function to convert numpy arrays to\n# torch tensor. Therefore, if the arguments `inputs` and `target` are\n# instance one of `pd.DataFrame` or `pd.Series`, they should be converted to\n# numpy arrays\nif isinstance(inputs, (pd.DataFrame, pd.Series)):\nself.inputs = inputs.to_numpy()\nelif isinstance(inputs, np.ndarray):\nself.inputs = inputs\nelse:\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB610.value}: The argument `inputs` should be \"\nf\"an instance one of np.ndarray, pd.DataFrame or pd.Series\")\n# Configuring self.target attribute\nif isinstance(target, (pd.DataFrame, pd.Series)):\nself.target = target.to_numpy()\nelif isinstance(inputs, np.ndarray):\nself.target = target\nelse:\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB610.value}: The argument `target` should be \"\nf\"an instance one of np.ndarray, pd.DataFrame or pd.Series\")\n# The lengths should be equal\nif len(self.inputs) != len(self.target):\nraise FedbiomedDatasetError(f\"{ErrorNumbers.FB610.value}: Length of input variables and target \"\nf\"variable does not match. Please make sure that they have \"\nf\"equal size while creating the method `training_data` of \"\nf\"TrainingPlan\")\n# Convert `inputs` adn `target` to Torch floats\nself.inputs = from_numpy(self.inputs).float()\nself.target = from_numpy(self.target).float()\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TabularDataset-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._tabular_dataset.TabularDataset.inputs","title":"inputs     <code>instance-attribute</code>","text":"<pre><code>inputs = from_numpy(self.inputs).float()\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._tabular_dataset.TabularDataset.target","title":"target     <code>instance-attribute</code>","text":"<pre><code>target = from_numpy(self.target).float()\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TabularDataset-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._tabular_dataset.TabularDataset.get_dataset_type","title":"<pre><code>get_dataset_type()\n</code></pre>  <code>staticmethod</code>","text":"Source code in <code>fedbiomed/common/data/_tabular_dataset.py</code> <pre><code>@staticmethod\ndef get_dataset_type() -&gt; DatasetTypes:\nreturn DatasetTypes.TABULAR\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager","title":"TorchDataManager","text":"CLASS  <pre><code>TorchDataManager(dataset, kwargs)\n</code></pre> <p>           Bases: <code>object</code></p> <p>Wrapper for PyTorch Dataset to manage loading operations for validation and train.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset object for torch.utils.data.DataLoader</p> required <code>**kwargs</code> <code>dict</code> <p>Arguments for PyTorch <code>DataLoader</code></p> <code>{}</code> <p>Raises:</p> Type Description <code>FedbiomedTorchDataManagerError</code> <p>If the argument <code>dataset</code> is not an instance of <code>torch.utils.data.Dataset</code></p> Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>def __init__(self, dataset: Dataset, **kwargs: dict):\n\"\"\"Construct  of class\n    Args:\n        dataset: Dataset object for torch.utils.data.DataLoader\n        **kwargs: Arguments for PyTorch `DataLoader`\n    Raises:\n        FedbiomedTorchDataManagerError: If the argument `dataset` is not an instance of `torch.utils.data.Dataset`\n    \"\"\"\n# TorchDataManager should get `dataset` argument as an instance of torch.utils.data.Dataset\nif not isinstance(dataset, Dataset):\nraise FedbiomedTorchDataManagerError(\nf\"{ErrorNumbers.FB608.value}: The attribute `dataset` should an instance \"\nf\"of `torch.utils.data.Dataset`, please use `Dataset` as parent class for\"\nf\"your custom torch dataset object\")\nself._dataset = dataset\nself._loader_arguments = kwargs\nself._subset_test: Union[Subset, None] = None\nself._subset_train: Union[Subset, None] = None\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager-attributes","title":"Attributes","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._torch_data_manager.TorchDataManager.dataset","title":"dataset     <code>property</code>","text":"<pre><code>dataset: Dataset\n</code></pre> <p>Gets dataset.</p> <p>Returns:</p> Type Description <code>Dataset</code> <p>PyTorch dataset instance</p>"},{"location":"developer/api/common/data/#fedbiomed.common.data.TorchDataManager-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data._torch_data_manager.TorchDataManager.load_all_samples","title":"<pre><code>load_all_samples()\n</code></pre>","text":"<p>Loading all samples as PyTorch DataLoader without splitting.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>Dataloader for entire datasets. <code>DataLoader</code> arguments will be retrieved from the <code>**kwargs</code> which is defined while initializing the class</p> Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>def load_all_samples(self) -&gt; DataLoader:\n\"\"\"Loading all samples as PyTorch DataLoader without splitting.\n    Returns:\n        Dataloader for entire datasets. `DataLoader` arguments will be retrieved from the `**kwargs` which\n            is defined while initializing the class\n    \"\"\"\nreturn self._create_torch_data_loader(self._dataset, **self._loader_arguments)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._torch_data_manager.TorchDataManager.split","title":"<pre><code>split(test_ratio)\n</code></pre>","text":"<p>Splitting PyTorch Dataset into train and validation.</p> <p>Parameters:</p> Name Type Description Default <code>test_ratio</code> <code>float</code> <p>Split ratio for validation set ratio. Rest of the samples will be used for training</p> required <p>Raises:</p> Type Description <code>FedbiomedTorchDataManagerError</code> <p>If the ratio is not in good format</p> <p>Returns:</p> Name Type Description <code>train_loader</code> <code>Union[DataLoader, None]</code> <p>DataLoader for training subset. <code>None</code> if the <code>test_ratio</code> is <code>1</code></p> <code>test_loader</code> <code>Union[DataLoader, None]</code> <p>DataLoader for validation subset. <code>None</code> if the <code>test_ratio</code> is <code>0</code></p> Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>def split(self, test_ratio: float) -&gt; Tuple[Union[DataLoader, None], Union[DataLoader, None]]:\n\"\"\" Splitting PyTorch Dataset into train and validation.\n    Args:\n         test_ratio: Split ratio for validation set ratio. Rest of the samples will be used for training\n    Raises:\n        FedbiomedTorchDataManagerError: If the ratio is not in good format\n    Returns:\n         train_loader: DataLoader for training subset. `None` if the `test_ratio` is `1`\n         test_loader: DataLoader for validation subset. `None` if the `test_ratio` is `0`\n    \"\"\"\n# Check the argument `ratio` is of type `float`\nif not isinstance(test_ratio, (float, int)):\nraise FedbiomedTorchDataManagerError(f'{ErrorNumbers.FB608.value}: The argument `ratio` should be '\nf'type `float` or `int` not {type(test_ratio)}')\n# Check ratio is valid for splitting\nif test_ratio &lt; 0 or test_ratio &gt; 1:\nraise FedbiomedTorchDataManagerError(f'{ErrorNumbers.FB608.value}: The argument `ratio` should be '\nf'equal or between 0 and 1, not {test_ratio}')\n# If `Dataset` has proper data attribute\n# try to get shape from self.data\nif not hasattr(self._dataset, '__len__'):\nraise FedbiomedTorchDataManagerError(f\"{ErrorNumbers.FB608.value}: Can not get number of samples from \"\nf\"{str(self._dataset)} without `__len__`.  Please make sure \"\nf\"that `__len__` method has been added to custom dataset. \"\nf\"This method should return total number of samples.\")\ntry:\nsamples = len(self._dataset)\nexcept AttributeError as e:\nraise FedbiomedTorchDataManagerError(f\"{ErrorNumbers.FB608.value}: Can not get number of samples from \"\nf\"{str(self._dataset)} due to undefined attribute, {str(e)}\")\nexcept TypeError as e:\nraise FedbiomedTorchDataManagerError(f\"{ErrorNumbers.FB608.value}: Can not get number of samples from \"\nf\"{str(self._dataset)}, {str(e)}\")\n# Calculate number of samples for train and validation subsets\ntest_samples = math.floor(samples * test_ratio)\ntrain_samples = samples - test_samples\nself._subset_train, self._subset_test = random_split(self._dataset, [train_samples, test_samples])\nloaders = (self._subset_loader(self._subset_train, **self._loader_arguments),\nself._subset_loader(self._subset_test, batch_size=len(self._subset_test)))\nreturn loaders\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._torch_data_manager.TorchDataManager.subset_test","title":"<pre><code>subset_test()\n</code></pre>","text":"<p>Gets validation subset of the dataset.</p> <p>Returns:</p> Type Description <code>Subset</code> <p>Validation subset</p> Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>def subset_test(self) -&gt; Subset:\n\"\"\"Gets validation subset of the dataset.\n    Returns:\n        Validation subset\n    \"\"\"\nreturn self._subset_test\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._torch_data_manager.TorchDataManager.subset_train","title":"<pre><code>subset_train()\n</code></pre>","text":"<p>Gets train subset of the dataset.</p> <p>Returns:</p> Type Description <code>Subset</code> <p>Train subset</p> Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>def subset_train(self) -&gt; Subset:\n\"\"\"Gets train subset of the dataset.\n    Returns:\n        Train subset\n    \"\"\"\nreturn self._subset_train\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data._torch_data_manager.TorchDataManager.to_sklearn","title":"<pre><code>to_sklearn()\n</code></pre>","text":"<p>Converts PyTorch <code>Dataset</code> to sklearn data manager of Fed-BioMed.</p> <p>Returns:</p> Type Description <code>SkLearnDataManager</code> <p>Data manager to use in SkLearn base training plans</p> Source code in <code>fedbiomed/common/data/_torch_data_manager.py</code> <pre><code>def to_sklearn(self) -&gt; SkLearnDataManager:\n\"\"\"Converts PyTorch `Dataset` to sklearn data manager of Fed-BioMed.\n    Returns:\n        Data manager to use in SkLearn base training plans\n    \"\"\"\nloader = self._create_torch_data_loader(self._dataset, batch_size=len(self._dataset))\n# Iterate over samples and get input variable and target variable\ninputs = next(iter(loader))[0].numpy()\ntarget = next(iter(loader))[1].numpy()\nreturn SkLearnDataManager(inputs=inputs, target=target, **self._loader_arguments)\n</code></pre>"},{"location":"developer/api/common/data/#fedbiomed.common.data-functions","title":"Functions","text":""},{"location":"developer/api/common/data/#fedbiomed.common.data.discover_flamby_datasets","title":"<pre><code>discover_flamby_datasets()\n</code></pre>","text":"<p>Automatically discover the available Flamby datasets based on the contents of the flamby.datasets module.</p> <p>Returns:</p> Name Type Description <code>Dict[int, str]</code> <p>a dictionary {index: dataset_name} where index is an int and dataset_name is the name of a flamby module</p> <code>Dict[int, str]</code> <p>corresponding to a dataset, represented as str. To import said module one must prepend with the correct</p> <code>path</code> <code>Dict[int, str]</code> <p><code>import flamby.datasets.dataset_name</code>.</p> Source code in <code>fedbiomed/common/data/_flamby_dataset.py</code> <pre><code>def discover_flamby_datasets() -&gt; Dict[int, str]:\n\"\"\"Automatically discover the available Flamby datasets based on the contents of the flamby.datasets module.\n    Returns:\n        a dictionary {index: dataset_name} where index is an int and dataset_name is the name of a flamby module\n        corresponding to a dataset, represented as str. To import said module one must prepend with the correct\n        path: `import flamby.datasets.dataset_name`.\n    \"\"\"\ndataset_list = [name for _, name, ispkg in pkgutil.iter_modules(flamby_datasets_module.__path__) if ispkg]\nreturn {i: name for i, name in enumerate(dataset_list)}\n</code></pre>"},{"location":"developer/api/common/environ/","title":"Environ","text":""},{"location":"developer/api/common/environ/#fedbiomed.common.environ","title":"fedbiomed.common.environ","text":"Module: <code>fedbiomed.common.environ</code> <p>All environment/configuration variables are provided by the Environ dictionary.</p> <p>Environ is a singleton class, meaning that only an instance of Environ is available.</p> <p>Descriptions of global/environment variables</p> <p>Researcher Global Variables:</p> <ul> <li>RESEARCHER_ID           : id of the researcher</li> <li>ID                      : equals to researcher id</li> <li>TENSORBOARD_RESULTS_DIR : path for writing tensorboard log files</li> <li>EXPERIMENTS_DIR         : folder for saving experiments</li> <li>MESSAGES_QUEUE_DIR      : Path for writing queue files</li> </ul> <p>Nodes Global Variables:</p> <ul> <li>NODE_ID                           : id of the node</li> <li>ID                                : equals to node id</li> <li>MESSAGES_QUEUE_DIR                : Path for queues</li> <li>DB_PATH                           : TinyDB database path where datasets/training_plans/loading plans are saved</li> <li>DEFAULT_TRAINING_PLANS_DIR        : Path of directory for storing default training plans</li> <li>TRAINING_PLANS_DIR                 : Path of directory for storing registered training plans</li> <li>TRAINING_PLAN_APPROVAL            : True if the node enables training plan approval</li> <li>ALLOW_DEFAULT_TRAINING_PLANS      : True if the node enables default training plans for training plan approval</li> </ul> <p>Common Global Variables:</p> <ul> <li>COMPONENT_TYPE          : Node or Researcher</li> <li>CONFIG_DIR              : Configuration file path</li> <li>VAR_DIR                 : Var directory of Fed-BioMed</li> <li>CACHE_DIR               : Cache directory of Fed-BioMed</li> <li>TMP_DIR                 : Temporary directory</li> <li>MQTT_BROKER             : MQTT broker IP address</li> <li>MQTT_BROKER_PORT        : MQTT broker port</li> <li>UPLOADS_URL             : Upload URL for file repository</li> <li>MPSPDZ_IP               : MP-SPDZ endpoint IP of component</li> <li>DEFAULT_BIPRIMES_DIR    : Path of directory for storing default secure aggregation biprimes</li> <li>ALLOW_DEFAULT_BIPRIMES  : True if the component enables the default secure aggregation biprimes</li> <li>PORT_INCREMENT_FILE     : File for storing next port to be allocated for MP-SPDZ</li> <li>CERT_DIR                : Directory for storing certificates for MP-SPDZ</li> <li>DEFAULT_BIPRIMES_DIR    : Directory for storing default biprimes files</li> </ul>"},{"location":"developer/api/common/environ/#fedbiomed.common.environ-classes","title":"Classes","text":""},{"location":"developer/api/common/environ/#fedbiomed.common.environ.Environ","title":"Environ","text":"CLASS  <pre><code>Environ(root_dir=None)\n</code></pre> <p>Singleton class contains all variables for researcher or node</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>if not provided the directory is deduced from the package location (specifying root_dir is mainly used by the test files)</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedEnvironError</code> <p>If component type is invalid</p> Source code in <code>fedbiomed/common/environ.py</code> <pre><code>def __init__(self, root_dir: str = None):\n\"\"\"Class constructor\n    Args:\n        root_dir: if not provided the directory is deduced from the package location\n            (specifying root_dir is mainly used by the test files)\n    Raises:\n        FedbiomedEnvironError: If component type is invalid\n    \"\"\"\n# dict with contains all configuration values\nself._values = {}\nself._cfg = configparser.ConfigParser()\nself._root_dir = root_dir\n</code></pre>"},{"location":"developer/api/common/environ/#fedbiomed.common.environ.Environ-functions","title":"Functions","text":""},{"location":"developer/api/common/environ/#fedbiomed.common.environ.Environ.check_and_set_config_file_version","title":"<pre><code>check_and_set_config_file_version(version_from_runtime)\n</code></pre>","text":"<p>Check compatibility of config file and set corresponding environment value.</p> <p>Parameters:</p> Name Type Description Default <code>version_from_runtime</code> <code>Union[str, FBM_Component_Version]</code> <p>the version hardcoded in common/constants.py</p> required Source code in <code>fedbiomed/common/environ.py</code> <pre><code>def check_and_set_config_file_version(self,\nversion_from_runtime: Union[str, FBM_Component_Version]):\n\"\"\"Check compatibility of config file and set corresponding environment value.\n    Args:\n        version_from_runtime: the version hardcoded in common/constants.py\n    \"\"\"\ntry:\nconfig_file_version = self.from_config('default', 'version')\nexcept FedbiomedEnvironError:\nconfig_file_version = fedbiomed.common.utils.__default_version__\nraise_for_version_compatibility(config_file_version, version_from_runtime,\nf\"Configuration file {self._values['CONFIG_FILE']}: \"\nf\"found version %s expected version %s\")\nself._values[\"CONFIG_FILE_VERSION\"] = config_file_version\n</code></pre>"},{"location":"developer/api/common/environ/#fedbiomed.common.environ.Environ.default_config_file","title":"<pre><code>default_config_file()\n</code></pre>  <code>abstractmethod</code>","text":"<p>Abstract method for retrieving default configuration file path</p> Source code in <code>fedbiomed/common/environ.py</code> <pre><code>@abstractmethod\ndef default_config_file(self) -&gt; str:\n\"\"\"Abstract method for retrieving default configuration file path\"\"\"\n</code></pre>"},{"location":"developer/api/common/environ/#fedbiomed.common.environ.Environ.from_config","title":"<pre><code>from_config(section, key)\n</code></pre>","text":"<p>Gets values from config file</p> <p>Parameters:</p> Name Type Description Default <code>section</code> <p>the section of the key</p> required <code>key</code> <p>the name of the key</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The value of the key</p> <p>Raises:</p> Type Description <code>FedbiomedEnvironError</code> <p>If the key does not exist in the configuration</p> Source code in <code>fedbiomed/common/environ.py</code> <pre><code>def from_config(self, section, key) -&gt; Any:\n\"\"\"Gets values from config file\n    Args:\n        section: the section of the key\n        key: the name of the key\n    Returns:\n        The value of the key\n    Raises:\n        FedbiomedEnvironError: If the key does not exist in the configuration\n    \"\"\"\ntry:\n_cfg_value = self._cfg.get(section, key)\nexcept configparser.Error:\n_msg = f\"{ErrorNumbers.FB600.value}: no {section}/{key} in config file. Please recreate a new config file\"\nlogger.error(_msg)\nraise FedbiomedEnvironError(_msg)\nreturn _cfg_value\n</code></pre>"},{"location":"developer/api/common/environ/#fedbiomed.common.environ.Environ.info","title":"<pre><code>info()\n</code></pre>  <code>abstractmethod</code>","text":"<p>Abstract method to return component information</p> Source code in <code>fedbiomed/common/environ.py</code> <pre><code>@abstractmethod\ndef info(self):\n\"\"\"Abstract method to return component information\"\"\"\n</code></pre>"},{"location":"developer/api/common/environ/#fedbiomed.common.environ.Environ.parse_write_config_file","title":"<pre><code>parse_write_config_file(new=False)\n</code></pre>","text":"<p>Parses configuration file.</p> <p>Create new config file if it is not existing.</p> <p>Parameters:</p> Name Type Description Default <code>new</code> <code>bool</code> <p>True if configuration file is not expected to exist</p> <code>False</code> Raise <p>FedbiomedEnvironError: cannot read configuration file</p> Source code in <code>fedbiomed/common/environ.py</code> <pre><code>def parse_write_config_file(self, new: bool = False):\n\"\"\"Parses configuration file.\n    Create new config file if it is not existing.\n    Args:\n        new: True if configuration file is not expected to exist\n    Raise:\n        FedbiomedEnvironError: cannot read configuration file\n    \"\"\"\n# Sets configuration file path\nself.set_config_file()\n# Parse configuration if it is existing\nif os.path.isfile(self._values[\"CONFIG_FILE\"]) and not new:\n# get values from .ini file\ntry:\nself._cfg.read(self._values[\"CONFIG_FILE\"])\nexcept configparser.Error:\n_msg = ErrorNumbers.FB600.value + \": cannot read config file, check file permissions\"\nlogger.critical(_msg)\nraise FedbiomedEnvironError(_msg)\n# Create new configuration file\nelse:\n# Create new config file\nself._set_component_specific_config_parameters()\n# Updates config file with MQTT configuration\nself._configure_mqtt()\n# Update config with secure aggregation parameters\nself._configure_secure_aggregation()\n# Writes config file to a file\nself._write_config_file()\n</code></pre>"},{"location":"developer/api/common/environ/#fedbiomed.common.environ.Environ.set_config_file","title":"<pre><code>set_config_file()\n</code></pre>","text":"<p>Sets configuration file</p> Source code in <code>fedbiomed/common/environ.py</code> <pre><code>def set_config_file(self):\n\"\"\"Sets configuration file \"\"\"\nconfig_file = os.getenv('CONFIG_FILE')\nif config_file:\nif not os.path.isabs(config_file):\nconfig_file = os.path.join(self._values['CONFIG_DIR'],\nos.getenv('CONFIG_FILE'))\nelse:\nconfig_file = self.default_config_file()\nself._values[\"CONFIG_FILE\"] = config_file\n</code></pre>"},{"location":"developer/api/common/environ/#fedbiomed.common.environ.Environ.setup_environment","title":"<pre><code>setup_environment()\n</code></pre>","text":"<p>Final environment setup function</p> Source code in <code>fedbiomed/common/environ.py</code> <pre><code>def setup_environment(self):\n\"\"\"Final environment setup function \"\"\"\n# Initialize common environment variables\nself._initialize_common_variables()\n# Parse config file or create if not existing\nself.parse_write_config_file()\n# Check that config version is compatible\nself._check_config_version()\n# Configuring network variables\nself._set_network_variables()\n# Initialize environment variables\nself._set_component_specific_variables()\n</code></pre>"},{"location":"developer/api/common/exceptions/","title":"Exceptions","text":""},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions","title":"fedbiomed.common.exceptions","text":"Module: <code>fedbiomed.common.exceptions</code> <p>All the fedbiomed errors/Exceptions</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions-classes","title":"Classes","text":""},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedAggregatorError","title":"FedbiomedAggregatorError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception specific to the Aggregator classes/subclasses.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedCertificateError","title":"FedbiomedCertificateError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Certificate error</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDPControllerError","title":"FedbiomedDPControllerError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exceptions specific for the class DPController</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDataLoadingPlanError","title":"FedbiomedDataLoadingPlanError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exceptions specific for the class fedbiomed.common.data.DataLoadingPlan.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDataLoadingPlanValueError","title":"FedbiomedDataLoadingPlanValueError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exceptions similar to Value Error for a DataLoadingPlan.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDataManagerError","title":"FedbiomedDataManagerError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception for DataManager errors.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDataQualityCheckError","title":"FedbiomedDataQualityCheckError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception raised when facing uncompatibles datatypes accross nodes</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDatasetError","title":"FedbiomedDatasetError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Generic exception for a Dataset class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDatasetManagerError","title":"FedbiomedDatasetManagerError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exceptions specific for the class DatasetManager.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedDatasetValueError","title":"FedbiomedDatasetValueError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>ValueErrors raised by any Dataset class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedEnvironError","title":"FedbiomedEnvironError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception specific to the Environ class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedError","title":"FedbiomedError","text":"<p>           Bases: <code>Exception</code></p> <p>Top class of all our exceptions.</p> <p>this allows to catch every Fedbiomed*Errors in a single except block</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedExperimentError","title":"FedbiomedExperimentError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception specific to the Experiment class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedFederatedDataSetError","title":"FedbiomedFederatedDataSetError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception specific to the FederatedDataSetError class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedLoadingBlockError","title":"FedbiomedLoadingBlockError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception specific to the DataLoadingBlock classes/subclasses.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedLoadingBlockValueError","title":"FedbiomedLoadingBlockValueError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception similar to ValueError for a DataLoadingBlock.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedLoggerError","title":"FedbiomedLoggerError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception specific to the Logger class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedMPCControllerError","title":"FedbiomedMPCControllerError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Certificate error</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedMessageError","title":"FedbiomedMessageError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception specific to the Message class, usually a badly formed message.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedMessagingError","title":"FedbiomedMessagingError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception specific to the Messaging (communication) class.</p> <p>Usually a problem with the communication framework</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedMetricError","title":"FedbiomedMetricError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception raised when evualution fails because of inconsistence in using the metric.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedModelError","title":"FedbiomedModelError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exceptions triggered from Model class</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedOptimizerError","title":"FedbiomedOptimizerError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception raised when an error is encountered within <code>Optimizer</code> code.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedRepositoryError","title":"FedbiomedRepositoryError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception of the <code>Repository</code> class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedResponsesError","title":"FedbiomedResponsesError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception specific to Responses class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedRoundError","title":"FedbiomedRoundError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exceptions specific for the node round class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedSecaggCrypterError","title":"FedbiomedSecaggCrypterError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Secure aggregation encryption error</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedSecaggError","title":"FedbiomedSecaggError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exceptions specific for the researcher secure aggregation class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedSecureAggregationError","title":"FedbiomedSecureAggregationError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Secure aggregation error</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedSilentTerminationError","title":"FedbiomedSilentTerminationError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception for silently terminating the researcher from a notebook.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedSkLearnDataManagerError","title":"FedbiomedSkLearnDataManagerError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exceptions specific for the class SkLearnDataset.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedStrategyError","title":"FedbiomedStrategyError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception specific to the Strategy class and subclasses.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedTaskQueueError","title":"FedbiomedTaskQueueError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception specific to the internal queuing system.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedTorchDataManagerError","title":"FedbiomedTorchDataManagerError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exceptions specific for the class TorchDataset.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedTrainingError","title":"FedbiomedTrainingError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception raised then training fails.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedTrainingPlanError","title":"FedbiomedTrainingPlanError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception specific to errors while getting source of the model class.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedTrainingPlanSecurityManagerError","title":"FedbiomedTrainingPlanSecurityManagerError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception specific to the TrainingPlanSecurityManager.</p> <p>(from fedbiomed.common.model_manager)</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedTypeError","title":"FedbiomedTypeError","text":"<p>           Bases: <code>FedbiomedError</code>, <code>TypeError</code></p> <p>TypeError for Fed-BioMed</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedUserInputError","title":"FedbiomedUserInputError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Exception raised then user input is invalid.</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedValueError","title":"FedbiomedValueError","text":"<p>           Bases: <code>FedbiomedError</code>, <code>ValueError</code></p> <p>ValueError for Fed-BioMed</p>"},{"location":"developer/api/common/exceptions/#fedbiomed.common.exceptions.FedbiomedVersionError","title":"FedbiomedVersionError","text":"<p>           Bases: <code>FedbiomedError</code></p> <p>Error in the versions of one of Fed-BioMed's components</p>"},{"location":"developer/api/common/json/","title":"Json","text":""},{"location":"developer/api/common/json/#fedbiomed.common.json","title":"fedbiomed.common.json","text":"Module: <code>fedbiomed.common.json</code> <p>This module defines message serializer and deserializer for sending / receiving / parsing messages through Messaging.</p> <p>Compared to the usual json module, it deals with some fedbiomed data types which are not serialized by default (eg: enumerations)</p>"},{"location":"developer/api/common/json/#fedbiomed.common.json-functions","title":"Functions","text":""},{"location":"developer/api/common/json/#fedbiomed.common.json.deserialize_msg","title":"<pre><code>deserialize_msg(msg)\n</code></pre>","text":"<p>Deserializes a JSON string or bytes message as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Union[str, bytes]</code> <p>message in JSON format but encoded as string or bytes</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Parsed message as python dictionary.</p> Source code in <code>fedbiomed/common/json.py</code> <pre><code>def deserialize_msg(msg: Union[str, bytes]) -&gt; dict:\n\"\"\"Deserializes a JSON string or bytes message as a dictionary.\n    Args:\n        msg: message in JSON format but encoded as string or bytes\n    Returns:\n        Parsed message as python dictionary.\n    \"\"\"\ndecode = json.loads(msg)\n# deserialize our own types/classes\ndecode = _deserialize_test_metric(decode)\n# errnum is present in ErrorMessage and is an Enum\n# which need to be deserialized\nif 'errnum' in decode:\nerrnum = decode['errnum']\nfound = False\nfor e in ErrorNumbers:\nif e.value == errnum:\nfound = True\ndecode['errnum'] = e\nbreak\nif not found:\n# error code sent by the node is unknown\ndecode['errnum'] = ErrorNumbers.FB999\nreturn decode\n</code></pre>"},{"location":"developer/api/common/json/#fedbiomed.common.json.serialize_msg","title":"<pre><code>serialize_msg(msg)\n</code></pre>","text":"<p>Serialize an object as a JSON message (applies for dict-like objects)</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict</code> <p>dict-like object containing the message to send.</p> required <p>Returns:</p> Type Description <code>str</code> <p>JSON parsed message ready to transmit.</p> Source code in <code>fedbiomed/common/json.py</code> <pre><code>def serialize_msg(msg: dict) -&gt; str:\n\"\"\"Serialize an object as a JSON message (applies for dict-like objects)\n    Args:\n        msg: dict-like object containing the message to send.\n    Returns:\n        JSON parsed message ready to transmit.\n    \"\"\"\n# serialize our own types/classes\nmsg = _serialize_training_args(msg)\nmsg = _serialize_test_metric(msg)\n# Errnum is present in ErrorMessage and is an Enum\n# which need to be serialized\nif 'errnum' in msg:\nmsg['errnum'] = msg['errnum'].value\nreturn json.dumps(msg)\n</code></pre>"},{"location":"developer/api/common/logger/","title":"Logger","text":""},{"location":"developer/api/common/logger/#fedbiomed.common.logger","title":"fedbiomed.common.logger","text":"Module: <code>fedbiomed.common.logger</code> <p>Global logger for fedbiomed</p> <p>Written above origin Logger class provided by python.</p> <p>Following features were added from to the original module:</p> <ul> <li>provides a logger instance of FedLogger, which is also a singleton, so it can be used \"as is\"</li> <li>provides a dedicated file handler</li> <li>provides a JSON/MQTT handler: all messages with priority greater than error are sent to the MQQT handler (this permit to send error messages from a node to a researcher)</li> <li>works on python scripts / ipython / notebook</li> <li>manages a dictionary of handlers. Default keys are 'CONSOLE', 'MQTT', 'FILE',   but any key is allowed (only one handler by key)</li> <li>allow changing log level globally, or on a specific handler (using its key)</li> <li>log levels can be provided as string instead of logging.* levels (no need to   import logging in caller's code) just as in the initial python logger</li> </ul> <p>A typical usage is:</p> <pre><code>from fedbiomed.common.logger import logger\n\nlogger.info(\"information message\")\n</code></pre> <p>All methods of the original python logger are provided. To name a few:</p> <ul> <li>logger.debug()</li> <li>logger.info()</li> <li>logger.warning()</li> <li>logger.error()</li> <li>logger.critical()</li> </ul> <p>Contrary to other Fed-BioMed classes, the API of FedLogger is compliant with the coding conventions used for logger (lowerCameCase)</p> <p>Dependency issue</p> <p>Please pay attention to not create dependency loop then importing other fedbiomed package</p>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger-attributes","title":"Attributes","text":""},{"location":"developer/api/common/logger/#fedbiomed.common.logger.DEFAULT_LOG_FILE","title":"DEFAULT_LOG_FILE     <code>module-attribute</code>","text":"<pre><code>DEFAULT_LOG_FILE = 'mylog.log'\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.DEFAULT_LOG_LEVEL","title":"DEFAULT_LOG_LEVEL     <code>module-attribute</code>","text":"<pre><code>DEFAULT_LOG_LEVEL = logging.WARNING\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.DEFAULT_LOG_TOPIC","title":"DEFAULT_LOG_TOPIC     <code>module-attribute</code>","text":"<pre><code>DEFAULT_LOG_TOPIC = 'general/logger'\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.logger","title":"logger     <code>module-attribute</code>","text":"<pre><code>logger = FedLogger()\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger-classes","title":"Classes","text":""},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger","title":"FedLogger","text":"CLASS  <pre><code>FedLogger(level=DEFAULT_LOG_LEVEL)\n</code></pre> <p>Base class for the logger. it uses python logging module by composition (only log() method is overwritten)</p> <p>All methods from the logging module can be accessed through the _logger member of the class if necessary (instead of overloading all the methods) (ex:  logger._logger.getEffectiveLevel() )</p> <p>Should not be imported</p> <p>An initial console logger is installed (so the logger has at minimum one handler)</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>initial loglevel. This loglevel will be the default for all handlers, if called without the default level</p> <code>DEFAULT_LOG_LEVEL</code> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def __init__(self, level: str = DEFAULT_LOG_LEVEL):\n\"\"\"Constructor of base class\n    An initial console logger is installed (so the logger has at minimum one handler)\n    Args:\n        level: initial loglevel. This loglevel will be the default for all handlers, if called\n            without the default level\n    \"\"\"\n# internal tables\n# transform string to logging.level\nself._nameToLevel = {\n\"DEBUG\": logging.DEBUG,\n\"INFO\": logging.INFO,\n\"WARNING\": logging.WARNING,\n\"ERROR\": logging.ERROR,\n\"CRITICAL\": logging.CRITICAL,\n}\n# transform logging.level to string\nself._levelToName = {\nlogging.DEBUG: \"DEBUG\",\nlogging.INFO: \"INFO\",\nlogging.WARNING: \"WARNING\",\nlogging.ERROR: \"ERROR\",\nlogging.CRITICAL: \"CRITICAL\"\n}\n# name this logger\nself._logger = logging.getLogger(\"fedbiomed\")\n# Do not propagate (avoids log duplication when third party libraries uses logging module)\nself._logger.propagate = False\nself._default_level = DEFAULT_LOG_LEVEL  # MANDATORY ! KEEP THIS PLEASE !!!\nself._default_level = self._internalLevelTranslator(level)\nself._logger.setLevel(self._default_level)\n# init the handlers list and add a console handler on startup\nself._handlers = {}\nself.addConsoleHandler()\npass\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger-functions","title":"Functions","text":""},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.addConsoleHandler","title":"<pre><code>addConsoleHandler(format='%(asctime)s %(name)s %(levelname)s - %(message)s', level=DEFAULT_LOG_LEVEL)\n</code></pre>","text":"<p>Adds a console handler</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>str</code> <p>the format string of the logger</p> <code>'%(asctime)s %(name)s %(levelname)s - %(message)s'</code> <code>level</code> <code>Any</code> <p>initial level of the logger for this handler (optional) if not given, the default level is set</p> <code>DEFAULT_LOG_LEVEL</code> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def addConsoleHandler(self,\nformat: str = '%(asctime)s %(name)s %(levelname)s - %(message)s',\nlevel: Any = DEFAULT_LOG_LEVEL):\n\"\"\"Adds a console handler\n    Args:\n        format: the format string of the logger\n        level: initial level of the logger for this handler (optional) if not given, the default level is set\n    \"\"\"\nhandler = logging.StreamHandler()\nhandler.setLevel(self._internalLevelTranslator(level))\nformatter = logging.Formatter(format)\nhandler.setFormatter(formatter)\nself._internalAddHandler(\"CONSOLE\", handler)\npass\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.addFileHandler","title":"<pre><code>addFileHandler(filename=DEFAULT_LOG_FILE, format='%(asctime)s %(name)s %(levelname)s - %(message)s', level=DEFAULT_LOG_LEVEL)\n</code></pre>","text":"<p>Adds a file handler</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>File to log to</p> <code>DEFAULT_LOG_FILE</code> <code>format</code> <code>str</code> <p>Log format</p> <code>'%(asctime)s %(name)s %(levelname)s - %(message)s'</code> <code>level</code> <code>any</code> <p>Initial level of the logger (optionnal)</p> <code>DEFAULT_LOG_LEVEL</code> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def addFileHandler(self,\nfilename: str = DEFAULT_LOG_FILE,\nformat: str = '%(asctime)s %(name)s %(levelname)s - %(message)s',\nlevel: any = DEFAULT_LOG_LEVEL):\n\"\"\"Adds a file handler\n    Args:\n        filename: File to log to\n        format: Log format\n        level: Initial level of the logger (optionnal)\n    \"\"\"\nhandler = logging.FileHandler(filename=filename, mode='a')\nhandler.setLevel(self._internalLevelTranslator(level))\nformatter = logging.Formatter(format)\nhandler.setFormatter(formatter)\nself._internalAddHandler(\"FILE\", handler)\npass\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.addMqttHandler","title":"<pre><code>addMqttHandler(mqtt=None, node_id=None, topic=DEFAULT_LOG_TOPIC, level=logging.ERROR)\n</code></pre>","text":"<p>Adds a mqtt handler, to publish error message on a topic</p> <p>Parameters:</p> Name Type Description Default <code>mqtt</code> <code>Any</code> <p>already opened MQTT object</p> <code>None</code> <code>node_id</code> <code>str</code> <p>id of the caller (necessary for msg formatting to the researcher)</p> <code>None</code> <code>topic</code> <code>Any</code> <p>topic to publish to (non-mandatory)</p> <code>DEFAULT_LOG_TOPIC</code> <code>level</code> <code>Any</code> <p>level of this handler (non-mandatory) level must be lower than ERROR to ensure that the research get all ERROR/CRITICAL messages</p> <code>logging.ERROR</code> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def addMqttHandler(self,\nmqtt: Any = None,\nnode_id: str = None,\ntopic: Any = DEFAULT_LOG_TOPIC,\nlevel: Any = logging.ERROR\n):\n\"\"\"Adds a mqtt handler, to publish error message on a topic\n    Args:\n        mqtt: already opened MQTT object\n        node_id: id of the caller (necessary for msg formatting to the researcher)\n        topic: topic to publish to (non-mandatory)\n        level: level of this handler (non-mandatory) level must be lower than ERROR to ensure that the\n            research get all ERROR/CRITICAL messages\n    \"\"\"\nhandler = _MqttHandler(\nmqtt=mqtt,\nnode_id=node_id,\ntopic=topic\n)\n# may be not necessary ?\nhandler.setLevel(self._internalLevelTranslator(level))\nformatter = _MqttFormatter(node_id)\nhandler.setFormatter(formatter)\nself._internalAddHandler(\"MQTT\", handler)\n# as a side effect this will set the minimal level to ERROR\nself.setLevel(level, \"MQTT\")\npass\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.delMqttHandler","title":"<pre><code>delMqttHandler()\n</code></pre>","text":"Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def delMqttHandler(self):\nself._internalAddHandler(\"MQTT\", None)\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.log","title":"<pre><code>log(level, msg)\n</code></pre>","text":"<p>Overrides the logging.log() method to allow the use of string instead of a logging.* level</p> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def log(self, level: Any, msg: str):\n\"\"\"Overrides the logging.log() method to allow the use of string instead of a logging.* level \"\"\"\nlevel = logger._internalLevelTranslator(level)\nself._logger.log(\nlevel,\nmsg\n)\n</code></pre>"},{"location":"developer/api/common/logger/#fedbiomed.common.logger.FedLogger.setLevel","title":"<pre><code>setLevel(level, htype=None)\n</code></pre>","text":"<p>Overrides the setLevel method, to deal with level given as a string and to change le level of one or all known handlers</p> <p>This also change the default level for all future handlers.</p> <p>Remark</p> <p>Level should not be lower than CRITICAL (meaning CRITICAL errors are always displayed)</p> <p>Example: <pre><code>setLevel( logging.DEBUG, 'FILE')\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>level</code> <p>level to modify, can be a string or a logging.* level (mandatory)</p> required <code>htype</code> <p>if provided (non-mandatory), change the level of the given handler. if not  provided (or None), change the level of all known handlers</p> <code>None</code> Source code in <code>fedbiomed/common/logger.py</code> <pre><code>def setLevel(self, level: Any, htype: Any = None):\n\"\"\"Overrides the setLevel method, to deal with level given as a string and to change le level of\n    one or all known handlers\n    This also change the default level for all future handlers.\n    !!! info \"Remark\"\n        Level should not be lower than CRITICAL (meaning CRITICAL errors are always displayed)\n        Example:\n        ```python\n        setLevel( logging.DEBUG, 'FILE')\n        ```\n    Args:\n        level : level to modify, can be a string or a logging.* level (mandatory)\n        htype : if provided (non-mandatory), change the level of the given handler. if not  provided (or None),\n            change the level of all known handlers\n    \"\"\"\nlevel = self._internalLevelTranslator(level)\nif htype is None:\n# store this level (for future handler adding)\nself._logger.setLevel(level)\nfor h in self._handlers:\nself._handlers[h].setLevel(level)\nreturn\nif htype in self._handlers:\nself._handlers[htype].setLevel(level)\nreturn\n# htype provided but no handler for this type exists\nself._logger.warning(htype + \" handler not initialized yet\")\n</code></pre>"},{"location":"developer/api/common/message/","title":"Message","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message","title":"fedbiomed.common.message","text":"Module: <code>fedbiomed.common.message</code> <p>Definition of messages exchanged by the researcher and the nodes</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message-classes","title":"Classes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply","title":"AddScalarReply    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a add_scalar message sent by the node.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that receives the reply</p> <code>job_id</code> <code>str</code> <p>ID of the Job that is sent by researcher</p> <code>train</code> <code>bool</code> <p>Declares whether scalar value is for training</p> <code>test</code> <code>bool</code> <p>Declares whether scalar value is for validation</p> <code>test_on_local_updates</code> <code>bool</code> <p>Declares whether validation is performed over locally updated parameters</p> <code>test_on_global_updates</code> <code>bool</code> <p>Declares whether validation is performed over aggregated parameters</p> <code>metric</code> <code>dict</code> <p>Evaluation metroc</p> <code>epoch</code> <code>(int, type(None))</code> <p>Scalar is received at</p> <code>total_samples</code> <code>int</code> <p>Number of all samples in dataset</p> <code>batch_samples</code> <code>int</code> <p>Number of samples in batch</p> <code>num_batches</code> <code>int</code> <p>Number of batches in single epoch</p> <code>iteration</code> <code>int</code> <p>Scalar is received at</p> <code>command</code> <code>str</code> <p>Reply command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.batch_samples","title":"batch_samples     <code>instance-attribute</code>","text":"<pre><code>batch_samples: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.epoch","title":"epoch     <code>instance-attribute</code>","text":"<pre><code>epoch: (int, type(None))\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.iteration","title":"iteration     <code>instance-attribute</code>","text":"<pre><code>iteration: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.job_id","title":"job_id     <code>instance-attribute</code>","text":"<pre><code>job_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.metric","title":"metric     <code>instance-attribute</code>","text":"<pre><code>metric: dict\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.node_id","title":"node_id     <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.num_batches","title":"num_batches     <code>instance-attribute</code>","text":"<pre><code>num_batches: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.num_samples_trained","title":"num_samples_trained     <code>instance-attribute</code>","text":"<pre><code>num_samples_trained: (int, type(None))\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.test","title":"test     <code>instance-attribute</code>","text":"<pre><code>test: bool\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.test_on_global_updates","title":"test_on_global_updates     <code>instance-attribute</code>","text":"<pre><code>test_on_global_updates: bool\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.test_on_local_updates","title":"test_on_local_updates     <code>instance-attribute</code>","text":"<pre><code>test_on_local_updates: bool\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.total_samples","title":"total_samples     <code>instance-attribute</code>","text":"<pre><code>total_samples: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.AddScalarReply.train","title":"train     <code>instance-attribute</code>","text":"<pre><code>train: bool\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply","title":"ApprovalReply    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes the TrainingPlan approval reply (acknoledge) from node to researcher.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that will receive the reply</p> <code>node_id</code> <code>str</code> <p>Node id that replys the request</p> <code>sequence</code> <code>int</code> <p>sequence number of the corresponding request</p> <code>status</code> <code>int</code> <p>status code received after uploading the training plan (usually HTTP status)</p> <code>command</code> <code>str</code> <p>Reply command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply.node_id","title":"node_id     <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply.sequence","title":"sequence     <code>instance-attribute</code>","text":"<pre><code>sequence: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply.status","title":"status     <code>instance-attribute</code>","text":"<pre><code>status: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalReply.success","title":"success     <code>instance-attribute</code>","text":"<pre><code>success: bool\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalRequest","title":"ApprovalRequest    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes the TrainingPlan approval request from researcher to node.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>id of the researcher that sends the request</p> <code>description</code> <code>str</code> <p>description of the training plan</p> <code>sequence</code> <code>int</code> <p>(unique) sequence number which identifies the message</p> <code>training_plan_url</code> <code>str</code> <p>URL where TrainingPlan is available</p> <code>command</code> <code>str</code> <p>request command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalRequest.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalRequest.description","title":"description     <code>instance-attribute</code>","text":"<pre><code>description: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalRequest.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalRequest.sequence","title":"sequence     <code>instance-attribute</code>","text":"<pre><code>sequence: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ApprovalRequest.training_plan_url","title":"training_plan_url     <code>instance-attribute</code>","text":"<pre><code>training_plan_url: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ErrorMessage","title":"ErrorMessage    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes an error message sent by the node.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that receives the error message</p> <code>node_id</code> <code>str</code> <p>ID of the node that sends error message</p> <code>errnum</code> <code>ErrorNumbers</code> <p>Error ID/Number</p> <code>extra_msg</code> <code>str</code> <p>Additional message regarding the error</p> <code>command</code> <code>str</code> <p>Reply command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ErrorMessage-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.ErrorMessage.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ErrorMessage.errnum","title":"errnum     <code>instance-attribute</code>","text":"<pre><code>errnum: ErrorNumbers\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ErrorMessage.extra_msg","title":"extra_msg     <code>instance-attribute</code>","text":"<pre><code>extra_msg: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ErrorMessage.node_id","title":"node_id     <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ErrorMessage.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply","title":"ListReply    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>This class describes a list reply message sent by the node that includes list of datasets. It is a reply for ListRequest message from the researcher.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that sends the request</p> <code>succes</code> <code>str</code> <p>True if the node process the request as expected, false if any exception occurs</p> <code>databases</code> <code>list</code> <p>List of datasets</p> <code>node_id</code> <code>str</code> <p>Node id that replys the request</p> <code>count</code> <code>int</code> <p>Number of datasets</p> <code>command</code> <code>str</code> <p>Reply command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply.count","title":"count     <code>instance-attribute</code>","text":"<pre><code>count: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply.databases","title":"databases     <code>instance-attribute</code>","text":"<pre><code>databases: list\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply.node_id","title":"node_id     <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListReply.success","title":"success     <code>instance-attribute</code>","text":"<pre><code>success: bool\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListRequest","title":"ListRequest    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a list request message sent by the researcher to nodes in order to list datasets belonging to each node.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that sends the request</p> <code>command</code> <code>str</code> <p>Request command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.ListRequest.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ListRequest.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.LogMessage","title":"LogMessage    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a log message sent by the node.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that will receive the log message</p> <code>node_id</code> <code>str</code> <p>ID of the node that sends log message</p> <code>level</code> <code>str</code> <p>Log level</p> <code>msg</code> <code>str</code> <p>Log message</p> <code>command</code> <code>str</code> <p>Reply command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.LogMessage-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.LogMessage.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.LogMessage.level","title":"level     <code>instance-attribute</code>","text":"<pre><code>level: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.LogMessage.msg","title":"msg     <code>instance-attribute</code>","text":"<pre><code>msg: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.LogMessage.node_id","title":"node_id     <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.LogMessage.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Message","title":"Message","text":"<p>           Bases: <code>object</code></p> <p>Base class for all fedbiomed messages providing all methods to access the messages</p> <p>The subclasses of this class will be pure data containers (no provided functions)</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Message-functions","title":"Functions","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.Message.get_dict","title":"<pre><code>get_dict()\n</code></pre>","text":"<p>Returns pairs (Message class attributes name, attributes values) into a dictionary</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Message as dictionary</p> Source code in <code>fedbiomed/common/message.py</code> <pre><code>def get_dict(self) -&gt; Dict[str, Any]:\n\"\"\"Returns pairs (Message class attributes name, attributes values) into a dictionary\n    Returns:\n        Message as dictionary\n    \"\"\"\nreturn self.__dict__\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.Message.get_param","title":"<pre><code>get_param(param)\n</code></pre>","text":"<p>Get the value of a given param</p> <p>Parameters:</p> Name Type Description Default <code>param</code> <code>str</code> <p>name of the param</p> required Source code in <code>fedbiomed/common/message.py</code> <pre><code>def get_param(self, param: str):\n\"\"\"Get the value of a given param\n    Args:\n        param: name of the param\n    \"\"\"\nreturn getattr(self, param)\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.MessageFactory","title":"MessageFactory","text":"<p>Pack message contents into the appropriate Message class.</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.MessageFactory-functions","title":"Functions","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.MessageFactory.format_incoming_message","title":"<pre><code>format_incoming_message(params)\n</code></pre>  <code>classmethod</code>","text":"<p>Format a dictionary representing an incoming message into the appropriate Message class.</p> <p>Packs the input into the appropriate Message class representing an incoming message. The type of Message class is inferred from the <code>command</code> key in the input dictionary. This function also validates:</p> <ul> <li>the legacy of the message</li> <li>the structure of the received message</li> </ul> <p>Attributes:</p> Name Type Description <code>params</code> <p>the dictionary of key-value pairs extracted from the received message.</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>if 'command' field is not present in <code>params</code></p> <code>FedbiomedMessageError</code> <p>if the component is not allowed to receive the message</p> <p>Returns:</p> Type Description <code>Message</code> <p>The received message formatted as an instance of the appropriate Message class</p> Source code in <code>fedbiomed/common/message.py</code> <pre><code>@classmethod\ndef format_incoming_message(cls, params: Dict[str, Any]) -&gt; Message:\n\"\"\"Format a dictionary representing an incoming message into the appropriate Message class.\n    Packs the input into the appropriate Message class representing an incoming message.\n    The type of Message class is inferred from the `command` key in the input dictionary.\n    This function also validates:\n    - the legacy of the message\n    - the structure of the received message\n    Attributes:\n        params: the dictionary of key-value pairs extracted from the received message.\n    Raises:\n        FedbiomedMessageError: if 'command' field is not present in `params`\n        FedbiomedMessageError: if the component is not allowed to receive the message\n    Returns:\n        The received message formatted as an instance of the appropriate Message class\n    \"\"\"\nMessageFactory._raise_for_missing_command(params)\nmessage_type = params['command']\nMessageFactory._validate_message_type_or_raise(message_type, cls.INCOMING_MESSAGE_TYPE_TO_CLASS_MAP)\nreturn cls.INCOMING_MESSAGE_TYPE_TO_CLASS_MAP[message_type](**params)\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.MessageFactory.format_outgoing_message","title":"<pre><code>format_outgoing_message(params)\n</code></pre>  <code>classmethod</code>","text":"<p>Format a dictionary representing an outgoing message into the appropriate Message class.</p> <p>Packs the input into the appropriate Message class representing an outbound message. The type of Message class is inferred from the <code>command</code> key in the input dictionary. This function also validates:</p> <ul> <li>the legacy of the message</li> <li>the structure of the received message</li> </ul> <p>Attributes:</p> Name Type Description <code>params</code> <p>the dictionary of key-value pairs to be packed into the outgoing message.</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>if 'command' field is not present in <code>params</code></p> <code>FedbiomedMessageError</code> <p>if the component is not allowed to send the message</p> <p>Returns:</p> Type Description <code>Message</code> <p>The outbound message formatted as an instance of the appropriate Message class</p> Source code in <code>fedbiomed/common/message.py</code> <pre><code>@classmethod\ndef format_outgoing_message(cls, params: Dict[str, Any]) -&gt; Message:\n\"\"\"Format a dictionary representing an outgoing message into the appropriate Message class.\n    Packs the input into the appropriate Message class representing an outbound message.\n    The type of Message class is inferred from the `command` key in the input dictionary.\n    This function also validates:\n    - the legacy of the message\n    - the structure of the received message\n    Attributes:\n        params: the dictionary of key-value pairs to be packed into the outgoing message.\n    Raises:\n        FedbiomedMessageError: if 'command' field is not present in `params`\n        FedbiomedMessageError: if the component is not allowed to send the message\n    Returns:\n        The outbound message formatted as an instance of the appropriate Message class\n    \"\"\"\nMessageFactory._raise_for_missing_command(params)\nmessage_type = params['command']\nMessageFactory._validate_message_type_or_raise(message_type, cls.OUTGOING_MESSAGE_TYPE_TO_CLASS_MAP)\nparams['protocol_version'] = str(__messaging_protocol_version__)  # inject procotol version only in outgoing msg\nreturn cls.OUTGOING_MESSAGE_TYPE_TO_CLASS_MAP[message_type](**params)\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.NodeMessages","title":"NodeMessages","text":"<p>           Bases: <code>MessageFactory</code></p> <p>Specializes MessageFactory for Node.</p> <p>Node send replies and receive requests.</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.NodeMessages-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.NodeMessages.INCOMING_MESSAGE_TYPE_TO_CLASS_MAP","title":"INCOMING_MESSAGE_TYPE_TO_CLASS_MAP     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INCOMING_MESSAGE_TYPE_TO_CLASS_MAP = {'train': TrainRequest, 'search': SearchRequest, 'ping': PingRequest, 'list': ListRequest, 'training-plan-status': TrainingPlanStatusRequest, 'approval': ApprovalRequest, 'secagg': SecaggRequest, 'secagg-delete': SecaggDeleteRequest}\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.NodeMessages.OUTGOING_MESSAGE_TYPE_TO_CLASS_MAP","title":"OUTGOING_MESSAGE_TYPE_TO_CLASS_MAP     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OUTGOING_MESSAGE_TYPE_TO_CLASS_MAP = {'train': TrainReply, 'search': SearchReply, 'pong': PingReply, 'log': LogMessage, 'error': ErrorMessage, 'add_scalar': AddScalarReply, 'list': ListReply, 'training-plan-status': TrainingPlanStatusReply, 'approval': ApprovalReply, 'secagg': SecaggReply, 'secagg-delete': SecaggDeleteReply}\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingReply","title":"PingReply    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>This class describes a ping message sent by the node.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that will receive the reply</p> <code>node_id</code> <code>str</code> <p>Node id that replys the request</p> <code>succes</code> <code>str</code> <p>True if the node process the request as expected, false if any exception occurs</p> <code>sequence</code> <code>int</code> <p>Ping sequence</p> <code>command</code> <code>str</code> <p>Reply command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.PingReply.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingReply.node_id","title":"node_id     <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingReply.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingReply.sequence","title":"sequence     <code>instance-attribute</code>","text":"<pre><code>sequence: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingReply.success","title":"success     <code>instance-attribute</code>","text":"<pre><code>success: bool\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingRequest","title":"PingRequest    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a ping message sent by the researcher</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that send ping reqeust</p> <code>sequence</code> <code>int</code> <p>Ping sequence</p> <code>command</code> <code>str</code> <p>Request command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.PingRequest.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingRequest.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.PingRequest.sequence","title":"sequence     <code>instance-attribute</code>","text":"<pre><code>sequence: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.RequiresProtocolVersion","title":"RequiresProtocolVersion    <code>dataclass</code>","text":"<p>Mixin class for messages that must be endowed with a version field.</p> <p>Attributes:</p> Name Type Description <code>protocol_version</code> <code>str</code> <p>version of the messaging protocol used</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.RequiresProtocolVersion-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.RequiresProtocolVersion.protocol_version","title":"protocol_version     <code>instance-attribute</code>","text":"<pre><code>protocol_version: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ResearcherMessages","title":"ResearcherMessages","text":"<p>           Bases: <code>MessageFactory</code></p> <p>Specializes MessageFactory for Researcher.</p> <p>Researchers send requests and receive replies.</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ResearcherMessages-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.ResearcherMessages.INCOMING_MESSAGE_TYPE_TO_CLASS_MAP","title":"INCOMING_MESSAGE_TYPE_TO_CLASS_MAP     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INCOMING_MESSAGE_TYPE_TO_CLASS_MAP = {'train': TrainReply, 'search': SearchReply, 'pong': PingReply, 'log': LogMessage, 'error': ErrorMessage, 'list': ListReply, 'add_scalar': AddScalarReply, 'training-plan-status': TrainingPlanStatusReply, 'approval': ApprovalReply, 'secagg': SecaggReply, 'secagg-delete': SecaggDeleteReply}\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.ResearcherMessages.OUTGOING_MESSAGE_TYPE_TO_CLASS_MAP","title":"OUTGOING_MESSAGE_TYPE_TO_CLASS_MAP     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OUTGOING_MESSAGE_TYPE_TO_CLASS_MAP = {'train': TrainRequest, 'search': SearchRequest, 'ping': PingRequest, 'list': ListRequest, 'training-plan-status': TrainingPlanStatusRequest, 'approval': ApprovalRequest, 'secagg': SecaggRequest, 'secagg-delete': SecaggDeleteRequest}\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchReply","title":"SearchReply    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a search message sent by the node</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that sends the request</p> <code>succes</code> <code>str</code> <p>True if the node process the request as expected, false if any exception occurs</p> <code>databases</code> <code>list</code> <p>List of datasets</p> <code>node_id</code> <code>str</code> <p>Node id that replys the request</p> <code>count</code> <code>int</code> <p>Number of datasets</p> <code>command</code> <code>str</code> <p>Reply command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchReply.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchReply.count","title":"count     <code>instance-attribute</code>","text":"<pre><code>count: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchReply.databases","title":"databases     <code>instance-attribute</code>","text":"<pre><code>databases: list\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchReply.node_id","title":"node_id     <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchReply.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchReply.success","title":"success     <code>instance-attribute</code>","text":"<pre><code>success: bool\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchRequest","title":"SearchRequest    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a search message sent by the researcher.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that sends the request</p> <code>tags</code> <code>list</code> <p>Tags for search request</p> <code>command</code> <code>str</code> <p>Request command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchRequest.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchRequest.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SearchRequest.tags","title":"tags     <code>instance-attribute</code>","text":"<pre><code>tags: list\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply","title":"SecaggDeleteReply    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a secagg context element delete reply message sent by the node</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that requests deletion</p> <code>secagg_id</code> <code>str</code> <p>ID of secagg context element that is sent by researcher</p> <code>sequence</code> <code>int</code> <p>(unique) sequence number which identifies the message</p> <code>success</code> <code>bool</code> <p>True if the node process the request as expected, false if any exception occurs</p> <code>node_id</code> <code>str</code> <p>Node id that replies to the request</p> <code>msg</code> <code>str</code> <p>Custom message</p> <code>command</code> <code>str</code> <p>Reply command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply.msg","title":"msg     <code>instance-attribute</code>","text":"<pre><code>msg: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply.node_id","title":"node_id     <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply.secagg_id","title":"secagg_id     <code>instance-attribute</code>","text":"<pre><code>secagg_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply.sequence","title":"sequence     <code>instance-attribute</code>","text":"<pre><code>sequence: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteReply.success","title":"success     <code>instance-attribute</code>","text":"<pre><code>success: bool\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteRequest","title":"SecaggDeleteRequest    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a secagg context element delete request message sent by the researcher</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that requests deletion</p> <code>secagg_id</code> <code>str</code> <p>ID of secagg context element that is sent by researcher</p> <code>sequence</code> <code>int</code> <p>(unique) sequence number which identifies the message</p> <code>element</code> <code>int</code> <p>Type of secagg context element</p> <code>job_id</code> <code>(str, type(None))</code> <p>Id of the Job to which this secagg context element is attached</p> <code>command</code> <code>str</code> <p>Request command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteRequest.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteRequest.element","title":"element     <code>instance-attribute</code>","text":"<pre><code>element: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteRequest.job_id","title":"job_id     <code>instance-attribute</code>","text":"<pre><code>job_id: (str, type(None))\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteRequest.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteRequest.secagg_id","title":"secagg_id     <code>instance-attribute</code>","text":"<pre><code>secagg_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggDeleteRequest.sequence","title":"sequence     <code>instance-attribute</code>","text":"<pre><code>sequence: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply","title":"SecaggReply    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a secagg context element setup reply message sent by the node</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that requests setup</p> <code>secagg_id</code> <code>str</code> <p>ID of secagg context element that is sent by researcher</p> <code>sequence</code> <code>int</code> <p>(unique) sequence number which identifies the message</p> <code>success</code> <code>bool</code> <p>True if the node process the request as expected, false if any exception occurs</p> <code>node_id</code> <code>str</code> <p>Node id that replies to the request</p> <code>msg</code> <code>str</code> <p>Custom message</p> <code>command</code> <code>str</code> <p>Reply command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply.msg","title":"msg     <code>instance-attribute</code>","text":"<pre><code>msg: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply.node_id","title":"node_id     <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply.secagg_id","title":"secagg_id     <code>instance-attribute</code>","text":"<pre><code>secagg_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply.sequence","title":"sequence     <code>instance-attribute</code>","text":"<pre><code>sequence: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggReply.success","title":"success     <code>instance-attribute</code>","text":"<pre><code>success: bool\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest","title":"SecaggRequest    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a secagg context element setup request message sent by the researcher</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that requests setup</p> <code>secagg_id</code> <code>str</code> <p>ID of secagg context element that is sent by researcher</p> <code>sequence</code> <code>int</code> <p>(unique) sequence number which identifies the message</p> <code>element</code> <code>int</code> <p>Type of secagg context element</p> <code>job_id</code> <code>(str, type(None))</code> <p>Id of the Job to which this secagg context element is attached</p> <code>parties</code> <code>list</code> <p>List of parties participating to the secagg context element setup</p> <code>command</code> <code>str</code> <p>Request command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest.element","title":"element     <code>instance-attribute</code>","text":"<pre><code>element: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest.job_id","title":"job_id     <code>instance-attribute</code>","text":"<pre><code>job_id: (str, type(None))\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest.parties","title":"parties     <code>instance-attribute</code>","text":"<pre><code>parties: list\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest.secagg_id","title":"secagg_id     <code>instance-attribute</code>","text":"<pre><code>secagg_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.SecaggRequest.sequence","title":"sequence     <code>instance-attribute</code>","text":"<pre><code>sequence: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply","title":"TrainReply    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a train message sent by the node.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that receives the reply</p> <code>job_id</code> <code>str</code> <p>Id of the Job that is sent by researcher</p> <code>success</code> <code>bool</code> <p>True if the node process the request as expected, false if any exception occurs</p> <code>node_id</code> <code>str</code> <p>Node id that replys the request</p> <code>dataset_id</code> <code>str</code> <p>id of the dataset that is used for training</p> <code>params_url</code> <code>str</code> <p>URL of parameters uploaded by node</p> <code>timing</code> <code>dict</code> <p>Timing statistics</p> <code>msg</code> <code>str</code> <p>Custom message</p> <code>command</code> <code>str</code> <p>Reply command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.dataset_id","title":"dataset_id     <code>instance-attribute</code>","text":"<pre><code>dataset_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.job_id","title":"job_id     <code>instance-attribute</code>","text":"<pre><code>job_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.msg","title":"msg     <code>instance-attribute</code>","text":"<pre><code>msg: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.node_id","title":"node_id     <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.params_url","title":"params_url     <code>instance-attribute</code>","text":"<pre><code>params_url: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.sample_size","title":"sample_size     <code>instance-attribute</code>","text":"<pre><code>sample_size: (int, type(None))\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.success","title":"success     <code>instance-attribute</code>","text":"<pre><code>success: bool\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainReply.timing","title":"timing     <code>instance-attribute</code>","text":"<pre><code>timing: dict\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest","title":"TrainRequest    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a train message sent by the researcher</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>ID of the researcher that requests training</p> <code>job_id</code> <code>str</code> <p>Id of the Job that is sent by researcher</p> <code>params_url</code> <code>str</code> <p>URL where model parameters are uploaded</p> <code>training_args</code> <code>dict</code> <p>Arguments for training routine</p> <code>dataset_id</code> <code>str</code> <p>id of the dataset that is used for training</p> <code>training</code> <code>bool</code> <p>Declares whether training will be performed</p> <code>model_args</code> <code>dict</code> <p>Arguments to initialize training plan class</p> <code>training_plan_url</code> <code>str</code> <p>URL where TrainingPlan is available</p> <code>training_plan_class</code> <code>str</code> <p>Class name of the training plan</p> <code>command</code> <code>str</code> <p>Reply command string</p> <code>aggregator_args</code> <code>dict</code> <p>??</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.aggregator_args","title":"aggregator_args     <code>instance-attribute</code>","text":"<pre><code>aggregator_args: dict\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.dataset_id","title":"dataset_id     <code>instance-attribute</code>","text":"<pre><code>dataset_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.job_id","title":"job_id     <code>instance-attribute</code>","text":"<pre><code>job_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.model_args","title":"model_args     <code>instance-attribute</code>","text":"<pre><code>model_args: dict\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.params_url","title":"params_url     <code>instance-attribute</code>","text":"<pre><code>params_url: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.round","title":"round     <code>instance-attribute</code>","text":"<pre><code>round: int\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.secagg_biprime_id","title":"secagg_biprime_id     <code>instance-attribute</code>","text":"<pre><code>secagg_biprime_id: (str, type(None))\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.secagg_clipping_range","title":"secagg_clipping_range     <code>instance-attribute</code>","text":"<pre><code>secagg_clipping_range: (int, type(None))\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.secagg_random","title":"secagg_random     <code>instance-attribute</code>","text":"<pre><code>secagg_random: (float, type(None))\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.secagg_servkey_id","title":"secagg_servkey_id     <code>instance-attribute</code>","text":"<pre><code>secagg_servkey_id: (str, type(None))\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.training","title":"training     <code>instance-attribute</code>","text":"<pre><code>training: bool\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.training_args","title":"training_args     <code>instance-attribute</code>","text":"<pre><code>training_args: dict\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.training_plan_class","title":"training_plan_class     <code>instance-attribute</code>","text":"<pre><code>training_plan_class: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainRequest.training_plan_url","title":"training_plan_url     <code>instance-attribute</code>","text":"<pre><code>training_plan_url: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply","title":"TrainingPlanStatusReply    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a training plan approve status check message sent by the node</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that sends the request</p> <code>node_id</code> <code>str</code> <p>Node id that replys the request</p> <code>job_id</code> <code>str</code> <p>job id related to the experiment</p> <code>succes</code> <code>str</code> <p>True if the node process the request as expected, false if any execption occurs</p> <code>approval_obligation</code> <p>Approval mode for node. True, if training plan approval is enabled/required in the node for training.</p> <code>is_approved</code> <p>True, if the requested training plan is one of the approved training plan by the node</p> <code>msg</code> <code>str</code> <p>Message from node based on state of the reply</p> <code>training_plan_url</code> <code>str</code> <p>The training plan that has been checked for approval</p> <code>command</code> <code>str</code> <p>Reply command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.approval_obligation","title":"approval_obligation     <code>instance-attribute</code>","text":"<pre><code>approval_obligation: bool\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.job_id","title":"job_id     <code>instance-attribute</code>","text":"<pre><code>job_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.msg","title":"msg     <code>instance-attribute</code>","text":"<pre><code>msg: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.node_id","title":"node_id     <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.status","title":"status     <code>instance-attribute</code>","text":"<pre><code>status: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.success","title":"success     <code>instance-attribute</code>","text":"<pre><code>success: bool\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusReply.training_plan_url","title":"training_plan_url     <code>instance-attribute</code>","text":"<pre><code>training_plan_url: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusRequest","title":"TrainingPlanStatusRequest    <code>dataclass</code>","text":"<p>           Bases: <code>Message</code>, <code>RequiresProtocolVersion</code></p> <p>Describes a training plan approve status check message sent by the researcher.</p> <p>Attributes:</p> Name Type Description <code>researcher_id</code> <code>str</code> <p>Id of the researcher that sends the request</p> <code>job_id</code> <code>str</code> <p>Job id related to the experiment.</p> <code>training_plan_url</code> <code>str</code> <p>The training plan that is going to be checked for approval</p> <code>command</code> <code>str</code> <p>Request command string</p> <p>Raises:</p> Type Description <code>FedbiomedMessageError</code> <p>triggered if message's fields validation failed</p>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusRequest-attributes","title":"Attributes","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusRequest.command","title":"command     <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusRequest.job_id","title":"job_id     <code>instance-attribute</code>","text":"<pre><code>job_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusRequest.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message.TrainingPlanStatusRequest.training_plan_url","title":"training_plan_url     <code>instance-attribute</code>","text":"<pre><code>training_plan_url: str\n</code></pre>"},{"location":"developer/api/common/message/#fedbiomed.common.message-functions","title":"Functions","text":""},{"location":"developer/api/common/message/#fedbiomed.common.message.catch_dataclass_exception","title":"<pre><code>catch_dataclass_exception(cls)\n</code></pre>","text":"<p>Encapsulates the init() method of dataclass in order to transform the exceptions sent by the dataclass (TypeError) into our own exception (FedbiomedMessageError)</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Callable</code> <p>Dataclass to validate</p> required Source code in <code>fedbiomed/common/message.py</code> <pre><code>def catch_dataclass_exception(cls: Callable):\n\"\"\"Encapsulates the __init__() method of dataclass in order to transform the exceptions sent\n    by the dataclass (TypeError) into our own exception (FedbiomedMessageError)\n    Args:\n        cls: Dataclass to validate\n    \"\"\"\ndef __cde_init__(self: Any, *args: list, **kwargs: dict):\n\"\"\"This is the __init__() replacement.\n        Its purpose is to catch the TypeError created by the __init__\n        method of the @dataclass decorator and replace this exception by  FedbiomedMessageError\n        Raises:\n          FedbiomedMessageError if number/type of arguments is wrong\n        \"\"\"\ntry:\nself.__class__.__dict__['__initial_init__'](self, *args, **kwargs)\nexcept TypeError as e:\n# this is the error raised by dataclass if number of parameter is wrong\n_msg = ErrorNumbers.FB601.value + \": bad number of parameters: \" + str(e)\nlogger.error(_msg)\nraise FedbiomedMessageError(_msg)\n@functools.wraps(cls)\ndef wrap(cls: Callable):\n\"\"\" Wrapper to the class given as parameter\n        Class wrapping should keep some attributes (__doc__, etc) of the initial class or the API documentation tools\n        will be mistaken\n        \"\"\"\ncls.__initial_init__ = cls.__init__\nsetattr(cls, \"__init__\", __cde_init__)\nreturn cls\nreturn wrap(cls)\n</code></pre>"},{"location":"developer/api/common/messaging/","title":"Messaging","text":""},{"location":"developer/api/common/messaging/#fedbiomed.common.messaging","title":"fedbiomed.common.messaging","text":"Module: <code>fedbiomed.common.messaging</code> <p>This module is a wrapper to the message bus used by Fed-BioMed.</p>"},{"location":"developer/api/common/messaging/#fedbiomed.common.messaging-classes","title":"Classes","text":""},{"location":"developer/api/common/messaging/#fedbiomed.common.messaging.Messaging","title":"Messaging","text":"CLASS  <pre><code>Messaging(on_message, messaging_type, messaging_id, mqtt_broker='localhost', mqtt_broker_port=1883)\n</code></pre> <p>Represents the messenger, (MQTT messaging facility).</p> <p>Creates an instance of MQTT Client, and MQTT message handler. Creates topics on which to send messages through Messanger. Topics in MQTT work as a channel allowing to filter shared information between connected clients</p> <p>Parameters:</p> Name Type Description Default <code>on_message</code> <code>Callable[[dict], None]</code> <p>Function that should be executed when a message is received</p> required <code>messaging_type</code> <code>ComponentType</code> <p>Describes incoming message sender. 1 for researcher, 2 for node</p> required <code>messaging_id</code> <code>Union[int, str]</code> <p>messaging id</p> required <code>mqtt_broker</code> <code>str</code> <p>IP address / URL. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>mqtt_broker_port</code> <code>int</code> <p>Defaults to 80 (http default port).</p> <code>1883</code> Source code in <code>fedbiomed/common/messaging.py</code> <pre><code>def __init__(self,\non_message: Callable[[dict], None],\nmessaging_type: ComponentType,\nmessaging_id: Union[int, str],\nmqtt_broker: str = 'localhost',\nmqtt_broker_port: int = 1883):\n\"\"\" Constructor of the messaging class.\n    Args:\n        on_message: Function that should be executed when a message is received\n        messaging_type: Describes incoming message sender. 1 for researcher, 2 for node\n        messaging_id: messaging id\n        mqtt_broker: IP address / URL. Defaults to \"localhost\".\n        mqtt_broker_port: Defaults to 80 (http default port).\n    \"\"\"\nself._messaging_type = messaging_type\nself._messaging_id = str(messaging_id)\nself._is_connected = False\nself._is_failed = False\n# Client() will generate a random client_id if not given\n# this means we choose not to use the {node,researcher}_id for this purpose\nself._mqtt = mqtt.Client()\n# defining a client.\n# defining MQTT 's `on_connect` and `on_message` handlers\n# (see MQTT paho documentation for further information\n# _ https://github.com/eclipse/paho.mqtt.python)\nself._mqtt.on_connect = self.on_connect\nself._mqtt.on_message = self.on_message\nself._mqtt.on_disconnect = self.on_disconnect\nself._mqtt_broker = mqtt_broker\nself._mqtt_broker_port = mqtt_broker_port\n# memorize mqqt parameters\nself._broker_host = mqtt_broker\nself._broker_port = mqtt_broker_port\n# protection for logger initialisation (mqqt handler)\nself._logger_handler_installed = False\nself._on_message_handler = on_message  # store the caller's mesg handler\nif on_message is None:\nlogger.warning(\"no message handler defined\")\nif self._messaging_type is ComponentType.RESEARCHER:\nself._default_send_topic = 'general/nodes'\nelif self._messaging_type is ComponentType.NODE:\nself._default_send_topic = 'general/researcher'\nelse:\nself._default_send_topic = None\n</code></pre>"},{"location":"developer/api/common/messaging/#fedbiomed.common.messaging.Messaging-functions","title":"Functions","text":""},{"location":"developer/api/common/messaging/#fedbiomed.common.messaging.Messaging.default_send_topic","title":"<pre><code>default_send_topic()\n</code></pre>","text":"<p>Gets for default_send_topic</p> <p>Returns:</p> Type Description <code>str</code> <p>Default topic</p> Source code in <code>fedbiomed/common/messaging.py</code> <pre><code>def default_send_topic(self) -&gt; str:\n\"\"\"Gets for default_send_topic\n    Returns:\n        Default topic\n    \"\"\"\nreturn self._default_send_topic\n</code></pre>"},{"location":"developer/api/common/messaging/#fedbiomed.common.messaging.Messaging.is_connected","title":"<pre><code>is_connected()\n</code></pre>","text":"<p>Gets the is_connected status flag</p> <p>Returns:</p> Type Description <code>bool</code> <p>Connection status</p> Source code in <code>fedbiomed/common/messaging.py</code> <pre><code>def is_connected(self) -&gt; bool:\n\"\"\"Gets the is_connected status flag\n    Returns:\n        Connection status\n    \"\"\"\nreturn self._is_connected\n</code></pre>"},{"location":"developer/api/common/messaging/#fedbiomed.common.messaging.Messaging.is_failed","title":"<pre><code>is_failed()\n</code></pre>","text":"<p>Gets the is_failed status flag</p> <p>Returns:</p> Type Description <code>bool</code> <p>Connection failure status</p> Source code in <code>fedbiomed/common/messaging.py</code> <pre><code>def is_failed(self) -&gt; bool:\n\"\"\"Gets the is_failed status flag\n    Returns:\n        Connection failure status\n    \"\"\"\nreturn self._is_failed\n</code></pre>"},{"location":"developer/api/common/messaging/#fedbiomed.common.messaging.Messaging.on_connect","title":"<pre><code>on_connect(client, userdata, flags, rc)\n</code></pre>","text":"<p>Callback for when the client receives a CONNECT response from the server.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>mqtt.Client</code> <p>mqtt on_message arg (unused)</p> required <code>userdata</code> <code>Any</code> <p>mqtt on_message arg, private user data (unused)</p> required <code>flags</code> <code>dict</code> <p>mqtt on_message arg, response flag sent by the broker (unused)</p> required <code>rc</code> <code>int</code> <p>mqtt on_message arg, connection result</p> required <p>Raises:</p> Type Description <code>FedbiomedMessagingError</code> <p>If connection is not successful</p> Source code in <code>fedbiomed/common/messaging.py</code> <pre><code>def on_connect(self,\nclient: mqtt.Client,\nuserdata: Any,\nflags: dict,\nrc: int):\n\"\"\"Callback for when the client receives a CONNECT response from the server.\n    Args:\n        client: mqtt on_message arg (unused)\n        userdata: mqtt on_message arg, private user data (unused)\n        flags: mqtt on_message arg, response flag sent by the broker (unused)\n        rc: mqtt on_message arg, connection result\n    Raises:\n        FedbiomedMessagingError: If connection is not successful\n    \"\"\"\nif rc == 0:\nlogger.info(\"Messaging \" + str(self._messaging_id) +\n\" successfully connected to the message broker, object = \" + str(self))\nelse:\nmsg = ErrorNumbers.FB101.value + \": \" + str(self._messaging_id) + \" could not connect to the message broker\"\nlogger.delMqttHandler()  # just in case !\nself._logger_handler_installed = False\nlogger.critical(msg)\nself._is_failed = True\nraise FedbiomedMessagingError(msg)\nif self._messaging_type is ComponentType.RESEARCHER:\nfor channel in ('general/researcher', 'general/monitoring'):\nresult, _ = self._mqtt.subscribe(channel)\nif result != mqtt.MQTT_ERR_SUCCESS:\nlogger.error(\"Messaging \" + str(self._messaging_id) + \"failed subscribe to channel\" + str(channel))\nself._is_failed = True\n# PoC subscribe also to error channel\nresult, _ = self._mqtt.subscribe('general/logger')\nif result != mqtt.MQTT_ERR_SUCCESS:\nlogger.error(\"Messaging \" + str(self._messaging_id) + \"failed subscribe to channel general/error\")\nself._is_failed = True\nelif self._messaging_type is ComponentType.NODE:\nfor channel in ('general/nodes', 'general/' + self._messaging_id):\nresult, _ = self._mqtt.subscribe(channel)\nif result != mqtt.MQTT_ERR_SUCCESS:\nlogger.error(\"Messaging \" + str(self._messaging_id) + \" failed subscribe to channel\" + str(channel))\nself._is_failed = True\nif not self._logger_handler_installed:\n# add the MQTT handler for logger\n# this should be done once.\n# This is sldo tested by the addHandler() method, but\n# it may raise a MQTT message (that we prefer not to send)\nlogger.addMqttHandler(mqtt=self._mqtt,\nnode_id=self._messaging_id)\n# to get Train/Epoch messages on console and on MQTT\nlogger.setLevel(\"DEBUG\")\n# Send messages to researcher starting from INFO level\nlogger.setLevel(\"INFO\", \"MQTT\")\nself._logger_handler_installed = True\nself._is_connected = True\n</code></pre>"},{"location":"developer/api/common/messaging/#fedbiomed.common.messaging.Messaging.on_disconnect","title":"<pre><code>on_disconnect(client, userdata, rc)\n</code></pre>","text":"<p>Calls-back on client is disconnected</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>mqtt.Client</code> <p>MQTT Client</p> required <code>userdata</code> <code>Any</code> <p>-</p> required <code>rc</code> <code>int</code> <p>Response code</p> required <p>Raises:</p> Type Description <code>SystemExit</code> <p>If disconnection status is not 0</p> Source code in <code>fedbiomed/common/messaging.py</code> <pre><code>def on_disconnect(self, client: mqtt.Client, userdata: Any, rc: int):\n\"\"\"Calls-back on client is disconnected\n    Args:\n        client:  MQTT Client\n        userdata: -\n        rc: Response code\n    Raises:\n        SystemExit: If disconnection status is not 0\n    \"\"\"\nself._is_connected = False\nif rc == 0:\n# should this ever happen ? we're not disconnecting intentionally yet\nlogger.info(\"Messaging \" + str(self._messaging_id) + \" disconnected without error\")\nelse:\n# see MQTT specs : when another client connects with same client_id,\n# the previous one is disconnected\n# https://docs.oasis-open.org/mqtt/mqtt/v5.0/os/mqtt-v5.0-os.html#_Toc3901205\nlogger.error(\"Messaging \" + str(self._messaging_id) + \" disconnected with error code rc = \" + str(rc) +\n\" - Hint: check for another instance of the same component running or for communication error\")\nself._is_failed = True\n# quit messaging to avoid connect/disconnect storm in case multiple nodes with same id\nraise SystemExit\n</code></pre>"},{"location":"developer/api/common/messaging/#fedbiomed.common.messaging.Messaging.on_message","title":"<pre><code>on_message(client, userdata, msg)\n</code></pre>","text":"<p>Callback called when a new MQTT message is received. The message is processed and forwarded to the node/researcher to be treated/stored/whatever</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>mqtt.Client</code> <p>mqtt on_message arg, client instance (unused)</p> required <code>userdata</code> <code>Any</code> <p>mqtt on_message arg (unused)</p> required <code>msg</code> <code>Union[str, bytes]</code> <p>mqtt on_message arg</p> required Source code in <code>fedbiomed/common/messaging.py</code> <pre><code>def on_message(self,\nclient: mqtt.Client,\nuserdata: Any,\nmsg: Union[str, bytes]):\n\"\"\"Callback called when a new MQTT message is received.\\\n    The message is processed and forwarded to the node/researcher to be treated/stored/whatever\n    Args:\n        client: mqtt on_message arg, client instance (unused)\n        userdata: mqtt on_message arg (unused)\n        msg: mqtt on_message arg\n    \"\"\"\nif self._on_message_handler is not None:\nmessage = json.deserialize_msg(msg.payload)\n# check version\nmsg_version = message.get('protocol_version', __default_version__)\nraise_for_version_compatibility(msg_version, __messaging_protocol_version__,\nf\"{ErrorNumbers.FB104.value}: Received message with protocol version %s \"\nf\"which is incompatible with the current protocol version %s\")\nself._on_message_handler(msg=message, topic=msg.topic)\nelse:\nlogger.warning(\"no message handler defined\")\n</code></pre>"},{"location":"developer/api/common/messaging/#fedbiomed.common.messaging.Messaging.send_error","title":"<pre><code>send_error(errnum, extra_msg='', researcher_id='&lt;unknown&gt;')\n</code></pre>","text":"<p>Sends error through mqtt</p> <p>Difference with send_message() is that we do extra tests before sending the message</p> <p>Parameters:</p> Name Type Description Default <code>errnum</code> <code>ErrorNumbers</code> <p>Error number</p> required <code>extra_msg</code> <code>str</code> <p>Extra error message</p> <code>''</code> <code>researcher_id</code> <code>str</code> <p>ID of the researcher that the message will be sent to</p> <code>'&lt;unknown&gt;'</code> <p>Raises:</p> Type Description <code>FedbiomedMessagingError</code> <p>If client is not connected</p> Source code in <code>fedbiomed/common/messaging.py</code> <pre><code>def send_error(self, errnum: ErrorNumbers, extra_msg: str = \"\", researcher_id: str = \"&lt;unknown&gt;\"):\n\"\"\"Sends error through mqtt\n    Difference with send_message() is that we do extra tests before sending the message\n    Args:\n        errnum: Error number\n        extra_msg: Extra error message\n        researcher_id: ID of the researcher that the message will be sent to\n    Raises:\n        FedbiomedMessagingError: If client is not connected\n    \"\"\"\nif self._messaging_type != ComponentType.NODE:\nlogger.warning(\"this component (\" +\nself._messaging_type +\n\") cannot send error message (\" +\nerrnum.value +\n\") through MQTT\")\nreturn\nif not self._is_connected:\nlogger.delMqttHandler()  # just in case\nself._logger_handler_installed = False\nmsg = \"MQTT not initialized yet (error to transmit=\" + errnum.value + \")\"\nlogger.critical(msg)\nraise FedbiomedMessagingError(msg)\n# format error message and send it\nmsg = dict(\ncommand='error',\nerrnum=errnum,\nnode_id=self._messaging_id,\nextra_msg=extra_msg,\nresearcher_id=researcher_id\n)\n# just check the syntax before sending\n_ = message.NodeMessages.format_outgoing_message(msg)\nself._mqtt.publish(\"general/researcher\", json.serialize_msg(msg))\n</code></pre>"},{"location":"developer/api/common/messaging/#fedbiomed.common.messaging.Messaging.send_message","title":"<pre><code>send_message(msg, client=None)\n</code></pre>","text":"<p>This method sends a message to a given client</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict</code> <p>the content of a message</p> required <code>client</code> <code>str</code> <p>defines the channel to which the message will be sent. Defaults to None (all clients)</p> <code>None</code> Source code in <code>fedbiomed/common/messaging.py</code> <pre><code>def send_message(self, msg: dict, client: str = None):\n\"\"\"This method sends a message to a given client\n    Args:\n        msg: the content of a message\n        client: defines the channel to which the message will be sent. Defaults to None (all clients)\n    \"\"\"\nif self._is_failed:\nlogger.error('Messaging has failed, will not try to send message')\nreturn\nelif not self._is_connected:\nlogger.error('Messaging is not connected, will not try to send message')\nreturn\nif client is None:\nchannel = self._default_send_topic\nelse:\nchannel = \"general/\" + str(client)\nif channel is not None:\nmessinfo = self._mqtt.publish(channel, json.serialize_msg(msg))\nif messinfo.rc != mqtt.MQTT_ERR_SUCCESS:\nlogger.error(\"Messaging \" +\nstr(self._messaging_id) +\n\" failed sending message with code rc = \",\nstr(messinfo.rc) +\n\" object = \" +\nstr(self) +\n\" message = \" +\nstr(msg))\nself._is_failed = True\nelse:\nlogger.warning(\"send_message: channel must be specific (None at the moment)\")\n</code></pre>"},{"location":"developer/api/common/messaging/#fedbiomed.common.messaging.Messaging.start","title":"<pre><code>start(block=False)\n</code></pre>","text":"<p>Calls the loop function of mqtt. Starts message handling by the library.</p> <p>Parameters:</p> Name Type Description Default <code>block</code> <code>bool</code> <p>if True; calls the loop_forever method in MQTT (blocking loop) else, calls the loop_start method (non-blocking loop). <code>loop_start</code> calls a background thread for messaging.</p> <p>See Paho MQTT documentation(https://github.com/eclipse/paho.mqtt.python) for further information.</p> <code>False</code> <p>Raises:</p> Type Description <code>FedbiomedMessagingError</code> <p>If it can not connect MQTT broker</p> Source code in <code>fedbiomed/common/messaging.py</code> <pre><code>def start(self, block: bool = False):\n\"\"\"Calls the loop function of mqtt. Starts message handling by the library.\n    Args:\n        block: if True; calls the loop_forever method in MQTT (blocking loop) else, calls the loop_start method\n            (non-blocking loop). `loop_start` calls a background thread for messaging.\n            See Paho MQTT documentation(https://github.com/eclipse/paho.mqtt.python) for further information.\n    Raises:\n        FedbiomedMessagingError: If it can not connect MQTT broker\n    \"\"\"\n# will try to connect even if is_failed or is_connected, to give a chance to resolve problems\ntry:\nself._mqtt.connect(self._mqtt_broker, self._mqtt_broker_port, keepalive=60)\nexcept (ConnectionRefusedError, TimeoutError, socket.timeout) as e:\nlogger.delMqttHandler()  # just in case !\nself._logger_handler_installed = False\nmsg = ErrorNumbers.FB101.value + \"(error = \" + str(e) + \")\"\nlogger.critical(msg)\nraise FedbiomedMessagingError(msg)\nif block:\n# TODO : not used, should probably be removed\nself._mqtt.loop_forever()\nelif not self._is_connected:\nself._mqtt.loop_start()\nwhile not self._is_connected:\npass\n</code></pre>"},{"location":"developer/api/common/messaging/#fedbiomed.common.messaging.Messaging.stop","title":"<pre><code>stop()\n</code></pre>","text":"<p>stops the loop started using <code>loop_start</code> method.</p> <p>The non-blocking loop triggered with <code>Messaging.start(block=True)</code> only. It stops the background thread for messaging.</p> Source code in <code>fedbiomed/common/messaging.py</code> <pre><code>def stop(self):\n\"\"\"stops the loop started using `loop_start` method.\n    The non-blocking loop triggered with `Messaging.start(block=True)` only. It stops the background\n    thread for messaging.\n    \"\"\"\n# will try a stop even if is_failed or not is_connected, to give a chance to clean state\nself._mqtt.loop_stop()\n</code></pre>"},{"location":"developer/api/common/metrics/","title":"Metrics","text":""},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics","title":"fedbiomed.common.metrics","text":"Module: <code>fedbiomed.common.metrics</code> <p>Provide test metrics, both MetricTypes to use in TrainingArgs but also calculation routines.</p>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics-classes","title":"Classes","text":""},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes","title":"MetricTypes","text":"CLASS  <pre><code>MetricTypes(idx, metric_category)\n</code></pre> <p>           Bases: <code>_BaseEnum</code></p> <p>List of Performance metrics used to evaluate the model.</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>def __init__(self, idx: int, metric_category: _MetricCategory) -&gt; None:\nself._idx = idx\nself._metric_category = metric_category\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes-attributes","title":"Attributes","text":""},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.ACCURACY","title":"ACCURACY     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ACCURACY = (0, _MetricCategory.CLASSIFICATION_LABELS)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.EXPLAINED_VARIANCE","title":"EXPLAINED_VARIANCE     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EXPLAINED_VARIANCE = (6, _MetricCategory.REGRESSION)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.F1_SCORE","title":"F1_SCORE     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>F1_SCORE = (1, _MetricCategory.CLASSIFICATION_LABELS)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.MEAN_ABSOLUTE_ERROR","title":"MEAN_ABSOLUTE_ERROR     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEAN_ABSOLUTE_ERROR = (5, _MetricCategory.REGRESSION)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.MEAN_SQUARE_ERROR","title":"MEAN_SQUARE_ERROR     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEAN_SQUARE_ERROR = (4, _MetricCategory.REGRESSION)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.PRECISION","title":"PRECISION     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PRECISION = (2, _MetricCategory.CLASSIFICATION_LABELS)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.RECALL","title":"RECALL     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RECALL = (3, _MetricCategory.CLASSIFICATION_LABELS)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes-functions","title":"Functions","text":""},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.get_all_metrics","title":"<pre><code>get_all_metrics()\n</code></pre>  <code>staticmethod</code>","text":"Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef get_all_metrics() -&gt; List[str]:\nreturn [metric.name for metric in MetricTypes]\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.get_metric_type_by_name","title":"<pre><code>get_metric_type_by_name(metric_name)\n</code></pre>  <code>staticmethod</code>","text":"Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef get_metric_type_by_name(metric_name: str):\nfor metric in MetricTypes:\nif metric.name == metric_name:\nreturn metric\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.MetricTypes.metric_category","title":"<pre><code>metric_category()\n</code></pre>","text":"Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>def metric_category(self) -&gt; _MetricCategory:\nreturn self._metric_category\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics","title":"Metrics","text":"CLASS  <pre><code>Metrics()\n</code></pre> <p>           Bases: <code>object</code></p> <p>Class of performance metrics used in validation evaluation.</p> Attrs <p>metrics: Provided metrics in form of <code>{ MetricTypes : skleran.metrics }</code></p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>def __init__(self):\n\"\"\"Constructs metric class with provided metric types: metric function\n    Attrs:\n        metrics: Provided metrics in form of `{ MetricTypes : skleran.metrics }`\n    \"\"\"\nself.metrics = {\nMetricTypes.ACCURACY.name: self.accuracy,\nMetricTypes.PRECISION.name: self.precision,\nMetricTypes.RECALL.name: self.recall,\nMetricTypes.F1_SCORE.name: self.f1_score,\nMetricTypes.MEAN_SQUARE_ERROR.name: self.mse,\nMetricTypes.MEAN_ABSOLUTE_ERROR.name: self.mae,\nMetricTypes.EXPLAINED_VARIANCE.name: self.explained_variance,\n}\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics-attributes","title":"Attributes","text":""},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.metrics","title":"metrics     <code>instance-attribute</code>","text":"<pre><code>metrics = {MetricTypes.ACCURACY.name: self.accuracy, MetricTypes.PRECISION.name: self.precision, MetricTypes.RECALL.name: self.recall, MetricTypes.F1_SCORE.name: self.f1_score, MetricTypes.MEAN_SQUARE_ERROR.name: self.mse, MetricTypes.MEAN_ABSOLUTE_ERROR.name: self.mae, MetricTypes.EXPLAINED_VARIANCE.name: self.explained_variance}\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics-functions","title":"Functions","text":""},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.accuracy","title":"<pre><code>accuracy(y_true, y_pred, kwargs)\n</code></pre>  <code>staticmethod</code>","text":"<p>Evaluate the accuracy score</p> <p>Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[np.ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[np.ndarray, list]</code> <p>Predicted values</p> required <code>**kwargs</code> <code>dict</code> <p>Extra arguments from <code>sklearn.metrics.accuracy_score</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>Accuracy score</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>raised if above sklearn method raises</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef accuracy(y_true: Union[np.ndarray, list],\ny_pred: Union[np.ndarray, list],\n**kwargs: dict) -&gt; float:\n\"\"\" Evaluate the accuracy score\n    Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        **kwargs: Extra arguments from [`sklearn.metrics.accuracy_score`][sklearn.metrics.accuracy_score]\n    Returns:\n        Accuracy score\n    Raises:\n        FedbiomedMetricError: raised if above sklearn method raises\n    \"\"\"\ntry:\ny_true, y_pred, _, _ = Metrics._configure_multiclass_parameters(y_true, y_pred, kwargs, 'ACCURACY')\nreturn metrics.accuracy_score(y_true, y_pred, **kwargs)\nexcept Exception as e:\nmsg = ErrorNumbers.FB611.value + \" Exception raised from SKLEARN metrics: \" + str(e)\nraise FedbiomedMetricError(msg)\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.evaluate","title":"<pre><code>evaluate(y_true, y_pred, metric, kwargs)\n</code></pre>","text":"<p>Perform evaluation based on given metric.</p> <p>This method configures given y_pred and y_true to make them compatible with default evaluation methods.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[np.ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[np.ndarray, list]</code> <p>Predicted values</p> required <code>metric</code> <code>MetricTypes</code> <p>An instance of MetricTypes to chose metric that will be used for evaluation</p> required <code>**kwargs</code> <code>dict</code> <p>The arguments specifics to each type of metrics.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[int, float]</code> <p>Result of the evaluation function</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>in case of invalid metric, y_true and y_pred types</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>def evaluate(self,\ny_true: Union[np.ndarray, list],\ny_pred: Union[np.ndarray, list],\nmetric: MetricTypes,\n**kwargs: dict) -&gt; Union[int, float]:\n\"\"\"Perform evaluation based on given metric.\n    This method configures given y_pred and y_true to make them compatible with default evaluation methods.\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        metric: An instance of MetricTypes to chose metric that will be used for evaluation\n        **kwargs: The arguments specifics to each type of metrics.\n    Returns:\n        Result of the evaluation function\n    Raises:\n        FedbiomedMetricError: in case of invalid metric, y_true and y_pred types\n    \"\"\"\nif not isinstance(metric, MetricTypes):\nraise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: Metric should instance of `MetricTypes`\")\nif y_true is not None and not isinstance(y_true, (np.ndarray, list)):\nraise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: The argument `y_true` should an instance \"\nf\"of `np.ndarray`, but got {type(y_true)} \")\nif y_pred is not None and not isinstance(y_pred, (np.ndarray, list)):\nraise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: The argument `y_pred` should an instance \"\nf\"of `np.ndarray`, but got {type(y_true)} \")\ny_true, y_pred = self._configure_y_true_pred_(y_true=y_true, y_pred=y_pred, metric=metric)\nresult = self.metrics[metric.name](y_true, y_pred, **kwargs)\nreturn result\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.explained_variance","title":"<pre><code>explained_variance(y_true, y_pred, kwargs)\n</code></pre>  <code>staticmethod</code>","text":"<p>Evaluate the Explained variance regression score.</p> <p>Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html]</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[np.ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[np.ndarray, list]</code> <p>Predicted values</p> required <code>**kwargs</code> <code>dict</code> <p>Extra arguments from <code>sklearn.metrics.explained_variance_score</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>EV score (float or ndarray of floats)</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>raised if above sklearn method for computing precision raises</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef explained_variance(y_true: Union[np.ndarray, list],\ny_pred: Union[np.ndarray, list],\n**kwargs: dict) -&gt; float:\n\"\"\"Evaluate the Explained variance regression score.\n    Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html]\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        **kwargs: Extra arguments from [`sklearn.metrics.explained_variance_score`]\n            [sklearn.metrics.explained_variance_score]\n    Returns:\n        EV score (float or ndarray of floats)\n    Raises:\n        FedbiomedMetricError: raised if above sklearn method for computing precision raises\n    \"\"\"\n# Set multiouput as raw_values is it is not defined by researcher\nif len(y_true.shape) &gt; 1:\nmulti_output = kwargs.get('multioutput', 'raw_values')\nelse:\nmulti_output = None\nkwargs.pop('multioutput', None)\ntry:\nreturn metrics.explained_variance_score(y_true, y_pred, multioutput=multi_output, **kwargs)\nexcept Exception as e:\nraise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: Error during calculation of `EXPLAINED_VARIANCE`\"\nf\" {str(e)}\")\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.f1_score","title":"<pre><code>f1_score(y_true, y_pred, kwargs)\n</code></pre>  <code>staticmethod</code>","text":"<p>Evaluate the F1 score.</p> <p>Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[np.ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[np.ndarray, list]</code> <p>Predicted values</p> required <code>**kwargs</code> <code>dict</code> <p>Extra arguments from <code>sklearn.metrics.f1_score</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>f1_score (float or array of float, shape = [n_unique_labels])</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>raised if above sklearn method for computing precision raises</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef f1_score(y_true: Union[np.ndarray, list],\ny_pred: Union[np.ndarray, list],\n**kwargs: dict) -&gt; float:\n\"\"\"Evaluate the F1 score.\n    Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        **kwargs: Extra arguments from [`sklearn.metrics.f1_score`][sklearn.metrics.f1_score]\n    Returns:\n        f1_score (float or array of float, shape = [n_unique_labels])\n    Raises:\n        FedbiomedMetricError: raised if above sklearn method for computing precision raises\n    \"\"\"\n# Get average and pob_label argument based on multiclass status\ny_true, y_pred, average, pos_label = Metrics._configure_multiclass_parameters(y_true,\ny_pred,\nkwargs,\n'F1_SCORE')\nkwargs.pop(\"average\", None)\nkwargs.pop(\"pos_label\", None)\ntry:\nreturn metrics.f1_score(y_true, y_pred, average=average, pos_label=pos_label, **kwargs)\nexcept Exception as e:\nraise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: Error during calculation of `F1_SCORE` {str(e)}\")\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.mae","title":"<pre><code>mae(y_true, y_pred, kwargs)\n</code></pre>  <code>staticmethod</code>","text":"<p>Evaluate the mean absolute error.</p> <p>Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[np.ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[np.ndarray, list]</code> <p>Predicted values</p> required <code>**kwargs</code> <code>dict</code> <p>Extra arguments from <code>sklearn.metrics.mean_absolute_error</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>MAE score (float or ndarray of floats)</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>raised if above sklearn method for computing precision raises</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef mae(y_true: Union[np.ndarray, list],\ny_pred: Union[np.ndarray, list],\n**kwargs: dict) -&gt; float:\n\"\"\"Evaluate the mean absolute error.\n    Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        **kwargs: Extra arguments from [`sklearn.metrics.mean_absolute_error`][sklearn.metrics.mean_absolute_error]\n    Returns:\n        MAE score (float or ndarray of floats)\n    Raises:\n        FedbiomedMetricError: raised if above sklearn method for computing precision raises\n    \"\"\"\n# Set multiouput as raw_values is it is not defined by researcher\nif len(y_true.shape) &gt; 1:\nmulti_output = kwargs.get('multioutput', 'raw_values')\nelse:\nmulti_output = None\nkwargs.pop('multioutput', None)\ntry:\nreturn metrics.mean_absolute_error(y_true, y_pred, multioutput=multi_output, **kwargs)\nexcept Exception as e:\nraise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: Error during calculation of `MEAN_ABSOLUTE_ERROR`\"\nf\" {str(e)}\")\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.mse","title":"<pre><code>mse(y_true, y_pred, kwargs)\n</code></pre>  <code>staticmethod</code>","text":"<p>Evaluate the mean squared error.</p> <p>Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[np.ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[np.ndarray, list]</code> <p>Predicted values</p> required <code>**kwargs</code> <code>dict</code> <p>Extra arguments from <code>sklearn.metrics.mean_squared_error</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>MSE score (float or ndarray of floats)</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>raised if above sklearn method for computing precision raises</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef mse(y_true: Union[np.ndarray, list],\ny_pred: Union[np.ndarray, list],\n**kwargs: dict) -&gt; float:\n\"\"\"Evaluate the mean squared error.\n    Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        **kwargs: Extra arguments from [`sklearn.metrics.mean_squared_error`][sklearn.metrics.mean_squared_error]\n    Returns:\n        MSE score (float or ndarray of floats)\n    Raises:\n        FedbiomedMetricError: raised if above sklearn method for computing precision raises\n    \"\"\"\n# Set multiouput as raw_values is it is not defined by researcher\nif len(y_true.shape) &gt; 1:\nmulti_output = kwargs.get('multioutput', 'raw_values')\nelse:\nmulti_output = None\nkwargs.pop('multioutput', None)\ntry:\nreturn metrics.mean_squared_error(y_true, y_pred, multioutput=multi_output, **kwargs)\nexcept Exception as e:\nraise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: Error during calculation of `MEAN_SQUARED_ERROR`\"\nf\" {str(e)}\")\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.precision","title":"<pre><code>precision(y_true, y_pred, kwargs)\n</code></pre>  <code>staticmethod</code>","text":"<p>Evaluate the precision score [source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html]</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[np.ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[np.ndarray, list]</code> <p>Predicted values</p> required <code>**kwargs</code> <code>dict</code> <p>Extra arguments from <code>sklearn.metrics.precision_score</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>precision (float, or array of float of shape (n_unique_labels,))</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>raised if above sklearn method for computing precision raises</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef precision(y_true: Union[np.ndarray, list],\ny_pred: Union[np.ndarray, list],\n**kwargs: dict) -&gt; float:\n\"\"\"Evaluate the precision score\n    [source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html]\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        **kwargs: Extra arguments from [`sklearn.metrics.precision_score`][sklearn.metrics.precision_score]\n    Returns:\n        precision (float, or array of float of shape (n_unique_labels,))\n    Raises:\n        FedbiomedMetricError: raised if above sklearn method for computing precision raises\n    \"\"\"\n# Get average and pob_label argument based on multiclass status\ny_true, y_pred, average, pos_label = Metrics._configure_multiclass_parameters(y_true,\ny_pred,\nkwargs,\n'PRECISION')\nkwargs.pop(\"average\", None)\nkwargs.pop(\"pos_label\", None)\ntry:\nreturn metrics.precision_score(y_true, y_pred, average=average, pos_label=pos_label, **kwargs)\nexcept Exception as e:\nraise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: Error during calculation of `PRECISION` \"\nf\"calculation: {str(e)}\")\n</code></pre>"},{"location":"developer/api/common/metrics/#fedbiomed.common.metrics.Metrics.recall","title":"<pre><code>recall(y_true, y_pred, kwargs)\n</code></pre>  <code>staticmethod</code>","text":"<p>Evaluate the recall. [source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html]</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[np.ndarray, list]</code> <p>True values</p> required <code>y_pred</code> <code>Union[np.ndarray, list]</code> <p>Predicted values</p> required <code>**kwargs</code> <code>dict</code> <p>Extra arguments from <code>sklearn.metrics.recall_score</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>recall (float (if average is not None) or array of float of shape (n_unique_labels,))</p> <p>Raises:</p> Type Description <code>FedbiomedMetricError</code> <p>raised if above sklearn method for computing precision raises</p> Source code in <code>fedbiomed/common/metrics.py</code> <pre><code>@staticmethod\ndef recall(y_true: Union[np.ndarray, list],\ny_pred: Union[np.ndarray, list],\n**kwargs: dict) -&gt; float:\n\"\"\"Evaluate the recall.\n    [source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html]\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        **kwargs: Extra arguments from [`sklearn.metrics.recall_score`][sklearn.metrics.recall_score]\n    Returns:\n        recall (float (if average is not None) or array of float of shape (n_unique_labels,))\n    Raises:\n        FedbiomedMetricError: raised if above sklearn method for computing precision raises\n    \"\"\"\n# Get average and pob_label argument based on multiclass status\ny_true, y_pred, average, pos_label = Metrics._configure_multiclass_parameters(y_true, y_pred, kwargs, 'RECALL')\nkwargs.pop(\"average\", None)\nkwargs.pop(\"pos_label\", None)\ntry:\nreturn metrics.recall_score(y_true, y_pred, average=average, pos_label=pos_label, **kwargs)\nexcept Exception as e:\nraise FedbiomedMetricError(f\"{ErrorNumbers.FB611.value}: Error during calculation of `RECALL` \"\nf\"calculation: {str(e)}\")\n</code></pre>"},{"location":"developer/api/common/models/","title":"Model","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models","title":"fedbiomed.common.models","text":"Module: <code>fedbiomed.common.models</code> <p>The <code>fedbiomed.common.models</code> module includes model abstraction classes that can be used with plain framework specific models.</p> <p>Please visit Declearn repository for the \"TorchVector\" and \"NumpyVector\" classes used in this module.</p>"},{"location":"developer/api/common/models/#fedbiomed.common.models-classes","title":"Classes","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel","title":"BaseSkLearnModel","text":"CLASS  <pre><code>BaseSkLearnModel(model)\n</code></pre> <p>           Bases: <code>Model</code></p> <p>Wrapper of Scikit learn models.</p> <p>This class implements all abstract methods from the <code>Model</code> API, but adds some scikit-learn-specific ones that need implementing by its children.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>BaseEstimator</code> <p>Wrapped model</p> <code>param_list</code> <code>List[str]</code> <p>List that contains layer attributes. Should be set when calling <code>set_init_params</code> method</p> <p>Class attributes:</p> Name Type Description <code>is_classification</code> <code>bool</code> <p>Boolean flag indicating whether the wrapped model is designed for classification or for regression supervised-learning tasks.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseEstimator</code> <p>Model object as an instance of BaseEstimator</p> required <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>if model is not as scikit learn BaseEstimator object</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def __init__(\nself,\nmodel: BaseEstimator,\n) -&gt; None:\n\"\"\"Instantiate the wrapper over a scikit-learn BaseEstimator.\n    Args:\n        model: Model object as an instance of [BaseEstimator][sklearn.base.BaseEstimator]\n    Raises:\n        FedbiomedModelError: if model is not as scikit learn [BaseEstimator][sklearn.base.BaseEstimator] object\n    \"\"\"\nsuper().__init__(model)\nself._gradients: Dict[str, np.ndarray] = {}\nself.param_list: List[str] = []\nself._optim_params: Dict[str, Any] = {}\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel-attributes","title":"Attributes","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.is_classification","title":"is_classification     <code>class-attribute</code>","text":"<pre><code>is_classification: bool\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.model","title":"model     <code>instance-attribute</code>","text":"<pre><code>model: BaseEstimator\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.param_list","title":"param_list     <code>instance-attribute</code>","text":"<pre><code>param_list: List[str] = []\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.BaseSkLearnModel-functions","title":"Functions","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.apply_updates","title":"<pre><code>apply_updates(updates)\n</code></pre>","text":"<p>Apply incoming updates to the wrapped model's parameters.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>Dict[str, np.ndarray]</code> <p>Model parameters' updates to add (apply) to existing parameters' values.</p> required Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def apply_updates(\nself,\nupdates: Dict[str, np.ndarray],\n) -&gt; None:\n\"\"\"Apply incoming updates to the wrapped model's parameters.\n    Args:\n        updates: Model parameters' updates to add (apply) to existing\n            parameters' values.\n    \"\"\"\nself._assert_dict_inputs(updates)\nfor key, val in updates.items():\nweights = getattr(self.model, key)\nsetattr(self.model, key, weights + val)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.disable_internal_optimizer","title":"<pre><code>disable_internal_optimizer()\n</code></pre>","text":"<p>Disable the scikit-learn internal optimizer.</p> <p>Calling this method alters the wrapped model so that raw gradients are computed and attached to it (rather than relying on scikit-learn to apply a learning rate that may be scheduled to vary along time).</p> <p>''' warning \"Call it only if using an external optimizer\"</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def disable_internal_optimizer(self) -&gt; None:\n\"\"\"Disable the scikit-learn internal optimizer.\n    Calling this method alters the wrapped model so that raw gradients are\n    computed and attached to it (rather than relying on scikit-learn to\n    apply a learning rate that may be scheduled to vary along time).\n    ''' warning \"Call it only if using an external optimizer\"\n    \"\"\"\n# Record initial params, then override optimizer ones.\nself._optim_params = self.get_params()\nself.set_params(**self._null_optim_params)\n# Warn about overridden values.\nchanged_params: List[str] = []\nfor key, val in self._null_optim_params.items():\nparam = self._optim_params.get(key)\nif param is not None and param != val:\nchanged_params.append(key)\nif changed_params:\nchanged = \",\\n\\t\".join(changed_params)\nlogger.warning(\n\"The following non-default model parameters were overridden \"\nf\"due to the disabling of the scikit-learn internal optimizer:\\n\\t{changed}\"\n)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.enable_internal_optimizer","title":"<pre><code>enable_internal_optimizer()\n</code></pre>","text":"<p>Enable the scikit-learn internal optimizer.</p> <p>Calling this method restores any model parameter previously overridden due to calling the counterpart <code>disable_internal_optimizer</code> method.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def enable_internal_optimizer(self) -&gt; None:\n\"\"\"Enable the scikit-learn internal optimizer.\n    Calling this method restores any model parameter previously overridden\n    due to calling the counterpart `disable_internal_optimizer` method.\n    \"\"\"\nif self._optim_params:\nself.set_params(**self._optim_params)\nlogger.debug(\"Internal Optimizer restored\")\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.export","title":"<pre><code>export(filename)\n</code></pre>","text":"<p>Export the wrapped model to a dump file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to the file where the model will be saved.</p> required <p>!!! info \"Notes\":     This method is designed to save the model to a local dump     file for easy re-use by the same user, possibly outside of     Fed-BioMed. It is not designed to produce trustworthy data     dumps and is not used to exchange models and their weights     as part of the federated learning process.</p> <p>!!! warning \"Warning\":     This method uses <code>joblib.dump</code>, which relies on pickle and     is therefore hard to trust by third-party loading methods.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def export(self, filename: str) -&gt; None:\n\"\"\"Export the wrapped model to a dump file.\n    Args:\n        filename: path to the file where the model will be saved.\n    !!! info \"Notes\":\n        This method is designed to save the model to a local dump\n        file for easy re-use by the same user, possibly outside of\n        Fed-BioMed. It is not designed to produce trustworthy data\n        dumps and is not used to exchange models and their weights\n        as part of the federated learning process.\n    !!! warning \"Warning\":\n        This method uses `joblib.dump`, which relies on pickle and\n        is therefore hard to trust by third-party loading methods.\n    \"\"\"\nwith open(filename, \"wb\") as file:\njoblib.dump(self.model, file)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.flatten","title":"<pre><code>flatten()\n</code></pre>","text":"<p>Gets weights as flatten vector</p> <p>Returns:</p> Name Type Description <code>to_list</code> <code>List[float]</code> <p>Convert np.ndarray to a list if it is True.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def flatten(self) -&gt; List[float]:\n\"\"\"Gets weights as flatten vector\n    Returns:\n        to_list: Convert np.ndarray to a list if it is True.\n    \"\"\"\nweights = self.get_weights()\nflatten = []\nfor _, w in weights.items():\nw_: List[float] = list(w.flatten().astype(float))\nflatten.extend(w_)\nreturn flatten\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.get_gradients","title":"<pre><code>get_gradients()\n</code></pre>","text":"<p>Return computed gradients attached to the model.</p> <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>If no gradients have been computed yet (i.e. the model has not been trained).</p> <p>Returns:</p> Type Description <code>Dict[str, np.ndarray]</code> <p>Gradients, as a dict mapping parameters' names to their gradient's numpy array.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def get_gradients(\nself,\n) -&gt; Dict[str, np.ndarray]:\n\"\"\"Return computed gradients attached to the model.\n    Raises:\n        FedbiomedModelError: If no gradients have been computed yet\n            (i.e. the model has not been trained).\n    Returns:\n        Gradients, as a dict mapping parameters' names to their\n            gradient's numpy array.\n    \"\"\"\nif not self._gradients:\nraise FedbiomedModelError(\nf\"{ErrorNumbers.FB622.value}. Cannot get gradients if the \"\n\"model has not been trained beforehand.\"\n)\ngradients = self._gradients\nreturn gradients\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.get_learning_rate","title":"<pre><code>get_learning_rate()\n</code></pre>  <code>abstractmethod</code>","text":"<p>Retrieves learning rate of the model. Method implementation will depend on the attribute used to set up these arbitrary arguments</p> <p>Returns:</p> Type Description <code>List[float]</code> <p>Initial learning rate value(s); a single value if only on learning rate has been used, and a list of several learning rates, one for each layer of the model.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>@abstractmethod\ndef get_learning_rate(self) -&gt; List[float]:\n\"\"\"Retrieves learning rate of the model. Method implementation will\n    depend on the attribute used to set up these arbitrary arguments\n    Returns:\n        Initial learning rate value(s); a single value if only on learning rate has been used, and\n            a list of several learning rates, one for each layer of the model.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.get_params","title":"<pre><code>get_params(value=None)\n</code></pre>","text":"<p>Return the wrapped scikit-learn model's hyperparameters.</p> <p>Please refer to [<code>baseEstimator documentation</code>] [https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html] <code>get_params</code> method for further details.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>if specified, returns a specific hyperparameter, otherwise, returns a dictionary with all the hyperparameters. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary mapping model hyperparameter names to their values</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def get_params(self, value: Any = None) -&gt; Dict[str, Any]:\n\"\"\"Return the wrapped scikit-learn model's hyperparameters.\n    Please refer to [`baseEstimator documentation`]\n    [https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html] `get_params` method\n    for further details.\n    Args:\n        value: if specified, returns a specific hyperparameter, otherwise, returns a dictionary\n            with all the hyperparameters. Defaults to None.\n    Returns:\n        Dictionary mapping model hyperparameter names to their values\n    \"\"\"\nif value is not None:\nreturn self.model.get_params().get(value)\nreturn self.model.get_params()\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.get_weights","title":"<pre><code>get_weights(only_trainable=False)\n</code></pre>","text":"<p>Return a copy of the model's trainable weights.</p> <p>Parameters:</p> Name Type Description Default <code>only_trainable</code> <code>bool</code> <p>Unused for scikit-learn models. (Whether to ignore non-trainable model parameters.)</p> <code>False</code> <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>If the model parameters are not initialized.</p> <p>Returns:</p> Type Description <code>Dict[str, np.ndarray]</code> <p>Model weights, as a dictionary mapping parameters' names to their numpy array, or as a declearn NumpyVector wrapping such a dict.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def get_weights(\nself,\nonly_trainable: bool = False,\n) -&gt; Dict[str, np.ndarray]:\n\"\"\"Return a copy of the model's trainable weights.\n    Args:\n        only_trainable: Unused for scikit-learn models. (Whether to ignore\n            non-trainable model parameters.)\n    Raises:\n        FedbiomedModelError: If the model parameters are not initialized.\n    Returns:\n        Model weights, as a dictionary mapping parameters' names to their\n            numpy array, or as a declearn NumpyVector wrapping such a dict.\n    \"\"\"\nif not self.param_list:\nraise FedbiomedModelError(\nf\"{ErrorNumbers.FB622.value}. Attribute `param_list` is empty. You should \"\nf\"have initialized the model beforehand (try calling `set_init_params`)\"\n)\n# Gather copies of the model weights.\nweights = {}  # type: Dict[str, np.ndarray]\ntry:\nfor key in self.param_list:\nval = getattr(self.model, key)\nif not isinstance(val, np.ndarray):\nraise FedbiomedModelError(\nf\"{ErrorNumbers.FB622.value}: SklearnModel parameter is not a numpy array.\"\n)\nweights[key] = val.copy()\nexcept AttributeError as err:\nraise FedbiomedModelError(\nf\"{ErrorNumbers.FB622.value}. Unable to access weights of BaseEstimator \"\nf\"model {self.model} (details {err})\"\n) from err\nreturn weights\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.init_training","title":"<pre><code>init_training()\n</code></pre>","text":"<p>Initialises the training by setting up attributes.</p> <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>raised if <code>param_list</code> has not been defined</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def init_training(self):\n\"\"\"Initialises the training by setting up attributes.\n    Raises:\n        FedbiomedModelError: raised if `param_list` has not been defined\n    \"\"\"\nif not self.param_list:\nraise FedbiomedModelError(\nf\"{ErrorNumbers.FB622.value}. Attribute `param_list` is empty. You should \"\nf\"have initialized the model beforehand (try calling `set_init_params`)\"\n)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.predict","title":"<pre><code>predict(inputs)\n</code></pre>","text":"<p>Computes prediction given input data.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>np.ndarray</code> <p>input data</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Model predictions</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def predict(\nself,\ninputs: np.ndarray,\n) -&gt; np.ndarray:\n\"\"\"Computes prediction given input data.\n    Args:\n        inputs: input data\n    Returns:\n        Model predictions\n    \"\"\"\nreturn self.model.predict(inputs)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.set_gradients","title":"<pre><code>set_gradients(gradients)\n</code></pre>","text":"Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def set_gradients(self, gradients: Dict[str, np.ndarray]) -&gt; None:\n# TODO: either document or remove this (useless) method\nself._gradients = gradients\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.set_init_params","title":"<pre><code>set_init_params(model_args)\n</code></pre>  <code>abstractmethod</code>","text":"<p>Zeroes scikit learn model parameters.</p> <p>Should be used before any training, as it sets the scikit learn model parameters and makes them accessible through the use of attributes. Model parameter attribute names will depend on the scikit learn model wrapped.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict</code> <p>dictionary that contains specifications for setting initial model</p> required Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>@abstractmethod\ndef set_init_params(self, model_args: Dict) -&gt; None:\n\"\"\"Zeroes scikit learn model parameters.\n    Should be used before any training, as it sets the scikit learn model parameters\n    and makes them accessible through the use of attributes. Model parameter attribute names\n    will depend on the scikit learn model wrapped.\n    Args:\n        model_args: dictionary that contains specifications for setting initial model\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.set_params","title":"<pre><code>set_params(params)\n</code></pre>","text":"<p>Assign some hyperparameters to the wrapped scikit-learn model.</p> <p>Please refer to BaseEstimator [https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html] <code>set_params</code> method for further details.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Any</code> <p>new hyperparameters to assign to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: dictionary containing new hyperparameter values.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def set_params(self, **params: Any) -&gt; Dict[str, Any]:\n\"\"\"Assign some hyperparameters to the wrapped scikit-learn model.\n    Please refer to [BaseEstimator][sklearn.base.BaseEstimator]\n    [https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html] `set_params` method\n    for further details.\n    Args:\n        params: new hyperparameters to assign to the model.\n    Returns:\n        Dict[str, Any]: dictionary containing new hyperparameter values.\n    \"\"\"\nself.model.set_params(**params)\nreturn params\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.set_weights","title":"<pre><code>set_weights(weights)\n</code></pre>","text":"<p>Assign new values to the model's trainable weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Dict[str, np.ndarray]</code> <p>Model weights, as a dict mapping parameters' names to their numpy array.</p> required Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def set_weights(\nself,\nweights: Dict[str, np.ndarray],\n) -&gt; None:\n\"\"\"Assign new values to the model's trainable weights.\n    Args:\n        weights: Model weights, as a dict mapping parameters' names\n            to their numpy array.\n    \"\"\"\nself._assert_dict_inputs(weights)\nfor key, val in weights.items():\nsetattr(self.model, key, val.copy())\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.train","title":"<pre><code>train(inputs, targets, stdout=None, kwargs)\n</code></pre>","text":"<p>Run a training step, and record associated gradients.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>np.ndarray</code> <p>inputs data.</p> required <code>targets</code> <code>np.ndarray</code> <p>targets, to be fit with inputs data.</p> required <code>stdout</code> <code>Optional[List[List[str]]]</code> <p>list of console outputs that have been collected during training, that contains losses values. Used to plot model losses. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>if training has not been initialized.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def train(\nself,\ninputs: np.ndarray,\ntargets: np.ndarray,\nstdout: Optional[List[List[str]]] = None,\n**kwargs,\n) -&gt; None:\n\"\"\"Run a training step, and record associated gradients.\n    Args:\n        inputs: inputs data.\n        targets: targets, to be fit with inputs data.\n        stdout: list of console outputs that have been collected\n            during training, that contains losses values.\n            Used to plot model losses. Defaults to None.\n    Raises:\n        FedbiomedModelError: if training has not been initialized.\n    \"\"\"\nbatch_size = inputs.shape[0]\nw_init = self.get_weights()\nw_updt = {key: np.zeros_like(val) for key, val in w_init.items()}\n# Iterate over the batch; accumulate sample-wise gradients (and loss).\nfor idx in range(batch_size):\n# Compute updated weights based on the sample. Capture loss prints.\nwith capture_stdout() as console:\nself.model.partial_fit(inputs[idx : idx + 1], targets[idx])\nif stdout is not None:\nstdout.append(console)\n# Accumulate updated weights (weights + sum of gradients).\n# Reset the model's weights and iteration counter.\nfor key in self.param_list:\nw_updt[key] += getattr(self.model, key)\nsetattr(self.model, key, w_init[key].copy())\nself.model.n_iter_ -= 1\n# Compute the batch-averaged, learning-rate-scaled gradients.\n# Note: w_init: {w_t}, w_updt: {w_t - eta_t * sum_{s=1}^B(grad_s)}\n#       hence eta_t * avg(grad_s) = w_init - (w_updt / B)\nself._gradients = {\nkey: w_init[key] - (w_updt[key] / batch_size)\nfor key in self.param_list\n}\n# ------------------------------ WARNINGS ----------------------------------\n#\n# Warning 1: if `disable_internal_optimizer` has not been called before, gradients won't be scaled\n# (you will get un-scaled gradients, that need to be scaled back by dividing gradients by the learning rate)\n# here is a way to do so (with `lrate` as the learning rate): \n# ```python\n# for key, val in self._gradients.items():\n#        val /= lrate\n# ````\n# Warning 2:  `_gradients` has different meanings, when using `disable_internal_optimizer`\n# if it is not called (ie when using native sklearn optimizer), it is not plain gradients, \n# but rather the quantity `lr * grads`\n# Finally, increment the model's iteration counter.\nself.model.n_iter_ += 1\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.BaseSkLearnModel.unflatten","title":"<pre><code>unflatten(weights_vector)\n</code></pre>","text":"<p>Unflatten vectorized model weights</p> <p>Parameters:</p> Name Type Description Default <code>weights_vector</code> <code>List[float]</code> <p>Vectorized model weights to convert dict</p> required <p>Returns:</p> Type Description <code>Dict[str, np.ndarray]</code> <p>Model dictionary</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def unflatten(\nself,\nweights_vector: List[float]\n) -&gt; Dict[str, np.ndarray]:\n\"\"\"Unflatten vectorized model weights\n    Args:\n        weights_vector: Vectorized model weights to convert dict\n    Returns:\n        Model dictionary\n    \"\"\"\nsuper().unflatten(weights_vector)\nweights_vector = np.array(weights_vector)\nweights = self.get_weights()\npointer = 0\nparams = {}\nfor key, w in weights.items():\nnum_param = w.size\nparams[key] = weights_vector[pointer: pointer + num_param].reshape(w.shape)\npointer += num_param\nreturn params\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.MLPSklearnModel","title":"MLPSklearnModel","text":"CLASS  <pre><code>MLPSklearnModel(model)\n</code></pre> <p>           Bases: <code>BaseSkLearnModel</code></p> <p>BaseSklearnModel abstract subclass for multi-layer perceptron models.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def __init__(self, model: BaseEstimator) -&gt; None:\nself._null_optim_params: Dict[str, Any] = {\n\"learning_rate_init\": 1.0,\n\"learning_rate\": \"constant\",\n}\nsuper().__init__(model)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.MLPSklearnModel-attributes","title":"Attributes","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.MLPSklearnModel.model","title":"model     <code>instance-attribute</code>","text":"<pre><code>model: Union[MLPClassifier, MLPRegressor]\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.MLPSklearnModel-functions","title":"Functions","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.MLPSklearnModel.get_learning_rate","title":"<pre><code>get_learning_rate()\n</code></pre>","text":"Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def get_learning_rate(self) -&gt; List[float]:\nreturn [self.model.learning_rate_init]\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model","title":"Model","text":"CLASS  <pre><code>Model(model)\n</code></pre> <p>           Bases: <code>Generic[_MT, DT]</code></p> <p>Model abstraction, that wraps and handles both native models</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Any</code> <p>native model, written in a framework supported by Fed-BioMed.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>_MT</code> <p>native model wrapped, of child-class-specific type.</p> required Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>def __init__(self, model: _MT):\n\"\"\"Constructor of Model abstract class\n    Args:\n        model: native model wrapped, of child-class-specific type.\n    \"\"\"\nself._validate_model_type(model)\nself.model: Any = model\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model-attributes","title":"Attributes","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models._model.Model.model","title":"model     <code>instance-attribute</code>","text":"<pre><code>model: Any = model\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.Model-functions","title":"Functions","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models._model.Model.apply_updates","title":"<pre><code>apply_updates(updates)\n</code></pre>  <code>abstractmethod</code>","text":"<p>Applies updates to the model.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>Dict[str, DT]</code> <p>model updates.</p> required Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef apply_updates(self, updates: Dict[str, DT]):\n\"\"\"Applies updates to the model.\n    Args:\n        updates: model updates.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._model.Model.export","title":"<pre><code>export(filename)\n</code></pre>  <code>abstractmethod</code>","text":"<p>Export the wrapped model to a dump file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to the file where the model will be saved.</p> required <p>!!! info \"Notes\":     This method is designed to save the model to a local dump     file for easy re-use by the same user, possibly outside of     Fed-BioMed. It is not designed to produce trustworthy data     dumps and is not used to exchange models and their weights     as part of the federated learning process.</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef export(self, filename: str) -&gt; None:\n\"\"\"Export the wrapped model to a dump file.\n    Args:\n        filename: path to the file where the model will be saved.\n    !!! info \"Notes\":\n        This method is designed to save the model to a local dump\n        file for easy re-use by the same user, possibly outside of\n        Fed-BioMed. It is not designed to produce trustworthy data\n        dumps and is not used to exchange models and their weights\n        as part of the federated learning process.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._model.Model.flatten","title":"<pre><code>flatten()\n</code></pre>  <code>abstractmethod</code>","text":"<p>Flattens model weights</p> <p>Returns:</p> Type Description <code>List[float]</code> <p>List of model weights as float.</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef flatten(self) -&gt; List[float]:\n\"\"\"Flattens model weights\n    Returns:\n        List of model weights as float.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._model.Model.get_gradients","title":"<pre><code>get_gradients()\n</code></pre>  <code>abstractmethod</code>","text":"<p>Return computed gradients attached to the model.</p> <p>Returns:</p> Type Description <code>Dict[str, DT]</code> <p>Gradients, as a dict mapping parameters' names to their gradient's value.</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef get_gradients(self) -&gt; Dict[str, DT]:\n\"\"\"Return computed gradients attached to the model.\n    Returns:\n        Gradients, as a dict mapping parameters' names to their\n            gradient's value.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._model.Model.get_weights","title":"<pre><code>get_weights()\n</code></pre>  <code>abstractmethod</code>","text":"<p>Return a copy of the model's trainable weights.</p> <p>Parameters:</p> Name Type Description Default <code>only_trainable</code> <p>Whether to ignore non-trainable model parameters from outputs (e.g. frozen neural network layers' parameters), or include all model parameters (the default).</p> required <p>Returns:</p> Type Description <code>Dict[str, DT]</code> <p>Model weights, as a dict mapping parameters' names to their value.</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef get_weights(self) -&gt; Dict[str, DT]:\n\"\"\"Return a copy of the model's trainable weights.\n    Args:\n        only_trainable: Whether to ignore non-trainable model parameters\n            from outputs (e.g. frozen neural network layers' parameters),\n            or include all model parameters (the default).\n    Returns:\n        Model weights, as a dict mapping parameters' names to their value.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._model.Model.init_training","title":"<pre><code>init_training()\n</code></pre>  <code>abstractmethod</code>","text":"<p>Initialize parameters before model training.</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef init_training(self):\n\"\"\"Initialize parameters before model training.\"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._model.Model.predict","title":"<pre><code>predict(inputs)\n</code></pre>  <code>abstractmethod</code>","text":"<p>Return model predictions given input values.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Any</code> <p>input values.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>predictions.</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef predict(self, inputs: Any) -&gt; Any:\n\"\"\"Return model predictions given input values.\n    Args:\n        inputs: input values.\n    Returns:\n        Any: predictions.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._model.Model.reload","title":"<pre><code>reload(filename)\n</code></pre>","text":"<p>Import and replace the wrapped model from a dump file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to the file where the model has been exported.</p> required <p>!!! info \"Notes\":     This method is designed to load the model from a local dump     file, that might not be in a trustworthy format. It should     therefore only be used to re-load data exported locally and     not received from someone else, including other FL peers.</p> <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>if the reloaded instance is of unproper type.</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>def reload(self, filename: str) -&gt; None:\n\"\"\"Import and replace the wrapped model from a dump file.\n    Args:\n        filename: path to the file where the model has been exported.\n    !!! info \"Notes\":\n        This method is designed to load the model from a local dump\n        file, that might not be in a trustworthy format. It should\n        therefore only be used to re-load data exported locally and\n        not received from someone else, including other FL peers.\n    Raises:\n        FedbiomedModelError: if the reloaded instance is of unproper type.\n    \"\"\"\nmodel = self._reload(filename)\nif not isinstance(model, self._model_type):\nerr_msg = (\nf\"{ErrorNumbers.FB622.value}: unproper type for imported model\"\nf\": expected '{self._model_type}', but 'got {type(model)}'.\"\n)\nlogger.critical(err_msg)\nraise FedbiomedModelError(err_msg)\nself.model = model\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._model.Model.set_model","title":"<pre><code>set_model(model)\n</code></pre>","text":"<p>Replace the wrapped model with a new one.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>_MT</code> <p>New model instance that needs assignment as the <code>model</code> attribute.</p> required Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>def set_model(self, model: _MT) -&gt; None:\n\"\"\"Replace the wrapped model with a new one.\n    Args:\n        model: New model instance that needs assignment as the `model`\n            attribute.\n    \"\"\"\nself._validate_model_type(model)\nself.model = model\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._model.Model.set_weights","title":"<pre><code>set_weights(weights)\n</code></pre>  <code>abstractmethod</code>","text":"<p>Assign new values to the model's trainable weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Dict[str, DT]</code> <p>Model weights, as a dict mapping parameters' names to their value.</p> required Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef set_weights(self, weights: Dict[str, DT]) -&gt; None:\n\"\"\"Assign new values to the model's trainable weights.\n    Args:\n        weights: Model weights, as a dict mapping parameters' names\n            to their value.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._model.Model.train","title":"<pre><code>train(inputs, targets, kwargs)\n</code></pre>  <code>abstractmethod</code>","text":"<p>Perform a training step given inputs and targets data.</p> <p>Warning</p> <p>Please run <code>init_training</code> method before running <code>train</code> method, so to initialize parameters needed for model training\"</p> <p>Warning</p> <p>This function usually does not update weights. You need to call <code>apply_updates</code> to ensure updates are applied to the model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Any</code> <p>input (training) data.</p> required <code>targets</code> <code>Any</code> <p>target values.</p> required Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef train(self, inputs: Any, targets: Any, **kwargs) -&gt; None:\n\"\"\"Perform a training step given inputs and targets data.\n    !!! warning \"Warning\"\n        Please run `init_training` method before running `train` method,\n        so to initialize parameters needed for model training\"\n    !!! warning \"Warning\"\n        This function usually does not update weights. You need to call\n        `apply_updates` to ensure updates are applied to the model.\n    Args:\n        inputs: input (training) data.\n        targets: target values.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._model.Model.unflatten","title":"<pre><code>unflatten(weights_vector)\n</code></pre>  <code>abstractmethod</code>","text":"<p>Revert flatten model weights back model-dict form.</p> <p>Parameters:</p> Name Type Description Default <code>weights_vector</code> <code>List[float]</code> <p>Vectorized model weights to convert dict</p> required <p>Returns:</p> Type Description <code>None</code> <p>Model dictionary</p> Source code in <code>fedbiomed/common/models/_model.py</code> <pre><code>@abstractmethod\ndef unflatten(\nself,\nweights_vector: List[float]\n) -&gt; None:\n\"\"\"Revert flatten model weights back model-dict form.\n    Args:\n        weights_vector: Vectorized model weights to convert dict\n    Returns:\n        Model dictionary\n    \"\"\"\nif not isinstance(weights_vector, list) or not all([isinstance(w, float) for w in weights_vector]):\nraise FedbiomedModelError(\nf\"{ErrorNumbers.FB622} `weights_vector should be 1D list of float containing flatten model parameters`\"\n)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDClassifierSKLearnModel","title":"SGDClassifierSKLearnModel","text":"<p>           Bases: <code>SGDSkLearnModel</code></p> <p>BaseSkLearnModel subclass for SGDClassifier models.</p>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDClassifierSKLearnModel-attributes","title":"Attributes","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.SGDClassifierSKLearnModel.is_classification","title":"is_classification     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_classification = True\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.SGDClassifierSKLearnModel.model","title":"model     <code>instance-attribute</code>","text":"<pre><code>model: SGDClassifier\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDClassifierSKLearnModel-functions","title":"Functions","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.SGDClassifierSKLearnModel.set_init_params","title":"<pre><code>set_init_params(model_args)\n</code></pre>","text":"<p>Initialize the model's trainable parameters.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def set_init_params(self, model_args: Dict[str, Any]) -&gt; None:\n\"\"\"Initialize the model's trainable parameters.\"\"\"\n# Set up zero-valued start weights, for binary of multiclass classif.\nn_classes = model_args[\"n_classes\"]\nif n_classes == 2:\ninit_params = {\n\"intercept_\": np.zeros((1,)),\n\"coef_\": np.zeros((1, model_args[\"n_features\"])),\n}\nelse:\ninit_params = {\n\"intercept_\": np.zeros((n_classes,)),\n\"coef_\": np.zeros((n_classes, model_args[\"n_features\"])),\n}\n# Assign these initialization parameters and retain their names.\nself.param_list = list(init_params)\nfor key, val in init_params.items():\nsetattr(self.model, key, val)\n# Also initialize the \"classes_\" slot with unique predictable labels.\n# FIXME: this assumes target values are integers in range(n_classes).\nsetattr(self.model, \"classes_\", np.arange(n_classes))\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDRegressorSKLearnModel","title":"SGDRegressorSKLearnModel","text":"<p>           Bases: <code>SGDSkLearnModel</code></p> <p>BaseSkLearnModel subclass for SGDRegressor models.</p>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDRegressorSKLearnModel-attributes","title":"Attributes","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.SGDRegressorSKLearnModel.is_classification","title":"is_classification     <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_classification = False\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.SGDRegressorSKLearnModel.model","title":"model     <code>instance-attribute</code>","text":"<pre><code>model: SGDRegressor\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SGDRegressorSKLearnModel-functions","title":"Functions","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models._sklearn.SGDRegressorSKLearnModel.set_init_params","title":"<pre><code>set_init_params(model_args)\n</code></pre>","text":"<p>Initialize the model's trainable parameters.</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def set_init_params(self, model_args: Dict[str, Any]):\n\"\"\"Initialize the model's trainable parameters.\"\"\"\ninit_params = {\n\"intercept_\": np.array([0.0]),\n\"coef_\": np.array([0.0] * model_args[\"n_features\"]),\n}\nself.param_list = list(init_params)\nfor key, val in init_params.items():\nsetattr(self.model, key, val)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.SkLearnModel","title":"SkLearnModel","text":"CLASS  <pre><code>SkLearnModel(model)\n</code></pre> <p>Sklearn model builder.</p> <p>It wraps one of Fed-BioMed <code>BaseSkLearnModel</code> object children, by passing a (BaseEstimator)(https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html) object to the constructor, as shown below.</p> <p>Usage <pre><code>    from sklearn.linear_model import SGDClassifier\n    model = SkLearnModel(SGDClassifier)\n    model.set_weights(some_weights)\n    type(model.model)\n    # Output: &lt;class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'&gt;\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>_instance</code> <code>BaseSkLearnModel</code> <p>instance of BaseSkLearnModel</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[BaseEstimator]</code> <p>non-initialized BaseEstimator object</p> required <p>Raises:</p> Type Description <code>FedbiomedModelError</code> <p>raised if model does not belong to the implemented models.</p> <code>FedbiomedModelError</code> <p>raised if <code>__name__</code> attribute does not belong to object. This may happen when passing an instantiated object instead of the class object (e.g. instance of SGDClassifier() instead of SGDClassifier object)</p> Source code in <code>fedbiomed/common/models/_sklearn.py</code> <pre><code>def __init__(self, model: Type[BaseEstimator]):\n\"\"\"Constructor of the model builder.\n    Args:\n        model: non-initialized [BaseEstimator][sklearn.base.BaseEstimator] object\n    Raises:\n        FedbiomedModelError: raised if model does not belong to the implemented models.\n        FedbiomedModelError: raised if `__name__` attribute does not belong to object. This may happen\n            when passing an instantiated object instead of the class object (e.g. instance of\n            SGDClassifier() instead of SGDClassifier object)\n    \"\"\"\nif not isinstance(model, type):\nraise FedbiomedModelError(\nf\"{ErrorNumbers.FB622.value}: 'SkLearnModel' received a '{type(model)}' instance as 'model' \"\n\"input while it was expecting a scikit-learn BaseEstimator subclass constructor.\"\n)\nif not issubclass(model, BaseEstimator):\nraise FedbiomedModelError(\nf\"{ErrorNumbers.FB622.value}: 'SkLearnModel' received a 'model' class that is not \"\nf\"a scikit-learn BaseEstimator subclass: '{model}'.\"\n)\nif model.__name__ not in SKLEARN_MODELS:\nraise FedbiomedModelError(\nf\"{ErrorNumbers.FB622.value}: 'SkLearnModel' received '{model}' as 'model' class, \"\nf\"support for which has not yet been implemented in Fed-BioMed.\"\n)\nself._instance: BaseSkLearnModel = SKLEARN_MODELS[model.__name__](model())\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel","title":"TorchModel","text":"CLASS  <pre><code>TorchModel(model)\n</code></pre> <p>           Bases: <code>Model</code></p> <p>PyTorch model wrapper that ease the handling of a pytorch model</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>torch.nn.Module</code> <p>torch.nn.Module. Pytorch model wrapped.</p> <code>init_params</code> <code>Dict[str, torch.Tensor]</code> <p>OrderedDict. Model initial parameters. Set when calling <code>init_training</code>.</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def __init__(self, model: torch.nn.Module) -&gt; None:\n\"\"\"Instantiates the wrapper over a torch Module instance.\"\"\"\nsuper().__init__(model)\nself.init_params: Dict[str, torch.Tensor] = {}\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel-attributes","title":"Attributes","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models._torch.TorchModel.init_params","title":"init_params     <code>instance-attribute</code>","text":"<pre><code>init_params: Dict[str, torch.Tensor] = {}\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._torch.TorchModel.model","title":"model     <code>instance-attribute</code>","text":"<pre><code>model: torch.nn.Module\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models.TorchModel-functions","title":"Functions","text":""},{"location":"developer/api/common/models/#fedbiomed.common.models._torch.TorchModel.add_corrections_to_gradients","title":"<pre><code>add_corrections_to_gradients(corrections)\n</code></pre>","text":"<p>Add values to the gradients currently attached to the model.</p> <p>Parameters:</p> Name Type Description Default <code>corrections</code> <code>Dict[str, torch.Tensor]</code> <p>corrections to be added to the model's gradients.</p> required Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def add_corrections_to_gradients(\nself,\ncorrections: Dict[str, torch.Tensor],\n) -&gt; None:\n\"\"\"Add values to the gradients currently attached to the model.\n    Args:\n        corrections: corrections to be added to the model's gradients.\n    \"\"\"\nself._assert_dict_inputs(corrections)\nfor name, update in corrections.items():\nparam = self.model.get_parameter(name)\nif param.grad is not None:\nparam.grad.add_(update.to(param.grad.device))\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._torch.TorchModel.apply_updates","title":"<pre><code>apply_updates(updates)\n</code></pre>","text":"<p>Apply incoming updates to the wrapped model's parameters.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>Dict[str, torch.Tensor]</code> <p>model updates to be added to the model.</p> required Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def apply_updates(\nself,\nupdates: Dict[str, torch.Tensor],\n) -&gt; None:\n\"\"\"Apply incoming updates to the wrapped model's parameters.\n    Args:\n        updates: model updates to be added to the model.\n    \"\"\"\nself._assert_dict_inputs(updates)\nwith torch.no_grad():\nfor name, update in updates.items():\nparam = self.model.get_parameter(name)\nparam.add_(update.to(param.device))\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._torch.TorchModel.export","title":"<pre><code>export(filename)\n</code></pre>","text":"<p>Export the wrapped model to a dump file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to the file where the model will be saved.</p> required <p>!!! info \"Notes\":     This method is designed to save the model to a local dump     file for easy re-use by the same user, possibly outside of     Fed-BioMed. It is not designed to produce trustworthy data     dumps and is not used to exchange models and their weights     as part of the federated learning process.</p> <p>!!! warning \"Warning\":     This method uses <code>torch.save</code>, which relies on pickle and     is therefore hard to trust by third-party loading methods.</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def export(self, filename: str) -&gt; None:\n\"\"\"Export the wrapped model to a dump file.\n    Args:\n        filename: path to the file where the model will be saved.\n    !!! info \"Notes\":\n        This method is designed to save the model to a local dump\n        file for easy re-use by the same user, possibly outside of\n        Fed-BioMed. It is not designed to produce trustworthy data\n        dumps and is not used to exchange models and their weights\n        as part of the federated learning process.\n    !!! warning \"Warning\":\n        This method uses `torch.save`, which relies on pickle and\n        is therefore hard to trust by third-party loading methods.\n    \"\"\"\ntorch.save(self.model, filename)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._torch.TorchModel.flatten","title":"<pre><code>flatten()\n</code></pre>","text":"<p>Gets weights as flatten vector</p> <p>Returns:</p> Name Type Description <code>to_list</code> <code>List[float]</code> <p>Convert np.ndarray to a list if it is True.</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def flatten(self) -&gt; List[float]:\n\"\"\"Gets weights as flatten vector\n    Returns:\n        to_list: Convert np.ndarray to a list if it is True.\n    \"\"\"\nparams: List[float] = torch.nn.utils.parameters_to_vector(\nself.model.parameters()\n).tolist()\nreturn params\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._torch.TorchModel.get_gradients","title":"<pre><code>get_gradients()\n</code></pre>","text":"<p>Return the gradients attached to the model.</p> <p>Returns:</p> Type Description <code>Dict[str, torch.Tensor]</code> <p>Gradients, as a dict mapping parameters' names to their gradient's torch tensor.</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def get_gradients(\nself,\n) -&gt; Dict[str, torch.Tensor]:\n\"\"\"Return the gradients attached to the model.\n    Returns:\n        Gradients, as a dict mapping parameters' names to their gradient's\n            torch tensor.\n    \"\"\"\ngradients = {\nname: param.grad.detach().clone()\nfor name, param in self.model.named_parameters()\nif (param.requires_grad and param.grad is not None)\n}\nif len(gradients) &lt; len(list(self.model.named_parameters())):\n# FIXME: this will be triggered when having some frozen weights\n#        even if training was properly conducted\nlogger.warning(\n\"Warning: can not retrieve all gradients from the model. \"\n\"Are you sure you have trained the model beforehand?\"\n)\nreturn gradients\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._torch.TorchModel.get_weights","title":"<pre><code>get_weights(only_trainable=False)\n</code></pre>","text":"<p>Return the model's parameters.</p> <p>Parameters:</p> Name Type Description Default <code>only_trainable</code> <code>bool</code> <p>Whether to ignore non-trainable model parameters from outputs (e.g. frozen neural network layers' parameters), or include all model parameters (the default).</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, torch.Tensor]</code> <p>Model weights, as a dictionary mapping parameters' names to their torch tensor.</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def get_weights(\nself,\nonly_trainable: bool = False,\n) -&gt; Dict[str, torch.Tensor]:\n\"\"\"Return the model's parameters.\n    Args:\n        only_trainable: Whether to ignore non-trainable model parameters\n            from outputs (e.g. frozen neural network layers' parameters),\n            or include all model parameters (the default).\n    Returns:\n        Model weights, as a dictionary mapping parameters' names to their\n            torch tensor.\n    \"\"\"\nparameters = {\nname: param.detach().clone()\nfor name, param in self.model.named_parameters()\nif param.requires_grad or not only_trainable\n}\nreturn parameters\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._torch.TorchModel.init_training","title":"<pre><code>init_training()\n</code></pre>","text":"<p>Initializes and sets attributes before the training.</p> <p>Initializes <code>init_params</code> as a copy of the initial parameters of the model</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def init_training(self) -&gt; None:\n\"\"\"Initializes and sets attributes before the training.\n    Initializes `init_params` as a copy of the initial parameters of the model\n    \"\"\"\n# initial aggregated model parameters\nself.init_params = {\nkey: param.data.detach().clone()\nfor key, param in self.model.named_parameters()\n}\nself.model.train()  # pytorch switch for training\nself.model.zero_grad()\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._torch.TorchModel.predict","title":"<pre><code>predict(inputs)\n</code></pre>","text":"<p>Computes prediction given input data.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>torch.Tensor</code> <p>input data</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Model predictions returned as a numpy array</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def predict(\nself,\ninputs: torch.Tensor,\n) -&gt; np.ndarray:\n\"\"\"Computes prediction given input data.\n    Args:\n        inputs: input data\n    Returns:\n        Model predictions returned as a numpy array\n    \"\"\"\nself.model.eval()  # pytorch switch for model inference-mode\nwith torch.no_grad():\npred = self.model(inputs)\nreturn pred.cpu().numpy()\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._torch.TorchModel.send_to_device","title":"<pre><code>send_to_device(device)\n</code></pre>","text":"<p>Sends model to device</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>torch.device</code> <p>device set for using GPU or CPU.</p> required Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def send_to_device(\nself,\ndevice: torch.device,\n) -&gt; None:\n\"\"\"Sends model to device\n    Args:\n        device: device set for using GPU or CPU.\n    \"\"\"\nself.model.to(device)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._torch.TorchModel.set_weights","title":"<pre><code>set_weights(weights)\n</code></pre>","text":"<p>Sets model weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Dict[str, torch.Tensor]</code> <p>Model weights, as a dict mapping parameters' names to their torch tensor.</p> required Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def set_weights(\nself,\nweights: Dict[str, torch.Tensor],\n) -&gt; None:\n\"\"\"Sets model weights.\n    Args:\n        weights: Model weights, as a dict mapping parameters' names\n            to their torch tensor.\n    \"\"\"\nself._assert_dict_inputs(weights)\nincompatible = self.model.load_state_dict(weights, strict=False)\n# Warn about (probably-)missing trainable weights.\n# Note: state_dict may include values that do not belong to the model's\n# parameters, and/or input weights may exclude non-trainable weights,\n# without requiring a warning.\nif incompatible.missing_keys:\nparams = {key for key, prm in self.model.named_parameters() if prm.requires_grad}\nmissing = params.intersection(incompatible.missing_keys)\nif missing:\nlogger.warning(\n\"'TorchModel.set_weights' received inputs that did not cover all\"\n\"trainable model parameters; missing weights: %s\",\nmissing\n)\n# Warn about invalid (hence, unused) inputs.\nif incompatible.unexpected_keys:\nlogger.warning(\n\"'TorchModel.set_weights' received inputs with unexpected names: %s\",\nincompatible.unexpected_keys\n)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._torch.TorchModel.train","title":"<pre><code>train(inputs, targets, kwargs)\n</code></pre>","text":"Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def train(\nself,\ninputs: torch.Tensor,\ntargets: torch.Tensor,\n**kwargs,\n) -&gt; None:\n# TODO: should we pass loss function here? and do the backward prop?\nif not self.init_params:\nraise FedbiomedModelError(\nf\"{ErrorNumbers.FB622.value}. Training has not been initialized, please initialize it beforehand\"\n)\n</code></pre>"},{"location":"developer/api/common/models/#fedbiomed.common.models._torch.TorchModel.unflatten","title":"<pre><code>unflatten(weights_vector)\n</code></pre>","text":"<p>Unflatten vectorized model weights using <code>vector_to_parameters</code></p> <p>This method does not manipulate current model weights modify model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>weights_vector</code> <code>List[float]</code> <p>Vectorized model weights to convert dict</p> required <p>Returns:</p> Type Description <code>Dict[str, torch.Tensor]</code> <p>Model dictionary</p> Source code in <code>fedbiomed/common/models/_torch.py</code> <pre><code>def unflatten(\nself,\nweights_vector: List[float]\n) -&gt; Dict[str, torch.Tensor]:\n\"\"\"Unflatten vectorized model weights using [`vector_to_parameters`][torch.nn.utils.vector_to_parameters]\n    This method does not manipulate current model weights modify model parameters.\n    Args:\n        weights_vector: Vectorized model weights to convert dict\n    Returns:\n        Model dictionary\n    \"\"\"\nsuper().unflatten(weights_vector)\n# Copy model to make sure global model parameters won't be overwritten\nmodel = copy.deepcopy(self.model)\nvector = torch.as_tensor(weights_vector).type(torch.DoubleTensor)\n# Following operation updates model parameters of copied model object\ntry:\ntorch.nn.utils.vector_to_parameters(vector, model.parameters())\nexcept TypeError as e:\nFedbiomedModelError(\nf\"{ErrorNumbers.FB622.value} Can not unflatten model parameters. {e}\"\n)\nreturn TorchModel(model).get_weights()\n</code></pre>"},{"location":"developer/api/common/optimizer/","title":"Optimizer","text":""},{"location":"developer/api/common/optimizer/#fedbiomed.common.optimizers","title":"fedbiomed.common.optimizers","text":"Module: <code>fedbiomed.common.optimizers</code>"},{"location":"developer/api/common/repository/","title":"Repository","text":""},{"location":"developer/api/common/repository/#fedbiomed.common.repository","title":"fedbiomed.common.repository","text":"Module: <code>fedbiomed.common.repository</code> <p>HTTP file repository from which to upload and download files.</p>"},{"location":"developer/api/common/repository/#fedbiomed.common.repository-classes","title":"Classes","text":""},{"location":"developer/api/common/repository/#fedbiomed.common.repository.Repository","title":"Repository","text":"CLASS  <pre><code>Repository(uploads_url, tmp_dir, cache_dir)\n</code></pre> <p>HTTP file repository from which to upload and download files.</p> <p>Files are uploaded from/downloaded to a temporary file (<code>tmp_dir</code>). Data uploaded should be:</p> <ul> <li>python code (*.py file) that describes model +     data handling/preprocessing</li> <li>model params (under *.pt format)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>uploads_url</code> <code>Union[Text, bytes]</code> <p>The URL where we upload files</p> required <code>tmp_dir</code> <code>str</code> <p>A directory for temporary files</p> required <code>cache_dir</code> <code>str</code> <p>Currently unused</p> required Source code in <code>fedbiomed/common/repository.py</code> <pre><code>def __init__(self,\nuploads_url: Union[Text, bytes],\ntmp_dir: str,\ncache_dir: str):\n\"\"\"Constructor of the class.\n    Args:\n        uploads_url: The URL where we upload files\n        tmp_dir: A directory for temporary files\n        cache_dir: Currently unused\n    \"\"\"\nself.uploads_url = uploads_url\nself.tmp_dir = tmp_dir\nself.cache_dir = cache_dir  # unused\n</code></pre>"},{"location":"developer/api/common/repository/#fedbiomed.common.repository.Repository-attributes","title":"Attributes","text":""},{"location":"developer/api/common/repository/#fedbiomed.common.repository.Repository.cache_dir","title":"cache_dir     <code>instance-attribute</code>","text":"<pre><code>cache_dir = cache_dir\n</code></pre>"},{"location":"developer/api/common/repository/#fedbiomed.common.repository.Repository.tmp_dir","title":"tmp_dir     <code>instance-attribute</code>","text":"<pre><code>tmp_dir = tmp_dir\n</code></pre>"},{"location":"developer/api/common/repository/#fedbiomed.common.repository.Repository.uploads_url","title":"uploads_url     <code>instance-attribute</code>","text":"<pre><code>uploads_url = uploads_url\n</code></pre>"},{"location":"developer/api/common/repository/#fedbiomed.common.repository.Repository-functions","title":"Functions","text":""},{"location":"developer/api/common/repository/#fedbiomed.common.repository.Repository.download_file","title":"<pre><code>download_file(url, filename)\n</code></pre>","text":"<p>Downloads a file from a HTTP file repository (through an HTTP GET request).</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>An url from which to download file</p> required <code>filename</code> <code>str</code> <p>The name of the temporary file</p> required <p>Returns:</p> Name Type Description <code>status</code> <code>int</code> <p>The HTTP status code</p> <code>filepath</code> <code>str</code> <p>The complete pathfile under which the temporary file is saved</p> Source code in <code>fedbiomed/common/repository.py</code> <pre><code>def download_file(self, url: str, filename: str) -&gt; Tuple[int, str]:\n\"\"\"Downloads a file from a HTTP file repository (through an HTTP GET request).\n    Args:\n        url: An url from which to download file\n        filename: The name of the temporary file\n    Returns:\n        status: The HTTP status code\n        filepath: The complete pathfile under which the temporary file is saved\n    \"\"\"\nres = self._request_handler(requests.get, url, filename)\nself._raise_for_status_handler(res, filename)\nfilepath = os.path.join(self.tmp_dir, filename)\ntry:\nopen(filepath, 'wb').write(res.content)\nexcept FileNotFoundError as err:\n_msg = ErrorNumbers.FB604.value + str(err) + ', cannot save the downloaded content into it'\nlogger.error(_msg)\nraise FedbiomedRepositoryError(_msg)\nexcept PermissionError:\n_msg = ErrorNumbers.FB604.value + f': Unable to read {filepath} due to unsatisfactory privileges'\n\", cannot write the downloaded content into it\"\nlogger.error(_msg)\nraise FedbiomedRepositoryError(_msg)\nexcept MemoryError:\n_msg = ErrorNumbers.FB604.value + f\" : cannot write on {filepath}: out of memory!\"\nlogger.error(_msg)\nraise FedbiomedRepositoryError(_msg)\nexcept OSError:\n_msg = ErrorNumbers.FB604.value + f': Cannot open file {filepath} after downloading'\nlogger.error(_msg)\nraise FedbiomedRepositoryError(_msg)\nreturn res.status_code, filepath\n</code></pre>"},{"location":"developer/api/common/repository/#fedbiomed.common.repository.Repository.upload_file","title":"<pre><code>upload_file(filename)\n</code></pre>","text":"<p>Uploads a file to an HTTP file repository (through an HTTP POST request).</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>A name/path of the file to upload.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The result of the request under JSON format.</p> <p>Raises:</p> Type Description <code>FedbiomedRepositoryError</code> <p>unable to read the file 'filename'</p> <code>FedbiomedRepositoryError</code> <p>POST HTTP request fails or returns an HTTP status 4xx (bad request) or 500 (internal server error)</p> <code>FedbiomedRepositoryError</code> <p>unable to deserialize JSON from the request</p> Source code in <code>fedbiomed/common/repository.py</code> <pre><code>def upload_file(self, filename: str) -&gt; Dict[str, Any]:\n\"\"\"Uploads a file to an HTTP file repository (through an HTTP POST request).\n    Args:\n        filename: A name/path of the file to upload.\n    Returns:\n        The result of the request under JSON format.\n    Raises:\n        FedbiomedRepositoryError: unable to read the file 'filename'\n        FedbiomedRepositoryError: POST HTTP request fails or returns\n            an HTTP status 4xx (bad request) or 500 (internal server error)\n        FedbiomedRepositoryError: unable to deserialize JSON from\n            the request\n    \"\"\"\n# first, we are trying to open the file `filename` and catch\n# any known exceptions related top `open` builtin function\ntry:\nfiles = {'file': open(filename, 'rb')}\nexcept FileNotFoundError:\n_msg = ErrorNumbers.FB604.value + f': File {filename} not found, cannot upload it'\nlogger.error(_msg)\nraise FedbiomedRepositoryError(_msg)\nexcept PermissionError:\n_msg = ErrorNumbers.FB604.value + f': Unable to read {filename} due to unsatisfactory privileges' + \\\n            \", cannot upload it\"\nlogger.error(_msg)\nraise FedbiomedRepositoryError(_msg)\nexcept OSError:\n_msg = ErrorNumbers.FB604.value + f': Cannot read file {filename} when uploading'\nlogger.error(_msg)\nraise FedbiomedRepositoryError(_msg)\n# second, we are issuing an HTTP 'POST' request to the HTTP server\n_res = self._request_handler(requests.post, self.uploads_url,\nfilename, files=files)\n# checking status of HTTP request\nself._raise_for_status_handler(_res, filename)\n# finally, we are deserializing message from JSON\ntry:\njson_res = _res.json()\nexcept JSONDecodeError:\n# might be triggered by `request` package when deserializing\n_msg = 'Unable to deserialize JSON from HTTP POST request (when uploading file)'\nlogger.error(_msg)\nraise FedbiomedRepositoryError(_msg)\nreturn json_res\n</code></pre>"},{"location":"developer/api/common/tasks_queue/","title":"TasksQueue","text":""},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue","title":"fedbiomed.common.tasks_queue","text":"Module: <code>fedbiomed.common.tasks_queue</code> <p>Queue module that contains task queue class that is a wrapper to the persistqueue python library.</p>"},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue-classes","title":"Classes","text":""},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue","title":"TasksQueue","text":"CLASS  <pre><code>TasksQueue(messages_queue_dir, tmp_dir)\n</code></pre> <p>A disk-persistent Queue object, ensuring queue will remain on disk even if program crashes.</p> <p>Relies on <code>persistqueue</code> package.</p> <p>Parameters:</p> Name Type Description Default <code>messages_queue_dir</code> <code>str</code> <p>directory where enqueued data should be persisted.</p> required <code>tmp_dir</code> <code>str</code> <p>indicates where temporary files should be stored.</p> required Source code in <code>fedbiomed/common/tasks_queue.py</code> <pre><code>def __init__(self, messages_queue_dir: str, tmp_dir: str):\n\"\"\"Construct disk-persistent Queue.\n    Args:\n        messages_queue_dir: directory where enqueued data should be persisted.\n        tmp_dir: indicates where temporary files should be stored.\n    \"\"\"\ntry:\nself.queue = persistqueue.Queue(messages_queue_dir, tempdir=tmp_dir)\nexcept ValueError as e:\nmsg = ErrorNumbers.FB603.value + \": cannot create queue (\" + str(e) + \")\"\nlogger.critical(msg)\nraise FedbiomedTaskQueueError(msg)\n</code></pre>"},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue-attributes","title":"Attributes","text":""},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue.queue","title":"queue     <code>instance-attribute</code>","text":"<pre><code>queue = persistqueue.Queue(messages_queue_dir, tempdir=tmp_dir)\n</code></pre>"},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue-functions","title":"Functions","text":""},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue.add","title":"<pre><code>add(task)\n</code></pre>","text":"<p>Adds a task to the queue</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>dict</code> <p>a dict describing the task to be added</p> required Source code in <code>fedbiomed/common/tasks_queue.py</code> <pre><code>def add(self, task: dict):\n\"\"\"Adds a task to the queue\n    Args:\n        task: a dict describing the task to be added\n    \"\"\"\ntry:\nself.queue.put(task)\nexcept persistqueue.exceptions.Full:\nmsg = ErrorNumbers.FB603.value + \": queue is full\"\nlogger.critical(msg)\nraise FedbiomedTaskQueueError(msg)\n</code></pre>"},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue.get","title":"<pre><code>get(block=True)\n</code></pre>","text":"<p>Get the current task in the queue</p> <p>Parameters:</p> Name Type Description Default <code>block</code> <code>Optional[bool]</code> <p>if True, block if necessary until an item is available. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary object stored in queue</p> <p>Raises:</p> Type Description <code>FedbiomedTaskQueueError</code> <p>If queue is empty</p> Source code in <code>fedbiomed/common/tasks_queue.py</code> <pre><code>def get(self, block: Optional[bool] = True) -&gt; dict:\n\"\"\"Get the current task in the queue\n    Args:\n        block: if True, block if necessary until an item is available. Defaults to True.\n    Returns:\n        Dictionary object stored in queue\n    Raises:\n        FedbiomedTaskQueueError: If queue is empty\n    \"\"\"\ntry:\nreturn self.queue.get(block)\nexcept persistqueue.exceptions.Empty:\nmsg = ErrorNumbers.FB603.value + \": queue is empty\"\nlogger.debug(msg)\nraise FedbiomedTaskQueueError(msg)\n</code></pre>"},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue.qsize","title":"<pre><code>qsize()\n</code></pre>","text":"<p>Retrieve the size of the queue</p> <p>Returns:</p> Type Description <code>int</code> <p>size of the queue</p> Source code in <code>fedbiomed/common/tasks_queue.py</code> <pre><code>def qsize(self) -&gt; int:\n\"\"\"Retrieve the size of the queue\n    Returns:\n        size of the queue\n    \"\"\"\nreturn self.queue.qsize()\n</code></pre>"},{"location":"developer/api/common/tasks_queue/#fedbiomed.common.tasks_queue.TasksQueue.task_done","title":"<pre><code>task_done()\n</code></pre>","text":"<p>Indicate whether a formerly enqueued task is complete</p> <p>Returns:</p> Type Description <code>Any</code> <p>True if task is complete</p> Source code in <code>fedbiomed/common/tasks_queue.py</code> <pre><code>def task_done(self) -&gt; Any:\n\"\"\"Indicate whether a formerly enqueued task is complete\n    Returns:\n        True if task is complete\n    \"\"\"\ntry:\nreturn self.queue.task_done()\nexcept ValueError:\n# persistqueue raises it if task_done called too many times we can ignore it\nreturn\n</code></pre>"},{"location":"developer/api/common/training_args/","title":"TrainingArgs","text":""},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args","title":"fedbiomed.common.training_args","text":"Module: <code>fedbiomed.common.training_args</code> <p>Provide a way to easily to manage training arguments.</p>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args-attributes","title":"Attributes","text":""},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.DPArgsValidator","title":"DPArgsValidator     <code>module-attribute</code>","text":"<pre><code>DPArgsValidator = SchemeValidator({'type': {'rules': [str, _validate_dp_type], 'required': True, 'default': 'central'}, 'sigma': {'rules': [float], 'required': True}, 'clip': {'rules': [float], 'required': True}})\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args-classes","title":"Classes","text":""},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs","title":"TrainingArgs","text":"CLASS  <pre><code>TrainingArgs(ta=None, extra_scheme=None, only_required=True)\n</code></pre> <p>Provide a container to manage training arguments.</p> <p>This class uses the Validator and SchemeValidator classes and provides a default scheme, which describes the arguments necessary to train/validate a TrainingPlan.</p> <p>It also permits to extend the TrainingArgs then testing new features by supplying an extra_scheme at TrainingArgs instantiation.</p> <p>Parameters:</p> Name Type Description Default <code>ta</code> <code>Dict</code> <p>dictionary describing the TrainingArgs scheme.     if empty dict or None, a minimal instance of TrainingArgs     will be initialized with default values for required keys</p> <code>None</code> <code>extra_scheme</code> <code>Dict</code> <p>user provided scheme extension, which add new rules or     update the scheme of the default training args.     Warning: this is a dangerous feature, provided to     developers, to ease the test of future Fed-Biomed features</p> <code>None</code> <code>only_required</code> <code>bool</code> <p>if True, the object is initialized only with required     values defined in the default_scheme (+ extra_scheme).     If False, then all default values will also be returned     (not only the required key/value pairs).</p> <code>True</code> <p>Raises:</p> Type Description <code>FedbiomedUserInputError</code> <p>in case of bad value or bad extra_scheme</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def __init__(self, ta: Dict = None, extra_scheme: Dict = None, only_required: bool = True):\n\"\"\"\n    Create a TrainingArgs from a Dict with input validation.\n    Args:\n        ta:     dictionary describing the TrainingArgs scheme.\n                if empty dict or None, a minimal instance of TrainingArgs\n                will be initialized with default values for required keys\n        extra_scheme: user provided scheme extension, which add new rules or\n                update the scheme of the default training args.\n                Warning: this is a dangerous feature, provided to\n                developers, to ease the test of future Fed-Biomed features\n        only_required: if True, the object is initialized only with required\n                values defined in the default_scheme (+ extra_scheme).\n                If False, then all default values will also be returned\n                (not only the required key/value pairs).\n    Raises:\n        FedbiomedUserInputError: in case of bad value or bad extra_scheme\n    \"\"\"\nself._scheme = TrainingArgs.default_scheme()\nif not isinstance(extra_scheme, dict):\nextra_scheme = {}\nfor k in extra_scheme:\nself._scheme[k] = extra_scheme[k]\ntry:\nself._sc = SchemeValidator(self._scheme)\nexcept RuleError as e:\n#\n# internal error (invalid scheme)\nmsg = ErrorNumbers.FB414.value + f\": {e}\"\nlogger.critical(msg)\nraise FedbiomedUserInputError(msg)\n# scheme is validated from here\nif ta is None:\nta = {}\ntry:\nself._ta = self._sc.populate_with_defaults(ta, only_required=only_required)\nexcept ValidatorError as e:\n# scheme has required keys without defined default value\nmsg = ErrorNumbers.FB414.value + f\": {e}\"\nlogger.critical(msg)\nraise FedbiomedUserInputError(msg)\ntry:\nself._sc.validate(self._ta)\nexcept ValidateError as e:\n# transform to a Fed-BioMed error\nmsg = ErrorNumbers.FB414.value + f\": {e}\"\nlogger.critical(msg)\nraise FedbiomedUserInputError(msg)\n# Validate DP arguments if it is existing in training arguments\nif self._ta[\"dp_args\"] is not None:\ntry:\nself._ta[\"dp_args\"] = DPArgsValidator.populate_with_defaults(self._ta[\"dp_args\"], only_required=False)\nDPArgsValidator.validate(self._ta[\"dp_args\"])\nexcept ValidateError as e:\nmsg = f\"{ErrorNumbers.FB414.value}: {e}\"\nlogger.critical(msg)\nraise FedbiomedUserInputError(msg)\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs-functions","title":"Functions","text":""},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.default_scheme","title":"<pre><code>default_scheme()\n</code></pre>  <code>classmethod</code>","text":"<p>Returns the default (base) scheme for TrainingArgs.</p> <p>A summary of the semantics of each argument is given below. Please refer to the source code of this function for additional information on typing and constraints.</p> argument meaning optimizer_args supplemental arguments for initializing the optimizer batch_size the number of samples in a batch epochs the number of epochs performed during local training on each node num_updates the number of model updates performed during local training on each node. Supersedes epochs if both are specified use_gpu toggle requesting the use of GPUs for local training on the node when available dry_run perform a single model update for testing on each node and correctly handle GPU execution batch_maxnum prematurely break after batch_maxnum model updates for each epoch (useful for testing) test_ratio the proportion of validation samples to total number of samples in the dataset test_on_local_updates toggles validation after local training test_on_global_updates toggles validation before local training test_metric metric to be used for validation test_metric_args supplemental arguments for the validation metric log_interval output a training logging entry every log_interval model updates fedprox_mu set the value of mu and enable FedProx correction dp_args arguments for Differential Privacy share_persistent_buffers toggle whether nodes share the full state_dict (when True) or only trainable parameters (False) in a TorchTrainingPlan Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>@classmethod\ndef default_scheme(cls) -&gt; Dict:\n\"\"\"\n    Returns the default (base) scheme for TrainingArgs.\n    A summary of the semantics of each argument is given below. Please refer to the source code of this function\n    for additional information on typing and constraints.\n    | argument | meaning |\n    | -------- | ------- |\n    | optimizer_args | supplemental arguments for initializing the optimizer |\n    | batch_size | the number of samples in a batch |\n    | epochs | the number of epochs performed during local training on each node |\n    | num_updates | the number of model updates performed during local training on each node. Supersedes epochs if both are specified |\n    | use_gpu | toggle requesting the use of GPUs for local training on the node when available |\n    | dry_run | perform a single model update for testing on each node and correctly handle GPU execution |\n    | batch_maxnum | prematurely break after batch_maxnum model updates for each epoch (useful for testing) |\n    | test_ratio | the proportion of validation samples to total number of samples in the dataset |\n    | test_on_local_updates | toggles validation after local training |\n    | test_on_global_updates | toggles validation before local training |\n    | test_metric | metric to be used for validation |\n    | test_metric_args | supplemental arguments for the validation metric |\n    | log_interval | output a training logging entry every log_interval model updates |\n    | fedprox_mu | set the value of mu and enable FedProx correction |\n    | dp_args | arguments for Differential Privacy |\n    | share_persistent_buffers | toggle whether nodes share the full state_dict (when True) or only trainable parameters (False) in a TorchTrainingPlan |\n    \"\"\"\nreturn {\n\"optimizer_args\": {\n\"rules\": [dict], \"required\": True, \"default\": {}\n},\n\"batch_size\": {\n\"rules\": [int], \"required\": True, \"default\": 1\n},\n\"epochs\": {\n\"rules\": [cls._nonnegative_integer_value_validator_hook('epochs')], \"required\": True, \"default\": None\n},\n\"num_updates\": {\n\"rules\": [cls._nonnegative_integer_value_validator_hook('num_updates')],\n\"required\": True, \"default\": None\n},\n\"dry_run\": {\n\"rules\": [bool], \"required\": True, \"default\": False\n},\n\"batch_maxnum\": {\n\"rules\": [cls._nonnegative_integer_value_validator_hook('batch_maxnum')],\n\"required\": True, \"default\": None\n},\n\"test_ratio\": {\n\"rules\": [float, cls._test_ratio_hook], \"required\": False, \"default\": 0.0\n},\n\"test_on_local_updates\": {\n\"rules\": [bool], \"required\": False, \"default\": False\n},\n\"test_on_global_updates\": {\n\"rules\": [bool], \"required\": False, \"default\": False\n},\n\"test_metric\": {\n\"rules\": [cls._metric_validation_hook], \"required\": False, \"default\": None\n},\n\"test_metric_args\": {\n\"rules\": [dict], \"required\": False, \"default\": {}\n},\n\"log_interval\": {\n\"rules\": [int], \"required\": False, \"default\": 10\n},\n\"fedprox_mu\": {\n\"rules\": [cls._fedprox_mu_validator], 'required': False, \"default\": None\n},\n\"use_gpu\": {\n\"rules\": [bool], 'required': False, \"default\": False\n},\n\"dp_args\": {\n\"rules\": [cls._validate_dp_args], \"required\": True, \"default\": None\n},\n\"share_persistent_buffers\": {\n\"rules\": [bool], \"required\": False, \"default\": True\n}\n}\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.default_value","title":"<pre><code>default_value(key)\n</code></pre>","text":"<p>Returns the default value for the key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key</p> required <p>Returns:</p> Name Type Description <code>value</code> <code>Any</code> <p>the default value associated to the key</p> <p>Raises:</p> Type Description <code>FedbiomedUserInputError</code> <p>in case of problem (invalid key or value)</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def default_value(self, key: str) -&gt; Any:\n\"\"\"\n    Returns the default value for the key.\n    Args:\n        key:  key\n    Returns:\n        value: the default value associated to the key\n    Raises:\n        FedbiomedUserInputError: in case of problem (invalid key or value)\n    \"\"\"\nif key in self._sc.scheme():\nif \"default\" in self._sc.scheme()[key]:\nreturn deepcopy(self._sc.scheme()[key][\"default\"])\nelse:\nmsg = ErrorNumbers.FB410.value + \\\n                  f\"no default value defined for key: {key}\"\nlogger.critical(msg)\nraise FedbiomedUserInputError(msg)\nelse:\nmsg = ErrorNumbers.FB410.value + \\\n              f\"no such key: {key}\"\nlogger.critical(msg)\nraise FedbiomedUserInputError(msg)\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.dict","title":"<pre><code>dict()\n</code></pre>","text":"<p>Returns a copy of the training_args as a dictionary.</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def dict(self):\n\"\"\"Returns a copy of the training_args as a dictionary.\"\"\"\nta = deepcopy(self._ta)\nif 'test_metric' in ta and \\\n            isinstance(ta['test_metric'], MetricTypes):\n# replace MetricType value by a string\nta['test_metric'] = ta['test_metric'].name\nreturn ta\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.dp_arguments","title":"<pre><code>dp_arguments()\n</code></pre>","text":"<p>Extracts the arguments for differential privacy</p> <p>Returns:</p> Type Description <p>Contains differential privacy arguments</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def dp_arguments(self):\n\"\"\"Extracts the arguments for differential privacy\n    Returns:\n        Contains differential privacy arguments\n    \"\"\"\nreturn self[\"dp_args\"]\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.get","title":"<pre><code>get(key, default=None)\n</code></pre>","text":"<p>Mimics the get() method of dict, provided for backward compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>a key for retrieving data fro the dictionary</p> required <code>default</code> <code>Any</code> <p>default value to return if key does not belong to dictionary</p> <code>None</code> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def get(self, key: str, default: Any = None) -&gt; Any:\n\"\"\"Mimics the get() method of dict, provided for backward compatibility.\n    Args:\n        key: a key for retrieving data fro the dictionary\n        default: default value to return if key does not belong to dictionary\n    \"\"\"\ntry:\nreturn deepcopy(self._ta[key])\nexcept KeyError:\n# TODO: test if provided defualt value is compliant with the scheme\nreturn default\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.loader_arguments","title":"<pre><code>loader_arguments()\n</code></pre>","text":"<p>Extracts data loader arguments</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Contains loader arguments for PyTorch dataloader</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def loader_arguments(self) -&gt; Dict:\n\"\"\" Extracts data loader arguments\n    Returns:\n        Contains loader arguments for PyTorch dataloader\n    \"\"\"\nkeys = [\"batch_size\"]\nreturn self._extract_args(keys)\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.optimizer_arguments","title":"<pre><code>optimizer_arguments()\n</code></pre>","text":"Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def optimizer_arguments(self) -&gt; Dict:\nreturn self[\"optimizer_args\"]\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.pure_training_arguments","title":"<pre><code>pure_training_arguments()\n</code></pre>","text":"<p>Extracts the arguments that are only necessary for training_routine</p> <p>Returns:</p> Type Description <p>Contains training argument for training routine</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def pure_training_arguments(self):\n\"\"\" Extracts the arguments that are only necessary for training_routine\n    Returns:\n        Contains training argument for training routine\n    \"\"\"\nkeys = [\"batch_maxnum\",\n\"fedprox_mu\",\n\"log_interval\",\n\"dry_run\",\n\"epochs\",\n\"use_gpu\",\n\"num_updates\",\n\"batch_size\"]\nreturn self._extract_args(keys)\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.scheme","title":"<pre><code>scheme()\n</code></pre>","text":"<p>Returns the scheme of a TrainingArgs instance.</p> <p>The scheme is not necessarily the default_scheme (returned by TrainingArgs.default_scheme().</p> <p>Returns:</p> Name Type Description <code>scheme</code> <code>Dict</code> <p>the current scheme used for validation</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def scheme(self) -&gt; Dict:\n\"\"\"\n    Returns the scheme of a TrainingArgs instance.\n    The scheme is not necessarily the default_scheme (returned by TrainingArgs.default_scheme().\n    Returns:\n        scheme:  the current scheme used for validation\n    \"\"\"\nreturn deepcopy(self._scheme)\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.testing_arguments","title":"<pre><code>testing_arguments()\n</code></pre>","text":"<p>Extract testing arguments from training arguments</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Testing arguments as dictionary</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def testing_arguments(self) -&gt; Dict:\n\"\"\" Extract testing arguments from training arguments\n    Returns:\n        Testing arguments as dictionary\n    \"\"\"\nkeys = ['test_ratio', 'test_on_local_updates', 'test_on_global_updates',\n'test_metric', 'test_metric_args']\nreturn self._extract_args(keys)\n</code></pre>"},{"location":"developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.update","title":"<pre><code>update(values)\n</code></pre>","text":"<p>Update multiple keys of the training arguments.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict</code> <p>a dictionnary of (key, value) to validate/update</p> required <p>Returns:</p> Type Description <code>TypeVar(TrainingArgs)</code> <p>the object itself after modification</p> <p>Raises:</p> Type Description <code>FedbiomedUserInputError</code> <p>in case of bad key or value in values</p> Source code in <code>fedbiomed/common/training_args.py</code> <pre><code>def update(self, values: Dict) -&gt; TypeVar(\"TrainingArgs\"):\n\"\"\"\n    Update multiple keys of the training arguments.\n    Args:\n        values:  a dictionnary of (key, value) to validate/update\n    Returns:\n        the object itself after modification\n    Raises:\n        FedbiomedUserInputError: in case of bad key or value in values\n    \"\"\"\nfor k in values:\nself.__setitem__(k, values[k])\nreturn self\n</code></pre>"},{"location":"developer/api/common/training_plans/","title":"TrainingPlans","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans","title":"fedbiomed.common.training_plans","text":"Module: <code>fedbiomed.common.training_plans</code> <p>The <code>fedbiomed.common.training_plans</code> module includes training plan classes that are used for federated training</p>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans-classes","title":"Classes","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan","title":"BaseTrainingPlan","text":"CLASS  <pre><code>BaseTrainingPlan()\n</code></pre> <p>Base class for training plan</p> <p>All concrete, framework- and/or model-specific training plans should inherit from this class, and implement:     * the <code>post_init</code> method:         to process model and training hyper-parameters     * the <code>training_routine</code> method:         to train the model for one round     * the <code>predict</code> method:         to compute predictions over a given batch     * (opt.) the <code>testing_step</code> method:         to override the evaluation behavior and compute         a batch-wise (set of) metric(s)</p> <p>Attributes:</p> Name Type Description <code>dataset_path</code> <code>Union[str, None]</code> <p>The path that indicates where dataset has been stored</p> <code>pre_processes</code> <code>Dict[str, PreProcessDict]</code> <p>Preprocess functions that will be applied to the training data at the beginning of the training routine.</p> <code>training_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader used in the training routine.</p> <code>testing_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader used in the validation routine.</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def __init__(self) -&gt; None:\n\"\"\"Construct the base training plan.\"\"\"\nself._dependencies: List[str] = []\nself.dataset_path: Union[str, None] = None\nself.pre_processes: Dict[str, PreProcessDict] = OrderedDict()\nself.training_data_loader: Union[DataLoader, NPDataLoader, None] = None\nself.testing_data_loader: Union[DataLoader, NPDataLoader, None] = None\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan-attributes","title":"Attributes","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.dataset_path","title":"dataset_path     <code>instance-attribute</code>","text":"<pre><code>dataset_path: Union[str, None] = None\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.pre_processes","title":"pre_processes     <code>instance-attribute</code>","text":"<pre><code>pre_processes: Dict[str, PreProcessDict] = OrderedDict()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.testing_data_loader","title":"testing_data_loader     <code>instance-attribute</code>","text":"<pre><code>testing_data_loader: Union[DataLoader, NPDataLoader, None] = None\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.training_data_loader","title":"training_data_loader     <code>instance-attribute</code>","text":"<pre><code>training_data_loader: Union[DataLoader, NPDataLoader, None] = None\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.BaseTrainingPlan-functions","title":"Functions","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.add_dependency","title":"<pre><code>add_dependency(dep)\n</code></pre>","text":"<p>Add new dependencies to the TrainingPlan.</p> <p>These dependencies are used while creating a python module.</p> <p>Parameters:</p> Name Type Description Default <code>dep</code> <code>List[str]</code> <p>Dependencies to add. Dependencies should be indicated as import statement strings, e.g. <code>\"from torch import nn\"</code>.</p> required Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def add_dependency(self, dep: List[str]) -&gt; None:\n\"\"\"Add new dependencies to the TrainingPlan.\n    These dependencies are used while creating a python module.\n    Args:\n        dep: Dependencies to add. Dependencies should be indicated as\n            import statement strings, e.g. `\"from torch import nn\"`.\n    \"\"\"\nfor val in dep:\nif val not in self._dependencies:\nself._dependencies.append(val)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.add_preprocess","title":"<pre><code>add_preprocess(method, process_type)\n</code></pre>","text":"<p>Register a pre-processing method to be executed on training data.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>Pre-processing method to be run before training.</p> required <code>process_type</code> <code>ProcessTypes</code> <p>Type of pre-processing that will be run. The expected signature of <code>method</code> and the arguments passed to it depend on this parameter.</p> required Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def add_preprocess(\nself,\nmethod: Callable,\nprocess_type: ProcessTypes\n) -&gt; None:\n\"\"\"Register a pre-processing method to be executed on training data.\n    Args:\n        method: Pre-processing method to be run before training.\n        process_type: Type of pre-processing that will be run.\n            The expected signature of `method` and the arguments\n            passed to it depend on this parameter.\n    \"\"\"\nif not callable(method):\nmsg = (\nf\"{ErrorNumbers.FB605.value}: error while adding \"\n\"preprocess, `method` should be callable.\"\n)\nlogger.critical(msg)\nraise FedbiomedTrainingPlanError(msg)\nif not isinstance(process_type, ProcessTypes):\nmsg = (\nf\"{ErrorNumbers.FB605.value}: error while adding \"\n\"preprocess, `process_type` should be an instance \"\n\"of `fedbiomed.common.constants.ProcessTypes`.\"\n)\nlogger.critical(msg)\nraise FedbiomedTrainingPlanError(msg)\n# NOTE: this may be revised into a list rather than OrderedDict\nself.pre_processes[method.__name__] = {\n'method': method,\n'process_type': process_type\n}\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.after_training_params","title":"<pre><code>after_training_params(flatten=False)\n</code></pre>","text":"<p>Return the wrapped model's parameters for aggregation.</p> <p>This method returns a dict containing parameters that need to be reported back and aggregated in a federated learning setting.</p> <p>It may also implement post-processing steps to make these parameters suitable for sharing with the researcher after training - hence its being used over <code>get_model_params</code> at the end of training rounds.</p> <p>Returns:</p> Type Description <code>Union[Dict[str, Any], List[float]]</code> <p>The trained parameters to aggregate.</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def after_training_params(\nself,\nflatten: bool = False\n) -&gt; Union[Dict[str, Any], List[float]]:\n\"\"\"Return the wrapped model's parameters for aggregation.\n    This method returns a dict containing parameters that need to be\n    reported back and aggregated in a federated learning setting.\n    It may also implement post-processing steps to make these parameters\n    suitable for sharing with the researcher after training - hence its\n    being used over `get_model_params` at the end of training rounds.\n    Returns:\n        The trained parameters to aggregate.\n    \"\"\"\n# Get flatten model parameters\nif flatten:\nreturn self._model.flatten()\nreturn self.get_model_params()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.export_model","title":"<pre><code>export_model(filename)\n</code></pre>","text":"<p>Export the wrapped model to a dump file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to the file where the model will be saved.</p> required <p>!!! info \"Notes\":     This method is designed to save the model to a local dump     file for easy re-use by the same user, possibly outside of     Fed-BioMed. It is not designed to produce trustworthy data     dumps and is not used to exchange models and their weights     as part of the federated learning process.</p> <pre><code>To save the model parameters for sharing as part of the FL process,\nuse the `after_training_params` method (or `get_model_params` one\noutside of a training context) and export results using\n[`Serializer`][fedbiomed.common.serializer.Serializer].\n</code></pre> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def export_model(self, filename: str) -&gt; None:\n\"\"\"Export the wrapped model to a dump file.\n    Args:\n        filename: path to the file where the model will be saved.\n    !!! info \"Notes\":\n        This method is designed to save the model to a local dump\n        file for easy re-use by the same user, possibly outside of\n        Fed-BioMed. It is not designed to produce trustworthy data\n        dumps and is not used to exchange models and their weights\n        as part of the federated learning process.\n        To save the model parameters for sharing as part of the FL process,\n        use the `after_training_params` method (or `get_model_params` one\n        outside of a training context) and export results using\n        [`Serializer`][fedbiomed.common.serializer.Serializer].\n    \"\"\"\nself._model.export(filename)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.get_model_params","title":"<pre><code>get_model_params()\n</code></pre>","text":"<p>Return a copy of the model's trainable weights.</p> <p>The type of data structure used to store weights depends on the actual framework of the wrapped model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Model weights, as a dictionary mapping parameters' names to their value.</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n\"\"\"Return a copy of the model's trainable weights.\n    The type of data structure used to store weights depends on the actual\n    framework of the wrapped model.\n    Returns:\n        Model weights, as a dictionary mapping parameters' names to their value.\n    \"\"\"\nreturn self._model.get_weights()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.import_model","title":"<pre><code>import_model(filename)\n</code></pre>","text":"<p>Import and replace the wrapped model from a dump file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to the file where the model has been exported.</p> required <p>!!! info \"Notes\":     This method is designed to load the model from a local dump     file, that might not be in a trustworthy format. It should     therefore only be used to re-load data exported locally and     not received from someone else, including other FL peers.</p> <pre><code>To load model parameters shared as part of the FL process, use the\n[`Serializer`][fedbiomed.common.serializer.Serializer] to read the\nnetwork-exchanged file, and the `set_model_params` method to assign\nthe loaded values into the wrapped model.\n</code></pre> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def import_model(self, filename: str) -&gt; None:\n\"\"\"Import and replace the wrapped model from a dump file.\n    Args:\n        filename: path to the file where the model has been exported.\n    !!! info \"Notes\":\n        This method is designed to load the model from a local dump\n        file, that might not be in a trustworthy format. It should\n        therefore only be used to re-load data exported locally and\n        not received from someone else, including other FL peers.\n        To load model parameters shared as part of the FL process, use the\n        [`Serializer`][fedbiomed.common.serializer.Serializer] to read the\n        network-exchanged file, and the `set_model_params` method to assign\n        the loaded values into the wrapped model.\n    \"\"\"\ntry:\nself._model.reload(filename)\nexcept FedbiomedModelError as exc:\nmsg = (\nf\"{ErrorNumbers.FB304.value}: failed to import a model from \"\nf\"a dump file: {exc}\"\n)\nlogger.critical(msg)\nraise FedbiomedTrainingPlanError(msg) from exc\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.init_dependencies","title":"<pre><code>init_dependencies()\n</code></pre>","text":"<p>Default method where dependencies are returned</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>Empty list as default</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def init_dependencies(self) -&gt; List[str]:\n\"\"\"Default method where dependencies are returned\n    Returns:\n        Empty list as default\n    \"\"\"\nreturn []\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.init_optimizer","title":"<pre><code>init_optimizer()\n</code></pre>  <code>abstractmethod</code>","text":"<p>Method for declaring optimizer by default</p> <p>Returns:</p> Type Description <code>Any</code> <p>either framework specific optimizer (or None) or </p> <code>Any</code> <p>FedBiomed [<code>Optimizers</code>][<code>fedbiomed.common.optimizers.Optimizer</code>]</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>@abstractmethod\ndef init_optimizer(self) -&gt; Any:\n\"\"\"Method for declaring optimizer by default\n    Returns:\n        either framework specific optimizer (or None) or \n        FedBiomed [`Optimizers`][`fedbiomed.common.optimizers.Optimizer`]\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.model","title":"<pre><code>model()\n</code></pre>  <code>abstractmethod</code>","text":"<p>Gets model instance of the training plan</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>@abstractmethod\ndef model(self):\n\"\"\"Gets model instance of the training plan\"\"\"\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.optimizer_args","title":"<pre><code>optimizer_args()\n</code></pre>","text":"<p>Retrieves optimizer arguments (to be overridden by children classes)</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Empty dictionary: (to be overridden in children classes)</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def optimizer_args(self) -&gt; Dict:\n\"\"\"Retrieves optimizer arguments (to be overridden\n    by children classes)\n    Returns:\n        Empty dictionary: (to be overridden in children classes)\n    \"\"\"\nreturn {}\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.post_init","title":"<pre><code>post_init(model_args, training_args, aggregator_args=None)\n</code></pre>  <code>abstractmethod</code>","text":"<p>Process model, training and optimizer arguments.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict[str, Any]</code> <p>Arguments defined to instantiate the wrapped model.</p> required <code>training_args</code> <code>Dict[str, Any]</code> <p>Arguments that are used in training routines such as epoch, dry_run etc. Please see <code>TrainingArgs</code></p> required <code>aggregator_args</code> <code>Optional[Dict[str, Any]]</code> <p>Arguments managed by and shared with the researcher-side aggregator.</p> <code>None</code> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>@abstractmethod\ndef post_init(\nself,\nmodel_args: Dict[str, Any],\ntraining_args: Dict[str, Any],\naggregator_args: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n\"\"\"Process model, training and optimizer arguments.\n    Args:\n        model_args: Arguments defined to instantiate the wrapped model.\n        training_args: Arguments that are used in training routines\n            such as epoch, dry_run etc.\n            Please see [`TrainingArgs`][fedbiomed.common.training_args.TrainingArgs]\n        aggregator_args: Arguments managed by and shared with the\n            researcher-side aggregator.\n    \"\"\"\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.save_code","title":"<pre><code>save_code(filepath)\n</code></pre>","text":"<p>Saves the class source/codes of the training plan class that is created byuser.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>path to the destination file</p> required <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanError</code> <p>raised when source of the model class cannot be assessed</p> <code>FedbiomedTrainingPlanError</code> <p>raised when model file cannot be created/opened/edited</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def save_code(self, filepath: str) -&gt; None:\n\"\"\"Saves the class source/codes of the training plan class that is created byuser.\n    Args:\n        filepath: path to the destination file\n    Raises:\n        FedbiomedTrainingPlanError: raised when source of the model class cannot be assessed\n        FedbiomedTrainingPlanError: raised when model file cannot be created/opened/edited\n    \"\"\"\ntry:\nclass_source = get_class_source(self.__class__)\nexcept FedbiomedError as exc:\nmsg = f\"{ErrorNumbers.FB605.value}: error while getting source of the model class: {exc}\"\nlogger.critical(msg)\nraise FedbiomedTrainingPlanError(msg) from exc\n# Preparing content of the module\ncontent = \"\\n\".join(self._dependencies)\ncontent += \"\\n\"\ncontent += class_source\ntry:\n# should we write it in binary (for the sake of space optimization)?\nwith open(filepath, \"w\", encoding=\"utf-8\") as file:\nfile.write(content)\nlogger.debug(\"Model file has been saved: \" + filepath)\nexcept PermissionError as exc:\n_msg = ErrorNumbers.FB605.value + f\" : Unable to read {filepath} due to unsatisfactory privileges\" + \\\n               \", can't write the model content into it\"\nlogger.critical(_msg)\nraise FedbiomedTrainingPlanError(_msg) from exc\nexcept MemoryError as exc:\n_msg = ErrorNumbers.FB605.value + f\" : Can't write model file on {filepath}: out of memory!\"\nlogger.critical(_msg)\nraise FedbiomedTrainingPlanError(_msg) from exc\nexcept OSError as exc:\n_msg = ErrorNumbers.FB605.value + f\" : Can't open file {filepath} to write model content\"\nlogger.critical(_msg)\nraise FedbiomedTrainingPlanError(_msg) from exc\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.set_aggregator_args","title":"<pre><code>set_aggregator_args(aggregator_args)\n</code></pre>","text":"Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def set_aggregator_args(self, aggregator_args: Dict[str, Any]):\nraise FedbiomedTrainingPlanError(\"method not implemented and needed\")\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.set_data_loaders","title":"<pre><code>set_data_loaders(train_data_loader, test_data_loader)\n</code></pre>","text":"<p>Sets data loaders</p> <p>Parameters:</p> Name Type Description Default <code>train_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader for training routine/loop</p> required <code>test_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader for validation routine</p> required Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def set_data_loaders(\nself,\ntrain_data_loader: Union[DataLoader, NPDataLoader, None],\ntest_data_loader: Union[DataLoader, NPDataLoader, None]\n) -&gt; None:\n\"\"\"Sets data loaders\n    Args:\n        train_data_loader: Data loader for training routine/loop\n        test_data_loader: Data loader for validation routine\n    \"\"\"\nself.training_data_loader = train_data_loader\nself.testing_data_loader = test_data_loader\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.set_dataset_path","title":"<pre><code>set_dataset_path(dataset_path)\n</code></pre>","text":"<p>Dataset path setter for TrainingPlan</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path where data is saved on the node. This method is called by the node that executes the training.</p> required Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def set_dataset_path(self, dataset_path: str) -&gt; None:\n\"\"\"Dataset path setter for TrainingPlan\n    Args:\n        dataset_path: The path where data is saved on the node.\n            This method is called by the node that executes the training.\n    \"\"\"\nself.dataset_path = dataset_path\nlogger.debug(f\"Dataset path has been set as {self.dataset_path}\")\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.set_model_params","title":"<pre><code>set_model_params(params)\n</code></pre>","text":"<p>Assign new values to the model's trainable parameters.</p> <p>The type of data structure used to store weights depends on the actual framework of the wrapped model.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict[str, Any]</code> <p>model weights, as a dictionary mapping parameters' names to their value.</p> required Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def set_model_params(self, params: Dict[str, Any]) -&gt; None:\n\"\"\"Assign new values to the model's trainable parameters.\n    The type of data structure used to store weights depends on the actual\n    framework of the wrapped model.\n    Args:\n        params: model weights, as a dictionary mapping parameters' names\n            to their value.\n    \"\"\"\nreturn self._model.set_weights(params)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.testing_routine","title":"<pre><code>testing_routine(metric, metric_args, history_monitor, before_train)\n</code></pre>","text":"<p>Evaluation routine, to be called once per round.</p> <p>Note</p> <p>If the training plan implements a <code>testing_step</code> method (the signature of which is func(data, target) -&gt; metrics) then it will be used rather than the input metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Optional[MetricTypes]</code> <p>The metric used for validation. If None, use MetricTypes.ACCURACY.</p> required <code>history_monitor</code> <code>Optional[HistoryMonitor]</code> <p>HistoryMonitor instance, used to record computed metrics and communicate them to the researcher (server).</p> required <code>before_train</code> <code>bool</code> <p>Whether the evaluation is being performed before local training occurs, of afterwards. This is merely reported back through <code>history_monitor</code>.</p> required Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def testing_routine(\nself,\nmetric: Optional[MetricTypes],\nmetric_args: Dict[str, Any],\nhistory_monitor: Optional['HistoryMonitor'],\nbefore_train: bool\n) -&gt; None:\n\"\"\"Evaluation routine, to be called once per round.\n    !!! info \"Note\"\n        If the training plan implements a `testing_step` method\n        (the signature of which is func(data, target) -&gt; metrics)\n        then it will be used rather than the input metric.\n    Args:\n        metric: The metric used for validation.\n            If None, use MetricTypes.ACCURACY.\n        history_monitor: HistoryMonitor instance,\n            used to record computed metrics and communicate them to\n            the researcher (server).\n        before_train: Whether the evaluation is being performed\n            before local training occurs, of afterwards. This is merely\n            reported back through `history_monitor`.\n    \"\"\"\n# TODO: Add preprocess option for testing_data_loader.\nif self.testing_data_loader is None:\nmsg = f\"{ErrorNumbers.FB605.value}: no validation dataset was set.\"\nlogger.critical(msg)\nraise FedbiomedTrainingPlanError(msg)\nn_batches = len(self.testing_data_loader)\nn_samples = len(self.testing_data_loader.dataset)\n# Set up a batch-wise metrics-computation function.\n# Either use an optionally-implemented custom training routine.\nif hasattr(self, \"testing_step\"):\nevaluate = getattr(self, \"testing_step\")\nmetric_name = \"Custom\"\n# Or use the provided `metric` (or its default value).\nelse:\nif metric is None:\nmetric = MetricTypes.ACCURACY\nmetric_controller = Metrics()\ndef evaluate(data, target):\nnonlocal metric, metric_args, metric_controller\noutput = self._model.predict(data)\nif isinstance(target, torch.Tensor):\ntarget = target.numpy()\nreturn metric_controller.evaluate(\ntarget, output, metric=metric, **metric_args\n)\nmetric_name = metric.name\n# Iterate over the validation dataset and run the defined routine.\nnum_samples_observed_till_now: int = 0\nfor idx, (data, target) in enumerate(self.testing_data_loader, 1):\nnum_samples_observed_till_now += self._infer_batch_size(data)\n# Run the evaluation step; catch and raise exceptions.\ntry:\nm_value = evaluate(data, target)\nexcept Exception as exc:\nmsg = (\nf\"{ErrorNumbers.FB605.value}: An error occurred \"\nf\"while computing the {metric_name} metric: {exc}\"\n)\nlogger.critical(msg)\nraise FedbiomedTrainingPlanError(msg) from exc\n# Log the computed value.\nlogger.debug(\nf\"Validation: Batch {idx}/{n_batches} \"\nf\"| Samples {num_samples_observed_till_now}/{n_samples} \"\nf\"| Metric[{metric_name}]: {m_value}\"\n)\n# Further parse, and report it (provided a monitor is set).\nif history_monitor is not None:\nm_dict = self._create_metric_result_dict(m_value, metric_name)\nhistory_monitor.add_scalar(\nmetric=m_dict,\niteration=idx,\nepoch=None,\ntest=True,\ntest_on_local_updates=(not before_train),\ntest_on_global_updates=before_train,\ntotal_samples=n_samples,\nbatch_samples=num_samples_observed_till_now,\nnum_batches=n_batches\n)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.training_data","title":"<pre><code>training_data()\n</code></pre>","text":"<p>All subclasses must provide a training_data routine the purpose of this actual code is to detect that it has been provided</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanError</code> <p>if called and not inherited</p> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>def training_data(self):\n\"\"\"All subclasses must provide a training_data routine the purpose of this actual code is to detect\n    that it has been provided\n    Raises:\n        FedbiomedTrainingPlanError: if called and not inherited\n    \"\"\"\nmsg = f\"{ErrorNumbers.FB303.value}: training_data must be implemented\"\nlogger.critical(msg)\nraise FedbiomedTrainingPlanError(msg)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._base_training_plan.BaseTrainingPlan.training_routine","title":"<pre><code>training_routine(history_monitor=None, node_args=None)\n</code></pre>  <code>abstractmethod</code>","text":"<p>Training routine, to be called once per round.</p> <p>Parameters:</p> Name Type Description Default <code>history_monitor</code> <code>Optional[HistoryMonitor]</code> <p>optional HistoryMonitor instance, recording training metadata.</p> <code>None</code> <code>node_args</code> <code>Optional[Dict[str, Any]]</code> <p>Command line arguments for node. These arguments can specify GPU use; however, this is not supported for scikit-learn models and thus will be ignored.</p> <code>None</code> Source code in <code>fedbiomed/common/training_plans/_base_training_plan.py</code> <pre><code>@abstractmethod\ndef training_routine(\nself,\nhistory_monitor: Optional['HistoryMonitor'] = None,\nnode_args: Optional[Dict[str, Any]] = None\n) -&gt; None:\n\"\"\"Training routine, to be called once per round.\n    Args:\n        history_monitor: optional HistoryMonitor\n            instance, recording training metadata.\n        node_args: Command line arguments for node.\n            These arguments can specify GPU use; however, this is not\n            supported for scikit-learn models and thus will be ignored.\n    \"\"\"\nreturn None\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.FedPerceptron","title":"FedPerceptron","text":"CLASS  <pre><code>FedPerceptron()\n</code></pre> <p>           Bases: <code>FedSGDClassifier</code></p> <p>Fed-BioMed training plan for scikit-learn Perceptron models.</p> <p>This class inherits from FedSGDClassifier, and forces the wrapped scikit-learn SGDClassifier model to use a \"perceptron\" loss, that makes it equivalent to an actual scikit-learn Perceptron model.</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_models.py</code> <pre><code>def __init__(self) -&gt; None:\n\"\"\"Class constructor.\"\"\"\nsuper().__init__()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.FedPerceptron-functions","title":"Functions","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._sklearn_models.FedPerceptron.post_init","title":"<pre><code>post_init(model_args, training_args, aggregator_args=None)\n</code></pre>","text":"Source code in <code>fedbiomed/common/training_plans/_sklearn_models.py</code> <pre><code>def post_init(\nself,\nmodel_args: Dict[str, Any],\ntraining_args: Dict[str, Any],\naggregator_args: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n# get default values of Perceptron model (different from SGDClassifier model default values)\nperceptron_default_values = Perceptron().get_params()\nsgd_classifier_default_values = SGDClassifier().get_params()\n# make sure loss used is perceptron loss - can not be changed by user\nmodel_args[\"loss\"] = \"perceptron\"\nsuper().post_init(model_args, training_args)\nself._model.set_params(loss=\"perceptron\")\n# collect default values of Perceptron and set it to the model FedPerceptron\nmodel_hyperparameters = self._model.get_params()\nfor hyperparameter_name, val in perceptron_default_values.items():\nif model_hyperparameters[hyperparameter_name] == sgd_classifier_default_values[hyperparameter_name]:\n# this means default parameter of SGDClassifier has not been changed by user\nself._model.set_params(**{hyperparameter_name: val})\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.FedSGDClassifier","title":"FedSGDClassifier","text":"CLASS  <pre><code>FedSGDClassifier()\n</code></pre> <p>           Bases: <code>SKLearnTrainingPlanPartialFit</code></p> <p>Fed-BioMed training plan for scikit-learn SGDClassifier models.</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_models.py</code> <pre><code>def __init__(self) -&gt; None:\n\"\"\"Initialize the sklearn SGDClassifier training plan.\"\"\"\nsuper().__init__()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.FedSGDRegressor","title":"FedSGDRegressor","text":"CLASS  <pre><code>FedSGDRegressor()\n</code></pre> <p>           Bases: <code>SKLearnTrainingPlanPartialFit</code></p> <p>Fed-BioMed training plan for scikit-learn SGDRegressor models.</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_models.py</code> <pre><code>def __init__(self) -&gt; None:\n\"\"\"Initialize the sklearn SGDRegressor training plan.\"\"\"\nsuper().__init__()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.SKLearnTrainingPlan","title":"SKLearnTrainingPlan","text":"CLASS  <pre><code>SKLearnTrainingPlan()\n</code></pre> <p>           Bases: <code>BaseTrainingPlan</code></p> <p>Base class for Fed-BioMed wrappers of sklearn classes.</p> <p>Classes that inherit from this abstract class must: - Specify a <code>_model_cls</code> class attribute that defines the type   of scikit-learn model being wrapped for training. - Implement a <code>set_init_params</code> method that:   - sets and assigns the model's initial trainable weights attributes.   - populates the <code>_param_list</code> attribute with names of these attributes. - Implement a <code>_training_routine</code> method that performs a training round   based on <code>self.train_data_loader</code> (which is a <code>NPDataLoader</code>).</p> <p>Attributes:</p> Name Type Description <code>dataset_path</code> <code>Optional[str]</code> <p>The path that indicates where dataset has been stored</p> <code>pre_processes</code> <code>Optional[str]</code> <p>Preprocess functions that will be applied to the training data at the beginning of the training routine.</p> <code>training_data_loader</code> <code>Optional[str]</code> <p>Data loader used in the training routine.</p> <code>testing_data_loader</code> <code>Optional[str]</code> <p>Data loader used in the validation routine.</p> <p>Notes</p> <p>The trained model may be exported via the <code>export_model</code> method, resulting in a dump file that may be reloded using <code>joblib.load</code> outside of Fed-BioMed.</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def __init__(self) -&gt; None:\n\"\"\"Initialize the SKLearnTrainingPlan.\"\"\"\nsuper().__init__()\nself._model: Union[SkLearnModel, None] = None\nself._training_args = {}  # type: Dict[str, Any]\nself.__type = TrainingPlans.SkLearnTrainingPlan\nself._batch_maxnum = 0\nself.dataset_path: Optional[str] = None\nself._optimizer: Optional[BaseOptimizer] = None\nself.add_dependency([\n\"import inspect\",\n\"import numpy as np\",\n\"import pandas as pd\",\n\"from fedbiomed.common.training_plans import SKLearnTrainingPlan\",\n\"from fedbiomed.common.data import DataManager\",\n])\nself.add_dependency(list(self._model_dep))\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.SKLearnTrainingPlan-attributes","title":"Attributes","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._sklearn_training_plan.SKLearnTrainingPlan.dataset_path","title":"dataset_path     <code>instance-attribute</code>","text":"<pre><code>dataset_path: Optional[str] = None\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.SKLearnTrainingPlan-functions","title":"Functions","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._sklearn_training_plan.SKLearnTrainingPlan.init_optimizer","title":"<pre><code>init_optimizer()\n</code></pre>","text":"<p>Creates and configures optimizer. By default, returns None (meaning native inner scikit learn optimization SGD based will be used).</p> <p>In the case a Declearn Optimizer is used, this method should be overridden in the Training Plan and return a Fedbiomed <code>Optimizer</code></p> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def init_optimizer(self) -&gt; Optional[FedOptimizer]:\n\"\"\"Creates and configures optimizer. By default, returns None (meaning native inner scikit\n    learn optimization SGD based will be used).\n    In the case a Declearn Optimizer is used, this method should be overridden in the Training Plan and return\n    a Fedbiomed [`Optimizer`][fedbiomed.common.optimizers.optimizer.Optimizer]\"\"\"\npass\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._sklearn_training_plan.SKLearnTrainingPlan.model","title":"<pre><code>model()\n</code></pre>","text":"<p>Retrieve the wrapped scikit-learn model instance.</p> <p>Returns:</p> Type Description <code>Optional[BaseEstimator]</code> <p>Scikit-learn model instance</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def model(self) -&gt; Optional[BaseEstimator]:\n\"\"\"Retrieve the wrapped scikit-learn model instance.\n    Returns:\n        Scikit-learn model instance\n    \"\"\"\nif self._model is not None:\nreturn self._model.model\nelse:\nreturn self._model\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._sklearn_training_plan.SKLearnTrainingPlan.model_args","title":"<pre><code>model_args()\n</code></pre>","text":"<p>Retrieve model arguments.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Model arguments</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def model_args(self) -&gt; Dict[str, Any]:\n\"\"\"Retrieve model arguments.\n    Returns:\n        Model arguments\n    \"\"\"\nreturn self._model_args\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._sklearn_training_plan.SKLearnTrainingPlan.optimizer_args","title":"<pre><code>optimizer_args()\n</code></pre>","text":"<p>Retrieves optimizer arguments</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Optimizer arguments</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def optimizer_args(self) -&gt; Dict:\n\"\"\"Retrieves optimizer arguments\n    Returns:\n        Optimizer arguments\n    \"\"\"\nreturn self._optimizer_args\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._sklearn_training_plan.SKLearnTrainingPlan.post_init","title":"<pre><code>post_init(model_args, training_args, aggregator_args=None)\n</code></pre>","text":"<p>Process model, training and optimizer arguments.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict[str, Any]</code> <p>Arguments defined to instantiate the wrapped model.</p> required <code>training_args</code> <code>TrainingArgs</code> <p>Arguments that are used in training routines such as epoch, dry_run etc. Please see <code>TrainingArgs</code></p> required <code>aggregator_args</code> <code>Optional[Dict[str, Any]]</code> <p>Arguments managed by and shared with the researcher-side aggregator.</p> <code>None</code> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def post_init(\nself,\nmodel_args: Dict[str, Any],\ntraining_args: TrainingArgs,\naggregator_args: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n\"\"\"Process model, training and optimizer arguments.\n    Args:\n        model_args: Arguments defined to instantiate the wrapped model.\n        training_args: Arguments that are used in training routines\n            such as epoch, dry_run etc.\n            Please see [`TrainingArgs`][fedbiomed.common.training_args.TrainingArgs]\n        aggregator_args: Arguments managed by and shared with the\n            researcher-side aggregator.\n    \"\"\"\nself._model = SkLearnModel(self._model_cls)\nmodel_args.setdefault(\"verbose\", 1)\nself._model_args = model_args\nself._aggregator_args = aggregator_args or {}\nself._optimizer_args = training_args.optimizer_arguments() or {}\nself._training_args = training_args.pure_training_arguments()\nself._batch_maxnum = self._training_args.get('batch_maxnum', self._batch_maxnum)\n# Add dependencies\nself._configure_dependencies()\n# configure optimizer (if provided in the TrainingPlan)\nself._configure_optimizer()\n# FIXME: should we do that in `_configure_optimizer`\n# from now on, `self._optimizer`` is not None\n# Override default model parameters based on `self._model_args`.\nparams = {\nkey: model_args.get(key, val)\nfor key, val in self._model.get_params().items()\n}\nself._model.set_params(**params)\n# Set up additional parameters (normally created by `self._model.fit`).\nself._model.set_init_params(model_args)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._sklearn_training_plan.SKLearnTrainingPlan.set_data_loaders","title":"<pre><code>set_data_loaders(train_data_loader, test_data_loader)\n</code></pre>","text":"<p>Sets data loaders</p> <p>Parameters:</p> Name Type Description Default <code>train_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader for training routine/loop</p> required <code>test_data_loader</code> <code>Union[DataLoader, NPDataLoader, None]</code> <p>Data loader for validation routine</p> required Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def set_data_loaders(\nself,\ntrain_data_loader: Union[DataLoader, NPDataLoader, None],\ntest_data_loader: Union[DataLoader, NPDataLoader, None]\n) -&gt; None:\n\"\"\"Sets data loaders\n    Args:\n        train_data_loader: Data loader for training routine/loop\n        test_data_loader: Data loader for validation routine\n    \"\"\"\nargs = (train_data_loader, test_data_loader)\nif not all(isinstance(data, NPDataLoader) for data in args):\nmsg = (\nf\"{ErrorNumbers.FB310.value}: SKLearnTrainingPlan expects \"\n\"NPDataLoader instances as training and testing data \"\nf\"loaders, but received {type(train_data_loader)} \"\nf\"and {type(test_data_loader)} respectively.\"\n)\nlogger.error(msg)\nraise FedbiomedTrainingPlanError(msg)\nself.training_data_loader = train_data_loader\nself.testing_data_loader = test_data_loader\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._sklearn_training_plan.SKLearnTrainingPlan.testing_routine","title":"<pre><code>testing_routine(metric, metric_args, history_monitor, before_train)\n</code></pre>","text":"<p>Evaluation routine, to be called once per round.</p> <p>Note</p> <p>If the training plan implements a <code>testing_step</code> method (the signature of which is func(data, target) -&gt; metrics) then it will be used rather than the input metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Optional[MetricTypes]</code> <p>The metric used for validation. If None, use MetricTypes.ACCURACY.</p> required <code>history_monitor</code> <code>Optional[HistoryMonitor]</code> <p>HistoryMonitor instance, used to record computed metrics and communicate them to the researcher (server).</p> required <code>before_train</code> <code>bool</code> <p>Whether the evaluation is being performed before local training occurs, of afterwards. This is merely reported back through <code>history_monitor</code>.</p> required Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def testing_routine(\nself,\nmetric: Optional[MetricTypes],\nmetric_args: Dict[str, Any],\nhistory_monitor: Optional['HistoryMonitor'],\nbefore_train: bool\n) -&gt; None:\n\"\"\"Evaluation routine, to be called once per round.\n    !!! info \"Note\"\n        If the training plan implements a `testing_step` method\n        (the signature of which is func(data, target) -&gt; metrics)\n        then it will be used rather than the input metric.\n    Args:\n        metric: The metric used for validation.\n            If None, use MetricTypes.ACCURACY.\n        history_monitor: HistoryMonitor instance,\n            used to record computed metrics and communicate them to\n            the researcher (server).\n        before_train: Whether the evaluation is being performed\n            before local training occurs, of afterwards. This is merely\n            reported back through `history_monitor`.\n    \"\"\"\n# Check that the testing data loader is of proper type.\nif not isinstance(self.testing_data_loader, NPDataLoader):\nmsg = (\nf\"{ErrorNumbers.FB310.value}: SKLearnTrainingPlan cannot be \"\n\"evaluated without a NPDataLoader as `testing_data_loader`.\"\n)\nlogger.error(msg)\nraise FedbiomedTrainingPlanError(msg)\n# If required, make up for the lack of specifications regarding target\n# classification labels.\nif self._model.is_classification and not hasattr(self.model(), 'classes_'):\nclasses = self._classes_from_concatenated_train_test()\nsetattr(self.model(), 'classes_', classes)\n# If required, select the default metric (accuracy or mse).\nif metric is None:\nif self._model.is_classification:\nmetric = MetricTypes.ACCURACY\nelse:\nmetric = MetricTypes.MEAN_SQUARE_ERROR\n# Delegate the actual evalation routine to the parent class.\nsuper().testing_routine(\nmetric, metric_args, history_monitor, before_train\n)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._sklearn_training_plan.SKLearnTrainingPlan.training_args","title":"<pre><code>training_args()\n</code></pre>","text":"<p>Retrieve training arguments.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Training arguments</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def training_args(self) -&gt; Dict[str, Any]:\n\"\"\"Retrieve training arguments.\n    Returns:\n        Training arguments\n    \"\"\"\nreturn self._training_args\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._sklearn_training_plan.SKLearnTrainingPlan.training_routine","title":"<pre><code>training_routine(history_monitor=None, node_args=None)\n</code></pre>","text":"<p>Training routine, to be called once per round.</p> <p>Parameters:</p> Name Type Description Default <code>history_monitor</code> <code>Optional[HistoryMonitor]</code> <p>optional HistoryMonitor instance, recording training metadata. Defaults to None.</p> <code>None</code> <code>node_args</code> <code>Optional[Dict[str, Any]]</code> <p>command line arguments for node. These arguments can specify GPU use; however, this is not supported for scikit-learn models and thus will be ignored.</p> <code>None</code> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def training_routine(\nself,\nhistory_monitor: Optional['HistoryMonitor'] = None,\nnode_args: Optional[Dict[str, Any]] = None\n) -&gt; None:\n\"\"\"Training routine, to be called once per round.\n    Args:\n        history_monitor: optional HistoryMonitor\n            instance, recording training metadata. Defaults to None.\n        node_args: command line arguments for node.\n            These arguments can specify GPU use; however, this is not\n            supported for scikit-learn models and thus will be ignored.\n    \"\"\"\nif self._optimizer is None:\nraise FedbiomedTrainingPlanError('Optimizer is None, please run `post_init` beforehand')\n# Run preprocesses\nself._preprocess()\nif not isinstance(self.training_data_loader, NPDataLoader):\nmsg = (\nf\"{ErrorNumbers.FB310.value}: SKLearnTrainingPlan cannot \"\n\"be trained without a NPDataLoader as `training_data_loader`.\"\n)\nlogger.critical(msg)\nraise FedbiomedTrainingPlanError(msg)\n# Run preprocessing operations.\nself._preprocess()\n# Warn if GPU-use was expected (as it is not supported).\nif node_args is not None and node_args.get('gpu_only', False):\nlogger.warning(\n'Node would like to force GPU usage, but sklearn training '\n'plan does not support it. Training on CPU.'\n)\n# Run the model-specific training routine.\ntry:\nreturn self._training_routine(history_monitor)\nexcept Exception as exc:\nmsg = (\nf\"{ErrorNumbers.FB605.value}: error while fitting \"\nf\"the model: {exc}\"\n)\nlogger.critical(msg)\nraise FedbiomedTrainingPlanError(msg)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._sklearn_training_plan.SKLearnTrainingPlan.type","title":"<pre><code>type()\n</code></pre>","text":"<p>Getter for training plan type</p> Source code in <code>fedbiomed/common/training_plans/_sklearn_training_plan.py</code> <pre><code>def type(self) -&gt; TrainingPlans:\n\"\"\"Getter for training plan type \"\"\"\nreturn self.__type\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan","title":"TorchTrainingPlan","text":"CLASS  <pre><code>TorchTrainingPlan()\n</code></pre> <p>           Bases: <code>BaseTrainingPlan</code></p> <p>Implements  TrainingPlan for torch NN framework</p> <p>An abstraction over pytorch module to run pytorch models and scripts on node side. Researcher model (resp. params) will be:</p> <ol> <li>saved  on a '.py' (resp. '.mpk') files,</li> <li>uploaded on a HTTP server (network layer),</li> <li>then Downloaded from the HTTP server on node side,</li> <li>finally, read and executed on node side.</li> </ol> <p>Researcher must define/override: - a <code>training_data()</code> function - a <code>training_step()</code> function</p> <p>Researcher may have to add extra dependencies/python imports, by using <code>add_dependencies</code> method.</p> <p>Attributes:</p> Name Type Description <code>dataset_path</code> <p>The path that indicates where dataset has been stored</p> <code>pre_processes</code> <p>Preprocess functions that will be applied to the training data at the beginning of the training routine.</p> <code>training_data_loader</code> <p>Data loader used in the training routine.</p> <code>testing_data_loader</code> <p>Data loader used in the validation routine.</p> <code>correction_state</code> <code>OrderedDict</code> <p>an OrderedDict of {'parameter name': torch.Tensor} where the keys correspond to the names of the model parameters contained in self._model.named_parameters(), and the values correspond to the correction to be applied to that parameter.</p> <p>Notes</p> <p>The trained model may be exported via the <code>export_model</code> method, resulting in a dump file that may be reloded using <code>torch.save</code> outside of Fed-BioMed.</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def __init__(self):\n\"\"\" Construct training plan \"\"\"\nsuper().__init__()\nself.__type = TrainingPlans.TorchTrainingPlan\n# Differential privacy support\nself._dp_controller: Optional[DPController] = None\nself._optimizer: Union[BaseOptimizer, None] = None\nself._model: Union[TorchModel, None] = None\nself._training_args: Optional[dict] = None\nself._model_args: Optional[dict] = None\nself._optimizer_args: Optional[dict] = None\nself._use_gpu: bool = False\nself._share_persistent_buffers = None\nself._batch_maxnum: int = 100\nself._fedprox_mu: Optional[float] = None\nself._log_interval: int = 10\nself._epochs: int = 1\nself._dry_run = False\nself._num_updates: Optional[int] = None\nself.correction_state: OrderedDict = OrderedDict()\nself.aggregator_name: str = None\n# TODO : add random seed init\n# self.random_seed_params = None\n# self.random_seed_shuffling_data = None\n# device to use: cpu/gpu\n# - all operations except training only use cpu\n# - researcher doesn't request to use gpu by default\nself._device_init: str = \"cpu\"\nself._device = self._device_init\n# list dependencies of the model\nself.add_dependency([\"import torch\",\n\"import torch.nn as nn\",\n\"import torch.nn.functional as F\",\n\"from fedbiomed.common.training_plans import TorchTrainingPlan\",\n\"from fedbiomed.common.data import DataManager\",\n\"from fedbiomed.common.constants import ProcessTypes\",\n\"from torch.utils.data import DataLoader\",\n\"from torchvision import datasets, transforms\"\n])\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan-attributes","title":"Attributes","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.aggregator_name","title":"aggregator_name     <code>instance-attribute</code>","text":"<pre><code>aggregator_name: str = None\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.correction_state","title":"correction_state     <code>instance-attribute</code>","text":"<pre><code>correction_state: OrderedDict = OrderedDict()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans.TorchTrainingPlan-functions","title":"Functions","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.after_training_params","title":"<pre><code>after_training_params(flatten=False)\n</code></pre>","text":"<p>Return the wrapped model's parameters for aggregation.</p> <p>This method returns a dict containing parameters that need to be reported back and aggregated in a federated learning setting.</p> <p>If the <code>postprocess</code> method exists (i.e. has been defined by end-users) it is called in the context of this method. DP-required adjustments are also set to happen as part of this method.</p> <p>If the researcher specified <code>share_persistent_buffers: False</code> in the training arguments, then we return only the output of Model.get_weights, which considers only the trainable parameters. Otherwise, the default behaviour is to return the complete <code>state_dict</code>.</p> <p>Returns:</p> Type Description <code>Dict[str, torch.Tensor]</code> <p>The trained parameters to aggregate.</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def after_training_params(self, flatten: bool = False) -&gt; Dict[str, torch.Tensor]:\n\"\"\"Return the wrapped model's parameters for aggregation.\n    This method returns a dict containing parameters that need to be\n    reported back and aggregated in a federated learning setting.\n    If the `postprocess` method exists (i.e. has been defined by end-users)\n    it is called in the context of this method. DP-required adjustments are\n    also set to happen as part of this method.\n    If the researcher specified `share_persistent_buffers: False` in the\n    training arguments, then we return only the output of\n    [Model.get_weights][fedbiomed.common.models.TorchModel.get_weights],\n    which considers only the trainable parameters.\n    Otherwise, the default behaviour is to return the complete `state_dict`.\n    Returns:\n        The trained parameters to aggregate.\n    \"\"\"\n# Either include non-parameter buffers to the outputs or not.\n# Note: this is mostly about sharing statistics from BatchNorm layers.\nif self._share_persistent_buffers:\nparams = dict(self._model.model.state_dict())\nelse:\nparams = super().after_training_params()\n# Check whether postprocess method exists, and use it.\nif hasattr(self, 'postprocess'):\nlogger.debug(\"running model.postprocess() method\")\ntry:\nparams = self.postprocess(self._model.model.state_dict())  # Post process\nexcept Exception as e:\nraise FedbiomedTrainingPlanError(f\"{ErrorNumbers.FB605.value}: Error while running post-process \"\nf\"{e}\") from e\n# Run (optional) DP controller adjustments as well.\nparams = self._dp_controller.after_training(params)\nif flatten:\nparams = self._model.flatten()\nreturn params\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.init_model","title":"<pre><code>init_model()\n</code></pre>  <code>abstractmethod</code>","text":"<p>Abstract method where model should be defined.</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>@abstractmethod\ndef init_model(self):\n\"\"\"Abstract method where model should be defined.\"\"\"\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.init_optimizer","title":"<pre><code>init_optimizer()\n</code></pre>","text":"<p>Abstract method for declaring optimizer by default</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def init_optimizer(self) -&gt; Union[FedOptimizer, torch.optim.Optimizer]:\n\"\"\"Abstract method for declaring optimizer by default \"\"\"\ntry:\nself._optimizer = torch.optim.Adam(self._model.model.parameters(), **self._optimizer_args)\nexcept AttributeError as e:\nraise FedbiomedTrainingPlanError(f\"{ErrorNumbers.FB605.value}: Invalid argument for default \"\nf\"optimizer Adam. Error: {e}\") from e\nreturn self._optimizer\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.initial_parameters","title":"<pre><code>initial_parameters()\n</code></pre>","text":"<p>Returns initial parameters without DP or training applied</p> <p>Returns:</p> Type Description <code>Dict</code> <p>State dictionary of torch Module</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def initial_parameters(self) -&gt; Dict:\n\"\"\"Returns initial parameters without DP or training applied\n    Returns:\n        State dictionary of torch Module\n    \"\"\"\nreturn self._model.init_params\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.model","title":"<pre><code>model()\n</code></pre>","text":"Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def model(self) -&gt; Optional[torch.nn.Module]:\nif self._model is not None:\nreturn self._model.model\nelse:\nreturn self._model\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.model_args","title":"<pre><code>model_args()\n</code></pre>","text":"<p>Retrieves model args</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Model arguments arguments</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def model_args(self) -&gt; Dict:\n\"\"\"Retrieves model args\n    Returns:\n        Model arguments arguments\n    \"\"\"\nreturn self._model_args\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.optimizer","title":"<pre><code>optimizer()\n</code></pre>","text":"Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def optimizer(self):\nreturn self._optimizer\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.optimizer_args","title":"<pre><code>optimizer_args()\n</code></pre>","text":"<p>Retrieves optimizer arguments</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Optimizer arguments</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def optimizer_args(self) -&gt; Dict:\n\"\"\"Retrieves optimizer arguments\n    Returns:\n        Optimizer arguments\n    \"\"\"\nself.update_optimizer_args()  # update `optimizer_args` (eg after training)\nreturn self._optimizer_args\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.post_init","title":"<pre><code>post_init(model_args, training_args, aggregator_args=None)\n</code></pre>","text":"<p>Process model, training and optimizer arguments.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict[str, Any]</code> <p>Arguments defined to instantiate the wrapped model.</p> required <code>training_args</code> <code>TrainingArgs</code> <p>Arguments that are used in training routines such as epoch, dry_run etc. Please see <code>TrainingArgs</code></p> required <code>aggregator_args</code> <code>Optional[Dict[str, Any]]</code> <p>Arguments managed by and shared with the researcher-side aggregator.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanError</code> <p>If the provided arguments do not match expectations, or if the optimizer, model and dependencies configuration goes wrong.</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def post_init(\nself,\nmodel_args: Dict[str, Any],\ntraining_args: TrainingArgs,\naggregator_args: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n\"\"\"Process model, training and optimizer arguments.\n    Args:\n        model_args: Arguments defined to instantiate the wrapped model.\n        training_args: Arguments that are used in training routines\n            such as epoch, dry_run etc.\n            Please see [`TrainingArgs`][fedbiomed.common.training_args.TrainingArgs]\n        aggregator_args: Arguments managed by and shared with the\n            researcher-side aggregator.\n    Raises:\n        FedbiomedTrainingPlanError: If the provided arguments do not\n            match expectations, or if the optimizer, model and dependencies\n            configuration goes wrong.\n    \"\"\"\n# Assign model arguments.\nself._model_args = model_args\n# Assign scalar attributes.\nself._optimizer_args = training_args.optimizer_arguments() or {}\nself._training_args = training_args.pure_training_arguments()\nself._use_gpu = self._training_args.get('use_gpu')\nself._batch_maxnum = self._training_args.get('batch_maxnum')\nself._log_interval = self._training_args.get('log_interval')\nself._epochs = self._training_args.get('epochs')\nself._num_updates = self._training_args.get('num_updates', 1)\nself._dry_run = self._training_args.get('dry_run')\nself._share_persistent_buffers = training_args.get('share_persistent_buffers', True)\n# Optionally set up differential privacy.\nself._dp_controller = DPController(training_args.dp_arguments() or None)\n# Add dependencies\nself._configure_dependencies()\n# Configure aggregator-related arguments\n# TODO: put fedprox mu inside strategy_args\nself._fedprox_mu = self._training_args.get('fedprox_mu')\nself.set_aggregator_args(aggregator_args or {})\n# Configure the model and optimizer.\nself._configure_model_and_optimizer()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.send_to_device","title":"<pre><code>send_to_device(to_send, device)\n</code></pre>","text":"<p>Send inputs to correct device for training.</p> <p>Recursively traverses lists, tuples and dicts until it meets a torch Tensor, then sends the Tensor to the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>Union[torch.Tensor, list, tuple, dict]</code> <p>the data to be sent to the device.</p> required <code>device</code> <code>torch.device</code> <p>the device to send the data to.</p> required <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanError</code> <p>when to_send is not the correct type</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def send_to_device(self,\nto_send: Union[torch.Tensor, list, tuple, dict],\ndevice: torch.device\n):\n\"\"\"Send inputs to correct device for training.\n    Recursively traverses lists, tuples and dicts until it meets a torch Tensor, then sends the Tensor\n    to the specified device.\n    Args:\n        to_send: the data to be sent to the device.\n        device: the device to send the data to.\n    Raises:\n       FedbiomedTrainingPlanError: when to_send is not the correct type\n    \"\"\"\nif isinstance(to_send, torch.Tensor):\nreturn to_send.to(device)\nelif isinstance(to_send, dict):\nreturn {key: self.send_to_device(val, device) for key, val in to_send.items()}\nelif isinstance(to_send, tuple):\nreturn tuple(self.send_to_device(d, device) for d in to_send)\nelif isinstance(to_send, list):\nreturn [self.send_to_device(d, device) for d in to_send]\nelse:\nraise FedbiomedTrainingPlanError(f'{ErrorNumbers.FB310.value} cannot send data to device. '\nf'Data must be a torch Tensor or a list, tuple or dict '\nf'ultimately containing Tensors.')\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.set_aggregator_args","title":"<pre><code>set_aggregator_args(aggregator_args)\n</code></pre>","text":"<p>Handles and loads aggregators arguments sent through MQTT and file exchanged system. If sent through file exchanged system, loads the arguments.</p> <p>Parameters:</p> Name Type Description Default <code>aggregator_args</code> <code>Dict[str, Any]</code> <p>dictionary mapping aggregator argument name with its value (eg</p> required Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def set_aggregator_args(self, aggregator_args: Dict[str, Any]):\n\"\"\"Handles and loads aggregators arguments sent through MQTT and\n    file exchanged system. If sent through file exchanged system, loads the arguments.\n    Args:\n        aggregator_args (Dict[str, Any]): dictionary mapping aggregator argument name with its value (eg\n        'aggregator_correction' with correction states)\n    \"\"\"\nself.aggregator_name = aggregator_args.get('aggregator_name', self.aggregator_name)\n# FIXME: this is too specific to Scaffold. Should be redesigned, or handled\n# by an aggregator handler that contains all keys for all strategies\n# implemented in fedbiomed\n# here we ae loading all args that have been sent from file exchange system\nfor arg_name, aggregator_arg in aggregator_args.items():\nif arg_name == 'aggregator_correction':\nif not isinstance(aggregator_arg, dict):\nraise FedbiomedTrainingPlanError(\nf\"{ErrorNumbers.FB309.value}: TorchTrainingPlan received \"\n\"invalid 'aggregator_correction' aggregator args.\"\n)\nself.correction_state = aggregator_arg\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.testing_routine","title":"<pre><code>testing_routine(metric, metric_args, history_monitor, before_train)\n</code></pre>","text":"<p>Evaluation routine, to be called once per round.</p> <p>Note</p> <p>If the training plan implements a <code>testing_step</code> method (the signature of which is func(data, target) -&gt; metrics) then it will be used rather than the input metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Optional[MetricTypes]</code> <p>The metric used for validation. If None, use MetricTypes.ACCURACY.</p> required <code>history_monitor</code> <code>Optional[HistoryMonitor]</code> <p>HistoryMonitor instance, used to record computed metrics and communicate them to the researcher (server).</p> required <code>before_train</code> <code>bool</code> <p>Whether the evaluation is being performed before local training occurs, of afterwards. This is merely reported back through <code>history_monitor</code>.</p> required Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def testing_routine(\nself,\nmetric: Optional[MetricTypes],\nmetric_args: Dict[str, Any],\nhistory_monitor: Optional['HistoryMonitor'],\nbefore_train: bool\n) -&gt; None:\n\"\"\"Evaluation routine, to be called once per round.\n    !!! info \"Note\"\n        If the training plan implements a `testing_step` method\n        (the signature of which is func(data, target) -&gt; metrics)\n        then it will be used rather than the input metric.\n    Args:\n        metric: The metric used for validation.\n            If None, use MetricTypes.ACCURACY.\n        history_monitor: HistoryMonitor instance,\n            used to record computed metrics and communicate them to\n            the researcher (server).\n        before_train: Whether the evaluation is being performed\n            before local training occurs, of afterwards. This is merely\n            reported back through `history_monitor`.\n    \"\"\"\nif not isinstance(self.model(), torch.nn.Module):\nmsg = (\nf\"{ErrorNumbers.FB320.value}: model should be a torch \"\nf\"nn.Module, but is of type {type(self.model())}\"\n)\nlogger.critical(msg)\nraise FedbiomedTrainingPlanError(msg)\ntry:\nwith torch.no_grad():\nsuper().testing_routine(\nmetric, metric_args, history_monitor, before_train\n)\nfinally:\nself.model().train()  # restore training behaviors\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.training_args","title":"<pre><code>training_args()\n</code></pre>","text":"<p>Retrieves training args</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Training arguments</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def training_args(self) -&gt; Dict:\n\"\"\"Retrieves training args\n    Returns:\n        Training arguments\n    \"\"\"\nreturn self._training_args\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.training_data","title":"<pre><code>training_data()\n</code></pre>  <code>abstractmethod</code>","text":"<p>Abstract method to return training data</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>@abstractmethod\ndef training_data(self):\n\"\"\"Abstract method to return training data\"\"\"\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.training_routine","title":"<pre><code>training_routine(history_monitor=None, node_args=None)\n</code></pre>","text":"<p>Training routine procedure.</p> <p>End-user should define;</p> <ul> <li>a <code>training_data()</code> function defining how sampling / handling data in node's dataset is done. It should     return a generator able to output tuple (batch_idx, (data, targets)) that is iterable for each batch.</li> <li>a <code>training_step()</code> function defining how cost is computed. It should output loss values for backpropagation.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>history_monitor</code> <code>Any</code> <p>Monitor handler for real-time feed. Defined by the Node and can't be overwritten</p> <code>None</code> <code>node_args</code> <code>Union[dict, None]</code> <p>command line arguments for node. Can include: - <code>gpu (bool)</code>: propose use a GPU device if any is available. Default False. - <code>gpu_num (Union[int, None])</code>: if not None, use the specified GPU device instead of default     GPU device if this GPU device is available. Default None. - <code>gpu_only (bool)</code>: force use of a GPU device if any available, even if researcher     doesn't request for using a GPU. Default False.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Total number of samples observed during the training.</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def training_routine(self,\nhistory_monitor: Any = None,\nnode_args: Union[dict, None] = None,\n) -&gt; int:\n# FIXME: add betas parameters for ADAM solver + momentum for SGD\n# FIXME 2: remove parameters specific for validation specified in the\n# training routine\n\"\"\"Training routine procedure.\n    End-user should define;\n    - a `training_data()` function defining how sampling / handling data in node's dataset is done. It should\n        return a generator able to output tuple (batch_idx, (data, targets)) that is iterable for each batch.\n    - a `training_step()` function defining how cost is computed. It should output loss values for backpropagation.\n    Args:\n        history_monitor: Monitor handler for real-time feed. Defined by the Node and can't be overwritten\n        node_args: command line arguments for node. Can include:\n            - `gpu (bool)`: propose use a GPU device if any is available. Default False.\n            - `gpu_num (Union[int, None])`: if not None, use the specified GPU device instead of default\n                GPU device if this GPU device is available. Default None.\n            - `gpu_only (bool)`: force use of a GPU device if any available, even if researcher\n                doesn't request for using a GPU. Default False.\n    Returns:\n        Total number of samples observed during the training.\n    \"\"\"\n#self.model().train()  # pytorch switch for training\nself._optimizer.init_training()\n# set correct type for node args\nnode_args = {} if not isinstance(node_args, dict) else node_args\n# send all model to device, ensures having all the requested tensors\nself._set_device(self._use_gpu, node_args)\nself._model.send_to_device(self._device)\n# Run preprocess when everything is ready before the training\nself._preprocess()\n# # initial aggregated model parameters\n# self._init_params = deepcopy(list(self.model().parameters()))\n# DP actions\nself._optimizer, self.training_data_loader = \\\n        self._dp_controller.before_training(optimizer= self._optimizer, loader=self.training_data_loader)\n# set number of training loop iterations\niterations_accountant = MiniBatchTrainingIterationsAccountant(self)\n# Training loop iterations\nfor epoch in iterations_accountant.iterate_epochs():\ntraining_data_iter: Iterator = iter(self.training_data_loader)\nfor batch_idx in iterations_accountant.iterate_batches():\n# retrieve data and target\ndata, target = next(training_data_iter)\n# update accounting for number of observed samples\nbatch_size = self._infer_batch_size(data)\niterations_accountant.increment_sample_counters(batch_size)\n# handle training on accelerator devices\ndata, target = self.send_to_device(data, self._device), self.send_to_device(target, self._device)\n# train this batch\ncorrected_loss, loss = self._train_over_batch(data, target)\n# Reporting\nif iterations_accountant.should_log_this_batch():\n# Retrieve reporting information: semantics differ whether num_updates or epochs were specified\nnum_samples, num_samples_max = iterations_accountant.reporting_on_num_samples()\nnum_iter, num_iter_max = iterations_accountant.reporting_on_num_iter()\nepoch_to_report = iterations_accountant.reporting_on_epoch()\nlogger.debug('Train {}| '\n'Iteration {}/{} | '\n'Samples {}/{} ({:.0f}%)\\tLoss: {:.6f}'.format(\nf'Epoch: {epoch_to_report} ' if epoch_to_report is not None else '',\nnum_iter,\nnum_iter_max,\nnum_samples,\nnum_samples_max,\n100. * num_iter / num_iter_max,\nloss.item())\n)\n# Send scalar values via general/feedback topic\nif history_monitor is not None:\n# the researcher only sees the average value of samples observed until now\nhistory_monitor.add_scalar(metric={'Loss': loss.item()},\niteration=num_iter,\nepoch=epoch_to_report,\ntrain=True,\nnum_samples_trained=num_samples,\nnum_batches=num_iter_max,\ntotal_samples=num_samples_max,\nbatch_samples=batch_size)\n# Handle dry run mode\nif self._dry_run:\nself._model.send_to_device(self._device_init)\ntorch.cuda.empty_cache()\nreturn iterations_accountant.num_samples_observed_in_total\n# release gpu usage as much as possible though:\n# - it should be done by deleting the object\n# - and some gpu memory remains used until process (cuda kernel ?) finishes\nself._model.send_to_device(self._device_init)\ntorch.cuda.empty_cache()\n# # test (to be removed)\n# assert id(self._optimizer.model.model) == id(self._model.model)\n# assert id(self._optimizer.model) == id(self._model)\n# for (layer, val), (layer2, val2) in zip(self._model.model.state_dict().items(), self._optimizer.model.model.state_dict().items()):\n#     assert layer == layer2\n#     print(val, layer2)\n#     assert torch.isclose(val, val2).all()\n# for attributes, values in self._model.__dict__.items():\n#     print(\"ATTRIBUTES\", values)\n#     assert values == getattr(self._optimizer.model, attributes)\n# for attributes, values in self._model.model.__dict__.items():\n#     print(\"ATTRIBUTES\", values)\n#     assert values == getattr(self._optimizer.model.model, attributes) \nreturn iterations_accountant.num_samples_observed_in_total\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.training_step","title":"<pre><code>training_step()\n</code></pre>  <code>abstractmethod</code>","text":"<p>Abstract method, all subclasses must provide a training_step.</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>@abstractmethod\ndef training_step(self):\n\"\"\"Abstract method, all subclasses must provide a training_step.\"\"\"\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.type","title":"<pre><code>type()\n</code></pre>","text":"<p>Gets training plan type</p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def type(self) -&gt; TrainingPlans.TorchTrainingPlan:\n\"\"\" Gets training plan type\"\"\"\nreturn self.__type\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._torchnn.TorchTrainingPlan.update_optimizer_args","title":"<pre><code>update_optimizer_args()\n</code></pre>","text":"<p>Updates <code>_optimizer_args</code> variable. Can prove useful to retrieve optimizer parameters after having trained a model, parameters which may have changed during training (eg learning rate).</p> Updated arguments <ul> <li>learning_rate</li> </ul> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>updated <code>_optimizer_args</code></p> Source code in <code>fedbiomed/common/training_plans/_torchnn.py</code> <pre><code>def update_optimizer_args(self) -&gt; Dict:\n\"\"\"\n    Updates `_optimizer_args` variable. Can prove useful\n    to retrieve optimizer parameters after having trained a\n    model, parameters which may have changed during training (eg learning rate).\n    Updated arguments:\n     - learning_rate\n    Returns:\n        Dict: updated `_optimizer_args`\n    \"\"\"\nif self._optimizer_args is None:\nself._optimizer_args = {}\nif self.aggregator_name is not None and self.aggregator_name.lower() == \"scaffold\":\nself._optimizer_args['lr'] = self._optimizer.get_learning_rate()\nreturn self._optimizer_args\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant","title":"MiniBatchTrainingIterationsAccountant","text":"<p>Accounting class for keeping track of training iterations.</p> This class has the following responsibilities <ul> <li>manage iterators for epochs and batches</li> <li>provide up-to-date values for reporting</li> <li>handle different semantics in case the researcher asked for num_updates or epochs</li> </ul> <p>We assume that the underlying implementation for the training loop is always made in terms of epochs and batches. So the primary purpose of this class is to provide a way to correctly convert the number of updates into epochs and batches.</p> <p>For reporting purposes, in the case of num_updates then we think of the training as a single big loop, while in the case of epochs and batches we think of it as two nested loops. This changes the meaning of the values outputted by the reporting functions (see their docstrings for more details).</p> <p>Attributes:</p> Name Type Description <code>_training_plan</code> <p>a reference to the training plan executing the training iterations</p> <code>cur_epoch</code> <code>int</code> <p>the index of the current epoch during iterations</p> <code>cur_batch</code> <code>int</code> <p>the index of the current batch during iterations</p> <code>epochs</code> <code>int</code> <p>the total number of epochs to be performed (we always perform one additional -- possibly empty -- epoch</p> <code>num_batches_per_epoch</code> <code>int</code> <p>the number of iterations per epoch</p> <code>num_batches_in_last_epoch</code> <code>int</code> <p>the number of iterations in the last epoch (can be zero)</p> <code>num_samples_observed_in_epoch</code> <code>int</code> <p>a counter for the number of samples observed in the current epoch, for reporting</p> <code>num_samples_observed_in_total</code> <code>int</code> <p>a counter for the number of samples observed total, for reporting</p> <p>Parameters:</p> Name Type Description Default <code>training_plan</code> <code>TBaseTrainingPlan</code> <p>a reference to the training plan that is executing the training iterations</p> required Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def __init__(self, training_plan: TBaseTrainingPlan):\n\"\"\"Initialize the class.\n    Arguments:\n        training_plan: a reference to the training plan that is executing the training iterations\n    \"\"\"\nself._training_plan = training_plan\nself.cur_epoch: int = 0\nself.cur_batch: int = 0\nself.epochs: int = 0\nself.num_batches_per_epoch: int = 0\nself.num_batches_in_last_epoch: int = 0\nself.num_samples_observed_in_epoch: int = 0\nself.num_samples_observed_in_total: int = 0\nself._n_training_iterations()\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant-attributes","title":"Attributes","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.cur_batch","title":"cur_batch     <code>instance-attribute</code>","text":"<pre><code>cur_batch: int = 0\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.cur_epoch","title":"cur_epoch     <code>instance-attribute</code>","text":"<pre><code>cur_epoch: int = 0\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.epochs","title":"epochs     <code>instance-attribute</code>","text":"<pre><code>epochs: int = 0\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.num_batches_in_last_epoch","title":"num_batches_in_last_epoch     <code>instance-attribute</code>","text":"<pre><code>num_batches_in_last_epoch: int = 0\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.num_batches_per_epoch","title":"num_batches_per_epoch     <code>instance-attribute</code>","text":"<pre><code>num_batches_per_epoch: int = 0\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.num_samples_observed_in_epoch","title":"num_samples_observed_in_epoch     <code>instance-attribute</code>","text":"<pre><code>num_samples_observed_in_epoch: int = 0\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.num_samples_observed_in_total","title":"num_samples_observed_in_total     <code>instance-attribute</code>","text":"<pre><code>num_samples_observed_in_total: int = 0\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant-classes","title":"Classes","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.BatchIter","title":"BatchIter","text":"CLASS  <pre><code>BatchIter(accountant)\n</code></pre> <p>Iterator over batches.</p> <p>Attributes:</p> Name Type Description <code>_accountant</code> <p>an instance of the class that created this iterator</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def __init__(self, accountant: TTrainingIterationsAccountant):\nself._accountant = accountant\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.EpochsIter","title":"EpochsIter","text":"CLASS  <pre><code>EpochsIter(accountant)\n</code></pre> <p>Iterator over epochs.</p> <p>Attributes:</p> Name Type Description <code>_accountant</code> <p>an instance of the class that created this iterator</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def __init__(self, accountant: TTrainingIterationsAccountant):\nself._accountant = accountant\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant-functions","title":"Functions","text":""},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.increment_sample_counters","title":"<pre><code>increment_sample_counters(n_samples)\n</code></pre>","text":"<p>Increments internal counter for numbers of observed samples</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def increment_sample_counters(self, n_samples: int):\n\"\"\"Increments internal counter for numbers of observed samples\"\"\"\nself.num_samples_observed_in_epoch += n_samples\nself.num_samples_observed_in_total += n_samples\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.iterate_batches","title":"<pre><code>iterate_batches()\n</code></pre>","text":"<p>Returns an instance of a batches iterator.</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def iterate_batches(self):\n\"\"\"Returns an instance of a batches iterator.\"\"\"\nreturn MiniBatchTrainingIterationsAccountant.BatchIter(self)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.iterate_epochs","title":"<pre><code>iterate_epochs()\n</code></pre>","text":"<p>Returns an instance of an epochs iterator.</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def iterate_epochs(self):\n\"\"\"Returns an instance of an epochs iterator.\"\"\"\nreturn MiniBatchTrainingIterationsAccountant.EpochsIter(self)\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.num_batches_in_this_epoch","title":"<pre><code>num_batches_in_this_epoch()\n</code></pre>","text":"<p>Returns the number of iterations to be performed in the current epoch</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def num_batches_in_this_epoch(self) -&gt; int:\n\"\"\"Returns the number of iterations to be performed in the current epoch\"\"\"\nif self.cur_epoch == self.epochs:\nreturn self.num_batches_in_last_epoch\nelse:\nreturn self.num_batches_per_epoch\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.reporting_on_epoch","title":"<pre><code>reporting_on_epoch()\n</code></pre>","text":"<p>Returns the optional index of the current epoch, for reporting.</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def reporting_on_epoch(self) -&gt; Optional[int]:\n\"\"\"Returns the optional index of the current epoch, for reporting.\"\"\"\nif self._training_plan.training_args()['num_updates'] is not None:\nreturn None\nelse:\nreturn self.cur_epoch\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.reporting_on_num_iter","title":"<pre><code>reporting_on_num_iter()\n</code></pre>","text":"<p>Outputs useful reporting information about the number of iterations</p> <p>If the researcher specified num_updates, then the iteration number will be the cumulated total, and similarly the maximum number of iterations will be equal to the requested number of updates. If the researcher specified epochs, then the iteration number will be the batch index in the current epoch, while the maximum number of iterations will be computed specifically for the current epoch.</p> <p>Returns:</p> Type Description <code>int</code> <p>the iteration number</p> <code>int</code> <p>the maximum number of iterations to be reported</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def reporting_on_num_iter(self) -&gt; Tuple[int, int]:\n\"\"\"Outputs useful reporting information about the number of iterations\n    If the researcher specified num_updates, then the iteration number will be the cumulated total, and\n    similarly the maximum number of iterations will be equal to the requested number of updates.\n    If the researcher specified epochs, then the iteration number will be the batch index in the current epoch,\n    while the maximum number of iterations will be computed specifically for the current epoch.\n    Returns:\n        the iteration number\n        the maximum number of iterations to be reported\n    \"\"\"\nif self._training_plan.training_args()['num_updates'] is not None:\nnum_iter = (self.cur_epoch - 1) * self.num_batches_per_epoch + self.cur_batch\ntotal_batches_to_be_observed = (self.epochs - 1) * self.num_batches_per_epoch + \\\n            self.num_batches_in_last_epoch\nnum_iter_max = total_batches_to_be_observed\nelse:\nnum_iter = self.cur_batch\nnum_iter_max = self.num_batches_per_epoch\nreturn num_iter, num_iter_max\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.reporting_on_num_samples","title":"<pre><code>reporting_on_num_samples()\n</code></pre>","text":"<p>Outputs useful reporting information about the number of observed samples</p> <p>If the researcher specified num_updates, then the number of observed samples will be the grand total, and similarly the maximum number of samples will be the grand total over all iterations. If the researcher specified epochs, then both values will be specific to the current epoch.</p> <p>Returns:</p> Type Description <code>int</code> <p>the number of samples observed until the current iteration</p> <code>int</code> <p>the maximum number of samples to be observed</p> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def reporting_on_num_samples(self) -&gt; Tuple[int, int]:\n\"\"\"Outputs useful reporting information about the number of observed samples\n    If the researcher specified num_updates, then the number of observed samples will be the grand total, and\n    similarly the maximum number of samples will be the grand total over all iterations.\n    If the researcher specified epochs, then both values will be specific to the current epoch.\n    Returns:\n        the number of samples observed until the current iteration\n        the maximum number of samples to be observed\n    \"\"\"\nif self._training_plan.training_args()['num_updates'] is not None:\nnum_samples = self.num_samples_observed_in_total\ntotal_batches_to_be_observed = (self.epochs - 1) * self.num_batches_per_epoch + \\\n            self.num_batches_in_last_epoch\ntotal_n_samples_to_be_observed = \\\n            self._training_plan.training_args()['batch_size'] * total_batches_to_be_observed\nnum_samples_max = total_n_samples_to_be_observed\nelse:\nnum_samples = self.num_samples_observed_in_epoch\nnum_samples_max = self._training_plan.training_args()['batch_size']*self.num_batches_in_this_epoch() if \\\n            self.cur_batch &lt; self.num_batches_in_this_epoch() else num_samples\nreturn num_samples, num_samples_max\n</code></pre>"},{"location":"developer/api/common/training_plans/#fedbiomed.common.training_plans._training_iterations.MiniBatchTrainingIterationsAccountant.should_log_this_batch","title":"<pre><code>should_log_this_batch()\n</code></pre>","text":"<p>Whether the current batch should be logged or not.</p> A batch shall be logged if at least one of the following conditions is True <ul> <li>the cumulative batch index is a multiple of the logging interval</li> <li>the dry_run condition was specified by the researcher</li> <li>it is the last batch of the epoch</li> <li>it is the first batch of the epoch</li> </ul> Source code in <code>fedbiomed/common/training_plans/_training_iterations.py</code> <pre><code>def should_log_this_batch(self) -&gt; bool:\n\"\"\"Whether the current batch should be logged or not.\n    A batch shall be logged if at least one of the following conditions is True:\n        - the cumulative batch index is a multiple of the logging interval\n        - the dry_run condition was specified by the researcher\n        - it is the last batch of the epoch\n        - it is the first batch of the epoch\n    \"\"\"\ncurrent_iter = (self.cur_epoch - 1) * self.num_batches_per_epoch + self.cur_batch\nreturn (current_iter % self._training_plan.training_args()['log_interval'] == 0 or\nself._training_plan.training_args()['dry_run'] or\nself.cur_batch &gt;= self.num_batches_in_this_epoch() or  # last batch\nself.cur_batch == 1)  # first batch\n</code></pre>"},{"location":"developer/api/common/utils/","title":"Utils","text":""},{"location":"developer/api/common/utils/#fedbiomed.common.utils","title":"fedbiomed.common.utils","text":"Module: <code>fedbiomed.common.utils</code>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils-attributes","title":"Attributes","text":""},{"location":"developer/api/common/utils/#fedbiomed.common.utils.CACHE_DIR","title":"CACHE_DIR     <code>module-attribute</code>","text":"<pre><code>CACHE_DIR = os.path.join(VAR_DIR, CACHE_FOLDER_NAME)\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.CONFIG_DIR","title":"CONFIG_DIR     <code>module-attribute</code>","text":"<pre><code>CONFIG_DIR = os.path.join(ROOT_DIR, CONFIG_FOLDER_NAME)\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.ROOT_DIR","title":"ROOT_DIR     <code>module-attribute</code>","text":"<pre><code>ROOT_DIR = _get_fedbiomed_root()\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.TMP_DIR","title":"TMP_DIR     <code>module-attribute</code>","text":"<pre><code>TMP_DIR = os.path.join(VAR_DIR, TMP_FOLDER_NAME)\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.VAR_DIR","title":"VAR_DIR     <code>module-attribute</code>","text":"<pre><code>VAR_DIR = os.path.join(ROOT_DIR, VAR_FOLDER_NAME)\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils-functions","title":"Functions","text":""},{"location":"developer/api/common/utils/#fedbiomed.common.utils.compute_dot_product","title":"<pre><code>compute_dot_product(model, params, device=None)\n</code></pre>","text":"<p>Compute the dot product between model and input parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>OrderedDict representing model state</p> required <code>params</code> <code>dict</code> <p>OrderedDict containing correction parameters</p> required <p>Returns:</p> Type Description <code>torch.tensor</code> <p>A tensor containing a single numerical value which is the dot product.</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def compute_dot_product(model: dict, params: dict, device: Optional[str] = None) -&gt; torch.tensor:\n\"\"\"Compute the dot product between model and input parameters.\n    Args:\n        model: OrderedDict representing model state\n        params: OrderedDict containing correction parameters\n    Returns:\n        A tensor containing a single numerical value which is the dot product.\n    \"\"\"\nmodel_p = model.values()\ncorrection_state = params.values()\nif device is None:\nif model_p:\ndevice = list(model_p)[0].device\nelse:\n# if device is not found, set it to `cpu`\ndevice = 'cpu'\ndot_prod = sum([torch.sum(m * torch.tensor(p).float().to(device)) for m, p in zip(model_p, correction_state)])\nreturn dot_prod\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.convert_iterator_to_list_of_python_floats","title":"<pre><code>convert_iterator_to_list_of_python_floats(iterator)\n</code></pre>","text":"<p>Converts numerical values of array-like object to float</p> <p>Parameters:</p> Name Type Description Default <code>iterator</code> <code>Iterator</code> <p>Array-like numeric object to convert numerics to float</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>Numerical elements as converted to List of floats</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def convert_iterator_to_list_of_python_floats(iterator: Iterator) -&gt; List[float]:\n\"\"\"Converts numerical values of array-like object to float\n    Args:\n        iterator: Array-like numeric object to convert numerics to float\n    Returns:\n        Numerical elements as converted to List of floats\n    \"\"\"\nif not isinstance(iterator, Iterable):\nraise FedbiomedError(f\"object {type(iterator)} is not iterable\")\nlist_of_floats = []\nif isinstance(iterator, dict):\n# specific processing for dictionaries\nfor val in iterator.values():\nlist_of_floats.append(convert_to_python_float(val))\nelse:\nfor it in iterator:\nlist_of_floats.append(convert_to_python_float(it))\nreturn list_of_floats\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.convert_to_python_float","title":"<pre><code>convert_to_python_float(value)\n</code></pre>","text":"<p>Convert numeric types to float</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[torch.Tensor, np.integer, np.floating, float, int]</code> <p>value to convert python type float</p> required <p>Returns:</p> Type Description <code>float</code> <p>Python float</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def convert_to_python_float(value: Union[torch.Tensor, np.integer, np.floating, float, int]) -&gt; float:\n\"\"\" Convert numeric types to float\n    Args:\n        value: value to convert python type float\n    Returns:\n        Python float\n    \"\"\"\nif not isinstance(value, (torch.Tensor, np.integer, np.floating, float, int)):\nraise FedbiomedError(f\"Converting {type(value)} to python to float is not supported.\")\n# if the result is a tensor, convert it back to numpy\nif isinstance(value, torch.Tensor):\nvalue = value.numpy()\nif isinstance(value, Iterable) and value.size &gt; 1:\nraise FedbiomedError(\"Can not convert array-type objects to float.\")\nreturn float(value)\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_all_existing_certificates","title":"<pre><code>get_all_existing_certificates()\n</code></pre>","text":"<p>Gets all existing certificates from Fed-BioMed <code>etc</code> directory.</p> <p>This method parse all available configs in <code>etc</code> directory.</p> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List of certificate objects that contain  component type as <code>component</code>, party id <code>id</code>, public key content</p> <code>List[Dict[str, str]]</code> <p>(not path)  as <code>certificate</code>.</p> Source code in <code>fedbiomed/common/utils/_config_utils.py</code> <pre><code>def get_all_existing_certificates() -&gt; List[Dict[str, str]]:\n\"\"\"Gets all existing certificates from Fed-BioMed `etc` directory.\n    This method parse all available configs in `etc` directory.\n    Returns:\n        List of certificate objects that contain  component type as `component`, party id `id`, public key content\n        (not path)  as `certificate`.\n    \"\"\"\nconfig_files = get_all_existing_config_files()\ncertificates = []\nfor config in config_files:\ncertificates.append(get_component_certificate_from_config(config))\nreturn certificates\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_all_existing_config_files","title":"<pre><code>get_all_existing_config_files()\n</code></pre>","text":"<p>Gets all existing config files from Fed-BioMed <code>etc</code> directory</p> Source code in <code>fedbiomed/common/utils/_config_utils.py</code> <pre><code>def get_all_existing_config_files():\n\"\"\"Gets all existing config files from Fed-BioMed `etc` directory\"\"\"\netc = os.path.join(ROOT_DIR, CONFIG_FOLDER_NAME, '')\nreturn [file for file in glob.glob(f\"{etc}*.ini\")]\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_class_source","title":"<pre><code>get_class_source(cls)\n</code></pre>","text":"<p>Get source of the class.</p> <p>It uses different methods for getting the class source based on shell type; IPython,Notebook shells or Python shell</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Callable</code> <p>The class to extract the source code from</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Source code of the given class</p> <p>Raises:</p> Type Description <code>FedbiomedError</code> <p>if argument is not a class</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def get_class_source(cls: Callable) -&gt; str:\n\"\"\"Get source of the class.\n    It uses different methods for getting the class source based on shell type; IPython,Notebook\n    shells or Python shell\n    Args:\n        cls: The class to extract the source code from\n    Returns:\n        str: Source code of the given class\n    Raises:\n        FedbiomedError: if argument is not a class\n    \"\"\"\nif not inspect.isclass(cls):\nraise FedbiomedError('The argument `cls` must be a python class')\n# Check ipython status\nstatus = is_ipython()\nif status:\nfile = get_ipython_class_file(cls)\ncodes = \"\".join(inspect.linecache.getlines(file))\nclass_code = extract_symbols(codes, cls.__name__)[0][0]\nreturn class_code\nelse:\nreturn inspect.getsource(cls)\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_component_certificate_from_config","title":"<pre><code>get_component_certificate_from_config(config_path)\n</code></pre>","text":"<p>Gets component certificate, id and component type by given config file path.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path where config file is located.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Certificate object that contains  component type as <code>component</code>, party id <code>id</code>, public key content (not path)  as <code>certificate</code></p> <p>Raises:</p> Type Description <code>FedbiomedError</code> <ul> <li>If config file does not contain <code>node_id</code> or <code>researcher_id</code> under <code>default</code> section.</li> <li>If config file does not contain <code>public_key</code> under <code>ssl</code> section.</li> <li>If certificate file is not found or not readable</li> </ul> Source code in <code>fedbiomed/common/utils/_config_utils.py</code> <pre><code>def get_component_certificate_from_config(\nconfig_path: str\n) -&gt; Dict[str, str]:\n\"\"\"Gets component certificate, id and component type by given config file path.\n    Args:\n        config_path: Path where config file is located.\n    Returns:\n        Certificate object that contains  component type as `component`, party id `id`, public key content\n            (not path)  as `certificate`\n    Raises:\n        FedbiomedError:\n            - If config file does not contain `node_id` or `researcher_id` under `default` section.\n            - If config file does not contain `public_key` under `ssl` section.\n            - If certificate file is not found or not readable\n    \"\"\"\nconfig = get_component_config(config_path)\ncomponent_id = config.get(\"default\", \"id\")\ncomponent_type = config.get(\"default\", \"component\")\nip = config.get(\"mpspdz\", \"mpspdz_ip\")\nport = config.get(\"mpspdz\", \"mpspdz_port\")\ncertificate_path = os.path.join(os.path.dirname(config_path), config.get(\"mpspdz\", \"public_key\"))\nif not os.path.isfile(certificate_path):\nraise FedbiomedError(f\"The certificate for component '{component_id}' not found in {certificate_path}\")\ntry:\nwith open(certificate_path, 'r') as file:\ncertificate = file.read()\nexcept Exception as e:\nraise FedbiomedError(f\"Error while reading certificate -&gt; {certificate_path}. Error: {e}\")\nreturn {\n\"party_id\": component_id,\n\"certificate\": certificate,\n\"ip\": ip,\n\"port\": port,\n\"component\": component_type\n}\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_component_config","title":"<pre><code>get_component_config(config_path)\n</code></pre>","text":"<p>Gets config object from given config path.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path where config file is stored.</p> required <p>Returns:</p> Type Description <code>configparser.ConfigParser</code> <p>Configuration object.</p> <p>Raises:</p> Type Description <code>FedbiomedError</code> <p>If config file is not readable or not existing.</p> Source code in <code>fedbiomed/common/utils/_config_utils.py</code> <pre><code>def get_component_config(\nconfig_path: str\n) -&gt; configparser.ConfigParser:\n\"\"\"Gets config object from given config path.\n    Args:\n        config_path: The path where config file is stored.\n    Returns:\n        Configuration object.\n    Raises:\n        FedbiomedError: If config file is not readable or not existing.\n    \"\"\"\nconfig = configparser.ConfigParser()\ntry:\nconfig.read(config_path)\nexcept Exception:\nraise FedbiomedError(f\"Can not read config file. Please make sure it is existing or it has valid format. \"\nf\"{config_path}\")\nreturn config\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_existing_component_db_names","title":"<pre><code>get_existing_component_db_names()\n</code></pre>","text":"<p>Gets DB_PATHs of all existing components in Fed-BioMed root</p> Source code in <code>fedbiomed/common/utils/_config_utils.py</code> <pre><code>def get_existing_component_db_names():\n\"\"\"Gets DB_PATHs of all existing components in Fed-BioMed root\"\"\"\nconfig_files = get_all_existing_config_files()\ndb_names = {}\nfor _config in config_files:\nconfig = get_component_config(_config)\ncomponent_id = config['default']['id']\ndb_name = f\"{DB_PREFIX}{component_id}\"\ndb_names = {**db_names, component_id: db_name}\nreturn db_names\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_ipython_class_file","title":"<pre><code>get_ipython_class_file(cls)\n</code></pre>","text":"<p>Get source file/cell-id of the class which is defined in ZMQInteractiveShell or TerminalInteractiveShell</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Callable</code> <p>Python class defined on the IPython kernel</p> required <p>Returns:</p> Type Description <code>str</code> <p>File path or id of Jupyter cell. On IPython's interactive shell, it returns cell ID</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def get_ipython_class_file(cls: Callable) -&gt; str:\n\"\"\"Get source file/cell-id of the class which is defined in ZMQInteractiveShell or TerminalInteractiveShell\n    Args:\n        cls: Python class defined on the IPython kernel\n    Returns:\n        File path or id of Jupyter cell. On IPython's interactive shell, it returns cell ID\n    \"\"\"\n# Lookup by parent module\nif hasattr(cls, '__module__'):\nobject_ = sys.modules.get(cls.__module__)\n# If module has `__file__` attribute\nif hasattr(object_, '__file__'):\nreturn object_.__file__\n# If parent module is __main__\nfor name, member in inspect.getmembers(cls):\nif inspect.isfunction(member) and cls.__qualname__ + '.' + member.__name__ == member.__qualname__:\nreturn inspect.getfile(member)\nelse:\nraise FedbiomedError(f'{cls} has no attribute `__module__`, source is not found.')\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.get_method_spec","title":"<pre><code>get_method_spec(method)\n</code></pre>","text":"<p>Helper to get argument specification</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>The function/method to extract argument specification from</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Specification of the method</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def get_method_spec(method: Callable) -&gt; dict:\n\"\"\" Helper to get argument specification\n    Args:\n        method: The function/method to extract argument specification from\n    Returns:\n         Specification of the method\n    \"\"\"\nmethod_spec = {}\nparameters = inspect.signature(method).parameters\nfor (key, val) in parameters.items():\nmethod_spec[key] = {\n'name': val.name,\n'default': None if val.default is inspect._empty else val.default,\n'annotation': None if val.default is inspect._empty else val.default\n}\nreturn method_spec\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.is_ipython","title":"<pre><code>is_ipython()\n</code></pre>","text":"<p>Function that checks whether the codes (function itself) is executed in ipython kernel or not</p> <p>Returns:</p> Type Description <code>bool</code> <p>True, if python interpreter is IPython</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def is_ipython() -&gt; bool:\n\"\"\"\n    Function that checks whether the codes (function itself) is executed in ipython kernel or not\n    Returns:\n        True, if python interpreter is IPython\n    \"\"\"\nipython_shells = ['ZMQInteractiveShell', 'TerminalInteractiveShell']\ntry:\nshell = get_ipython().__class__.__name__\nif shell in ipython_shells:\nreturn True\nelse:\nreturn False\nexcept NameError:\nreturn False\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.matching_parties_biprime","title":"<pre><code>matching_parties_biprime(context, parties)\n</code></pre>","text":"<p>Check if parties of given context are compatible with the parties of a secagg biprime element.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>dict</code> <p>context to be compared with the secagg biprime element parties</p> required <code>parties</code> <code>list</code> <p>the secagg biprime element parties</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if this context can be used with this element, False if not.</p> Source code in <code>fedbiomed/common/utils/_secagg_utils.py</code> <pre><code>def matching_parties_biprime(context: dict, parties: list) -&gt; bool:\n\"\"\"Check if parties of given context are compatible with the parties\n        of a secagg biprime element.\n        Args:\n            context: context to be compared with the secagg biprime element parties\n            parties: the secagg biprime element parties\n        Returns:\n            True if this context can be used with this element, False if not.\n    \"\"\"\n# Need to ensure that:\n# - either the existing element is not attached to specific parties (None)\n# - or existing element was established for the same parties or a superset of the parties\nreturn (\n# Commented tests can be assumed from calling functions\n#\n# isinstance(context, dict) and\n# 'parties' in context and \n# isinstance(parties, list) and\n(\ncontext['parties'] is None or (\n# isinstance(context['parties'], list) and\nset(parties).issubset(set(context['parties']))\n)))\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.matching_parties_servkey","title":"<pre><code>matching_parties_servkey(context, parties)\n</code></pre>","text":"<p>Check if parties of given context are compatible with the parties of a secagg servkey element.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>dict</code> <p>context to be compared with the secagg servkey element parties</p> required <code>parties</code> <code>list</code> <p>the secagg servkey element parties</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if this context can be used with this element, False if not.</p> Source code in <code>fedbiomed/common/utils/_secagg_utils.py</code> <pre><code>def matching_parties_servkey(context: dict, parties: list) -&gt; bool:\n\"\"\"Check if parties of given context are compatible with the parties\n        of a secagg servkey element.\n        Args:\n            context: context to be compared with the secagg servkey element parties\n            parties: the secagg servkey element parties\n        Returns:\n            True if this context can be used with this element, False if not.\n        \"\"\"\n# Need to ensure that:\n# - existing element was established for the same parties\n# - first party needs to be the same for both\n# - set of other parties needs to be the same for both (order can differ)\n#\n# eg: [ 'un', 'deux', 'trois' ] compatible with [ 'un', 'trois', 'deux' ]\n# but not with [ 'deux', 'un', 'trois' ]\nreturn (\n# Commented tests can be assumed from calling functions\n#\n# isinstance(context, dict) and\n# 'parties' in context and \n# isinstance(context['parties'], list) and\n# len(context['parties']) &gt;= 1 and\n# isinstance(parties, list) and\nparties[0] == context['parties'][0] and\nset(parties[1:]) == set(context['parties'][1:]))\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.raise_for_version_compatibility","title":"<pre><code>raise_for_version_compatibility(their_version, our_version, error_msg=None)\n</code></pre>","text":"<p>Check version compatibility and behave accordingly.</p> <p>Raises an exception if the versions are incompatible, otherwise outputs a warning or info message.</p> <p>Parameters:</p> Name Type Description Default <code>their_version</code> <code>Union[FBM_Component_Version, str]</code> <p>the version that we detected in the component</p> required <code>our_version</code> <code>Union[FBM_Component_Version, str]</code> <p>the version of the component within the current runtime</p> required <code>error_msg</code> <code>Optional[str]</code> <p>an optional error message. It may contain two %s placeholders which will be substituted with the values of their_version and our_version.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedVersionError</code> <p>if the versions are incompatible</p> Source code in <code>fedbiomed/common/utils/_versions.py</code> <pre><code>def raise_for_version_compatibility(their_version: Union[FBM_Component_Version, str],\nour_version: Union[FBM_Component_Version, str],\nerror_msg: Optional[str] = None) -&gt; None:\n\"\"\"Check version compatibility and behave accordingly.\n    Raises an exception if the versions are incompatible, otherwise outputs a warning or info message.\n    Args:\n        their_version: the version that we detected in the component\n        our_version: the version of the component within the current runtime\n        error_msg: an optional error message. It may contain two %s placeholders which will be substituted with\n            the values of their_version and our_version.\n    Raises:\n        FedbiomedVersionError: if the versions are incompatible\n    \"\"\"\nif isinstance(our_version, str):\nour_version = FBM_Component_Version(our_version)\nif isinstance(their_version, str):\ntheir_version = FBM_Component_Version(their_version)\nif not isinstance(our_version, FBM_Component_Version):\nmsg = f\"{ErrorNumbers.FB625.value}: Component version has incorrect type `our_version` type={str(type(our_version))} value={our_version}\"\nlogger.critical(msg)\nraise FedbiomedVersionError(msg)\nif not isinstance(their_version, FBM_Component_Version):\nmsg = f\"{ErrorNumbers.FB625.value}: Component version has incorrect type `their_version` type={str(type(their_version))} value={their_version}\"\nlogger.critical(msg)\nraise FedbiomedVersionError(msg)\nif our_version != their_version:\n# note: the checks below rely on the short-circuiting behaviour of the or operator\n# (e.g. when checking our_version.minor &lt; their_version.minor we have the guarantee that\n# our_version.major == their_version.major\nif our_version.major != their_version.major or \\\n                our_version.minor &lt; their_version.minor or \\\n                (our_version.minor == their_version.minor and our_version.micro &lt; their_version.micro):\nmsg = _create_msg_for_version_check(\nf\"{ErrorNumbers.FB625.value}: Found incompatible version %s, expected version %s\" if error_msg is None else error_msg,\ntheir_version,\nour_version\n)\nlogger.critical(msg)\nraise FedbiomedVersionError(msg)\nelse:\nmsg = _create_msg_for_version_check(\n\"Found version %s, expected version %s\",\ntheir_version,\nour_version\n)\nlogger.warning(msg)\n</code></pre>"},{"location":"developer/api/common/utils/#fedbiomed.common.utils.read_file","title":"<pre><code>read_file(path)\n</code></pre>","text":"<p>Read given file</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Path to file to be read</p> required <p>Raises:</p> Type Description <code>FedbiomedError</code> <p>If the file is not existing or readable.</p> Source code in <code>fedbiomed/common/utils/_utils.py</code> <pre><code>def read_file(path):\n\"\"\"Read given file\n    Args:\n        path: Path to file to be read\n    Raises:\n        FedbiomedError: If the file is not existing or readable.\n    \"\"\"\ntry:\nwith open(path, \"r\") as file:\ncontent = file.read()\nfile.close()\nexcept Exception as e:\nraise FedbiomedError(\nf\"Can not read file {path}. Error: {e}\"\n)\nelse:\nreturn content\n</code></pre>"},{"location":"developer/api/common/validator/","title":"Validator","text":""},{"location":"developer/api/common/validator/#fedbiomed.common.validator","title":"fedbiomed.common.validator","text":"Module: <code>fedbiomed.common.validator</code> <p>Provide Validator ans SchemeValidator classes for validating parameters against a set of validation rules.</p> <p>This module provides two \"validation\" classes and two Error classes (exceptions):</p> <p>Validator:</p> <p>This class manage a rulebook of rules which can afterwards be accessed by their (registered) name.</p> <p>Values can be checked against the rules.</p> <p>Typical example:</p> <pre><code>  def my_validation_funct( value ):\n      if some_python_code:\n          return False\n      else:\n          return True\n\n  v = Validator()\n  v.register( \"funky_name\", my_validation_funct)\n  v.register( \"float_type\", float)\n\n  val = 3.14\n  v.validate( val, \"funky_name\")\n  v.validate( val, \"float_type\")\n  v.validate( val, float)\n\n  v.validate( \"{ 'a': 1 }\", dict)\n  ...\n</code></pre> <p>SchemeValidator:</p> <p>This class provides json validation against a scheme describing the expected json content.</p> <p>The scheme needs to follow a specific format, which describes each allowed fields and their characteristics: - a list of associated validators to check against (aka Validator instances) - the field requirement (required on not) - a default value (which will be used if the field is required but not provided)</p> <p>A SchemeValidator is accepted by the Validator class.</p> <p>Typical example:</p> <pre><code>  # direct use\n  scheme = { \"a\" : { \"rules\" : [float], \"required\": True } }\n\n  sc = SchemeValidator(scheme)\n\n  value =  { \"a\": 3.14 }\n  sc.validate(value)\n\n\n  # use also the Validator class\n  v = Validator()\n\n  v.register( \"message_a\", sc )\n  v.validate( value, \"message_a\" )\n\n  # remark: all these lines are equivalent\n  v.register( \"message_a\", sc )\n  v.register( \"message_a\", SchemeValidator( scheme) )\n  v.register( \"message_a\", scheme )\n</code></pre> <p>RuleError:</p> <p>This error is raised then the provided value is badly specified.</p> <p>ValidateError:</p> <p>This error is raised then a value does not comply to defined rule(s)</p>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator-classes","title":"Classes","text":""},{"location":"developer/api/common/validator/#fedbiomed.common.validator.RuleError","title":"RuleError","text":"<p>           Bases: <code>ValidatorError</code></p> <p>Error raised then the rule is badly defined.</p>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.SchemeValidator","title":"SchemeValidator","text":"CLASS  <pre><code>SchemeValidator(scheme)\n</code></pre> <p>           Bases: <code>object</code></p> <p>Validation class for scheme (grammar) which describes a json content.</p> <p>this class uses Validator's class base functions</p> <p>it requires a json grammar as argument and validate it's again the requested json description scheme</p> <p>A valid json description is also a dictionary with the following grammar:</p> <pre><code>{\n  \"var_name\": {\n                \"rules\": [ validator1, validator2, ...] ,\n                \"default\": a_default_value,\n                \"required\": True/False\n              },\n  ...\n}\n</code></pre> <p>the \"rules\" field is mandatory \"default\" and \"required\" fields are optional.</p> <p>Example:</p> <p>This is a valid scheme: <pre><code>{ \"a\" : { \"rules\" : [float], \"required\": True } }\n</code></pre></p> <p>The following json complies to this scheme: <pre><code>{ \"a\": 3.14 }\n</code></pre></p> <p>The following does not: <pre><code>{ \"a\": True }\n{ \"b\": 3.14 }\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>scheme</code> <code>Dict[str, Dict]</code> <p>scheme to validate</p> required <p>Raises:</p> Type Description <code>RuleError</code> <p>if provided scheme is invalid</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def __init__(self, scheme: Dict[str, Dict]):\n\"\"\"\n    Create a SchemeValidator instance, and validate the scheme passed as input.\n    it requires a json grammar as argument and validate\n    it's again the requested json description scheme\n    A valid json description is also a dictionary\n    with the following grammar:\n    ```python\n    {\n      \"var_name\": {\n                    \"rules\": [ validator1, validator2, ...] ,\n                    \"default\": a_default_value,\n                    \"required\": True/False\n                  },\n      ...\n    }\n    ```\n    the \"rules\" field is mandatory\n    \"default\" and \"required\" fields are optional.\n    Example:\n    This is a valid scheme:\n    ```python\n    { \"a\" : { \"rules\" : [float], \"required\": True } }\n    ```\n    The following json complies to this scheme:\n    ```python\n    { \"a\": 3.14 }\n    ```\n    The following does not:\n    ```python\n    { \"a\": True }\n    { \"b\": 3.14 }\n    ```\n    Args:\n        scheme:     scheme to validate\n    Raises:\n        RuleError: if provided scheme is invalid\n    \"\"\"\nstatus = self.__validate_scheme(scheme)\nif isinstance(status, bool) and status:\nself._scheme = scheme\nself._is_valid = True\nelse:\nself._scheme = None   # type: ignore\nself._is_valid = False\nraise RuleError(f\"scheme is not valid: {status}\")\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.SchemeValidator-functions","title":"Functions","text":""},{"location":"developer/api/common/validator/#fedbiomed.common.validator.SchemeValidator.is_valid","title":"<pre><code>is_valid()\n</code></pre>","text":"<p>Status of the scheme passed at creation time.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if scheme is valid, False instead</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def is_valid(self) -&gt; bool:\n\"\"\"\n    Status of the scheme passed at creation time.\n    Returns:\n        True if scheme is valid, False instead\n    \"\"\"\nreturn (self._scheme is not None) or self._is_valid\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.SchemeValidator.populate_with_defaults","title":"<pre><code>populate_with_defaults(value, only_required=True)\n</code></pre>","text":"<p>Inject default values defined in the rule to a given dictionary.</p> <p>Parse the given json value and add default value if key was required but not provided. Of course, the default values must be provided in the scheme.</p> <p>Warning: this does not parse the result against the scheme. It has to be done by the user.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>dict</code> <p>a json data to verify/populate</p> required <code>only_required</code> <code>bool</code> <p>if True, only force required key. If False, update all            keys with default values in the scheme. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict</code> <p>(dict) a json populated with default values, returns an empty dict if something is wrong</p> <p>Raises:</p> Type Description <code>RuleError</code> <p>if scheme provided at init contains a required rules without default value</p> <code>ValidatorError</code> <p>if input value was not a dict</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def populate_with_defaults(self, value: Dict, only_required: bool = True) -&gt; Dict:\n\"\"\"\n    Inject default values defined in the rule to a given dictionary.\n    Parse the given json value and add default value if key was required\n    but not provided.\n    Of course, the default values must be provided in the scheme.\n    Warning: this does not parse the result against the scheme. It has\n    to be done by the user.\n    Args:\n        value (dict):   a json data to verify/populate\n        only_required (bool): if True, only force required key. If False, update all\n                       keys with default values in the scheme. Defaults to True.\n    Returns:\n        (dict) a json populated with default values, returns an empty dict if something is wrong\n    Raises:\n        RuleError: if scheme provided at init contains a required rules without default value\n        ValidatorError: if input value was not a dict\n    \"\"\"\nif not self.is_valid():  # pragma: no cover\nreturn {}\n# check the value against the scheme\nif isinstance(value, dict):\nresult = deepcopy(value)\nelse:\nraise ValidatorError(\"input value is not a dict\")\nfor k, v in self._scheme.items():\nif 'required' in v and v['required'] is True:\nif k in value:\nresult[k] = value[k]\nelse:\nif 'default' in v:\nresult[k] = v['default']\nelse:\nraise RuleError(f\"scheme does not define a default value for required key: {k}\")\nelse:\nif not only_required:\nif k in value:\nresult[k] = value[k]\nelse:\nif 'default' in v:\nresult[k] = v['default']\nreturn result\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.SchemeValidator.scheme","title":"<pre><code>scheme()\n</code></pre>","text":"<p>Scheme getter.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict]</code> <p>scheme passed at init if valid, None instead</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def scheme(self) -&gt; Dict[str, Dict]:\n\"\"\"\n    Scheme getter.\n    Returns:\n        scheme passed at __init__ if valid, None instead\n    \"\"\"\nreturn self._scheme\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.SchemeValidator.validate","title":"<pre><code>validate(value)\n</code></pre>","text":"<p>Validate a value against the scheme passed at creation time.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>dict</code> <p>value (dict) to validate against the scheme passed    at init</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if value is valid</p> <p>Raises:</p> Type Description <code>ValidateError</code> <p>if provided value is invalid</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def validate(self, value: Dict) -&gt; bool:\n\"\"\"\n    Validate a value against the scheme passed at creation time.\n    Args:\n         value (dict):  value (dict) to validate against the scheme passed\n                 at __init__\n    Returns:\n        True if value is valid\n    Raises:\n        ValidateError: if provided value is invalid\n    \"\"\"\n# TODO: raises error messages\n# or store error string in self._error and provide a error() method\nif not self.is_valid():  # pragma: no cover\nreturn False\nif not isinstance(value, dict):\nraise ValidateError(\"value is not a dict\")\n# check the value against the scheme\nfor k, v in self._scheme.items():\nif 'required' in v and v['required'] is True and k not in value:\nraise ValidateError(f\"{k} key is required\")\nfor k in value:\nif k not in self._scheme:\nraise ValidateError(f\"undefined key ({k}) in scheme\")\nfor hook in self._scheme[k]['rules']:\nif not Validator().validate(value[k], hook):\n# this should already have raised an error\nraise ValidateError(f\"invalid value ({value[k]}) for key: {k}\")  # pragma: nocover\nreturn True\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.ValidateError","title":"ValidateError","text":"<p>           Bases: <code>ValidatorError</code></p> <p>Error raised then validating a value against a rule.</p>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.Validator","title":"Validator","text":"CLASS  <pre><code>Validator()\n</code></pre> <p>           Bases: <code>object</code></p> <p>Container class for validation functions accessible via their names.</p> <p>this class: - manages a catalog of tuples  ( \"name\", validation_hook )   The validation_hook is validated at registration phase. - permit to validate a value against     - a (named) registered hook     - a direct validation hook passed as argument to validate()     - a SchemeValidator for json validation     - typechecking</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def __init__(self):\n\"\"\"\n    Create an instance of Validator. For now, nothing to do.\n    \"\"\"\npass\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.Validator-functions","title":"Functions","text":""},{"location":"developer/api/common/validator/#fedbiomed.common.validator.Validator.delete","title":"<pre><code>delete(rule)\n</code></pre>","text":"<p>Delete a rule from the rulebook.</p> <p>Parameters:</p> Name Type Description Default <code>rule</code> <code>str</code> <p>name (string) of a possibly registered hook</p> required Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def delete(self, rule: str) -&gt; None:\n\"\"\"\n    Delete a rule from the rulebook.\n    Args:\n        rule:   name (string) of a possibly registered hook\n    \"\"\"\nif rule in self._validation_rulebook:\ndel self._validation_rulebook[rule]\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.Validator.is_known_rule","title":"<pre><code>is_known_rule(rule)\n</code></pre>","text":"<p>Information about rule registration.</p> <p>Parameters:</p> Name Type Description Default <code>rule</code> <code>str</code> <p>name <code>str</code> of a possibly registered hook</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if rule is registered, False instead</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def is_known_rule(self, rule: str) -&gt; bool:\n\"\"\"\n    Information about rule registration.\n    Args:\n        rule:   name [`str`][str] of a possibly registered hook\n    Returns:\n        True if rule is registered, False instead\n    \"\"\"\nreturn rule in self._validation_rulebook\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.Validator.register","title":"<pre><code>register(rule, hook, override=False)\n</code></pre>","text":"<p>Add a rule/validation_function to the rulebook.</p> <p>if the rule (entry of the catalog) was already registered, it will be rejected, except if override is True</p> <p>Parameters:</p> Name Type Description Default <code>rule</code> <code>str</code> <p>registration name (string)</p> required <code>hook</code> <code>Any</code> <p>validation hook to register (the hook is checked against        the accepted hook types)</p> required <code>override</code> <code>bool</code> <p>if True, still register the rule even if it existed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if rule is accepted, False instead if rule exists and overrride is False</p> <p>Raises:</p> Type Description <code>RuleError</code> <p>if provided rule name or hook is invalid</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def register(self, rule: str, hook: Any, override: bool = False) -&gt; bool:\n\"\"\"\n    Add a rule/validation_function to the rulebook.\n    if the rule (entry of the catalog) was already registered,\n    it will be rejected, except if override is True\n    Args:\n        rule:      registration name (string)\n        hook:      validation hook to register (the hook is checked against\n                   the accepted hook types)\n        override:  if True, still register the rule even if it existed. Defaults to False.\n    Returns:\n        True if rule is accepted, False instead if rule exists and overrride is False\n    Raises:\n        RuleError: if provided rule name or hook is invalid\n    \"\"\"\nif not isinstance(rule, str):\nraise RuleError(\"rule name must be a string\")\nif not override and rule in self._validation_rulebook:\nsys.stdout.write(f\"WARNING - Validator: rule is already registered: {rule} \\n\")\nreturn False\nif not Validator._is_hook_type_valid(hook):\nraise RuleError(\"action associated to the rule is not allowed\")\n# hook is a dict, we transform it to a SchemeValidator\nif isinstance(hook, dict):\ntry:\nsv = SchemeValidator(hook)\nexcept RuleError as e:\nraise RuleError(f\"validator is an invalid dict: {e}\")\nhook = sv\n# rule description is valid -&gt; register it\nself._validation_rulebook[rule] = hook\nreturn True\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.Validator.rule","title":"<pre><code>rule(rule)\n</code></pre>","text":"<p>Return a presumably stored rule.</p> <p>Parameters:</p> Name Type Description Default <code>rule</code> <code>str</code> <p>name (<code>str</code>) of a possibly registered hook</p> required <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>the hook associated to the rule name if registered, None if not registered</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def rule(self, rule: str) -&gt; Union[str, None]:\n\"\"\"\n    Return a presumably stored rule.\n    Args:\n        rule:   name ([`str`][str]) of a possibly registered hook\n    Returns:\n        the hook associated to the rule name if registered, None if not registered\n    \"\"\"\nif rule in self._validation_rulebook:\nreturn self._validation_rulebook[rule]\nelse:\nreturn None\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.Validator.validate","title":"<pre><code>validate(value, rule, strict=True)\n</code></pre>","text":"<p>Validate a value against a validation rule.</p> <p>The rule may be one of: - (registered) rule - a provided function, - a simple type checking - a SchemeValidator</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>value to check</p> required <code>rule</code> <code>Any</code> <p>validation hook (registered name, typecheck, direct hook, ...)</p> required <code>strict</code> <code>bool</code> <p>Raises error if rule is not defined. Otherwise, prints a warning message.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if rule exists and value is compliant.</p> <p>Raises:</p> Type Description <code>ValidateError</code> <p>if provided value does not comply to the rule</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def validate(self, value: Any, rule: Any, strict: bool = True) -&gt; bool:\n\"\"\"\n    Validate a value against a validation rule.\n    The rule may be one of:\n    - (registered) rule\n    - a provided function,\n    - a simple type checking\n    - a SchemeValidator\n    Args:\n        value:   value to check\n        rule:    validation hook (registered name, typecheck, direct hook, ...)\n        strict:  Raises error if rule is not defined. Otherwise, prints a warning message.\n    Returns:\n        True if rule exists and value is compliant.\n    Raises:\n        ValidateError: if provided value does not comply to the rule\n    \"\"\"\n# rule is in the rulebook -&gt; execute the rule associated function\nif isinstance(rule, str) and rule in self._validation_rulebook:\nstatus, error = Validator._hook_execute(value,\nself._validation_rulebook[rule])\nif not status:\nraise ValidateError(error)\nreturn status\n# rule is an unknown string\nif isinstance(rule, str):\nif strict:\nraise ValidateError(f\"unknown rule: {rule}\")\nelse:\nsys.stdout.write(f\"WARNING - Validator(): unknown rule: {rule} \\n\")\nreturn True\n# consider the rule as a direct rule definition\nstatus, error = Validator._hook_execute(value, rule)\nif not status:\nraise ValidateError(error)\nreturn status\n</code></pre>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator.ValidatorError","title":"ValidatorError","text":"<p>           Bases: <code>Exception</code></p> <p>Top class of all Validator/SchemaValidator exception.</p>"},{"location":"developer/api/common/validator/#fedbiomed.common.validator-functions","title":"Functions","text":""},{"location":"developer/api/common/validator/#fedbiomed.common.validator.validator_decorator","title":"<pre><code>validator_decorator(func)\n</code></pre>","text":"<p>Ease the writing of validation function/hooks.</p> <p>The decorator catches the output of the validator hook and replace it with a tuple(<code>bool</code>, <code>str</code>) as expected by the Validator class.</p> <p>It creates an error message if not provided by the decorated function. The error message is forced to if the decorated function returns True</p> <p>If the validator is not used to decorate a validation function/hook, then the user feedback will be less precise for the end-user but this will not change the accuracy (True/False) of the feedback.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>function to decorate</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>decorated function</p> Source code in <code>fedbiomed/common/validator.py</code> <pre><code>def validator_decorator(func: Callable) -&gt; Callable:\n\"\"\"\n    Ease the writing of validation function/hooks.\n    The decorator catches the output of the validator hook and replace\n    it with a tuple([`bool`][bool], [`str`][str]) as expected by\n    the Validator class.\n    It creates an error message if not provided by the decorated function.\n    The error message is forced to if the decorated function returns True\n    If the validator is not used to decorate a validation function/hook,\n    then the user feedback will be less precise for the end-user but this\n    will not change the accuracy (True/False) of the feedback.\n    Args:\n       func:  function to decorate\n    Returns:\n       decorated function\n    \"\"\"\n@functools.wraps(func)\ndef wrapper(*args, **kwargs):\n# execute the wrapped function\nstatus = func(*args, **kwargs)\n# we expect a tuple [boolean, str] as output of func()\n# but we try to be resilient to function that simply return boolean\n# and create the tuple in case that it is not provided\nerror = f\"validation error then calling: {func.__name__}\"\nif isinstance(status, tuple):\nstatus, *error = status\nif status:\nreturn status, None\nelse:\nerror = ''.join(error)\nreturn status, error\nreturn wrapper\n</code></pre>"},{"location":"developer/api/common/versions/","title":"Versions","text":""},{"location":"developer/api/common/versions/#fedbiomed.common.utils._versions","title":"fedbiomed.common.utils._versions","text":"Module: <code>fedbiomed.common.utils._versions</code> <p>Utility functions for handling version compatibility.</p> <p>This module contains functions that help managing the versions of different Fed-BioMed components.</p> <p>See https://fedbiomed.org/latest/user-guide/deployment/versions for more information</p> <p>Instructions for developers</p> <p>If you make a change that changes the format/metadata/structure of one of the components below, you must update the version.</p> <p>Components concerned by versioning:</p> <ul> <li>config files (researcher and node)</li> <li>messaging protocol (i.e. structure of Message classes)</li> <li>breakpoints</li> </ul> <p>Instructions for updating the version</p> <ol> <li>bump the version in common/constants.py: if your change breaks backward compatibility you must increase the     major version, else the minor version. Micro versions are supported but their use is currently discouraged.</li> <li>Update the Changelog page with a short description     of your change, ideally including instructions on how to manually migrate from the previous version.</li> </ol>"},{"location":"developer/api/common/versions/#fedbiomed.common.utils._versions-attributes","title":"Attributes","text":""},{"location":"developer/api/common/versions/#fedbiomed.common.utils._versions.FBM_Component_Version","title":"FBM_Component_Version     <code>module-attribute</code>","text":"<pre><code>FBM_Component_Version = Version\n</code></pre> <p>The Type of objects representing version numbers in Fed-BioMed</p>"},{"location":"developer/api/common/versions/#fedbiomed.common.utils._versions-functions","title":"Functions","text":""},{"location":"developer/api/common/versions/#fedbiomed.common.utils._versions.raise_for_version_compatibility","title":"<pre><code>raise_for_version_compatibility(their_version, our_version, error_msg=None)\n</code></pre>","text":"<p>Check version compatibility and behave accordingly.</p> <p>Raises an exception if the versions are incompatible, otherwise outputs a warning or info message.</p> <p>Parameters:</p> Name Type Description Default <code>their_version</code> <code>Union[FBM_Component_Version, str]</code> <p>the version that we detected in the component</p> required <code>our_version</code> <code>Union[FBM_Component_Version, str]</code> <p>the version of the component within the current runtime</p> required <code>error_msg</code> <code>Optional[str]</code> <p>an optional error message. It may contain two %s placeholders which will be substituted with the values of their_version and our_version.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedVersionError</code> <p>if the versions are incompatible</p> Source code in <code>fedbiomed/common/utils/_versions.py</code> <pre><code>def raise_for_version_compatibility(their_version: Union[FBM_Component_Version, str],\nour_version: Union[FBM_Component_Version, str],\nerror_msg: Optional[str] = None) -&gt; None:\n\"\"\"Check version compatibility and behave accordingly.\n    Raises an exception if the versions are incompatible, otherwise outputs a warning or info message.\n    Args:\n        their_version: the version that we detected in the component\n        our_version: the version of the component within the current runtime\n        error_msg: an optional error message. It may contain two %s placeholders which will be substituted with\n            the values of their_version and our_version.\n    Raises:\n        FedbiomedVersionError: if the versions are incompatible\n    \"\"\"\nif isinstance(our_version, str):\nour_version = FBM_Component_Version(our_version)\nif isinstance(their_version, str):\ntheir_version = FBM_Component_Version(their_version)\nif not isinstance(our_version, FBM_Component_Version):\nmsg = f\"{ErrorNumbers.FB625.value}: Component version has incorrect type `our_version` type={str(type(our_version))} value={our_version}\"\nlogger.critical(msg)\nraise FedbiomedVersionError(msg)\nif not isinstance(their_version, FBM_Component_Version):\nmsg = f\"{ErrorNumbers.FB625.value}: Component version has incorrect type `their_version` type={str(type(their_version))} value={their_version}\"\nlogger.critical(msg)\nraise FedbiomedVersionError(msg)\nif our_version != their_version:\n# note: the checks below rely on the short-circuiting behaviour of the or operator\n# (e.g. when checking our_version.minor &lt; their_version.minor we have the guarantee that\n# our_version.major == their_version.major\nif our_version.major != their_version.major or \\\n                our_version.minor &lt; their_version.minor or \\\n                (our_version.minor == their_version.minor and our_version.micro &lt; their_version.micro):\nmsg = _create_msg_for_version_check(\nf\"{ErrorNumbers.FB625.value}: Found incompatible version %s, expected version %s\" if error_msg is None else error_msg,\ntheir_version,\nour_version\n)\nlogger.critical(msg)\nraise FedbiomedVersionError(msg)\nelse:\nmsg = _create_msg_for_version_check(\n\"Found version %s, expected version %s\",\ntheir_version,\nour_version\n)\nlogger.warning(msg)\n</code></pre>"},{"location":"developer/api/node/cli/","title":"CLI","text":""},{"location":"developer/api/node/cli/#fedbiomed.node.cli","title":"fedbiomed.node.cli","text":"Module: <code>fedbiomed.node.cli</code> <p>Command line user interface for the node component</p>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli-functions","title":"Functions","text":""},{"location":"developer/api/node/cli/#fedbiomed.node.cli.launch_cli","title":"<pre><code>launch_cli()\n</code></pre>","text":"<p>Parses command line input for the node component and launches node accordingly.</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def launch_cli():\n\"\"\"Parses command line input for the node component and launches node accordingly.\n    \"\"\"\ncli = CommonCLI()\ncli.set_environ(environ=environ)\ncli.initialize_certificate_parser()\ncli.initialize_create_configuration()\n# Register description for CLI\ncli.description = f'{__intro__}:A CLI app for fedbiomed researchers.'\ncli.parser.add_argument('-a', '--add',\nhelp='Add and configure local dataset (interactive)',\naction='store_true')\ncli.parser.add_argument('-am', '--add-mnist',\nhelp='Add MNIST local dataset (non-interactive)',\ntype=str, nargs='?', const='', metavar='path_mnist',\naction='store')\n# this option provides a json file describing the data to add\ncli.parser.add_argument('-adff', '--add-dataset-from-file',\nhelp='Add a local dataset described by json file (non-interactive)',\ntype=str,\naction='store')\ncli.parser.add_argument('-d', '--delete',\nhelp='Delete existing local dataset (interactive)',\naction='store_true')\ncli.parser.add_argument('-da', '--delete-all',\nhelp='Delete all existing local datasets (non interactive)',\naction='store_true')\ncli.parser.add_argument('-dm', '--delete-mnist',\nhelp='Delete existing MNIST local dataset (non-interactive)',\naction='store_true')\ncli.parser.add_argument('-l', '--list',\nhelp='List my shared_data',\naction='store_true')\ncli.parser.add_argument('-s', '--start-node',\nhelp='Start fedbiomed node.',\naction='store_true')\ncli.parser.add_argument('-rtp', '--register-training-plan',\nhelp='Register and approve a training plan from a local file.',\naction='store_true')\ncli.parser.add_argument('-atp', '--approve-training-plan',\nhelp='Approve a training plan (requested, default or registered)',\naction='store_true')\ncli.parser.add_argument('-rjtp', '--reject-training-plan',\nhelp='Reject a training plan (requested, default or registered)',\naction='store_true')\ncli.parser.add_argument('-utp', '--update-training-plan',\nhelp='Update training plan file (for a training plan registered from a local file)',\naction='store_true')\ncli.parser.add_argument('-dtp', '--delete-training-plan',\nhelp='Delete a training plan from database (not for default training plans)',\naction='store_true')\ncli.parser.add_argument('-ltps', '--list-training-plans',\nhelp='List all training plans (requested, default or registered)',\naction='store_true')\ncli.parser.add_argument('-vtp', '--view-training-plan',\nhelp='View a training plan source code (requested, default or registered)',\naction='store_true')\ncli.parser.add_argument('-g', '--gpu',\nhelp='Use of a GPU device, if any available (default: dont use GPU)',\naction='store_true')\ncli.parser.add_argument('-gn', '--gpu-num',\nhelp='Use GPU device with the specified number instead of default device, if available',\ntype=int,\naction='store')\ncli.parser.add_argument('-go', '--gpu-only',\nhelp='Force use of a GPU device, if any available, even if researcher doesnt ' +\n'request it (default: dont use GPU)',\naction='store_true')\nprint(__intro__)\nprint('\\t- \ud83c\udd94 Your node ID:', environ['NODE_ID'], '\\n')\n# Parse CLI arguments after the arguments are ready\ncli.parse_args()\nif cli.arguments.add:\nadd_database()\nelif cli.arguments.add_mnist is not None:\nadd_database(interactive=False, path=cli.arguments.add_mnist)\nelif cli.arguments.add_dataset_from_file is not None:\nprint(\"Dataset description file provided: adding these data\")\ntry:\nwith open(cli.arguments.add_dataset_from_file) as json_file:\ndata = json.load(json_file)\nexcept:\nlogger.critical(\"cannot read dataset json file: \" + cli.arguments.add_dataset_from_file)\nsys.exit(-1)\n# verify that json file is complete\nfor k in [\"path\", \"data_type\", \"description\", \"tags\", \"name\"]:\nif k not in data:\nlogger.critical(\"dataset json file corrupted: \" + cli.arguments.add_dataset_from_file)\n# dataset path can be defined:\n# - as an absolute path -&gt; take it as it is\n# - as a relative path  -&gt; add the ROOT_DIR in front of it\n# - using an OS environment variable -&gt; transform it\n#\nelements = data[\"path\"].split(os.path.sep)\nif elements[0].startswith(\"$\"):\n# expand OS environment variable\nvar = elements[0][1:]\nif var in os.environ:\nvar = os.environ[var]\nelements[0] = var\nelse:\nlogger.info(\"Unknown env var: \" + var)\nelements[0] = \"\"\nelif elements[0]:\n# p is relative (does not start with /)\n# prepend with topdir\nelements = [environ[\"ROOT_DIR\"]] + elements\n# rebuild the path with these (eventually) new elements\ndata[\"path\"] = os.path.join(os.path.sep, *elements)\n# add the dataset to local database (not interactive)\nadd_database(interactive=False,\npath=data[\"path\"],\ndata_type=data[\"data_type\"],\ndescription=data[\"description\"],\ntags=data[\"tags\"],\nname=data[\"name\"],\ndataset_parameters=data.get(\"dataset_parameters\")\n)\nelif cli.arguments.list:\nprint('Listing your data available')\ndata = dataset_manager.list_my_data(verbose=True)\nif len(data) == 0:\nprint('No data has been set up.')\nelif cli.arguments.delete:\ndelete_database()\nelif cli.arguments.delete_all:\ndelete_all_database()\nelif cli.arguments.delete_mnist:\ndelete_database(interactive=False)\nelif cli.arguments.register_training_plan:\nregister_training_plan()\nelif cli.arguments.approve_training_plan:\napprove_training_plan()\nelif cli.arguments.reject_training_plan:\nreject_training_plan()\nelif cli.arguments.update_training_plan:\nupdate_training_plan()\nelif cli.arguments.delete_training_plan:\ndelete_training_plan()\nelif cli.arguments.list_training_plans:\ntp_security_manager.list_training_plans(verbose=True)\nelif cli.arguments.view_training_plan:\nview_training_plan()\nelif cli.arguments.start_node:\n# convert to node arguments structure format expected in Round()\nnode_args = {\n'gpu': (cli.arguments.gpu_num is not None) or (cli.arguments.gpu is True) or\n(cli.arguments.gpu_only is True),\n'gpu_num': cli.arguments.gpu_num,\n'gpu_only': (cli.arguments.gpu_only is True)\n}\nlaunch_node(node_args)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.launch_node","title":"<pre><code>launch_node(node_args=None)\n</code></pre>","text":"<p>Launches a node in a separate process.</p> <p>Process ends when user triggers a KeyboardInterrupt exception (CTRL+C).</p> <p>Parameters:</p> Name Type Description Default <code>node_args</code> <code>Union[dict, None]</code> <p>Command line arguments for node See <code>Round()</code> for details.</p> <code>None</code> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def launch_node(node_args: Union[dict, None] = None):\n\"\"\"Launches a node in a separate process.\n    Process ends when user triggers a KeyboardInterrupt exception (CTRL+C).\n    Args:\n        node_args: Command line arguments for node\n            See `Round()` for details.\n    \"\"\"\np = Process(target=manage_node, name='node-' + environ['NODE_ID'], args=(node_args,))\np.daemon = True\np.start()\nlogger.info(\"Node started as process with pid = \" + str(p.pid))\ntry:\nprint('To stop press Ctrl + C.')\np.join()\nexcept KeyboardInterrupt:\np.terminate()\n# give time to the node to send a MQTT message\ntime.sleep(1)\nwhile p.is_alive():\nlogger.info(\"Terminating process id =\" + str(p.pid))\ntime.sleep(1)\n# (above) p.exitcode returns None if not finished yet\nlogger.info('Exited with code ' + str(p.exitcode))\nsys.exit(0)\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.main","title":"<pre><code>main()\n</code></pre>","text":"<p>Entry point for the node.</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def main():\n\"\"\"Entry point for the node.\n    \"\"\"\ntry:\nlaunch_cli()\nexcept KeyboardInterrupt:\n# send error message to researcher via logger.error()\nlogger.critical('Operation cancelled by user.')\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.manage_node","title":"<pre><code>manage_node(node_args=None)\n</code></pre>","text":"<p>Runs the node component and blocks until the node terminates.</p> <p>Intended to be launched by the node in a separate process/thread.</p> <p>Instantiates <code>Node</code> and <code>DatasetManager</code> object, start exchaning messages with the researcher via the <code>Node</code>, passes control to the <code>Node</code>.</p> <p>Parameters:</p> Name Type Description Default <code>node_args</code> <code>Union[dict, None]</code> <p>command line arguments for node. See <code>Round()</code> for details.</p> <code>None</code> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def manage_node(node_args: Union[dict, None] = None):\n\"\"\"Runs the node component and blocks until the node terminates.\n    Intended to be launched by the node in a separate process/thread.\n    Instantiates `Node` and `DatasetManager` object, start exchaning\n    messages with the researcher via the `Node`, passes control to the `Node`.\n    Args:\n        node_args: command line arguments for node.\n            See `Round()` for details.\n    \"\"\"\nglobal node\ntry:\nsignal.signal(signal.SIGTERM, node_signal_handler)\nlogger.info('Launching node...')\n# Register default training plans and update hashes\nif environ[\"TRAINING_PLAN_APPROVAL\"]:\n# This methods updates hashes if hashing algorithm has changed\ntp_security_manager.check_hashes_for_registered_training_plans()\nif environ[\"ALLOW_DEFAULT_TRAINING_PLANS\"]:\nlogger.info('Loading default training plans')\ntp_security_manager.register_update_default_training_plans()\nelse:\nlogger.warning('Training plan approval for train request is not activated. ' +\n'This might cause security problems. Please, consider to enable training plan approval.')\nlogger.info('Starting communication channel with network')\nnode = Node(dataset_manager=dataset_manager,\ntp_security_manager=tp_security_manager,\nnode_args=node_args)\nnode.start_messaging(block=False)\nlogger.info('Starting task manager')\nnode.task_manager()  # handling training tasks in queue\nexcept FedbiomedError:\nlogger.critical(\"Node stopped.\")\n# we may add extra information for the user depending on the error\nexcept Exception as e:\n# must send info to the researcher (no mqqt should be handled by the previous FedbiomedError)\nnode.send_error(ErrorNumbers.FB300, extra_msg=\"Error = \" + str(e))\nlogger.critical(\"Node stopped.\")\nfinally:\n# this is triggered by the signal.SIGTERM handler SystemExit(0)\n#\n# cleaning staff should be done here\npass\n</code></pre>"},{"location":"developer/api/node/cli/#fedbiomed.node.cli.node_signal_handler","title":"<pre><code>node_signal_handler(signum, frame)\n</code></pre>","text":"<p>Signal handler that terminates the process.</p> <p>Parameters:</p> Name Type Description Default <code>signum</code> <code>int</code> <p>Signal number received.</p> required <code>frame</code> <code>Union[FrameType, None]</code> <p>Frame object received. Currently unused</p> required <p>Raises:</p> Type Description <code>SystemExit</code> <p>Always raised.</p> Source code in <code>fedbiomed/node/cli.py</code> <pre><code>def node_signal_handler(signum: int, frame: Union[FrameType, None]):\n\"\"\"Signal handler that terminates the process.\n    Args:\n        signum: Signal number received.\n        frame: Frame object received. Currently unused\n    Raises:\n       SystemExit: Always raised.\n    \"\"\"\n# get the (running) Node object\nglobal node\nif node:\nnode.send_error(ErrorNumbers.FB312)\nelse:\nlogger.error(\"Cannot send error message to researcher (node not initialized yet)\")\nlogger.critical(\"Node stopped in signal_handler, probably by user decision (Ctrl C)\")\ntime.sleep(1)\nsys.exit(signum)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/","title":"DatasetManager","text":""},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager","title":"fedbiomed.node.dataset_manager","text":"Module: <code>fedbiomed.node.dataset_manager</code> <p>Interfaces with the node component database.</p>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager-classes","title":"Classes","text":""},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager","title":"DatasetManager","text":"CLASS  <pre><code>DatasetManager()\n</code></pre> <p>Interfaces with the node component database.</p> <p>Facility for storing data, retrieving data and getting data info for the node. Currently uses TinyDB.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def __init__(self):\n\"\"\"Constructor of the class.\n    \"\"\"\nself._db = TinyDB(environ['DB_PATH'])\nself._database = Query()\n# don't use DB read cache to ensure coherence\n# (eg when mixing CLI commands with a GUI session)\nself._dataset_table = self._db.table(name='Datasets', cache_size=0)\nself._dlp_table = self._db.table(name='Data_Loading_Plans', cache_size=0)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager-functions","title":"Functions","text":""},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.add_database","title":"<pre><code>add_database(name, data_type, tags, description, path=None, dataset_id=None, dataset_parameters=None, data_loading_plan=None, save_dlp=True)\n</code></pre>","text":"<p>Adds a new dataset contained in a file to node's database.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the dataset</p> required <code>data_type</code> <code>str</code> <p>File extension/format of the dataset (*.csv, images, ...)</p> required <code>tags</code> <code>Union[tuple, list]</code> <p>Tags of the dataset.</p> required <code>description</code> <code>str</code> <p>Human readable description of the dataset.</p> required <code>path</code> <code>Optional[str]</code> <p>Path to the dataset. Defaults to None.</p> <code>None</code> <code>dataset_id</code> <code>Optional[str]</code> <p>Id of the dataset. Defaults to None.</p> <code>None</code> <code>dataset_parameters</code> <code>Optional[dict]</code> <p>a dictionary of additional (customized) parameters, or None</p> <code>None</code> <code>data_loading_plan</code> <code>Optional[DataLoadingPlan]</code> <p>a DataLoadingPlan to be linked to this dataset, or None</p> <code>None</code> <code>save_dlp</code> <code>bool</code> <p>if True, save the <code>data_loading_plan</code></p> <code>True</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p><code>data_type</code> is not supported.</p> <code>FedbiomedDatasetManagerError</code> <p>path does not exist or dataset was not saved properly.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def add_database(self,\nname: str,\ndata_type: str,\ntags: Union[tuple, list],\ndescription: str,\npath: Optional[str] = None,\ndataset_id: Optional[str] = None,\ndataset_parameters : Optional[dict] = None,\ndata_loading_plan: Optional[DataLoadingPlan] = None,\nsave_dlp: bool = True):\n\"\"\"Adds a new dataset contained in a file to node's database.\n    Args:\n        name: Name of the dataset\n        data_type: File extension/format of the\n            dataset (*.csv, images, ...)\n        tags: Tags of the dataset.\n        description: Human readable description of the dataset.\n        path: Path to the dataset. Defaults to None.\n        dataset_id: Id of the dataset. Defaults to None.\n        dataset_parameters: a dictionary of additional (customized) parameters, or None\n        data_loading_plan: a DataLoadingPlan to be linked to this dataset, or None\n        save_dlp: if True, save the `data_loading_plan`\n    Raises:\n        NotImplementedError: `data_type` is not supported.\n        FedbiomedDatasetManagerError: path does not exist or dataset was not saved properly.\n    \"\"\"\n# Accept tilde as home folder\nif path is not None:\npath = os.path.expanduser(path)\n# Check that there are not existing dataset with conflicting tags\nconflicting = self.search_conflicting_tags(tags)\nif len(conflicting) &gt; 0:\nmsg = f\"{ErrorNumbers.FB322.value}, one or more registered dataset has conflicting tags: \" \\\n            f\" {' '.join([ c['name'] for c in conflicting ])}\"\nlogger.critical(msg)\nraise FedbiomedDatasetManagerError(msg)\ndtypes = []  # empty list for Image datasets\ndata_types = ['csv', 'default', 'mednist', 'images', 'medical-folder', 'flamby']\nif data_type not in data_types:\nraise NotImplementedError(f'Data type {data_type} is not'\n' a compatible data type. '\nf'Compatible data types are: {data_types}')\nelif data_type == 'flamby':\n# check that data loading plan is present and well formed\nif data_loading_plan is None or \\\n                FlambyLoadingBlockTypes.FLAMBY_DATASET_METADATA not in data_loading_plan:\nmsg = f\"{ErrorNumbers.FB316.value}. A DataLoadingPlan containing \" \\\n                  f\"{FlambyLoadingBlockTypes.FLAMBY_DATASET_METADATA.value} is required for adding a FLamby dataset \" \\\n                  f\"to the database.\"\nlogger.critical(msg)\nraise FedbiomedDatasetManagerError(msg)\n# initialize a dataset and link to the flamby data. If all goes well, compute shape.\ntry:\ndataset = FlambyDataset()\ndataset.set_dlp(data_loading_plan)  # initializes fed_class as a side effect\nexcept FedbiomedError as e:\nraise FedbiomedDatasetManagerError(f\"Can not create FLamby dataset. {e}\")\nelse:\nshape = dataset.shape()\nif data_type == 'default':\nassert os.path.isdir(path), f'Folder {path} for Default Dataset does not exist.'\nshape = self.load_default_database(name, path)\nelif data_type == 'mednist':\nassert os.path.isdir(path), f'Folder {path} for MedNIST Dataset does not exist.'\nshape = self.load_mednist_database(path)\npath = os.path.join(path, 'MedNIST')\nelif data_type == 'csv':\nassert os.path.isfile(path), f'Path provided ({path}) does not correspond to a CSV file.'\ndataset = self.load_csv_dataset(path)\nshape = dataset.shape\ndtypes = self.get_csv_data_types(dataset)\nelif data_type == 'images':\nassert os.path.isdir(path), f'Folder {path} for Images Dataset does not exist.'\nshape = self.load_images_dataset(path)\nelif data_type == 'medical-folder':\nif not os.path.isdir(path):\nraise FedbiomedDatasetManagerError(f'Folder {path} for Medical Folder Dataset does not exist.')\nif \"tabular_file\" not in dataset_parameters:\nlogger.info(\"Medical Folder Dataset will be loaded without reference/demographics data.\")\nelse:\nif not os.path.isfile(dataset_parameters['tabular_file']):\nraise FedbiomedDatasetManagerError(f'Path {dataset_parameters[\"tabular_file\"]} does not '\nf'correspond a file.')\nif \"index_col\" not in dataset_parameters:\nraise FedbiomedDatasetManagerError('Index column is not provided')\ntry:\n# load using the MedicalFolderController to ensure all available modalities are inspected\ncontroller = MedicalFolderController(root=path)\nif data_loading_plan is not None:\ncontroller.set_dlp(data_loading_plan)\ndataset = controller.load_MedicalFolder(tabular_file=dataset_parameters.get('tabular_file', None),\nindex_col=dataset_parameters.get('index_col', None))\nexcept FedbiomedError as e:\nraise FedbiomedDatasetManagerError(f\"Can not create Medical Folder dataset. {e}\")\nelse:\nshape = dataset.shape()\n# try to read one sample and raise if it doesn't work\ntry:\n_ = dataset.get_nontransformed_item(0)\nexcept Exception as e:\nraise FedbiomedDatasetManagerError(f'Medical Folder Dataset was not saved properly and '\nf'cannot be read. {e}')\nif not dataset_id:\ndataset_id = 'dataset_' + str(uuid.uuid4())\nnew_database = dict(name=name, data_type=data_type, tags=tags,\ndescription=description, shape=shape,\npath=path, dataset_id=dataset_id, dtypes=dtypes,\ndataset_parameters=dataset_parameters)\nif save_dlp:\ndlp_id = self.save_data_loading_plan(data_loading_plan)\nelif isinstance(data_loading_plan, DataLoadingPlan):\ndlp_id = data_loading_plan.dlp_id\nelse:\ndlp_id = None\nif dlp_id is not None:\nnew_database['dlp_id'] = dlp_id\nself._dataset_table.insert(new_database)\nreturn dataset_id\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.get_by_id","title":"<pre><code>get_by_id(dataset_id)\n</code></pre>","text":"<p>Searches for a dataset with given dataset_id.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>A dataset id</p> required <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>A <code>dict</code> containing the dataset's description if a dataset with this <code>dataset_id</code></p> <code>Union[dict, None]</code> <p>exists in the database. <code>None</code> if no such dataset exists in the database.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def get_by_id(self, dataset_id: str) -&gt; Union[dict, None]:\n\"\"\"Searches for a dataset with given dataset_id.\n    Args:\n        dataset_id:  A dataset id\n    Returns:\n        A `dict` containing the dataset's description if a dataset with this `dataset_id`\n        exists in the database. `None` if no such dataset exists in the database. \n    \"\"\"\nresult = self._dataset_table.get(self._database.dataset_id == dataset_id)\nreturn result\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.get_csv_data_types","title":"<pre><code>get_csv_data_types(dataset)\n</code></pre>","text":"<p>Gets data types of each variable in dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>pd.DataFrame</code> <p>A Pandas dataset.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings containing data types.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def get_csv_data_types(self, dataset: pd.DataFrame) -&gt; List[str]:\n\"\"\"Gets data types of each variable in dataset.\n    Args:\n        dataset: A Pandas dataset.\n    Returns:\n        A list of strings containing data types.\n    \"\"\"\ntypes = [str(t) for t in dataset.dtypes]\nreturn types\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.get_data_loading_blocks_by_ids","title":"<pre><code>get_data_loading_blocks_by_ids(dlb_ids)\n</code></pre>","text":"<p>Search for a list of DataLoadingBlockTypes, each corresponding to one given id.</p> <p>Note that in case of conflicting ids (which should not happen), this function will silently return a random one with the sought id.</p> <p>DataLoadingBlock IDs always start with 'serialized_data_loading_block_' and should be unique in the database.</p> <p>Parameters:</p> Name Type Description Default <code>dlb_ids</code> <code>List[str]</code> <p>(List[str]) a list of DataLoadingBlock IDs</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>A list of dictionaries, each one containing the DataLoadingBlock metadata corresponding to one given id.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def get_data_loading_blocks_by_ids(self, dlb_ids: List[str]) -&gt; List[dict]:\n\"\"\"Search for a list of DataLoadingBlockTypes, each corresponding to one given id.\n    Note that in case of conflicting ids (which should not happen), this function will silently return a random\n    one with the sought id.\n    DataLoadingBlock IDs always start with 'serialized_data_loading_block_' and should be unique in the database.\n    Args:\n        dlb_ids: (List[str]) a list of DataLoadingBlock IDs\n    Returns:\n        A list of dictionaries, each one containing the DataLoadingBlock metadata corresponding to one given id.\n    \"\"\"\nreturn self._dlp_table.search(self._database.dlb_id.one_of(dlb_ids))\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.get_dlp_by_id","title":"<pre><code>get_dlp_by_id(dlp_id)\n</code></pre>","text":"<p>Search for a DataLoadingPlan with a given id.</p> <p>Note that in case of conflicting ids (which should not happen), this function will silently return a random one with the sought id.</p> <p>DataLoadingPlan IDs always start with 'dlp_' and should be unique in the database.</p> <p>Parameters:</p> Name Type Description Default <code>dlp_id</code> <code>str</code> <p>(str) the DataLoadingPlan id</p> required <p>Returns:</p> Type Description <code>Tuple[dict, List[dict]]</code> <p>A Tuple containing a dictionary with the DataLoadingPlan metadata corresponding to the given id.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def get_dlp_by_id(self, dlp_id: str) -&gt; Tuple[dict, List[dict]]:\n\"\"\"Search for a DataLoadingPlan with a given id.\n    Note that in case of conflicting ids (which should not happen), this function will silently return a random\n    one with the sought id.\n    DataLoadingPlan IDs always start with 'dlp_' and should be unique in the database.\n    Args:\n        dlp_id: (str) the DataLoadingPlan id\n    Returns:\n        A Tuple containing a dictionary with the DataLoadingPlan metadata corresponding to the given id.\n    \"\"\"\ndlp_metadata = self._dlp_table.get(self._database.dlp_id == dlp_id)\nreturn dlp_metadata, self._dlp_table.search(\nself._database.dlb_id.one_of(dlp_metadata['loading_blocks'].values()))\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.get_torch_dataset_shape","title":"<pre><code>get_torch_dataset_shape(dataset)\n</code></pre>","text":"<p>Gets info about dataset shape.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>torch.utils.data.Dataset</code> <p>A Pytorch dataset</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>A list of int containing [, ]. Example for MNIST: [60000, 1, 28, 28], where =60000 and =1, 28, 28 Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def get_torch_dataset_shape(self, dataset: torch.utils.data.Dataset) -&gt; List[int]:\n\"\"\"Gets info about dataset shape.\n    Args:\n        dataset: A Pytorch dataset\n    Returns:\n        A list of int containing\n            [&lt;nb_of_data&gt;, &lt;dimension_of_first_input_data&gt;].\n            Example for MNIST: [60000, 1, 28, 28], where &lt;nb_of_data&gt;=60000\n            and &lt;dimension_of_first_input_data&gt;=1, 28, 28\n    \"\"\"\nreturn [len(dataset)] + list(dataset[0][0].shape)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.list_dlp","title":"<pre><code>list_dlp(target_dataset_type=None)\n</code></pre>","text":"<p>Return all existing DataLoadingPlans.</p> <p>Parameters:</p> Name Type Description Default <code>target_dataset_type</code> <code>Optional[str]</code> <p>(str or None) if specified, return only dlps matching the requested target type.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[dict]</code> <p>An array of dict, each dict is a DataLoadingPlan</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def list_dlp(self, target_dataset_type: Optional[str] = None) -&gt; List[dict]:\n\"\"\"Return all existing DataLoadingPlans.\n    Args:\n        target_dataset_type: (str or None) if specified, return only dlps matching the requested target type.\n    Returns:\n        An array of dict, each dict is a DataLoadingPlan\n    \"\"\"\nif target_dataset_type is not None:\nif not isinstance(target_dataset_type, str):\nraise FedbiomedDatasetManagerError(f\"Wrong input type for target_dataset_type. \"\nf\"Expected str, got {type(target_dataset_type)} instead.\")\nif target_dataset_type not in [t.value for t in DatasetTypes]:\nraise FedbiomedDatasetManagerError(\"target_dataset_type should be of the values defined in \"\n\"fedbiomed.common.constants.DatasetTypes\")\ndlps = self._dlp_table.search(\n(self._database.dlp_id.exists()) &amp;\n(self._database.dlp_name.exists()) &amp;\n(self._database.target_dataset_type == target_dataset_type))\nelse:\ndlps = self._dlp_table.search(\n(self._database.dlp_id.exists()) &amp; (self._database.dlp_name.exists()))\nreturn [dict(dlp) for dlp in dlps]\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.list_my_data","title":"<pre><code>list_my_data(verbose=True)\n</code></pre>","text":"<p>Lists all datasets on the node.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>Give verbose output. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[dict]</code> <p>All datasets in the node's database.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def list_my_data(self, verbose: bool = True) -&gt; List[dict]:\n\"\"\"Lists all datasets on the node.\n    Args:\n        verbose: Give verbose output. Defaults to True.\n    Returns:\n        All datasets in the node's database.\n    \"\"\"\nmy_data = self._dataset_table.all()\n# Do not display dtypes\nfor doc in my_data:\ndoc.pop('dtypes')\nif verbose:\nprint(tabulate(my_data, headers='keys'))\nreturn my_data\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.load_as_dataloader","title":"<pre><code>load_as_dataloader(dataset)\n</code></pre>","text":"<p>Loads content of an image dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>dict</code> <p>Description of the dataset.</p> required <p>Returns:</p> Type Description <code>torch.utils.data.Dataset</code> <p>Content of the dataset.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def load_as_dataloader(self, dataset: dict) -&gt; torch.utils.data.Dataset:\n\"\"\"Loads content of an image dataset.\n    Args:\n        dataset: Description of the dataset.\n    Returns:\n        Content of the dataset.\n    \"\"\"\nname = dataset['data_type']\nif name == 'default':\nreturn self.load_default_database(name=dataset['name'],\npath=dataset['path'],\nas_dataset=True)\nelif name == 'images':\nreturn self.load_images_dataset(folder_path=dataset['path'],\nas_dataset=True)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.load_csv_dataset","title":"<pre><code>load_csv_dataset(path)\n</code></pre>","text":"<p>Loads a CSV dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the CSV file.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Pandas DataFrame with the content of the file.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def load_csv_dataset(self, path: str) -&gt; pd.DataFrame:\n\"\"\"Loads a CSV dataset.\n    Args:\n        path: Path to the CSV file.\n    Returns:\n        Pandas DataFrame with the content of the file.\n    \"\"\"\nreturn self.read_csv(path)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.load_data","title":"<pre><code>load_data(tags, mode)\n</code></pre>","text":"<p>Loads content of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Union[tuple, list]</code> <p>Tags describing the dataset to load.</p> required <code>mode</code> <code>str</code> <p>Return format for the dataset content.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p><code>mode</code> is not implemented yet.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Content of the dataset. Its type depends on the <code>mode</code> and dataset.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def load_data(self, tags: Union[tuple, list], mode: str) -&gt; Any:\n\"\"\"Loads content of a dataset.\n    Args:\n        tags: Tags describing the dataset to load.\n        mode: Return format for the dataset content.\n    Raises:\n        NotImplementedError: `mode` is not implemented yet.\n    Returns:\n        Content of the dataset. Its type depends on the `mode` and dataset.\n    \"\"\"\n# Verify is mode is available\nmode = mode.lower()\nmodes = ['pandas', 'torch_dataset', 'torch_tensor', 'numpy']\nif mode not in modes:\nraise NotImplementedError(f'Data mode `{mode}` was not found.'\nf' Data modes available: {modes}')\n# Look for dataset in database\ndataset = self.search_by_tags(tags)[0]\nprint(dataset)\nassert len(dataset) &gt; 0, f'Dataset with tags {tags} was not found.'\ndataset_path = dataset['path']\n# If path is a file, you will aim to read it with\nif os.path.isfile(dataset_path):\ndf = self.read_csv(dataset_path, index_col=0)\n# Load data as requested\nif mode == 'pandas':\nreturn df\nelif mode == 'numpy':\nreturn df._get_numeric_data().values\nelif mode == 'torch_tensor':\nreturn torch.from_numpy(df._get_numeric_data().values)\nelif os.path.isdir(dataset_path):\nif mode == 'torch_dataset':\nreturn self.load_as_dataloader(dataset)\nelif mode == 'torch_tensor':\nraise NotImplementedError('We are working on this'\n' implementation!')\nelif mode == 'numpy':\nraise NotImplementedError('We are working on this'\n'implementation!')\nelse:\nraise NotImplementedError(f'Mode `{mode}` has not been'\n' implemented on this version.')\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.load_default_database","title":"<pre><code>load_default_database(name, path, as_dataset=False)\n</code></pre>","text":"<p>Loads a default dataset.</p> <p>Currently, only MNIST dataset is used as the default dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the default dataset. Currently, only MNIST is accepted.</p> required <code>path</code> <code>str</code> <p>Pathfile to MNIST dataset.</p> required <code>as_dataset</code> <code>bool</code> <p>Whether to return the complete dataset (True) or dataset dimensions (False). Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Name is not matching with the name of a default dataset.</p> <p>Returns:</p> Type Description <code>Union[List[int], torch.utils.data.Dataset]</code> <p>Depends on the value of the parameter <code>as_dataset</code>: If</p> <code>Union[List[int], torch.utils.data.Dataset]</code> <p>set to True,  returns dataset (type: torch.utils.data.Dataset).</p> <code>Union[List[int], torch.utils.data.Dataset]</code> <p>If set to False, returns the size of the dataset stored inside</p> <code>Union[List[int], torch.utils.data.Dataset]</code> <p>a list (type: List[int]).</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def load_default_database(self,\nname: str,\npath: str,\nas_dataset: bool = False) -&gt; Union[List[int],\ntorch.utils.data.Dataset]:\n\"\"\"Loads a default dataset.\n    Currently, only MNIST dataset is used as the default dataset.\n    Args:\n        name: Name of the default dataset. Currently,\n            only MNIST is accepted.\n        path: Pathfile to MNIST dataset.\n        as_dataset: Whether to return\n            the complete dataset (True) or dataset dimensions (False).\n            Defaults to False.\n    Raises:\n        NotImplementedError: Name is not matching with\n            the name of a default dataset.\n    Returns:\n        Depends on the value of the parameter `as_dataset`: If\n        set to True,  returns dataset (type: torch.utils.data.Dataset).\n        If set to False, returns the size of the dataset stored inside\n        a list (type: List[int]).\n    \"\"\"\nkwargs = dict(root=path, download=True, transform=transforms.ToTensor())\nif 'mnist' in name.lower():\ndataset = datasets.MNIST(**kwargs)\nelse:\nraise NotImplementedError(f'Default dataset `{name}` has'\n'not been implemented.')\nif as_dataset:\nreturn dataset\nelse:\nreturn self.get_torch_dataset_shape(dataset)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.load_images_dataset","title":"<pre><code>load_images_dataset(folder_path, as_dataset=False)\n</code></pre>","text":"<p>Loads an image dataset.</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>Path to the directory containing the images.</p> required <code>as_dataset</code> <code>bool</code> <p>Whether to return the complete dataset (True) or dataset dimensions (False). Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List[int], torch.utils.data.Dataset]</code> <p>Depends on the value of the parameter <code>as_dataset</code>: If</p> <code>Union[List[int], torch.utils.data.Dataset]</code> <p>set to True,  returns dataset (type: torch.utils.data.Dataset).</p> <code>Union[List[int], torch.utils.data.Dataset]</code> <p>If set to False, returns the size of the dataset stored inside</p> <code>Union[List[int], torch.utils.data.Dataset]</code> <p>a list (type: List[int])</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def load_images_dataset(self,\nfolder_path: str,\nas_dataset: bool = False) -&gt; Union[List[int],\ntorch.utils.data.Dataset]:\n\"\"\"Loads an image dataset.\n    Args:\n        folder_path: Path to the directory containing the images.\n        as_dataset: Whether to return\n            the complete dataset (True) or dataset dimensions (False).\n            Defaults to False.\n    Returns:\n        Depends on the value of the parameter `as_dataset`: If\n        set to True,  returns dataset (type: torch.utils.data.Dataset).\n        If set to False, returns the size of the dataset stored inside\n        a list (type: List[int])\n    \"\"\"\ntry:\ndataset = datasets.ImageFolder(folder_path,\ntransform=transforms.ToTensor())\nexcept Exception as e:\n_msg = ErrorNumbers.FB315.value +\\\n            \"\\nThe following error was raised while loading dataset from the selected\" \\\n            \" path:  \" + str(e) + \"\\nPlease make sure that the selected folder is not empty \\\n            and doesn't have any empty class folder\"\nlogger.error(_msg)\nraise FedbiomedDatasetManagerError(_msg)\nif as_dataset:\nreturn dataset\nelse:\nreturn self.get_torch_dataset_shape(dataset)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.load_mednist_database","title":"<pre><code>load_mednist_database(path, as_dataset=False)\n</code></pre>","text":"<p>Loads the MedNist dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Pathfile to save a local copy of the MedNist dataset.</p> required <code>as_dataset</code> <code>bool</code> <p>Whether to return the complete dataset (True) or dataset dimensions (False). Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>FedbiomedDatasetManagerError</code> <p>One of the following cases:</p> <ul> <li>tarfile cannot be downloaded</li> <li>downloaded tarfile cannot     be extracted</li> <li>MedNIST path is empty</li> <li>one of the classes path is empty</li> </ul> <p>Returns:</p> Type Description <code>Union[List[int], torch.utils.data.Dataset]</code> <p>Depends on the value of the parameter <code>as_dataset</code>: If</p> <code>Union[List[int], torch.utils.data.Dataset]</code> <p>set to True,  returns dataset (type: torch.utils.data.Dataset).</p> <code>Union[List[int], torch.utils.data.Dataset]</code> <p>If set to False, returns the size of the dataset stored inside</p> <code>Union[List[int], torch.utils.data.Dataset]</code> <p>a list (type: List[int])</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def load_mednist_database(self,\npath: str,\nas_dataset: bool = False) -&gt; Union[List[int],\ntorch.utils.data.Dataset]:\n\"\"\"Loads the MedNist dataset.\n    Args:\n        path: Pathfile to save a local copy of the MedNist dataset.\n        as_dataset: Whether to return\n            the complete dataset (True) or dataset dimensions (False).\n            Defaults to False.\n    Raises:\n        FedbiomedDatasetManagerError: One of the following cases:\n            - tarfile cannot be downloaded\n            - downloaded tarfile cannot\n                be extracted\n            - MedNIST path is empty\n            - one of the classes path is empty\n    Returns:\n        Depends on the value of the parameter `as_dataset`: If\n        set to True,  returns dataset (type: torch.utils.data.Dataset).\n        If set to False, returns the size of the dataset stored inside\n        a list (type: List[int])\n    \"\"\"\ndownload_path = os.path.join(path, 'MedNIST')\nif not os.path.isdir(download_path):\nurl = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz\"\nfilepath = os.path.join(path, 'MedNIST.tar.gz')\ntry:\nlogger.info(\"Now downloading MEDNIST...\")\nurlretrieve(url, filepath)\nwith tarfile.open(filepath) as tar_file:\nlogger.info(\"Now extracting MEDNIST...\")\ntar_file.extractall(path)\nos.remove(filepath)\nexcept (URLError, HTTPError, ContentTooShortError, OSError, tarfile.TarError,\nMemoryError) as e:\n_msg = ErrorNumbers.FB315.value + \"\\nThe following error was raised while downloading MedNIST dataset\"\\\n                + \"from the MONAI repo:  \" + str(e)\nlogger.error(_msg)\nraise FedbiomedDatasetManagerError(_msg)\ntry:\ndataset = datasets.ImageFolder(download_path,\ntransform=transforms.ToTensor())\nexcept (FileNotFoundError, RuntimeError) as e:\n_msg = ErrorNumbers.FB315.value + \"\\nThe following error was raised while loading MedNIST dataset from\"\\\n            \"the selected path:  \" + str(e) + \"\\nPlease make sure that the selected MedNIST folder is not empty \\\n               or choose another path.\"\nlogger.error(_msg)\nraise FedbiomedDatasetManagerError(_msg)\nexcept Exception as e:\n_msg = ErrorNumbers.FB315.value + \"\\nThe following error was raised while loading MedNIST dataset\" + str(e)\nlogger.error(_msg)\nraise FedbiomedDatasetManagerError(_msg)\nif as_dataset:\nreturn dataset\nelse:\nreturn self.get_torch_dataset_shape(dataset)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.modify_database_info","title":"<pre><code>modify_database_info(dataset_id, modified_dataset)\n</code></pre>","text":"<p>Modifies a dataset in the database.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>ID of the dataset to modify.</p> required <code>modified_dataset</code> <code>dict</code> <p>New dataset description to replace the existing one.</p> required <p>Raises:</p> Type Description <code>FedbiomedDatasetManagerError</code> <p>conflicting tags with existing dataset</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def modify_database_info(self,\ndataset_id: str,\nmodified_dataset: dict):\n\"\"\"Modifies a dataset in the database.\n    Args:\n        dataset_id: ID of the dataset to modify.\n        modified_dataset: New dataset description to replace the existing one.\n    Raises:\n        FedbiomedDatasetManagerError: conflicting tags with existing dataset\n    \"\"\"\n# Check that there are not existing dataset with conflicting tags\nif 'tags' in modified_dataset:\nconflicting = self.search_conflicting_tags(modified_dataset['tags'])\nconflicting_ids = [ c['dataset_id'] for c in conflicting ]\n# the dataset to modify is ignored (can conflict with its previous tags)\nif dataset_id in conflicting_ids:\nconflicting_ids.remove(dataset_id)\nif len(conflicting_ids) &gt; 0:\nmsg = f\"{ErrorNumbers.FB322.value}, one or more registered dataset has conflicting tags: \" \\\n                f\" {' '.join([ c['name'] for c in conflicting if c['dataset_id'] != dataset_id ])}\"\nlogger.critical(msg)\nraise FedbiomedDatasetManagerError(msg)\nself._dataset_table.update(modified_dataset, self._database.dataset_id == dataset_id)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.obfuscate_private_information","title":"<pre><code>obfuscate_private_information(database_metadata)\n</code></pre>  <code>staticmethod</code>","text":"<p>Remove privacy-sensitive information, to prepare for sharing with a researcher.</p> <p>Removes any information that could be considered privacy-sensitive by the node. The typical use-case is to prevent sharing this information with a researcher through a reply message.</p> <p>Parameters:</p> Name Type Description Default <code>database_metadata</code> <code>Iterable[dict]</code> <p>an iterable of metadata information objects, one per dataset. Each metadata object should be in the format af key-value pairs, such as e.g. a dict.</p> required <p>Returns:</p> Type Description <code>Iterable[dict]</code> <p>the updated iterable of metadata information objects without privacy-sensitive information</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>@staticmethod\ndef obfuscate_private_information(database_metadata: Iterable[dict]) -&gt; Iterable[dict]:\n\"\"\"Remove privacy-sensitive information, to prepare for sharing with a researcher.\n    Removes any information that could be considered privacy-sensitive by the node. The typical use-case is to\n    prevent sharing this information with a researcher through a reply message.\n    Args:\n        database_metadata: an iterable of metadata information objects, one per dataset. Each metadata object\n            should be in the format af key-value pairs, such as e.g. a dict.\n    Returns:\n         the updated iterable of metadata information objects without privacy-sensitive information\n    \"\"\"\nfor d in database_metadata:\ntry:\n# common obfuscations\nd.pop('path', None)\n# obfuscations specific for each data type\nif 'data_type' in d:\nif d['data_type'] == 'medical-folder':\nif 'dataset_parameters' in d:\nd['dataset_parameters'].pop('tabular_file', None)\nexcept AttributeError:\nraise FedbiomedDatasetManagerError(f\"Object of type {type(d)} does not support pop or getitem method \"\nf\"in obfuscate_private_information.\")\nreturn database_metadata\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.read_csv","title":"<pre><code>read_csv(csv_file, index_col=None)\n</code></pre>","text":"<p>Gets content of a CSV file.</p> <p>Reads a *.csv file and outputs its data into a pandas DataFrame. Finds automatically the CSV delimiter by parsing the first line.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>File name / path</p> required <code>index_col</code> <code>Union[int, None]</code> <p>Column that contains CSV file index. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Pandas DataFrame with data contained in CSV file.</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def read_csv(self, csv_file: str, index_col: Union[int, None] = None) -&gt; pd.DataFrame:\n\"\"\"Gets content of a CSV file.\n    Reads a *.csv file and outputs its data into a pandas DataFrame.\n    Finds automatically the CSV delimiter by parsing the first line.\n    Args:\n        csv_file: File name / path\n        index_col: Column that contains CSV file index.\n            Defaults to None.\n    Returns:\n        Pandas DataFrame with data contained in CSV file.\n    \"\"\"\n# Automatically identify separator and header\nsniffer = csv.Sniffer()\nwith open(csv_file, 'r') as file:\ndelimiter = sniffer.sniff(file.readline()).delimiter\nfile.seek(0)\nheader = 0 if sniffer.has_header(file.read()) else None\nreturn pd.read_csv(csv_file, index_col=index_col, sep=delimiter, header=header)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.remove_database","title":"<pre><code>remove_database(tags)\n</code></pre>","text":"<p>Removes datasets from database.</p> <p>Only datasets matching the <code>tags</code> should be removed.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Union[tuple, list]</code> <p>Dataset description tags.</p> required Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def remove_database(self, tags: Union[tuple, list]):\n\"\"\"Removes datasets from database.\n    Only datasets matching the `tags` should be removed.\n    Args:\n        tags: Dataset description tags.\n    \"\"\"\ndoc_ids = [doc.doc_id for doc in self.search_by_tags(tags)]\nself._dataset_table.remove(doc_ids=doc_ids)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.remove_dlp_by_id","title":"<pre><code>remove_dlp_by_id(dlp_id)\n</code></pre>","text":"<p>Removes a data loading plan (DLP) from the database.</p> <p>Only DLP with matching ID is removed from the database. There should be at most one.</p> <p>If <code>remove_dlbs</code> is True, also remove the attached DLBs. You should ensure they are not used by another DLP, no verification is made.</p> <p>Parameters:</p> Name Type Description Default <code>dlp_id</code> <code>str</code> <p>the DataLoadingPlan id</p> required Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def remove_dlp_by_id(self, dlp_id: str):\n\"\"\"Removes a data loading plan (DLP) from the database.\n    Only DLP with matching ID is removed from the database. There should be at most one.\n    If `remove_dlbs` is True, also remove the attached DLBs. You should ensure\n    they are not used by another DLP, no verification is made.\n    Args:\n        dlp_id: the DataLoadingPlan id\n    \"\"\"\nif not isinstance(dlp_id, str):\n_msg = ErrorNumbers.FB316.value + f\": Bad type for dlp '{type(dlp_id)}', expecting str\"\nlogger.error(_msg)\nraise FedbiomedDatasetManagerError(_msg)\nif not str:\n_msg = ErrorNumbers.FB316.value + \": Bad value for dlp, expecting non empty str\"\nlogger.error(_msg)\nraise FedbiomedDatasetManagerError(_msg)\n_ , dlbs = self.get_dlp_by_id(dlp_id)\ntry:\nself._dlp_table.remove(self._database.dlp_id == dlp_id)\nfor dlb in dlbs:\nself._dlp_table.remove(self._database.dlb_id == dlb['dlb_id'])\nexcept Exception as e:\n_msg = ErrorNumbers.FB316.value + f\": Error during remove of DLP {dlp_id}: {e}\"\nlogger.error(_msg)\nraise FedbiomedDatasetManagerError(_msg)\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.save_data_loading_block","title":"<pre><code>save_data_loading_block(dlb)\n</code></pre>","text":"Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def save_data_loading_block(self, dlb: DataLoadingBlock) -&gt; None:\n# seems unused\nself._dlp_table.insert(dlb.serialize())\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.save_data_loading_plan","title":"<pre><code>save_data_loading_plan(data_loading_plan)\n</code></pre>","text":"<p>Save a DataLoadingPlan to the database.</p> <p>This function saves a DataLoadingPlan to the database, and returns its ID.</p> <p>Raises:</p> Type Description <code>FedbiomedDatasetManagerError</code> <p>bad data loading plan name (size, not unique)</p> <p>Parameters:</p> Name Type Description Default <code>data_loading_plan</code> <code>Optional[DataLoadingPlan]</code> <p>the DataLoadingPlan to be saved, or None.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The <code>dlp_id</code> if a DLP was saved, or None</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def save_data_loading_plan(self,\ndata_loading_plan: Optional[DataLoadingPlan]\n) -&gt; dict:\n\"\"\"Save a DataLoadingPlan to the database.\n    This function saves a DataLoadingPlan to the database, and returns its ID.\n    Raises:\n        FedbiomedDatasetManagerError: bad data loading plan name (size, not unique)\n    Args:\n        data_loading_plan: the DataLoadingPlan to be saved, or None.\n    Returns:\n        The `dlp_id` if a DLP was saved, or None\n    \"\"\"\nif data_loading_plan is None:\nreturn None\nif len(data_loading_plan.desc) &lt; 4:\n_msg = ErrorNumbers.FB316.value + \": Cannot save data loading plan, \" + \\\n            \"DLP name needs to have at least 4 characters.\"\nlogger.error(_msg)\nraise FedbiomedDatasetManagerError(_msg)\n_dlp_same_name = self._dlp_table.search(\n(self._database.dlp_id.exists()) &amp; (self._database.dlp_name.exists()) &amp;\n(self._database.dlp_name == data_loading_plan.desc))\nif _dlp_same_name:\n_msg = ErrorNumbers.FB316.value + \": Cannot save data loading plan, \" + \\\n            \"DLP name needs to be unique.\"\nlogger.error(_msg)\nraise FedbiomedDatasetManagerError(_msg)\ndlp_metadata, loading_blocks_metadata = data_loading_plan.serialize()\nself._dlp_table.insert(dlp_metadata)\nself._dlp_table.insert_multiple(loading_blocks_metadata)\nreturn data_loading_plan.dlp_id\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.search_by_tags","title":"<pre><code>search_by_tags(tags)\n</code></pre>","text":"<p>Searches for data with given tags.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Union[tuple, list]</code> <p>List of tags</p> required <p>Returns:</p> Type Description <code>list</code> <p>The list of matching datasets</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def search_by_tags(self, tags: Union[tuple, list]) -&gt; list:\n\"\"\"Searches for data with given tags.\n    Args:\n        tags:  List of tags\n    Returns:\n        The list of matching datasets\n    \"\"\"\nreturn self._dataset_table.search(self._database.tags.all(tags))\n</code></pre>"},{"location":"developer/api/node/dataset_manager/#fedbiomed.node.dataset_manager.DatasetManager.search_conflicting_tags","title":"<pre><code>search_conflicting_tags(tags)\n</code></pre>","text":"<p>Searches for registered data that have conflicting tags with the given tags</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Union[tuple, list]</code> <p>List of tags</p> required <p>Returns:</p> Type Description <code>list</code> <p>The list of conflicting datasets</p> Source code in <code>fedbiomed/node/dataset_manager.py</code> <pre><code>def search_conflicting_tags(self, tags: Union[tuple, list]) -&gt; list:\n\"\"\"Searches for registered data that have conflicting tags with the given tags\n    Args:\n        tags:  List of tags\n    Returns:\n        The list of conflicting datasets\n    \"\"\"\ndef _conflicting_tags(val):\nreturn all(t in val for t in tags) or all(t in tags for t in val)\nreturn self._dataset_table.search(self._database.tags.test(_conflicting_tags))\n</code></pre>"},{"location":"developer/api/node/history_monitor/","title":"HistoryMonitor","text":""},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor","title":"fedbiomed.node.history_monitor","text":"Module: <code>fedbiomed.node.history_monitor</code> <p>Send information from node to researcher during the training</p>"},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor-classes","title":"Classes","text":""},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor.HistoryMonitor","title":"HistoryMonitor","text":"CLASS  <pre><code>HistoryMonitor(job_id, researcher_id, client)\n</code></pre> <p>Send information from node to researcher during the training</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>TODO</p> required <code>researcher_id</code> <code>str</code> <p>TODO</p> required <code>client</code> <code>Messaging</code> <p>TODO</p> required Source code in <code>fedbiomed/node/history_monitor.py</code> <pre><code>def __init__(self,\njob_id: str,\nresearcher_id: str,\nclient: Messaging):\n\"\"\"Simple constructor for the class.\n    Args:\n        job_id: TODO\n        researcher_id: TODO\n        client: TODO\n    \"\"\"\nself.job_id = job_id\nself.researcher_id = researcher_id\nself.messaging = client\n</code></pre>"},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor.HistoryMonitor-attributes","title":"Attributes","text":""},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor.HistoryMonitor.job_id","title":"job_id     <code>instance-attribute</code>","text":"<pre><code>job_id = job_id\n</code></pre>"},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor.HistoryMonitor.messaging","title":"messaging     <code>instance-attribute</code>","text":"<pre><code>messaging = client\n</code></pre>"},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor.HistoryMonitor.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id = researcher_id\n</code></pre>"},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor.HistoryMonitor-functions","title":"Functions","text":""},{"location":"developer/api/node/history_monitor/#fedbiomed.node.history_monitor.HistoryMonitor.add_scalar","title":"<pre><code>add_scalar(metric, iteration, epoch, total_samples, batch_samples, num_batches, num_samples_trained=None, train=False, test=False, test_on_global_updates=False, test_on_local_updates=False)\n</code></pre>","text":"<p>Adds a scalar value to the monitor, and sends an 'AddScalarReply'     response to researcher.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Dict[str, Union[int, float]]</code> <p>recorded value</p> required <code>iteration</code> <code>int</code> <p>current epoch iteration.</p> required <code>epoch</code> <code>int</code> <p>current epoch</p> required <code>total_samples</code> <code>int</code> <p>TODO</p> required <code>batch_samples</code> <code>int</code> <p>TODO</p> required <code>num_batches</code> <code>int</code> <p>TODO</p> required <code>num_samples_trained</code> <code>int</code> <p>TODO</p> <code>None</code> <code>train</code> <code>bool</code> <p>TODO</p> <code>False</code> <code>test</code> <code>bool</code> <p>TODO</p> <code>False</code> <code>test_on_global_updates</code> <code>bool</code> <p>TODO</p> <code>False</code> <code>test_on_local_updates</code> <code>bool</code> <p>TODO</p> <code>False</code> Source code in <code>fedbiomed/node/history_monitor.py</code> <pre><code>def add_scalar(\nself,\nmetric: Dict[str, Union[int, float]],\niteration: int,\nepoch: int,\ntotal_samples: int,\nbatch_samples: int,\nnum_batches: int,\nnum_samples_trained: int = None,\ntrain: bool = False,\ntest: bool = False,\ntest_on_global_updates: bool = False,\ntest_on_local_updates: bool = False\n) -&gt; None:\n\"\"\"Adds a scalar value to the monitor, and sends an 'AddScalarReply'\n        response to researcher.\n    Args:\n        metric:  recorded value\n        iteration: current epoch iteration.\n        epoch: current epoch\n        total_samples: TODO\n        batch_samples: TODO\n        num_batches: TODO\n        num_samples_trained: TODO\n        train: TODO\n        test: TODO\n        test_on_global_updates: TODO\n        test_on_local_updates: TODO\n    \"\"\"\nself.messaging.send_message(NodeMessages.format_outgoing_message({\n'node_id': environ['NODE_ID'],\n'job_id': self.job_id,\n'researcher_id': self.researcher_id,\n'train': train,\n'test': test,\n'test_on_global_updates': test_on_global_updates,\n'test_on_local_updates': test_on_local_updates,\n'metric': metric,\n'iteration': iteration,\n'epoch': epoch,\n'num_samples_trained': num_samples_trained,\n'total_samples': total_samples,\n'batch_samples': batch_samples,\n'num_batches': num_batches,\n'command': 'add_scalar'\n}).get_dict(), client='monitoring')\n</code></pre>"},{"location":"developer/api/node/node/","title":"Node","text":""},{"location":"developer/api/node/node/#fedbiomed.node.node","title":"fedbiomed.node.node","text":"Module: <code>fedbiomed.node.node</code> <p>Core code of the node component.</p>"},{"location":"developer/api/node/node/#fedbiomed.node.node-classes","title":"Classes","text":""},{"location":"developer/api/node/node/#fedbiomed.node.node.Node","title":"Node","text":"CLASS  <pre><code>Node(dataset_manager, tp_security_manager, node_args=None)\n</code></pre> <p>Core code of the node component.</p> <p>Defines the behaviour of the node, while communicating with the researcher through the <code>Messaging</code>, parsing messages from the researcher, etiher treating them instantly or queuing them, executing tasks requested by researcher stored in the queue.</p> <p>Attributes:</p> Name Type Description <code>dataset_manager</code> <p><code>DatasetManager</code> object for managing the node's datasets.</p> <code>tp_security_manager</code> <p><code>TrainingPlanSecurityManager</code> object managing the node's training plans.</p> <code>node_args</code> <p>Command line arguments for node.</p> Source code in <code>fedbiomed/node/node.py</code> <pre><code>def __init__(self,\ndataset_manager: DatasetManager,\ntp_security_manager: TrainingPlanSecurityManager,\nnode_args: Union[dict, None] = None):\n\"\"\"Constructor of the class.\n    Attributes:\n        dataset_manager: `DatasetManager` object for managing the node's datasets.\n        tp_security_manager: `TrainingPlanSecurityManager` object managing the node's training plans.\n        node_args: Command line arguments for node.\n    \"\"\"\nself.tasks_queue = TasksQueue(environ['MESSAGES_QUEUE_DIR'], environ['TMP_DIR'])\nself.messaging = Messaging(self.on_message, ComponentType.NODE,\nenviron['NODE_ID'], environ['MQTT_BROKER'], environ['MQTT_BROKER_PORT'])\nself.dataset_manager = dataset_manager\nself.tp_security_manager = tp_security_manager\nself.node_args = node_args\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node-attributes","title":"Attributes","text":""},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.dataset_manager","title":"dataset_manager     <code>instance-attribute</code>","text":"<pre><code>dataset_manager = dataset_manager\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.messaging","title":"messaging     <code>instance-attribute</code>","text":"<pre><code>messaging = Messaging(self.on_message, ComponentType.NODE, environ['NODE_ID'], environ['MQTT_BROKER'], environ['MQTT_BROKER_PORT'])\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.node_args","title":"node_args     <code>instance-attribute</code>","text":"<pre><code>node_args = node_args\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.tasks_queue","title":"tasks_queue     <code>instance-attribute</code>","text":"<pre><code>tasks_queue = TasksQueue(environ['MESSAGES_QUEUE_DIR'], environ['TMP_DIR'])\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.tp_security_manager","title":"tp_security_manager     <code>instance-attribute</code>","text":"<pre><code>tp_security_manager = tp_security_manager\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node-functions","title":"Functions","text":""},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.add_task","title":"<pre><code>add_task(task)\n</code></pre>","text":"<p>Adds a task to the pending tasks queue.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>dict</code> <p>A <code>Message</code> object describing a training task</p> required Source code in <code>fedbiomed/node/node.py</code> <pre><code>def add_task(self, task: dict):\n\"\"\"Adds a task to the pending tasks queue.\n    Args:\n        task: A `Message` object describing a training task\n    \"\"\"\nself.tasks_queue.add(task)\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.on_message","title":"<pre><code>on_message(msg, topic=None)\n</code></pre>","text":"<p>Handler to be used with <code>Messaging</code> class (ie the messager).</p> <p>Called when a  message arrives through the <code>Messaging</code>. It reads and triggers instructions received by node from Researcher, mainly: - ping requests, - train requests (then a new task will be added on node's task queue), - search requests (for searching data in node's database).</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict</code> <p>Incoming message from Researcher. Must contain key named <code>command</code>, describing the nature of the command (ping requests, train requests, or search requests). Should be formatted as a <code>Message</code>.</p> required <code>topic</code> <code>str</code> <p>Messaging topic name, decision (specially on researcher) may be done regarding of the topic. Currently unused.</p> <code>None</code> Source code in <code>fedbiomed/node/node.py</code> <pre><code>def on_message(self, msg: dict, topic: str = None):\n\"\"\"Handler to be used with `Messaging` class (ie the messager).\n    Called when a  message arrives through the `Messaging`.\n    It reads and triggers instructions received by node from Researcher,\n    mainly:\n    - ping requests,\n    - train requests (then a new task will be added on node's task queue),\n    - search requests (for searching data in node's database).\n    Args:\n        msg: Incoming message from Researcher.\n            Must contain key named `command`, describing the nature\n            of the command (ping requests, train requests,\n            or search requests).\n            Should be formatted as a `Message`.\n        topic: Messaging topic name, decision (specially on researcher) may\n            be done regarding of the topic. Currently unused.\n    \"\"\"\n# TODO: describe all exceptions defined in this method\nmsg_print = {key: value for key, value in msg.items() if key != 'aggregator_args'}\nlogger.debug('Message received: ' + str(msg_print))\ntry:\n# get the request from the received message (from researcher)\ncommand = msg['command']\nrequest = NodeMessages.format_incoming_message(msg).get_dict()\nif command in ['train', 'secagg']:\n# add training task to queue\nself.add_task(request)\nelif command == 'secagg-delete':\nself._task_secagg_delete(NodeMessages.format_incoming_message(msg))\nelif command == 'ping':\nself.messaging.send_message(\nNodeMessages.format_outgoing_message(\n{\n'researcher_id': msg['researcher_id'],\n'node_id': environ['NODE_ID'],\n'success': True,\n'sequence': msg['sequence'],\n'command': 'pong'\n}).get_dict())\nelif command == 'search':\n# Look for databases matching the tags\ndatabases = self.dataset_manager.search_by_tags(msg['tags'])\nif len(databases) != 0:\ndatabases = self.dataset_manager.obfuscate_private_information(databases)\n# FIXME: what happens if len(database) == 0\nself.messaging.send_message(NodeMessages.format_outgoing_message(\n{'success': True,\n'command': 'search',\n'node_id': environ['NODE_ID'],\n'researcher_id': msg['researcher_id'],\n'databases': databases,\n'count': len(databases)}).get_dict())\nelif command == 'list':\n# Get list of all datasets\ndatabases = self.dataset_manager.list_my_data(verbose=False)\ndatabases = self.dataset_manager.obfuscate_private_information(databases)\nself.messaging.send_message(NodeMessages.format_outgoing_message(\n{'success': True,\n'command': 'list',\n'node_id': environ['NODE_ID'],\n'researcher_id': msg['researcher_id'],\n'databases': databases,\n'count': len(databases),\n}).get_dict())\nelif command == 'approval':\n# Ask for training plan approval\nself.tp_security_manager.reply_training_plan_approval_request(request, self.messaging)\nelif command == 'training-plan-status':\n# Check is training plan approved\nself.tp_security_manager.reply_training_plan_status_request(request, self.messaging)\nelse:\nraise NotImplementedError('Command not found')\nexcept decoder.JSONDecodeError:\nresid = msg.get('researcher_id', 'unknown_researcher_id')\nself.send_error(ErrorNumbers.FB301,\nextra_msg=\"Not able to deserialize the message\",\nresearcher_id=resid)\nexcept NotImplementedError:\nresid = msg.get('researcher_id', 'unknown_researcher_id')\nself.send_error(ErrorNumbers.FB301,\nextra_msg=f\"Command `{command}` is not implemented\",\nresearcher_id=resid)\nexcept KeyError:\n# FIXME: this error could be raised for other missing keys (eg\n# researcher_id, ....)\nresid = msg.get('researcher_id', 'unknown_researcher_id')\nself.send_error(ErrorNumbers.FB301,\nextra_msg=\"'command' property was not found\",\nresearcher_id=resid)\nexcept FedbiomedMessageError:  # Message was not properly formatted\nresid = msg.get('researcher_id', 'unknown_researcher_id')\nself.send_error(ErrorNumbers.FB301,\nextra_msg='Message was not properly formatted',\nresearcher_id=resid)\nexcept TypeError:  # Message was not serializable\nresid = msg.get('researcher_id', 'unknown_researcher_id')\nself.send_error(ErrorNumbers.FB301,\nextra_msg='Message was not serializable',\nresearcher_id=resid)\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.parser_task_train","title":"<pre><code>parser_task_train(msg)\n</code></pre>","text":"<p>Parses a given training task message to create a round instance</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>TrainRequest</code> <p><code>TrainRequest</code> message object to parse</p> required <p>Returns:</p> Type Description <code>Union[Round, None]</code> <p>a <code>Round</code> object for the training to perform, or None if no training</p> Source code in <code>fedbiomed/node/node.py</code> <pre><code>def parser_task_train(self, msg: TrainRequest) -&gt; Union[Round, None]:\n\"\"\"Parses a given training task message to create a round instance\n    Args:\n        msg: `TrainRequest` message object to parse\n    Returns:\n        a `Round` object for the training to perform, or None if no training \n    \"\"\"\nround = None\n# msg becomes a TrainRequest object\nhist_monitor = HistoryMonitor(job_id=msg.get_param('job_id'),\nresearcher_id=msg.get_param('researcher_id'),\nclient=self.messaging)\n# Get arguments for the model and training\nmodel_kwargs = msg.get_param('model_args') or {}\ntraining_kwargs = msg.get_param('training_args') or {}\ntraining_status = msg.get_param('training') or False\ntraining_plan_url = msg.get_param('training_plan_url')\ntraining_plan_class = msg.get_param('training_plan_class')\nparams_url = msg.get_param('params_url')\njob_id = msg.get_param('job_id')\nresearcher_id = msg.get_param('researcher_id')\naggregator_args = msg.get_param('aggregator_args') or None\nround_number = msg.get_param('round') or 0\nassert training_plan_url is not None, 'URL for training plan on repository not found.'\nassert validators.url(\ntraining_plan_url), 'URL for training plan on repository is not valid.'\nassert training_plan_class is not None, 'classname for the training plan and training routine ' \\\n                                            'was not found in message.'\nassert isinstance(\ntraining_plan_class,\nstr), '`training_plan_class` must be a string corresponding to the classname for the training plan ' \\\n              'and training routine in the repository'\ndataset_id = msg.get_param('dataset_id')\ndata = self.dataset_manager.get_by_id(dataset_id)\nif data is None or 'path' not in data.keys():\n# FIXME: 'the condition above depends on database model\n# if database model changes (ie `path` field removed/\n# modified);\n# condition above is likely to be false\nlogger.error('Did not found proper data in local datasets ' +\nf'on node={environ[\"NODE_ID\"]}')\nself.messaging.send_message(NodeMessages.format_outgoing_message(\n{'command': \"error\",\n'node_id': environ['NODE_ID'],\n'researcher_id': researcher_id,\n'errnum': ErrorNumbers.FB313,\n'extra_msg': \"Did not found proper data in local datasets\"}\n).get_dict())\nelse:\ndlp_and_loading_block_metadata = None\nif 'dlp_id' in data:\ndlp_and_loading_block_metadata = self.dataset_manager.get_dlp_by_id(data['dlp_id'])\nround = Round(\nmodel_kwargs,\ntraining_kwargs,\ntraining_status,\ndata,\ntraining_plan_url,\ntraining_plan_class,\nparams_url,\njob_id,\nresearcher_id,\nhist_monitor,\naggregator_args,\nself.node_args,\nround_number=round_number,\ndlp_and_loading_block_metadata=dlp_and_loading_block_metadata\n)\nreturn round\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.reply","title":"<pre><code>reply(msg)\n</code></pre>","text":"<p>Send reply to researcher</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict</code> required Source code in <code>fedbiomed/node/node.py</code> <pre><code>def reply(self, msg: dict):\n\"\"\"Send reply to researcher\n    Args:\n        msg:\n    \"\"\"\ntry:\nreply = NodeMessages.format_outgoing_message(\n{'node_id': environ['ID'],\n**msg}\n).get_dict()\nexcept FedbiomedMessageError as e:\nlogger.error(f\"{ErrorNumbers.FB601.value}: {e}\")\nself.send_error(errnum=ErrorNumbers.FB601, extra_msg=f\"{ErrorNumbers.FB601.value}: Can not reply \"\nf\"due to incorrect message type {e}.\")\nexcept Exception as e:\nlogger.error(f\"{ErrorNumbers.FB601.value} Unexpected error while creating node reply message {e}\")\nself.send_error(errnum=ErrorNumbers.FB601, extra_msg=f\"{ErrorNumbers.FB601.value}: \"\nf\"Unexpected error occurred\")\nelse:\nself.messaging.send_message(reply)\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.send_error","title":"<pre><code>send_error(errnum, extra_msg='', researcher_id='&lt;unknown&gt;')\n</code></pre>","text":"<p>Sends an error message.</p> <p>It is a wrapper of <code>Messaging.send_error()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>errnum</code> <code>ErrorNumbers</code> <p>Code of the error.</p> required <code>extra_msg</code> <code>str</code> <p>Additional human readable error message.</p> <code>''</code> <code>researcher_id</code> <code>str</code> <p>Destination researcher.</p> <code>'&lt;unknown&gt;'</code> Source code in <code>fedbiomed/node/node.py</code> <pre><code>def send_error(\nself,\nerrnum: ErrorNumbers,\nextra_msg: str = \"\",\nresearcher_id: str = \"&lt;unknown&gt;\"\n):\n\"\"\"Sends an error message.\n    It is a wrapper of `Messaging.send_error()`.\n    Args:\n        errnum: Code of the error.\n        extra_msg: Additional human readable error message.\n        researcher_id: Destination researcher.\n    \"\"\"\n#\nself.messaging.send_error(errnum=errnum, extra_msg=extra_msg, researcher_id=researcher_id)\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.start_messaging","title":"<pre><code>start_messaging(block=False)\n</code></pre>","text":"<p>Calls the start method of messaging class.</p> <p>Parameters:</p> Name Type Description Default <code>block</code> <code>Optional[bool]</code> <p>Whether messager is blocking (or not). Defaults to False.</p> <code>False</code> Source code in <code>fedbiomed/node/node.py</code> <pre><code>def start_messaging(self, block: Optional[bool] = False):\n\"\"\"Calls the start method of messaging class.\n    Args:\n        block: Whether messager is blocking (or not). Defaults to False.\n    \"\"\"\nself.messaging.start(block)\n</code></pre>"},{"location":"developer/api/node/node/#fedbiomed.node.node.Node.task_manager","title":"<pre><code>task_manager()\n</code></pre>","text":"<p>Manages training tasks in the queue.</p> Source code in <code>fedbiomed/node/node.py</code> <pre><code>def task_manager(self):\n\"\"\"Manages training tasks in the queue.\n    \"\"\"\nwhile True:\nitem = self.tasks_queue.get()\nitem_print = {key: value for key, value in item.items() if key != 'aggregator_args'}\nlogger.debug('[TASKS QUEUE] Item:' + str(item_print))\ntry:\nitem = NodeMessages.format_incoming_message(item)\ncommand = item.get_param('command')\nexcept Exception as e:\n# send an error message back to network if something wrong occured\nself.messaging.send_message(\nNodeMessages.format_outgoing_message(\n{\n'command': 'error',\n'extra_msg': str(e),\n'node_id': environ['NODE_ID'],\n'researcher_id': 'NOT_SET',\n'errnum': ErrorNumbers.FB300\n}\n).get_dict()\n)\nelse:\nif command == 'train':\ntry:\nround = self.parser_task_train(item)\n# once task is out of queue, initiate training rounds\nif round is not None:\n# iterate over each dataset found\n# in the current round (here round refers\n# to a round to be done on a specific dataset).\nmsg = round.run_model_training(\nsecagg_arguments={\n'secagg_servkey_id': item.get_param('secagg_servkey_id'),\n'secagg_biprime_id': item.get_param('secagg_biprime_id'),\n'secagg_random': item.get_param('secagg_random'),\n'secagg_clipping_range': item.get_param('secagg_clipping_range')\n}\n)\nself.messaging.send_message(msg)\nexcept Exception as e:\n# send an error message back to network if something\n# wrong occured\nself.messaging.send_message(\nNodeMessages.format_outgoing_message(\n{\n'command': 'error',\n'extra_msg': str(e),\n'node_id': environ['NODE_ID'],\n'researcher_id': 'NOT_SET',\n'errnum': ErrorNumbers.FB300\n}\n).get_dict()\n)\nlogger.debug(f\"{ErrorNumbers.FB300}: {e}\")\nelif command == 'secagg':\nself._task_secagg(item)\nelse:\nerrmess = f'{ErrorNumbers.FB319.value}: \"{command}\"'\nlogger.error(errmess)\nself.send_error(errnum=ErrorNumbers.FB319, extra_msg=errmess)\nself.tasks_queue.task_done()\n</code></pre>"},{"location":"developer/api/node/round/","title":"Round","text":""},{"location":"developer/api/node/round/#fedbiomed.node.round","title":"fedbiomed.node.round","text":"Module: <code>fedbiomed.node.round</code> <p>implementation of Round class of the node component</p>"},{"location":"developer/api/node/round/#fedbiomed.node.round-classes","title":"Classes","text":""},{"location":"developer/api/node/round/#fedbiomed.node.round.Round","title":"Round","text":"CLASS  <pre><code>Round(model_kwargs=None, training_kwargs=None, training=True, dataset=None, training_plan_url=None, training_plan_class=None, params_url=None, job_id=None, researcher_id=None, history_monitor=None, aggregator_args=None, node_args=None, round_number=0, dlp_and_loading_block_metadata=None)\n</code></pre> <p>This class represents the training part execute by a node in a given round</p> <p>Parameters:</p> Name Type Description Default <code>model_kwargs</code> <code>dict</code> <p>contains model args</p> <code>None</code> <code>training_kwargs</code> <code>dict</code> <p>contains training arguments</p> <code>None</code> <code>dataset</code> <code>dict</code> <p>dataset details to use in this round. It contains the dataset name, dataset's id, data path, its shape, its description...</p> <code>None</code> <code>training_plan_url</code> <code>str</code> <p>url from which to download training plan file</p> <code>None</code> <code>training_plan_class</code> <code>str</code> <p>name of the training plan (eg 'MyTrainingPlan')</p> <code>None</code> <code>params_url</code> <code>str</code> <p>url from which to upload/download model params</p> <code>None</code> <code>job_id</code> <code>str</code> <p>job id</p> <code>None</code> <code>researcher_id</code> <code>str</code> <p>researcher id</p> <code>None</code> <code>history_monitor</code> <code>HistoryMonitor</code> <p>Sends real-time feed-back to end-user during training</p> <code>None</code> <code>correction_state</code> <p>correction state applied in case of SCAFFOLD aggregation strategy</p> required <code>node_args</code> <code>Union[dict, None]</code> <p>command line arguments for node. Can include: - <code>gpu (bool)</code>: propose use a GPU device if any is available. - <code>gpu_num (Union[int, None])</code>: if not None, use the specified GPU device instead of default     GPU device if this GPU device is available. - <code>gpu_only (bool)</code>: force use of a GPU device if any available, even if researcher     doesn't request for using a GPU.</p> <code>None</code> Source code in <code>fedbiomed/node/round.py</code> <pre><code>def __init__(self,\nmodel_kwargs: dict = None,\ntraining_kwargs: dict = None,\ntraining: bool = True,\ndataset: dict = None,\ntraining_plan_url: str = None,\ntraining_plan_class: str = None,\nparams_url: str = None,\njob_id: str = None,\nresearcher_id: str = None,\nhistory_monitor: HistoryMonitor = None,\naggregator_args: dict = None,\nnode_args: Union[dict, None] = None,\nround_number: int = 0,\ndlp_and_loading_block_metadata: Optional[Tuple[dict, List[dict]]] = None):\n\"\"\"Constructor of the class\n    Args:\n        model_kwargs: contains model args\n        training_kwargs: contains training arguments\n        dataset: dataset details to use in this round. It contains the dataset name, dataset's id,\n            data path, its shape, its description...\n        training_plan_url: url from which to download training plan file\n        training_plan_class: name of the training plan (eg 'MyTrainingPlan')\n        params_url: url from which to upload/download model params\n        job_id: job id\n        researcher_id: researcher id\n        history_monitor: Sends real-time feed-back to end-user during training\n        correction_state: correction state applied in case of SCAFFOLD aggregation strategy\n        node_args: command line arguments for node. Can include:\n            - `gpu (bool)`: propose use a GPU device if any is available.\n            - `gpu_num (Union[int, None])`: if not None, use the specified GPU device instead of default\n                GPU device if this GPU device is available.\n            - `gpu_only (bool)`: force use of a GPU device if any available, even if researcher\n                doesn't request for using a GPU.\n    \"\"\"\nself._use_secagg: bool = False\nself.dataset = dataset\nself.training_plan_url = training_plan_url\nself.training_plan_class = training_plan_class\nself.params_url = params_url\nself.job_id = job_id\nself.researcher_id = researcher_id\nself.history_monitor = history_monitor\nself.aggregator_args: Optional[Dict[str, Any]] = aggregator_args\nself.tp_security_manager = TrainingPlanSecurityManager()\nself.node_args = node_args\nself.repository = Repository(environ['UPLOADS_URL'], environ['TMP_DIR'], environ['CACHE_DIR'])\nself.training_plan = None\nself.training = training\nself._dlp_and_loading_block_metadata = dlp_and_loading_block_metadata\nself.training_kwargs = training_kwargs\nself.model_arguments = model_kwargs\nself.testing_arguments = None\nself.loader_arguments = None\nself.training_arguments = None\nself._secagg_crypter = SecaggCrypter()\nself._secagg_clipping_range = None\nself._round = round_number\nself._biprime = None\nself._servkey = None\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round-attributes","title":"Attributes","text":""},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.aggregator_args","title":"aggregator_args     <code>instance-attribute</code>","text":"<pre><code>aggregator_args: Optional[Dict[str, Any]] = aggregator_args\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.dataset","title":"dataset     <code>instance-attribute</code>","text":"<pre><code>dataset = dataset\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.history_monitor","title":"history_monitor     <code>instance-attribute</code>","text":"<pre><code>history_monitor = history_monitor\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.job_id","title":"job_id     <code>instance-attribute</code>","text":"<pre><code>job_id = job_id\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.loader_arguments","title":"loader_arguments     <code>instance-attribute</code>","text":"<pre><code>loader_arguments = None\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.model_arguments","title":"model_arguments     <code>instance-attribute</code>","text":"<pre><code>model_arguments = model_kwargs\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.node_args","title":"node_args     <code>instance-attribute</code>","text":"<pre><code>node_args = node_args\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.params_url","title":"params_url     <code>instance-attribute</code>","text":"<pre><code>params_url = params_url\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.repository","title":"repository     <code>instance-attribute</code>","text":"<pre><code>repository = Repository(environ['UPLOADS_URL'], environ['TMP_DIR'], environ['CACHE_DIR'])\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.researcher_id","title":"researcher_id     <code>instance-attribute</code>","text":"<pre><code>researcher_id = researcher_id\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.testing_arguments","title":"testing_arguments     <code>instance-attribute</code>","text":"<pre><code>testing_arguments = None\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.tp_security_manager","title":"tp_security_manager     <code>instance-attribute</code>","text":"<pre><code>tp_security_manager = TrainingPlanSecurityManager()\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.training","title":"training     <code>instance-attribute</code>","text":"<pre><code>training = training\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.training_arguments","title":"training_arguments     <code>instance-attribute</code>","text":"<pre><code>training_arguments = None\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.training_kwargs","title":"training_kwargs     <code>instance-attribute</code>","text":"<pre><code>training_kwargs = training_kwargs\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.training_plan","title":"training_plan     <code>instance-attribute</code>","text":"<pre><code>training_plan = None\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.training_plan_class","title":"training_plan_class     <code>instance-attribute</code>","text":"<pre><code>training_plan_class = training_plan_class\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.training_plan_url","title":"training_plan_url     <code>instance-attribute</code>","text":"<pre><code>training_plan_url = training_plan_url\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round-functions","title":"Functions","text":""},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.download_aggregator_args","title":"<pre><code>download_aggregator_args()\n</code></pre>","text":"<p>Retrieves aggregator arguments, that are sent through file exchange service</p> <p>Returns:</p> Type Description <code>Tuple[bool, str]</code> <p>Tuple[bool, str]: a tuple containing: a bool that indicates the success of operation a string containing the error message</p> Source code in <code>fedbiomed/node/round.py</code> <pre><code>def download_aggregator_args(self) -&gt; Tuple[bool, str]:\n\"\"\"Retrieves aggregator arguments, that are sent through file exchange service\n    Returns:\n        Tuple[bool, str]: a tuple containing:\n            a bool that indicates the success of operation\n            a string containing the error message\n    \"\"\"\n# download heavy aggregator args (if any)\nif self.aggregator_args is not None:\nfor arg_name, aggregator_arg in self.aggregator_args.items():\nif isinstance(aggregator_arg, dict):\nurl = aggregator_arg.get('url', False)\nif any((url, arg_name)):\n# if both `filename` and `arg_name` fields are present, it means that parameters\n# should be retrieved using file\n# exchanged system\nsuccess, param_path, error_msg = self.download_file(url, f\"{arg_name}_{uuid.uuid4()}.mpk\")\nif not success:\nreturn success, error_msg\nelse:\n# FIXME: should we load parameters here or in the training plan\nself.aggregator_args[arg_name] = {'param_path': param_path,\n# 'params': training_plan.load(param_path,\n# update_model=True)\n}\nself.aggregator_args[arg_name] = Serializer.load(param_path)\nreturn True, ''\nelse:\nreturn True, \"no file downloads required for aggregator args\"\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.download_file","title":"<pre><code>download_file(url, file_path)\n</code></pre>","text":"<p>Downloads file from file exchange system</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>url used to download file</p> required <code>file_path</code> <code>str</code> <p>file path used to store the downloaded content</p> required <p>Returns:</p> Type Description <code>Tuple[bool, str, str]</code> <p>Tuple[bool, str, str]: tuple that contains: bool that indicates the success of the download str that returns the complete path file str containing the error message (if any). Returns empty string if operation successful.</p> Source code in <code>fedbiomed/node/round.py</code> <pre><code>def download_file(self, url: str, file_path: str) -&gt; Tuple[bool, str, str]:\n\"\"\"Downloads file from file exchange system\n    Args:\n        url (str): url used to download file\n        file_path (str): file path used to store the downloaded content\n    Returns:\n        Tuple[bool, str, str]: tuple that contains:\n            bool that indicates the success of the download\n            str that returns the complete path file\n            str containing the error message (if any). Returns empty\n            string if operation successful.\n    \"\"\"\nstatus, params_path = self.repository.download_file(url, file_path)\nif (status != 200) or params_path is None:\nerror_message = f\"Cannot download param file: {url}\"\nreturn False, '', error_message\nelse:\nreturn True, params_path, ''\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.initialize_validate_training_arguments","title":"<pre><code>initialize_validate_training_arguments()\n</code></pre>","text":"<p>Validates and separates training argument for experiment round</p> Source code in <code>fedbiomed/node/round.py</code> <pre><code>def initialize_validate_training_arguments(self) -&gt; None:\n\"\"\"Validates and separates training argument for experiment round\"\"\"\nself.training_arguments = TrainingArgs(self.training_kwargs, only_required=False)\nself.testing_arguments = self.training_arguments.testing_arguments()\nself.loader_arguments = self.training_arguments.loader_arguments()\n</code></pre>"},{"location":"developer/api/node/round/#fedbiomed.node.round.Round.run_model_training","title":"<pre><code>run_model_training(secagg_arguments=None)\n</code></pre>","text":"<p>This method downloads training plan file; then runs the training of a model and finally uploads model params to the file repository</p> <p>Parameters:</p> Name Type Description Default <code>secagg_arguments</code> <code>Union[Dict, None]</code> <ul> <li>secagg_servkey_id: Secure aggregation Servkey context id. None means that the parameters     are not going to be encrypted</li> <li>secagg_biprime_id: Secure aggregation Biprime context ID.</li> <li>secagg_random: Float value to validate secure aggregation on the researcher side</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Returns the corresponding node message, training reply instance</p> Source code in <code>fedbiomed/node/round.py</code> <pre><code>def run_model_training(\nself,\nsecagg_arguments: Union[Dict, None] = None,\n) -&gt; Dict[str, Any]:\n\"\"\"This method downloads training plan file; then runs the training of a model\n    and finally uploads model params to the file repository\n    Args:\n        secagg_arguments:\n            - secagg_servkey_id: Secure aggregation Servkey context id. None means that the parameters\n                are not going to be encrypted\n            - secagg_biprime_id: Secure aggregation Biprime context ID.\n            - secagg_random: Float value to validate secure aggregation on the researcher side\n    Returns:\n        Returns the corresponding node message, training reply instance\n    \"\"\"\nis_failed = False\n# Validate secagg status. Raises error if the training request is compatible with\n# secure aggregation settings\ntry:\nsecagg_arguments = {} if secagg_arguments is None else secagg_arguments\nself._use_secagg = self._configure_secagg(\nsecagg_servkey_id=secagg_arguments.get('secagg_servkey_id'),\nsecagg_biprime_id=secagg_arguments.get('secagg_biprime_id'),\nsecagg_random=secagg_arguments.get('secagg_random')\n)\nexcept FedbiomedRoundError as e:\nreturn self._send_round_reply(success=False, message=str(e))\n# Initialize and validate requested experiment/training arguments\ntry:\nself.initialize_validate_training_arguments()\nexcept FedbiomedUserInputError as e:\nreturn self._send_round_reply(success=False, message=repr(e))\nexcept Exception as e:\nmsg = 'Unexpected error while validating training argument'\nlogger.debug(f\"{msg}: {repr(e)}\")\nreturn self._send_round_reply(success=False, message=f'{msg}. Please contact system provider')\ntry:\n# module name cannot contain dashes\nimport_module = 'training_plan_' + str(uuid.uuid4().hex)\nstatus, _ = self.repository.download_file(self.training_plan_url,\nimport_module + '.py')\nif status != 200:\nerror_message = \"Cannot download training plan file: \" + self.training_plan_url\nreturn self._send_round_reply(success=False, message=error_message)\nelse:\nif environ[\"TRAINING_PLAN_APPROVAL\"]:\napproved, training_plan_ = self.tp_security_manager.check_training_plan_status(\nos.path.join(environ[\"TMP_DIR\"], import_module + '.py'),\nTrainingPlanApprovalStatus.APPROVED)\nif not approved:\nerror_message = f'Requested training plan is not approved by the node: {environ[\"NODE_ID\"]}'\nreturn self._send_round_reply(success=False, message=error_message)\nelse:\nlogger.info(f'Training plan has been approved by the node {training_plan_[\"name\"]}')\nif not is_failed:\nsuccess, params_path, error_msg = self.download_file(self.params_url, f\"my_model_{uuid.uuid4()}.mpk\")\nif success:\n# retrieving aggregator args\nsuccess, error_msg = self.download_aggregator_args()\nif not success:\nreturn self._send_round_reply(success=False, message=error_msg)\nexcept Exception as e:\nis_failed = True\n# FIXME: this will trigger if model is not approved by node\nerror_message = f\"Cannot download training plan files: {repr(e)}\"\nreturn self._send_round_reply(success=False, message=error_message)\n# import module, declare the training plan, load parameters\ntry:\nsys.path.insert(0, environ['TMP_DIR'])\nmodule = importlib.import_module(import_module)\ntrain_class = getattr(module, self.training_plan_class)\nself.training_plan = train_class()\nsys.path.pop(0)\nexcept Exception as e:\nerror_message = f\"Cannot instantiate training plan object: {repr(e)}\"\nreturn self._send_round_reply(success=False, message=error_message)\ntry:\nself.training_plan.post_init(model_args=self.model_arguments,\ntraining_args=self.training_arguments,\naggregator_args=self.aggregator_args)\nexcept Exception as e:\nerror_message = f\"Can't initialize training plan with the arguments: {repr(e)}\"\nreturn self._send_round_reply(success=False, message=error_message)\n# import model params into the training plan instance\ntry:\nparams = Serializer.load(params_path)[\"model_weights\"]\nself.training_plan.set_model_params(params)\nexcept Exception as e:\nerror_message = f\"Cannot initialize model parameters: {repr(e)}\"\nreturn self._send_round_reply(success=False, message=error_message)\n# Split training and validation data\ntry:\nself._set_training_testing_data_loaders()\nexcept FedbiomedError as e:\nerror_message = f\"Can not create validation/train data: {repr(e)}\"\nreturn self._send_round_reply(success=False, message=error_message)\nexcept Exception as e:\nerror_message = f\"Undetermined error while creating data for training/validation. Can not create \" \\\n                        f\"validation/train data: {repr(e)}\"\nreturn self._send_round_reply(success=False, message=error_message)\n# Validation Before Training\nif self.testing_arguments.get('test_on_global_updates', False) is not False:\n# Last control to make sure validation data loader is set.\nif self.training_plan.testing_data_loader is not None:\ntry:\nself.training_plan.testing_routine(metric=self.testing_arguments.get('test_metric', None),\nmetric_args=self.testing_arguments.get('test_metric_args', {}),\nhistory_monitor=self.history_monitor,\nbefore_train=True)\nexcept FedbiomedError as e:\nlogger.error(f\"{ErrorNumbers.FB314}: During the validation phase on global parameter updates; \"\nf\"{repr(e)}\")\nexcept Exception as e:\nlogger.error(f\"Undetermined error during the testing phase on global parameter updates: \"\nf\"{repr(e)}\")\nelse:\nlogger.error(f\"{ErrorNumbers.FB314}: Can not execute validation routine due to missing testing dataset\"\nf\"Please make sure that `test_ratio` has been set correctly\")\n# If training is activated.\nif self.training:\nif self.training_plan.training_data_loader is not None:\ntry:\nresults = {}\nrtime_before = time.perf_counter()\nptime_before = time.process_time()\nself.training_plan.training_routine(history_monitor=self.history_monitor,\nnode_args=self.node_args)\nrtime_after = time.perf_counter()\nptime_after = time.process_time()\nexcept Exception as e:\nerror_message = f\"Cannot train model in round: {repr(e)}\"\nreturn self._send_round_reply(success=False, message=error_message)\n# Validation after training\nif self.testing_arguments.get('test_on_local_updates', False) is not False:\nif self.training_plan.testing_data_loader is not None:\ntry:\nself.training_plan.testing_routine(metric=self.testing_arguments.get('test_metric', None),\nmetric_args=self.testing_arguments.get('test_metric_args',\n{}),\nhistory_monitor=self.history_monitor,\nbefore_train=False)\nexcept FedbiomedError as e:\nlogger.error(\nf\"{ErrorNumbers.FB314.value}: During the validation phase on local parameter updates; \"\nf\"{repr(e)}\")\nexcept Exception as e:\nlogger.error(f\"Undetermined error during the validation phase on local parameter updates\"\nf\"{repr(e)}\")\nelse:\nlogger.error(\nf\"{ErrorNumbers.FB314.value}: Can not execute validation routine due to missing testing \"\nf\"dataset please make sure that test_ratio has been set correctly\")\nsample_size = len(self.training_plan.training_data_loader.dataset)\nresults[\"encrypted\"] = False\nmodel_weights = self.training_plan.after_training_params(flatten=self._use_secagg)\nif self._use_secagg:\nlogger.info(\"Encrypting model parameters. This process can take some time depending on model size.\")\nencrypt = functools.partial(\nself._secagg_crypter.encrypt,\nnum_nodes=len(self._servkey[\"parties\"]) - 1,  # -1: don't count researcher\ncurrent_round=self._round,\nkey=self._servkey[\"context\"][\"server_key\"],\nbiprime=self._biprime[\"context\"][\"biprime\"],\nweight=sample_size,\nclipping_range=secagg_arguments.get('secagg_clipping_range')\n)\nmodel_weights = encrypt(params=model_weights)\nresults[\"encrypted\"] = True\nresults[\"encryption_factor\"] = encrypt(params=[secagg_arguments[\"secagg_random\"]])\nlogger.info(\"Encryption is completed!\")\nresults['researcher_id'] = self.researcher_id\nresults['job_id'] = self.job_id\nresults['model_weights'] = model_weights\nresults['node_id'] = environ['NODE_ID']\nresults['optimizer_args'] = self.training_plan.optimizer_args()\ntry:\n# TODO: add validation status to these results?\n# Dump the results to a msgpack file.\nfilename = os.path.join(environ[\"TMP_DIR\"], f\"node_params_{uuid.uuid4()}.mpk\")\nSerializer.dump(results, filename)\n# Upload that file to the remote repository.\nres = self.repository.upload_file(filename)\nlogger.info(\"results uploaded successfully \")\nexcept Exception as exc:\nreturn self._send_round_reply(success=False, message=f\"Cannot upload results: {exc}\")\n# end : clean the namespace\ntry:\ndel self.training_plan\ndel import_module\nexcept Exception as e:\nlogger.debug(f'Exception raise while deleting training plan instance: {repr(e)}')\nreturn self._send_round_reply(success=True,\ntiming={'rtime_training': rtime_after - rtime_before,\n'ptime_training': ptime_after - ptime_before},\nparams_url=res['file'],\nsample_size=sample_size)\nelse:\n# Only for validation\nreturn self._send_round_reply(success=True)\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/","title":"TrainingPlanSecurityManager","text":""},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager","title":"fedbiomed.node.training_plan_security_manager","text":"Module: <code>fedbiomed.node.training_plan_security_manager</code> <p>Manages training plan approval for a node.</p>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager-attributes","title":"Attributes","text":""},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.HASH_FUNCTIONS","title":"HASH_FUNCTIONS     <code>module-attribute</code>","text":"<pre><code>HASH_FUNCTIONS = {HashingAlgorithms.SHA256.value: hashlib.sha256, HashingAlgorithms.SHA384.value: hashlib.sha384, HashingAlgorithms.SHA512.value: hashlib.sha512, HashingAlgorithms.SHA3_256.value: hashlib.sha3_256, HashingAlgorithms.SHA3_384.value: hashlib.sha3_384, HashingAlgorithms.SHA3_512.value: hashlib.sha3_512, HashingAlgorithms.BLAKE2B.value: hashlib.blake2s, HashingAlgorithms.BLAKE2S.value: hashlib.blake2s}\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.trainingPlansSearchScheme","title":"trainingPlansSearchScheme     <code>module-attribute</code>","text":"<pre><code>trainingPlansSearchScheme = SchemeValidator({'by': {'rules': [str], 'required': True}, 'text': {'rules': [str], 'required': True}})\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager-classes","title":"Classes","text":""},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager","title":"TrainingPlanSecurityManager","text":"CLASS  <pre><code>TrainingPlanSecurityManager()\n</code></pre> <p>Manages training plan approval for a node.</p> <p>Creates a DB object for the table named as <code>Training plans</code> and builds a query object to query the database.</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def __init__(self):\n\"\"\"Class constructor for TrainingPlanSecurityManager.\n    Creates a DB object for the table named as `Training plans` and builds a query object to query\n    the database.\n    \"\"\"\nself._tinydb = TinyDB(environ[\"DB_PATH\"])\n# dont use DB read cache for coherence when updating from multiple sources (eg: GUI and CLI)\nself._db = self._tinydb.table(name=\"TrainingPlans\", cache_size=0)\nself._database = Query()\nself._repo = Repository(environ['UPLOADS_URL'], environ['TMP_DIR'], environ['CACHE_DIR'])\nself._tags_to_remove = ['training_plan_path',\n'hash',\n'date_modified',\n'date_created']\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager-functions","title":"Functions","text":""},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.approve_training_plan","title":"<pre><code>approve_training_plan(training_plan_id, extra_notes=None)\n</code></pre>","text":"<p>Approves a training plan stored into the database given its [<code>training_plan_id</code>]</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_id</code> <code>str</code> <p>id of the training plan.</p> required <code>extra_notes</code> <code>Union[str, None]</code> <p>notes detailing why training plan has been approved. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>True</code> <p>Currently always returns True</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def approve_training_plan(self, training_plan_id: str, extra_notes: Union[str, None] = None) -&gt; True:\n\"\"\"Approves a training plan stored into the database given its [`training_plan_id`]\n    Args:\n        training_plan_id: id of the training plan.\n        extra_notes: notes detailing why training plan has been approved. Defaults to None.\n    Returns:\n        Currently always returns True\n    \"\"\"\nres = self._update_training_plan_status(training_plan_id,\nTrainingPlanApprovalStatus.APPROVED,\nextra_notes)\nreturn res\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.check_hashes_for_registered_training_plans","title":"<pre><code>check_hashes_for_registered_training_plans()\n</code></pre>","text":"<p>Checks registered training plans (training plans either rejected or approved).</p> <p>Makes sure training plan files exists and hashing algorithm is matched with specified algorithm in the config file.</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>cannot update training plan list in database</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def check_hashes_for_registered_training_plans(self):\n\"\"\"Checks registered training plans (training plans either rejected or approved).\n    Makes sure training plan files exists and hashing algorithm is matched with specified\n    algorithm in the config file.\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: cannot update training plan list in database\n    \"\"\"\ntry:\ntraining_plans = self._db.search(self._database.training_plan_type.all(TrainingPlanStatus.REGISTERED.value))\nexcept Exception as e:\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + f\"database search operation failed, with following error: {str(e)}\")\nlogger.info('Checking hashes for registered training plans')\nif not training_plans:\nlogger.info('There are no training plans registered')\nelse:\nfor training_plan in training_plans:\n# If training plan file is exists\nif os.path.isfile(training_plan['training_plan_path']):\nif training_plan['algorithm'] != environ['HASHING_ALGORITHM']:\nlogger.info(\nf'Recreating hashing for : {training_plan[\"name\"]} \\t {training_plan[\"training_plan_id\"]}')\nhashing, algorithm = self._create_hash(training_plan['training_plan_path'])\n# Verify no such training plan already exists in DB\nself._check_training_plan_not_existing(None, None, hashing, algorithm)\nrtime = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S.%f\")\ntry:\nself._db.update({'hash': hashing,\n'algorithm': algorithm,\n'date_last_action': rtime},\nself._database.training_plan_id.all(training_plan[\"training_plan_id\"]))\nexcept Exception as err:\nraise FedbiomedTrainingPlanSecurityManagerError(ErrorNumbers.FB606.value +\n\": database update failed, with error \"\nf\" {str(err)}\")\nelse:\n# Remove doc because training plan file is not existing anymore\nlogger.info(\nf'Training plan : {training_plan[\"name\"]} could not found in : '\nf'{training_plan[\"training_plan_path\"]}, will be removed')\ntry:\nself._db.remove(doc_ids=[training_plan.doc_id])\nexcept Exception as err:\nraise FedbiomedTrainingPlanSecurityManagerError(\nf\"{ErrorNumbers.FB606.value}: database remove operation failed, with following error: \",\nf\"{err}\")\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.check_training_plan_status","title":"<pre><code>check_training_plan_status(training_plan_path, state)\n</code></pre>","text":"<p>Checks whether training plan exists in database and has the specified status.</p> <p>Sends a query to database to search for hash of requested training plan. If the hash matches with one of the training plans hashes in the DB, and if training plan has the specified status {approved, rejected, pending} or training_plan_type {registered, requested, default}.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_path</code> <code>str</code> <p>The path of requested training plan file by researcher after downloading training plan file from file repository.</p> required <code>state</code> <code>Union[TrainingPlanApprovalStatus, TrainingPlanStatus, None]</code> <p>training plan status or training plan type, to check against training plan. <code>None</code> accepts any training plan status or type.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, Dict[str, Any]]</code> <p>A tuple (is_status, training plan) where</p> <ul> <li>status: Whether training plan exists in database     with specified status (returns True) or not (False)</li> <li>training_plan: Dictionary containing fields     related to the training plan. If database search request failed,     returns None instead.</li> </ul> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>bad argument type or value</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>database access problem</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def check_training_plan_status(self,\ntraining_plan_path: str,\nstate: Union[TrainingPlanApprovalStatus, TrainingPlanStatus, None]) \\\n        -&gt; Tuple[bool, Dict[str, Any]]:\n\"\"\"Checks whether training plan exists in database and has the specified status.\n    Sends a query to database to search for hash of requested training plan.\n    If the hash matches with one of the\n    training plans hashes in the DB, and if training plan has the specified status {approved, rejected, pending}\n    or training_plan_type {registered, requested, default}.\n    Args:\n        training_plan_path: The path of requested training plan file by researcher after downloading\n            training plan file from file repository.\n        state: training plan status or training plan type, to check against training plan. `None` accepts\n            any training plan status or type.\n    Returns:\n        A tuple (is_status, training plan) where\n            - status: Whether training plan exists in database\n                with specified status (returns True) or not (False)\n            - training_plan: Dictionary containing fields\n                related to the training plan. If database search request failed,\n                returns None instead.\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: bad argument type or value\n        FedbiomedTrainingPlanSecurityManagerError: database access problem\n    \"\"\"\n# Create hash for requested training plan\nreq_training_plan_hash, _ = self._create_hash(training_plan_path)\n# If node allows defaults training plans search hash for all training plan types\n# otherwise search only for `registered` training plans\nif state is None:\n_all_training_plans_with_status = None\nelif isinstance(state, TrainingPlanApprovalStatus):\n_all_training_plans_with_status = (self._database.training_plan_status == state.value)\nelif isinstance(state, TrainingPlanStatus):\n_all_training_plans_with_status = (self._database.training_plan_type == state.value)\nelse:\nraise FedbiomedTrainingPlanSecurityManagerError(\nf\"{ErrorNumbers.FB606.value} + status should be either TrainingPlanApprovalStatus or \"\nf\"TrainingPlanStatus, but got {type(state)}\"\n)\n_all_training_plans_which_have_req_hash = (self._database.hash == req_training_plan_hash)\n# TODO: more robust implementation\n# current implementation (with `get`) makes supposition that there is at most\n# one training plan with a given hash in the database\ntry:\nif _all_training_plans_with_status is None:\n# check only against hash\ntraining_plan = self._db.get(_all_training_plans_which_have_req_hash)\nelse:\n# check against hash and status\ntraining_plan = self._db.get(_all_training_plans_with_status &amp; _all_training_plans_which_have_req_hash)\nexcept Exception as e:\nraise FedbiomedTrainingPlanSecurityManagerError(\nf\"{ErrorNumbers.FB606.value} database remove operation failed, with following error: {e}\"\n)\nif training_plan:\nis_status = True\nelse:\nis_status = False\ntraining_plan = None\nreturn is_status, training_plan\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.create_txt_training_plan_from_py","title":"<pre><code>create_txt_training_plan_from_py(training_plan_path)\n</code></pre>  <code>staticmethod</code>","text":"<p>Creates a text training plan file (.txt extension) from a python (.py) training plan file, in the directory where the python training plan file belongs to.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_path</code> <code>str</code> <p>path to the training plan file (with *.py) extension</p> required <p>Returns:</p> Name Type Description <code>training_plan_path_txt</code> <code>str</code> <p>path to new training plan file (with *.txt extension)</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>@staticmethod\ndef create_txt_training_plan_from_py(training_plan_path: str) -&gt; str:\n\"\"\"Creates a text training plan file (*.txt extension) from a python (*.py) training plan file,\n    in the directory where the python training plan file belongs to.\n    Args:\n        training_plan_path (str): path to the training plan file (with *.py) extension\n    Returns:\n        training_plan_path_txt (str): path to new training plan file (with *.txt extension)\n    \"\"\"\n# remove '*.py' extension of `training_plan_path` and rename it into `*.txt`\ntraining_plan_path_txt, _ = os.path.splitext(training_plan_path)\ntraining_plan_path_txt += '.txt'\n# save the content of the training plan into a plain '*.txt' file\nshutil.copyfile(training_plan_path, training_plan_path_txt)\nreturn training_plan_path_txt\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.delete_training_plan","title":"<pre><code>delete_training_plan(training_plan_id)\n</code></pre>","text":"<p>Removes training plan file from database.</p> <p>Only removes <code>registered</code> and <code>requested</code> type of training plans from the database. Does not remove the corresponding training plan file from the disk. Default training plans should be removed from the directory</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_id</code> <code>str</code> <p>The id of the registered training plan.</p> required <p>Returns:</p> Type Description <code>True</code> <p>Currently always returns True.</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>bad type for parameter</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>cannot read or remove training plan from the database</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>training plan is not a <code>registered</code> training plan (thus a <code>default</code> training plan)</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def delete_training_plan(self, training_plan_id: str) -&gt; True:\n\"\"\"Removes training plan file from database.\n    Only removes `registered` and `requested` type of training plans from the database.\n    Does not remove the corresponding training plan file from the disk.\n    Default training plans should be removed from the directory\n    Args:\n        training_plan_id: The id of the registered training plan.\n    Returns:\n        Currently always returns True.\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: bad type for parameter\n        FedbiomedTrainingPlanSecurityManagerError: cannot read or remove training plan from the database\n        FedbiomedTrainingPlanSecurityManagerError: training plan is not a `registered` training plan\n            (thus a `default` training plan)\n    \"\"\"\nif not isinstance(training_plan_id, str):\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + \": parameter training_plan_id (str) has bad \"\nf\"type {type(training_plan_id)}\")\ntry:\ntraining_plan = self._db.get(self._database.training_plan_id == training_plan_id)\nexcept Exception as err:\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + \": cannot get training plan from database.\"\nf\"Details: {str(err)}\")\nif training_plan is None:\nraise FedbiomedTrainingPlanSecurityManagerError(ErrorNumbers.FB606.value +\nf\": training plan {training_plan_id} not in database\")\nif training_plan['training_plan_type'] != TrainingPlanStatus.DEFAULT.value:\ntry:\nself._db.remove(doc_ids=[training_plan.doc_id])\nexcept Exception as err:\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + f\": cannot remove training plan from database. Details: {str(err)}\"\n)\nelse:\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + 'For default training plans, please remove training plan file from '\n'`default_training_plans` and restart your node')\nreturn True\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.get_training_plan_by_id","title":"<pre><code>get_training_plan_by_id(training_plan_id, secure=True, content=False)\n</code></pre>","text":"<p>Get a training plan in database given his <code>training_plan_id</code>.</p> <p>Also add a <code>content</code> key to the returned dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_id</code> <code>str</code> <p>id of the training plan to pick from the database</p> required <code>secure</code> <code>bool</code> <p>if <code>True</code> then strip some security sensitive fields</p> <code>True</code> <code>content</code> <code>bool</code> <p>if <code>True</code> add content of training plan in <code>content</code> key of returned training plan. If <code>False</code> then <code>content</code> key value is <code>None</code></p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Dict[str, Any], None]</code> <p>training plan entry from database through a query based on the training plan_id.</p> <code>Union[Dict[str, Any], None]</code> <p>If there is no training plan matching [<code>training_plan_id</code>], returns None</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>bad argument type</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>database access problem</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def get_training_plan_by_id(self,\ntraining_plan_id: str,\nsecure: bool = True,\ncontent: bool = False) -&gt; Union[Dict[str, Any], None]:\n\"\"\"Get a training plan in database given his `training_plan_id`.\n    Also add a `content` key to the returned dictionary.\n    Args:\n        training_plan_id: id of the training plan to pick from the database\n        secure: if `True` then strip some security sensitive fields\n        content: if `True` add content of training plan in `content` key of returned training plan. If `False` then\n            `content` key value is `None`\n    Returns:\n        training plan entry from database through a query based on the training plan_id.\n        If there is no training plan matching [`training_plan_id`], returns None\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: bad argument type\n        FedbiomedTrainingPlanSecurityManagerError: database access problem\n    \"\"\"\nif not isinstance(training_plan_id, str):\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + f': training_plan_id {training_plan_id} is not a string')\ntry:\ntraining_plan = self._db.get(self._database.training_plan_id == training_plan_id)\nexcept Exception as e:\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + f\"database get operation failed, with following error: {str(e)}\")\nif isinstance(training_plan, dict):\nif content:\nwith open(training_plan[\"training_plan_path\"], 'r') as file:\ntraining_plan_content = file.read()\nelse:\ntraining_plan_content = None\nif secure and training_plan is not None:\nself._remove_sensible_keys_from_request(training_plan)\ntraining_plan.update({\"content\": training_plan_content})\nreturn training_plan\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.get_training_plan_by_name","title":"<pre><code>get_training_plan_by_name(training_plan_name)\n</code></pre>","text":"<p>Gets training plan from database, by its name</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_name</code> <code>str</code> <p>name of the training plan entry to search in the database</p> required <p>Returns:</p> Type Description <code>Union[Dict[str, Any], None]</code> <p>training plan entry found in the database matching <code>training_plan_name</code>. Otherwise, returns None.</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>bad argument type</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>cannot read database.</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def get_training_plan_by_name(self, training_plan_name: str) -&gt; Union[Dict[str, Any], None]:\n\"\"\"Gets training plan from database, by its name\n    Args:\n        training_plan_name: name of the training plan entry to search in the database\n    Returns:\n        training plan entry found in the database matching `training_plan_name`. Otherwise, returns None.\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: bad argument type\n        FedbiomedTrainingPlanSecurityManagerError: cannot read database.\n    \"\"\"\nif not isinstance(training_plan_name, str):\nraise FedbiomedTrainingPlanSecurityManagerError(\nf\"{ErrorNumbers.FB606.value} training plan name {training_plan_name} is not a string\"\n)\n# TODO: more robust implementation\n# names in database should be unique, but we don't verify it\n# (and do we properly enforce it ?)\ntry:\ntraining_plan = self._db.get(self._database.name == training_plan_name)\nexcept Exception as e:\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + ': cannot search database for training plan '\nf' \"{training_plan_name}\", error is \"{e}\"')\nif not training_plan:\ntraining_plan = None\nreturn training_plan\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.get_training_plan_from_database","title":"<pre><code>get_training_plan_from_database(training_plan_path)\n</code></pre>","text":"<p>Gets training plan from database, by its hash</p> <p>Training plan file MUST be a *.txt file.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_path</code> <code>str</code> <p>training plan path where the file is saved, in order to compute its hash.</p> required <p>Returns:</p> Name Type Description <code>training_plan</code> <code>Union[Dict[str, Any], None]</code> <p>training plan entry found in the dataset if query in database succeed. Otherwise, returns</p> <code>Union[Dict[str, Any], None]</code> <p>None.</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>bad argument type</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>database access problem</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def get_training_plan_from_database(self,\ntraining_plan_path: str\n) -&gt; Union[Dict[str, Any], None]:\n\"\"\"Gets training plan from database, by its hash\n    !!! info \"Training plan file MUST be a *.txt file.\"\n    Args:\n        training_plan_path: training plan path where the file is saved, in order to compute its hash.\n    Returns:\n        training_plan: training plan entry found in the dataset if query in database succeed. Otherwise, returns\n        None.\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: bad argument type\n        FedbiomedTrainingPlanSecurityManagerError: database access problem\n    \"\"\"\nif not isinstance(training_plan_path, str):\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + \" : no training_plan_path specified\")\nreq_training_plan_hash, _ = self._create_hash(training_plan_path)\n_all_training_plans_which_have_req_hash = (self._database.hash == req_training_plan_hash)\n# TODO: more robust implementation\n# hashes in database should be unique, but we don't verify it\n# (and do we properly enforce it ?)\ntry:\ntraining_plan = self._db.get(_all_training_plans_which_have_req_hash)\nexcept Exception as e:\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + f\"database get operation failed, with following error: {str(e)}\")\nif not training_plan:\ntraining_plan = None\nreturn training_plan\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.list_training_plans","title":"<pre><code>list_training_plans(sort_by=None, select_status=None, verbose=True, search=None)\n</code></pre>","text":"<p>Lists approved training plan files</p> <p>Parameters:</p> Name Type Description Default <code>sort_by</code> <code>Union[str, None]</code> <p>when specified, sort results by alphabetical order, provided sort_by is an entry in the database.</p> <code>None</code> <code>select_status</code> <code>Union[None, TrainingPlanApprovalStatus, List[TrainingPlanApprovalStatus]]</code> <p>filter list by training plan status or list of training plan statuses</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>When it is True, print list of training plan in tabular format. Default is True.</p> <code>True</code> <code>search</code> <code>Union[dict, None]</code> <p>Dictionary that contains <code>text</code> property to declare the text that wil be search and <code>by</code> property to declare text will be search on which field</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of training plans that have been found as <code>registered</code>. Each training plan is in fact a dictionary containing fields (note that following fields are removed :'training_plan_path', 'hash', dates due to privacy reasons).</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>bad type for parameter</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>database access error</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def list_training_plans(\nself,\nsort_by: Union[str, None] = None,\nselect_status: Union[None, TrainingPlanApprovalStatus, List[TrainingPlanApprovalStatus]] = None,\nverbose: bool = True,\nsearch: Union[dict, None] = None\n) -&gt; List[Dict[str, Any]]:\n\"\"\"Lists approved training plan files\n    Args:\n        sort_by: when specified, sort results by alphabetical order,\n            provided sort_by is an entry in the database.\n        select_status: filter list by training plan status or list of training plan statuses\n        verbose: When it is True, print list of training plan in tabular format.\n            Default is True.\n        search: Dictionary that contains `text` property to declare the text that wil be search and `by`\n            property to declare text will be search on which field\n    Returns:\n        A list of training plans that have\n            been found as `registered`. Each training plan is in fact a dictionary\n            containing fields (note that following fields are removed :'training_plan_path',\n            'hash', dates due to privacy reasons).\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: bad type for parameter\n        FedbiomedTrainingPlanSecurityManagerError: database access error\n    \"\"\"\nif sort_by is not None and not isinstance(sort_by, str):\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + f\": parameter sort_by has bad type {type(sort_by)}\")\nif not isinstance(verbose, bool):\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + f\": parameter verbose has bad type {type(verbose)}\")\n# in case select_status is a list, we filter later with elements are TrainingPlanApprovalStatus\nif select_status is not None and not isinstance(select_status, TrainingPlanApprovalStatus) and \\\n            not isinstance(select_status, list):\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + f\": parameter select_status has bad type {type(select_status)}\")\nif search is not None and not isinstance(search, dict):\nraise FedbiomedTrainingPlanSecurityManagerError(f\"{ErrorNumbers.FB606.value}: `search` argument should be \"\nf\"dictionary that contains `text` and `by` (that indicates \"\nf\"field to search on)\")\nif search:\ntry:\ntrainingPlansSearchScheme.validate(search)\nexcept ValidateError as e:\nraise FedbiomedTrainingPlanSecurityManagerError(\nf\"{ErrorNumbers.FB606.value}: `search` argument is not valid. {e}\")\nif isinstance(select_status, (TrainingPlanApprovalStatus, list)):\n# filtering training plan based on their status\nif not isinstance(select_status, list):\n# convert everything into a list\nselect_status = [select_status]\nselect_status = [x.value for x in select_status if isinstance(x, TrainingPlanApprovalStatus)]\n# extract value from TrainingPlanApprovalStatus\ntry:\nif search:\ntraining_plans = self._db.search(self._database.training_plan_status.one_of(select_status) &amp;\nself._database[search[\"by\"]].matches(search[\"text\"],\nflags=re.IGNORECASE))\nelse:\ntraining_plans = self._db.search(self._database.training_plan_status.one_of(select_status))\nexcept Exception as err:\nraise FedbiomedTrainingPlanSecurityManagerError(\nf\"{ErrorNumbers.FB606.value}: request failed when looking for a training plan into database with \"\nf\"error: {err}\"\n)\nelse:\ntry:\nif search:\ntraining_plans = self._db.search(\nself._database[search[\"by\"]].matches(search[\"text\"], flags=re.IGNORECASE))\nelse:\ntraining_plans = self._db.all()\nexcept Exception as e:\nraise FedbiomedTrainingPlanSecurityManagerError(\nf\"{ErrorNumbers.FB606.value} database full read operation failed, with following error: {str(e)}\"\n)\n# Drop some keys for security reasons\nfor doc in training_plans:\nself._remove_sensible_keys_from_request(doc)\nif sort_by is not None:\n# sorting training plan fields by column attributes\ntry:\nis_entry_exists = self._db.search(self._database[sort_by].exists())\nexcept Exception as e:\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + f\"database search operation failed, with following error: {str(e)}\")\nif is_entry_exists and sort_by not in self._tags_to_remove:\ntraining_plans = sorted(training_plans, key=lambda x: (x[sort_by] is None, x[sort_by]))\nelse:\nlogger.warning(f\"Field {sort_by} is not available in dataset\")\nif verbose:\nprint(tabulate(training_plans, headers='keys'))\nreturn training_plans\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.register_training_plan","title":"<pre><code>register_training_plan(name, description, path, training_plan_type=TrainingPlanStatus.REGISTERED.value, training_plan_id=None, researcher_id=None)\n</code></pre>","text":"<p>Approves/registers training plan file through CLI.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Training plan file name. The name should be unique. Otherwise, methods throws an Exception FedbiomedTrainingPlanSecurityManagerError</p> required <code>description</code> <code>str</code> <p>Description for training plan file.</p> required <code>path</code> <code>str</code> <p>Exact path for the training plan that will be registered</p> required <code>training_plan_type</code> <code>str</code> <p>Default is <code>registered</code>. It means that training plan has been registered by a user/hospital. Other value can be <code>default</code> which indicates that training plan is default (training plans for tutorials/examples)</p> <code>TrainingPlanStatus.REGISTERED.value</code> <code>training_plan_id</code> <code>str</code> <p>Pre-defined id for training plan. Default is None. When it is Nonde method creates unique id for the training plan.</p> <code>None</code> <code>researcher_id</code> <code>str</code> <p>ID of the researcher who is owner/requester of the training plan file</p> <code>None</code> <p>Returns:</p> Type Description <code>True</code> <p>Currently always returns True</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p><code>training_plan_type</code> is not <code>registered</code> or <code>default</code></p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>training plan is already registered into database</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>training plan name is already used for saving another training plan</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>database access problem</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def register_training_plan(self,\nname: str,\ndescription: str,\npath: str,\ntraining_plan_type: str = TrainingPlanStatus.REGISTERED.value,\ntraining_plan_id: str = None,\nresearcher_id: str = None\n) -&gt; True:\n\"\"\"Approves/registers training plan file through CLI.\n    Args:\n        name: Training plan file name. The name should be unique. Otherwise, methods\n            throws an Exception FedbiomedTrainingPlanSecurityManagerError\n        description: Description for training plan file.\n        path: Exact path for the training plan that will be registered\n        training_plan_type: Default is `registered`. It means that training plan has been registered\n            by a user/hospital. Other value can be `default` which indicates\n            that training plan is default (training plans for tutorials/examples)\n        training_plan_id: Pre-defined id for training plan. Default is None. When it is Nonde method\n            creates unique id for the training plan.\n        researcher_id: ID of the researcher who is owner/requester of the training plan file\n    Returns:\n        Currently always returns True\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: `training_plan_type` is not `registered` or `default`\n        FedbiomedTrainingPlanSecurityManagerError: training plan is already registered into database\n        FedbiomedTrainingPlanSecurityManagerError: training plan name is already used for saving another training plan\n        FedbiomedTrainingPlanSecurityManagerError: database access problem\n    \"\"\"\n# Check training plan type is valid\nif training_plan_type not in TrainingPlanStatus.list():\nraise FedbiomedTrainingPlanSecurityManagerError(\nf'Unknown training plan (training_plan_type) type: {training_plan_type}')\nif not training_plan_id:\ntraining_plan_id = 'training_plan_' + str(uuid.uuid4())\ntraining_plan_hash, algorithm = self._create_hash(path)\n# Verify no such training plan is already registered\nself._check_training_plan_not_existing(name, path, training_plan_hash, algorithm)\n# Training plan file creation date\nctime = datetime.fromtimestamp(os.path.getctime(path)).strftime(\"%d-%m-%Y %H:%M:%S.%f\")\n# Training plan file modification date\nmtime = datetime.fromtimestamp(os.path.getmtime(path)).strftime(\"%d-%m-%Y %H:%M:%S.%f\")\n# Training plan file registration date\nrtime = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S.%f\")\ntraining_plan_record = dict(name=name, description=description,\nhash=training_plan_hash, training_plan_path=path,\ntraining_plan_id=training_plan_id, training_plan_type=training_plan_type,\ntraining_plan_status=TrainingPlanApprovalStatus.APPROVED.value,\nalgorithm=algorithm,\nresearcher_id=researcher_id,\ndate_created=ctime,\ndate_modified=mtime,\ndate_registered=rtime,\ndate_last_action=rtime\n)\ntry:\nself._db.insert(training_plan_record)\nexcept Exception as err:\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + \" : database insertion failed with\"\nf\" following error: {str(err)}\")\nreturn True\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.register_update_default_training_plans","title":"<pre><code>register_update_default_training_plans()\n</code></pre>","text":"<p>Registers or updates default training plans.</p> <p>Launched when the node is started through CLI, if environ['ALLOW_DEFAULT_TRAINING_PLANS'] is enabled. Checks the files saved into <code>default_training_plans</code> directory and update/register them based on following conditions:</p> <ul> <li>Registers if there is a new training plan file which isn't saved into db.</li> <li>Updates if training plan is modified or if hashing algorithm has changed in config file.</li> </ul> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>cannot read or update training plan database</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def register_update_default_training_plans(self):\n\"\"\"Registers or updates default training plans.\n    Launched when the node is started through CLI, if environ['ALLOW_DEFAULT_TRAINING_PLANS'] is enabled.\n    Checks the files saved into `default_training_plans` directory and update/register them based on following\n    conditions:\n    - Registers if there is a new training plan file which isn't saved into db.\n    - Updates if training plan is modified or if hashing algorithm has changed in config file.\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: cannot read or update training plan database\n    \"\"\"\n# Get training plan files saved in the directory\ntraining_plans_file = os.listdir(environ['DEFAULT_TRAINING_PLANS_DIR'])\n# Get only default training plans from DB\ntry:\ntraining_plans = self._db.search(self._database.training_plan_type == 'default')\nexcept Exception as e:\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + f\"database search operation failed, with following error: {str(e)}\")\n# Get training plan names from list of training plans\ntraining_plans_name_db = [training_plan.get('name') for training_plan in training_plans if\nisinstance(training_plan, dict)]\n# Default training plans not in database\ntraining_plans_not_saved = list(set(training_plans_file) - set(training_plans_name_db))\n# Default training plans that have been deleted from file system but not in DB\ntraining_plans_deleted = list(set(training_plans_name_db) - set(training_plans_file))\n# Training plans have already saved and exist in the database\ntraining_plans_exists = list(set(training_plans_file) - set(training_plans_not_saved))\n# Register new default training plans\nfor training_plan in training_plans_not_saved:\nself.register_training_plan(name=training_plan,\ndescription=\"Default training plan\",\npath=os.path.join(environ['DEFAULT_TRAINING_PLANS_DIR'], training_plan),\ntraining_plan_type='default')\n# Remove training plans that have been removed from file system\nfor training_plan_name in training_plans_deleted:\ntry:\ntraining_plan_doc = self._db.get(self._database.name == training_plan_name)\nlogger.info('Removed default training plan file has been detected,'\nf' it will be removed from DB as well: {training_plan_name}')\nself._db.remove(doc_ids=[training_plan_doc.doc_id])\nexcept Exception as err:\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + \": failed to update database, \"\nf\" with error {str(err)}\")\n# Update training plans\nfor training_plan in training_plans_exists:\npath = os.path.join(environ['DEFAULT_TRAINING_PLANS_DIR'], training_plan)\nmtime = datetime.fromtimestamp(os.path.getmtime(path))\ntry:\ntraining_plan_info = self._db.get(self._database.name == training_plan)\nexcept Exception as err:\nraise FedbiomedTrainingPlanSecurityManagerError(ErrorNumbers.FB606.value +\nf\": failed to get training_plan info for training plan {training_plan}\"\nf\"Details : {str(err)}\")\n# Check if hashing algorithm has changed\ntry:\nhash, algorithm = self._create_hash(os.path.join(environ['DEFAULT_TRAINING_PLANS_DIR'], training_plan))\nif training_plan_info['algorithm'] != environ['HASHING_ALGORITHM']:\n# Verify no such training plan already exists in DB\nself._check_training_plan_not_existing(None, None, hash, algorithm)\nlogger.info(\nf'Recreating hashing for : {training_plan_info[\"name\"]} \\t {training_plan_info[\"training_plan_id\"]}')\nself._db.update({'hash': hash, 'algorithm': algorithm,\n'date_last_action': datetime.now().strftime(\"%d-%m-%Y %H:%M:%S.%f\")},\nself._database.training_plan_path == path)\n# If default training plan file is modified update hashing\nelif mtime &gt; datetime.strptime(training_plan_info['date_modified'], \"%d-%m-%Y %H:%M:%S.%f\"):\n# only check when hash changes\n# else we have error because this training plan exists in database with same hash\nif hash != training_plan_info['hash']:\n# Verify no such training plan already exists in DB\nself._check_training_plan_not_existing(None, None, hash, algorithm)\nlogger.info(\nf\"Modified default training plan file has been detected. Hashing will be updated for: {training_plan}\")\nself._db.update({'hash': hash, 'algorithm': algorithm,\n'date_modified': mtime.strftime(\"%d-%m-%Y %H:%M:%S.%f\"),\n'date_last_action': datetime.now().strftime(\"%d-%m-%Y %H:%M:%S.%f\")},\nself._database.training_plan_path == path)\nexcept Exception as err:\n# triggered if database update failed (see `update` method in tinydb code)\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + \": Failed to update database, with error: \"\nf\"{str(err)}\")\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.reject_training_plan","title":"<pre><code>reject_training_plan(training_plan_id, extra_notes=None)\n</code></pre>","text":"<p>Approves a training plan stored into the database given its [<code>training_plan_id</code>]</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_id</code> <code>str</code> <p>id of the training plan.</p> required <code>extra_notes</code> <code>Union[str, None]</code> <p>notes detailing why training plan has been rejected. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>True</code> <p>Currently always returns True</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def reject_training_plan(self, training_plan_id: str, extra_notes: Union[str, None] = None) -&gt; True:\n\"\"\"Approves a training plan stored into the database given its [`training_plan_id`]\n    Args:\n        training_plan_id: id of the training plan.\n        extra_notes: notes detailing why training plan has been rejected. Defaults to None.\n    Returns:\n        Currently always returns True\n    \"\"\"\nres = self._update_training_plan_status(training_plan_id,\nTrainingPlanApprovalStatus.REJECTED,\nextra_notes)\nreturn res\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.reply_training_plan_approval_request","title":"<pre><code>reply_training_plan_approval_request(msg, messaging)\n</code></pre>","text":"<p>Submits a training plan file (TrainingPlan) for approval. Needs an action from Node</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict</code> <p>approval request message, received from Researcher</p> required <code>messaging</code> <code>Messaging</code> <p>MQTT client to send reply  to researcher</p> required Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def reply_training_plan_approval_request(self, msg: dict, messaging: Messaging):\n\"\"\"Submits a training plan file (TrainingPlan) for approval. Needs an action from Node\n    Args:\n        msg: approval request message, received from Researcher\n        messaging: MQTT client to send reply  to researcher\n    \"\"\"\nreply = {\n'researcher_id': msg['researcher_id'],\n'node_id': environ['NODE_ID'],\n# 'training_plan_url': msg['training_plan_url'],\n'sequence': msg['sequence'],\n'status': 0,  # HTTP status (set by default to 0, non-existing HTTP status code)\n'command': 'approval'\n}\nis_existant = False\ndownloadable_checkable = True\ntry:\n# training_plan_id = str(uuid.uuid4())\ntraining_plan_name = \"training_plan_\" + str(uuid.uuid4())\nstatus, tmp_file = self._repo.download_file(msg['training_plan_url'], training_plan_name + '.py')\nreply['status'] = status\n# check if training plan has already been registered into database\ntraining_plan_to_check = self.create_txt_training_plan_from_py(tmp_file)\nis_existant, _ = self.check_training_plan_status(training_plan_to_check, None)\nexcept FedbiomedRepositoryError as fed_err:\nlogger.error(f\"Cannot download training plan from server due to error: {fed_err}\")\ndownloadable_checkable = False\nexcept FedbiomedTrainingPlanSecurityManagerError as fed_err:\ndownloadable_checkable = False\nlogger.error(\nf\"Can not check whether training plan has already be registered or not due to error: {fed_err}\")\nif not is_existant and downloadable_checkable:\n# move training plan into corresponding directory (from TMP_DIR to TRAINING_PLANS_DIR)\ntry:\nlogger.debug(\"Storing TrainingPlan into requested training plan directory\")\ntraining_plan_path = os.path.join(environ['TRAINING_PLANS_DIR'], training_plan_name + '.py')\nshutil.move(tmp_file, training_plan_path)\n# Training plan file creation date\nctime = datetime.fromtimestamp(os.path.getctime(training_plan_path)).strftime(\"%d-%m-%Y %H:%M:%S.%f\")\nexcept (PermissionError, FileNotFoundError, OSError) as err:\nreply['success'] = False\nlogger.error(f\"Cannot save training plan '{msg['description']} 'into directory due to error : {err}\")\nelse:\ntry:\ntraining_plan_hash, hash_algo = self._create_hash(training_plan_to_check)\ntraining_plan_object = dict(name=training_plan_name,\ndescription=msg['description'],\nhash=training_plan_hash,\ntraining_plan_path=training_plan_path,\ntraining_plan_id=training_plan_name,\ntraining_plan_type=TrainingPlanStatus.REQUESTED.value,\ntraining_plan_status=TrainingPlanApprovalStatus.PENDING.value,\nalgorithm=hash_algo,\ndate_created=ctime,\ndate_modified=ctime,\ndate_registered=ctime,\ndate_last_action=None,\nresearcher_id=msg['researcher_id'],\nnotes=None\n)\nself._db.upsert(training_plan_object, self._database.hash == training_plan_hash)\n# `upsert` stands for update and insert in TinyDB. This prevents any duplicate, that can happen\n# if same training plan is sent twice to Node for approval\nexcept Exception as err:\nreply['success'] = False\nlogger.error(f\"Cannot add training plan '{msg['description']} 'into database due to error : {err}\")\nelse:\nreply['success'] = True\nlogger.debug(f\"Training plan '{msg['description']}' successfully received by Node for approval\")\nelif is_existant and downloadable_checkable:\nif self.check_training_plan_status(training_plan_to_check, TrainingPlanApprovalStatus.PENDING)[0]:\nlogger.info(f\"Training plan '{msg['description']}' already sent for Approval (status Pending). \"\n\"Please wait for Node approval.\")\nelif self.check_training_plan_status(training_plan_to_check, TrainingPlanApprovalStatus.APPROVED)[0]:\nlogger.info(\nf\"Training plan '{msg['description']}' is already Approved. Ready to train on this training plan.\")\nelse:\nlogger.warning(f\"Training plan '{msg['description']}' already exists in database. Aborting\")\nreply['success'] = True\nelse:\n# case where training plan is non-downloadable or non-checkable\nreply['success'] = False\n# Send training plan approval acknowledge answer to researcher\nmessaging.send_message(NodeMessages.format_outgoing_message(reply).get_dict())\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.reply_training_plan_status_request","title":"<pre><code>reply_training_plan_status_request(msg, messaging)\n</code></pre>","text":"<p>Returns requested training plan file status {approved, rejected, pending} and sends TrainingPlanStatusReply to researcher.</p> <p>Called directly from Node.py when it receives TrainingPlanStatusRequest.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict</code> <p>Message that is received from researcher. Formatted as TrainingPlanStatusRequest</p> required <code>messaging</code> <code>Messaging</code> <p>MQTT client to send reply  to researcher</p> required Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def reply_training_plan_status_request(self, msg: dict, messaging: Messaging):\n\"\"\"Returns requested training plan file status {approved, rejected, pending}\n    and sends TrainingPlanStatusReply to researcher.\n    Called directly from Node.py when it receives TrainingPlanStatusRequest.\n    Args:\n        msg: Message that is received from researcher.\n            Formatted as TrainingPlanStatusRequest\n        messaging: MQTT client to send reply  to researcher\n    \"\"\"\n# Main header for the training plan status request\nheader = {\n'researcher_id': msg['researcher_id'],\n'node_id': environ['NODE_ID'],\n'job_id': msg['job_id'],\n'training_plan_url': msg['training_plan_url'],\n'command': 'training-plan-status'\n}\ntry:\n# Create training plan file with id and download\ntraining_plan_name = 'my_training_plan_' + str(uuid.uuid4().hex)\nstatus, training_plan_file = self._repo.download_file(msg['training_plan_url'], training_plan_name + '.py')\nif status != 200:\n# FIXME: should 'approval_obligation' be always false when training plan cannot be downloaded,\n#  regardless of environment variable \"TRAINING_PLAN_APPROVAL\"?\nreply = {**header,\n'success': False,\n'approval_obligation': False,\n'status': 'Error',\n'msg': f'Can not download training plan file. {msg[\"training_plan_url\"]}'}\nelse:\ntraining_plan = self.get_training_plan_from_database(training_plan_file)\nif training_plan is not None:\ntraining_plan_status = training_plan.get('training_plan_status', 'Not Registered')\nelse:\ntraining_plan_status = 'Not Registered'\nif environ[\"TRAINING_PLAN_APPROVAL\"]:\nif training_plan_status == TrainingPlanApprovalStatus.APPROVED.value:\nmsg = \"Training plan has been approved by the node, training can start\"\nelif training_plan_status == TrainingPlanApprovalStatus.PENDING.value:\nmsg = \"Training plan is pending: waiting for a review\"\nelif training_plan_status == TrainingPlanApprovalStatus.REJECTED.value:\nmsg = \"Training plan has been rejected by the node, training is not possible\"\nelse:\nmsg = f\"Unknown training plan not in database (status {training_plan_status})\"\nreply = {**header,\n'success': True,\n'approval_obligation': True,\n'status': training_plan_status,\n'msg': msg}\nelse:\nreply = {**header,\n'success': True,\n'approval_obligation': False,\n'status': training_plan_status,\n'msg': 'This node does not require training plan approval (maybe for debugging purposes).'}\nexcept FedbiomedTrainingPlanSecurityManagerError as fed_err:\nreply = {**header,\n'success': False,\n'approval_obligation': False,\n'status': 'Error',\n'msg': ErrorNumbers.FB606.value +\nf': Cannot check if training plan has been registered. Details {fed_err}'}\nexcept FedbiomedRepositoryError as fed_err:\nreply = {**header,\n'success': False,\n'approval_obligation': False,\n'status': 'Error',\n'msg': f'{ErrorNumbers.FB604.value}: An error occurred when downloading training plan file. '\nf'{msg[\"training_plan_url\"]} , {fed_err}'}\nexcept Exception as e:\nreply = {**header,\n'success': False,\n'approval_obligation': False,\n'status': 'Error',\n'msg': f'{ErrorNumbers.FB606.value}: An unknown error occurred when downloading training plan '\nf'file. {msg[\"training_plan_url\"]} , {e}'}\n# finally:\n#     # Send check training plan status answer to researcher\nmessaging.send_message(NodeMessages.format_outgoing_message(reply).get_dict())\nreturn\n</code></pre>"},{"location":"developer/api/node/training_plan_security_manager/#fedbiomed.node.training_plan_security_manager.TrainingPlanSecurityManager.update_training_plan_hash","title":"<pre><code>update_training_plan_hash(training_plan_id, path)\n</code></pre>","text":"<p>Updates an existing training plan entry in training plan database.</p> <p>Training plan entry cannot be a default training plan.</p> <p>The training plan entry to update is indicated by its <code>training_plan_id</code> The new training plan file for the training plan is specified from <code>path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_id</code> <code>str</code> <p>id of the training plan to update</p> required <code>path</code> <code>str</code> <p>path where new training plan file is stored</p> required <p>Returns:</p> Type Description <code>True</code> <p>Currently always returns True.</p> <p>Raises:</p> Type Description <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>try to update a default training plan</p> <code>FedbiomedTrainingPlanSecurityManagerError</code> <p>cannot read or update the training plan in database</p> Source code in <code>fedbiomed/node/training_plan_security_manager.py</code> <pre><code>def update_training_plan_hash(self, training_plan_id: str, path: str) -&gt; True:\n\"\"\"Updates an existing training plan entry in training plan database.\n    Training plan entry cannot be a default training plan.\n    The training plan entry to update is indicated by its `training_plan_id`\n    The new training plan file for the training plan is specified from `path`.\n    Args:\n        training_plan_id: id of the training plan to update\n        path: path where new training plan file is stored\n    Returns:\n        Currently always returns True.\n    Raises:\n        FedbiomedTrainingPlanSecurityManagerError: try to update a default training plan\n        FedbiomedTrainingPlanSecurityManagerError: cannot read or update the training plan in database\n    \"\"\"\n# Register training plan\ntry:\ntraining_plan = self._db.get(self._database.training_plan_id == training_plan_id)\nexcept Exception as err:\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + \": get request on database failed.\"\nf\" Details: {str(err)}\")\nif training_plan['training_plan_type'] != TrainingPlanStatus.DEFAULT.value:\nhash, algorithm = self._create_hash(path)\n# Verify no such training plan already exists in DB\nself._check_training_plan_not_existing(None, path, hash, algorithm)\n# Get modification date\nmtime = datetime.fromtimestamp(os.path.getmtime(path))\n# Get creation date\nctime = datetime.fromtimestamp(os.path.getctime(path))\ntry:\nself._db.update({'hash': hash, 'algorithm': algorithm,\n'date_modified': mtime.strftime(\"%d-%m-%Y %H:%M:%S.%f\"),\n'date_created': ctime.strftime(\"%d-%m-%Y %H:%M:%S.%f\"),\n'date_last_action': datetime.now().strftime(\"%d-%m-%Y %H:%M:%S.%f\"),\n'training_plan_path': path},\nself._database.training_plan_id == training_plan_id)\nexcept Exception as err:\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + \": update database failed. Details :\"\nf\"{str(err)}\")\nelse:\nraise FedbiomedTrainingPlanSecurityManagerError(\nErrorNumbers.FB606.value + 'You cannot update default training plans. Please '\n'update them through their files saved in `default_training_plans` directory '\n'and restart your node')\nreturn True\n</code></pre>"},{"location":"developer/api/researcher/aggregators/","title":"Aggregators","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators","title":"fedbiomed.researcher.aggregators","text":"Module: <code>fedbiomed.researcher.aggregators</code>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators-classes","title":"Classes","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Aggregator","title":"Aggregator","text":"CLASS  <pre><code>Aggregator()\n</code></pre> <p>Defines methods for aggregating strategy (eg FedAvg, FedProx, SCAFFOLD, ...).</p> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def __init__(self):\nself._aggregator_args: dict = None\nself._fds: FederatedDataSet = None\nself._training_plan_type: TrainingPlans = None\nself._secagg_crypter = SecaggCrypter()\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Aggregator-functions","title":"Functions","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.aggregator.Aggregator.aggregate","title":"<pre><code>aggregate(model_params, weights, args, kwargs)\n</code></pre>","text":"<p>Strategy to aggregate models</p> <p>Parameters:</p> Name Type Description Default <code>model_params</code> <code>list</code> <p>List of model parameters received from each node</p> required <code>weights</code> <code>list</code> <p>Weight for each node-model-parameter set</p> required <p>Raises:</p> Type Description <code>FedbiomedAggregatorError</code> <p>If the method is not defined by inheritor</p> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def aggregate(self, model_params: list, weights: list, *args, **kwargs) -&gt; Dict:\n\"\"\"\n    Strategy to aggregate models\n    Args:\n        model_params: List of model parameters received from each node\n        weights: Weight for each node-model-parameter set\n    Raises:\n        FedbiomedAggregatorError: If the method is not defined by inheritor\n    \"\"\"\nmsg = ErrorNumbers.FB401.value + \\\n        \": aggregate method should be overloaded by the choosen strategy\"\nlogger.critical(msg)\nraise FedbiomedAggregatorError(msg)\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.aggregator.Aggregator.check_values","title":"<pre><code>check_values(args, kwargs)\n</code></pre>","text":"Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def check_values(self, *args, **kwargs) -&gt; True:\nreturn True\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.aggregator.Aggregator.create_aggregator_args","title":"<pre><code>create_aggregator_args(args, kwargs)\n</code></pre>","text":"<p>Returns aggregator arguments that are expecting by the nodes</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>contains <code>Aggregator</code> parameters that will be sent through MQTT message     service</p> <code>dict</code> <code>dict</code> <p>contains parameters that will be sent through file exchange message.     Both dictionaries are mapping node_id to 'Aggregator` parameters specific     to each Node.</p> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def create_aggregator_args(self, *args, **kwargs) -&gt; Tuple[dict, dict]:\n\"\"\"Returns aggregator arguments that are expecting by the nodes\n    Returns:\n        dict: contains `Aggregator` parameters that will be sent through MQTT message\n                service\n        dict: contains parameters that will be sent through file exchange message.\n                Both dictionaries are mapping node_id to 'Aggregator` parameters specific\n                to each Node.\n    \"\"\"\nreturn self._aggregator_args or {}, {}\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.aggregator.Aggregator.load_state","title":"<pre><code>load_state(state, kwargs)\n</code></pre>","text":"<p>use for breakpoints. load the aggregator state</p> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def load_state(self, state: Dict[str, Any], **kwargs) -&gt; None:\n\"\"\"\n    use for breakpoints. load the aggregator state\n    \"\"\"\nself._aggregator_args = state['parameters']\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.aggregator.Aggregator.save_state","title":"<pre><code>save_state(breakpoint_path=None, aggregator_args_create)\n</code></pre>","text":"<p>use for breakpoints. save the aggregator state</p> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def save_state(\nself,\nbreakpoint_path: Optional[str] = None,\n**aggregator_args_create: Any,\n) -&gt; Dict[str, Any]:\n\"\"\"\n    use for breakpoints. save the aggregator state\n    \"\"\"\naggregator_args_thr_msg, aggregator_args_thr_files = self.create_aggregator_args(**aggregator_args_create)\nif aggregator_args_thr_msg:\nif self._aggregator_args is None:\nself._aggregator_args = {}\nself._aggregator_args.update(aggregator_args_thr_msg)\n# aggregator_args = copy.deepcopy(self._aggregator_args)\nif breakpoint_path is not None and aggregator_args_thr_files:\nfor node_id, node_arg in aggregator_args_thr_files.items():\nif isinstance(node_arg, dict):\nfor arg_name, aggregator_arg in node_arg.items():\nif arg_name != 'aggregator_name': # do not save `aggregator_name` as a file\nfilename = self._save_arg_to_file(breakpoint_path, arg_name, node_id, aggregator_arg)\nself._aggregator_args.setdefault(arg_name, {})\nself._aggregator_args[arg_name][node_id] = filename  # replacing value by a path towards a file\nelse:\nfilename = self._save_arg_to_file(breakpoint_path, arg_name, node_id, node_arg)\nself._aggregator_args[arg_name] = filename\nstate = {\n\"class\": type(self).__name__,\n\"module\": self.__module__,\n\"parameters\": self._aggregator_args\n}\nreturn state\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.aggregator.Aggregator.secure_aggregation","title":"<pre><code>secure_aggregation(params, encryption_factors, secagg_random, aggregation_round, total_sample_size, training_plan)\n</code></pre>","text":"<p>Apply aggregation for encrypted model parameters</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>List[List[int]]</code> <p>List containing list of encrypted parameters of each node</p> required <code>encryption_factors</code> <code>List[Dict[str, List[int]]]</code> <p>List of encrypted integers to validate encryption</p> required <code>secagg_random</code> <code>float</code> <p>Randomly generated float value to validate secure aggregation correctness</p> required <code>aggregation_round</code> <code>int</code> <p>The round of the aggregation.</p> required <code>total_sample_size</code> <code>int</code> <p>Sum of sample sizes used for training</p> required <code>training_plan</code> <code>BaseTrainingPlan</code> <p>Training plan instance used for the training.</p> required <p>Returns:</p> Type Description <p>aggregated model parameters</p> Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def secure_aggregation(\nself,\nparams: List[List[int]],\nencryption_factors: List[Dict[str, List[int]]],\nsecagg_random: float,\naggregation_round: int,\ntotal_sample_size: int,\ntraining_plan: 'BaseTrainingPlan'\n):\n\"\"\" Apply aggregation for encrypted model parameters\n    Args:\n        params: List containing list of encrypted parameters of each node\n        encryption_factors: List of encrypted integers to validate encryption\n        secagg_random: Randomly generated float value to validate secure aggregation correctness\n        aggregation_round: The round of the aggregation.\n        total_sample_size: Sum of sample sizes used for training\n        training_plan: Training plan instance used for the training.\n    Returns:\n        aggregated model parameters\n    \"\"\"\n# TODO: verify with secagg context number of parties\nnum_nodes = len(params)\n# TODO: Use server key here\nkey = -(len(params) * 10)\n# IMPORTANT = Keep this key for testing purposes\nkey = -4521514305280526329525552501850970498079782904248225896786295610941010325354834129826500373412436986239012584207113747347251251180530850751209537684586944643780840182990869969844131477709433555348941386442841023261287875379985666260596635843322044109172782411303407030194453287409138194338286254652273563418119335656859169132074431378389356392955315045979603414700450628308979043208779867835835935403213000649039155952076869962677675951924910959437120608553858253906942559260892494214955907017206115207769238347962438107202114814163305602442458693305475834199715587932463252324681290310458316249381037969151400784780\nlogger.info(\"Securely aggregating model parameters...\")\naggregate = functools.partial(self._secagg_crypter.aggregate,\ncurrent_round=aggregation_round,\nnum_nodes=num_nodes,\nkey=key,\ntotal_sample_size=total_sample_size\n)\n# Validation\nencryption_factors = [f for k, f in encryption_factors.items()]\nvalidation: List[int] = aggregate(params=encryption_factors)\nif len(validation) != 1 or not math.isclose(validation[0], secagg_random, abs_tol=0.01):\nraise FedbiomedAggregatorError(\"Aggregation is failed due to incorrect decryption.\")\naggregated_params = aggregate(params=params)\n# Convert model params\nmodel = training_plan._model\nmodel_params = model.unflatten(aggregated_params)\nreturn model_params\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.aggregator.Aggregator.set_fds","title":"<pre><code>set_fds(fds)\n</code></pre>","text":"Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def set_fds(self, fds: FederatedDataSet) -&gt; FederatedDataSet:\nself._fds = fds\nreturn self._fds\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.aggregator.Aggregator.set_training_plan_type","title":"<pre><code>set_training_plan_type(training_plan_type)\n</code></pre>","text":"Source code in <code>fedbiomed/researcher/aggregators/aggregator.py</code> <pre><code>def set_training_plan_type(self, training_plan_type: TrainingPlans) -&gt; TrainingPlans:\nself._training_plan_type = training_plan_type\nreturn self._training_plan_type\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.FedAverage","title":"FedAverage","text":"CLASS  <pre><code>FedAverage()\n</code></pre> <p>           Bases: <code>Aggregator</code></p> <p>Defines the Federated averaging strategy</p> Source code in <code>fedbiomed/researcher/aggregators/fedavg.py</code> <pre><code>def __init__(self):\n\"\"\"Construct `FedAverage` object as an instance of [`Aggregator`]\n    [fedbiomed.researcher.aggregators.Aggregator].\n    \"\"\"\nsuper(FedAverage, self).__init__()\nself.aggregator_name = \"FedAverage\"\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.FedAverage-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.fedavg.FedAverage.aggregator_name","title":"aggregator_name     <code>instance-attribute</code>","text":"<pre><code>aggregator_name = 'FedAverage'\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.FedAverage-functions","title":"Functions","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.fedavg.FedAverage.aggregate","title":"<pre><code>aggregate(model_params, weights, args, kwargs)\n</code></pre>","text":"<p>Aggregates  local models sent by participating nodes into a global model, following Federated Averaging strategy.</p> <p>weights is a list of single-item dictionaries, each dictionary has the node id as key, and the weight as value. model_params is a list of single-item dictionaries, each dictionary has the node is as key, and a framework-specific representation of the model parameters as value.</p> <p>Parameters:</p> Name Type Description Default <code>model_params</code> <code>Dict[str, Dict[str, Union[torch.Tensor, numpy.ndarray]]]</code> <p>contains each model layers</p> required <code>weights</code> <code>Dict[str, float]</code> <p>contains all weights of a given layer.</p> required <p>Returns:</p> Type Description <code>Mapping[str, Union[torch.Tensor, numpy.ndarray]]</code> <p>Aggregated parameters</p> Source code in <code>fedbiomed/researcher/aggregators/fedavg.py</code> <pre><code>def aggregate(\nself,\nmodel_params: Dict[str, Dict[str, Union['torch.Tensor', 'numpy.ndarray']]],\nweights: Dict[str, float],\n*args,\n**kwargs\n) -&gt; Mapping[str, Union['torch.Tensor', 'numpy.ndarray']]:\n\"\"\" Aggregates  local models sent by participating nodes into a global model, following Federated Averaging\n    strategy.\n    weights is a list of single-item dictionaries, each dictionary has the node id as key, and the weight as value.\n    model_params is a list of single-item dictionaries, each dictionary has the node is as key,\n    and a framework-specific representation of the model parameters as value.\n    Args:\n        model_params: contains each model layers\n        weights: contains all weights of a given layer.\n    Returns:\n        Aggregated parameters\n    \"\"\"\nmodel_params_processed = []\nweights_processed = []\nfor node_id, params in model_params.items():\nif node_id not in weights:\nraise FedbiomedAggregatorError(\nf\"{ErrorNumbers.FB401.value}. Can not find corresponding calculated weight for the \"\nf\"node {node_id}. Aggregation is aborted.\"\n)\nweight = weights[node_id]\nmodel_params_processed.append(params)\nweights_processed.append(weight)\nif any([x &lt; 0. or x &gt; 1. for x in weights_processed]) or sum(weights_processed) == 0:\nraise FedbiomedAggregatorError(\nf\"{ErrorNumbers.FB401.value}. Aggregation aborted due to sum of the weights is equal to 0 {weights}. \"\nf\"Sample sizes received from nodes might be corrupted.\"\n)\nagg_params = federated_averaging(model_params_processed, weights_processed)\nreturn agg_params\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold","title":"Scaffold","text":"CLASS  <pre><code>Scaffold(server_lr=1.0, fds=None)\n</code></pre> <p>           Bases: <code>Aggregator</code></p> <p>Defines the Scaffold strategy</p> <p>Despite being an algorithm of choice for federated learning, it is observed that FedAvg suffers from <code>client-drift</code> when the data is heterogeneous (non-iid), resulting in unstable and slow convergence. SCAFFOLD uses control variates (variance reduction) to correct for the <code>client-drift</code> in its local updates. Intuitively, SCAFFOLD estimates the update direction for the server model (c) and the update direction for each client (c_i). The difference (c - c_i) is then an estimate of the client-drift which is used to correct the local update.</p>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold--fed-biomed-implementation-details","title":"Fed-BioMed implementation details","text":"<p>Our implementation is heavily influenced by our design choice to prevent storing any state on the nodes between FL rounds. In particular, this means that the computation of the control variates (i.e. the correction states) needs to be performed centrally by the aggregator. Roughly, our implementation follows these steps (following the notation of the original Scaffold paper):</p> <ol> <li>let \\(\\delta_i = \\mathbf{c}_i - \\mathbf{c} \\)</li> <li>foreach(round):</li> <li>sample \\( S \\) nodes participating in this round out of \\( N \\) total</li> <li>the server communicates the global model \\( \\mathbf{x} \\) and the correction states \\( \\delta_i \\) to all clients</li> <li>parallel on each client</li> <li>initialize local model \\( \\mathbf{y}_i = \\mathbf{x} \\)</li> <li>foreach(update) until K updates have been performed</li> <li>obtain a data batch</li> <li>compute the gradients for this batch \\( g(\\mathbf{y}_i) \\)</li> <li>apply correction term to gradients \\( g(\\mathbf{y}_i) -= \\delta_i \\)</li> <li>update model with one optimizer step e.g. for SGD \\( \\mathbf{y}_i -= \\eta_i g(\\mathbf{y}_i) \\)</li> <li>end foreach(update)</li> <li>communicate updated model \\( \\mathbf{y}_i \\) and learning rate \\( \\eta_i \\)</li> <li>end parallel section on each client</li> <li>the server computes the node-wise model update \\( \\mathbf{\\Delta y}_i =  \\mathbf{x} - \\mathbf{y}_i \\)</li> <li>the server updates the node-wise states \\( \\mathbf{c}_i = \\delta_i + (\\mathbf{\\Delta y}_i) / (\\eta_i K) \\)</li> <li>the server updates the global state \\( \\mathbf{c} = (1/N) \\sum_{i \\in N} \\mathbf{c}_i \\)</li> <li>the server updates the node-wise correction state \\(\\delta_i = \\mathbf{c}_i - \\mathbf{c} \\)</li> <li>the server updates the global model by averaging \\( \\mathbf{x} = \\mathbf{x} - (\\eta/|S|) \\sum_{i \\in S} \\mathbf{\\Delta y}_i \\)</li> <li>end foreach(round)</li> </ol> <p>This diagram provides a visual representation of the algorithm.</p> <p>References:</p> <ul> <li>Scaffold: Stochastic Controlled Averaging for Federated Learning</li> <li>TCT: Convexifying Federated Learning using Bootstrapped Neural Tangent Kernels</li> </ul> <p>Attributes:</p> Name Type Description <code>aggregator_name</code> <code>str</code> <p>name of the aggregator</p> <code>server_lr</code> <code>float</code> <p>value of the server learning rate</p> <code>global_state</code> <code>Dict[str, Union[torch.Tensor, np.ndarray]]</code> <p>a dictionary representing the global correction state \\( \\mathbf{c} \\) in the format   {parameter name: correction value}</p> <code>nodes_states</code> <code>Dict[str, Dict[str, Union[torch.Tensor, np.ndarray]]]</code> <p>a nested dictionary   of correction parameters obtained for each client, in the format {node id: node-wise corrections}. The   node-wise corrections are a dictionary in the format {parameter name: correction value} where the   model parameters are those contained in each node's model.named_parameters().</p> <code>nodes_deltas</code> <code>Dict[str, Dict[str, Union[torch.Tensor, np.ndarray]]]</code> <p>a nested dictionary of deltas for each client, in the same format as nodes_states. The deltas   are defined as \\(\\delta_i = \\mathbf{c}_i - \\mathbf{c} \\)</p> <code>nodes_lr</code> <code>Dict[str, Dict[str, float]]</code> <p>dictionary of learning rates observed at end of the latest round, in the format   {node id: learning rate}</p> <p>Parameters:</p> Name Type Description Default <code>server_lr</code> <code>float</code> <p>server's (or Researcher's) learning rate. Defaults to 1..</p> <code>1.0</code> <code>fds</code> <code>FederatedDataset</code> <p>FederatedDataset obtained after a <code>search</code> request. Defaults to None.</p> <code>None</code> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def __init__(self, server_lr: float = 1., fds: Optional[FederatedDataSet] = None):\n\"\"\"Constructs `Scaffold` object as an instance of [`Aggregator`]\n    [fedbiomed.researcher.aggregators.Aggregator].\n    Args:\n        server_lr (float): server's (or Researcher's) learning rate. Defaults to 1..\n        fds (FederatedDataset, optional): FederatedDataset obtained after a `search` request. Defaults to None.\n    \"\"\"\nsuper().__init__()\nself.aggregator_name: str = \"Scaffold\"\nif server_lr == 0.:\nraise FedbiomedAggregatorError(\"SCAFFOLD Error: Server learning rate cannot be equal to 0\")\nself.server_lr: float = server_lr\nself.global_state: Dict[str, Union[torch.Tensor, np.ndarray]] = {}\nself.nodes_states: Dict[str, Dict[str, Union[torch.Tensor, np.ndarray]]] = {}\nself.nodes_deltas: Dict[str, Dict[str, Union[torch.Tensor, np.ndarray]]] = {}\nself.nodes_lr: Dict[str, Dict[str, float]] = {}\nif fds is not None:\nself.set_fds(fds)\nself._aggregator_args = {}  # we need `_aggregator_args` to be not None\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.aggregator_name","title":"aggregator_name     <code>instance-attribute</code>","text":"<pre><code>aggregator_name: str = 'Scaffold'\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.global_state","title":"global_state     <code>instance-attribute</code>","text":"<pre><code>global_state: Dict[str, Union[torch.Tensor, np.ndarray]] = {}\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.nodes_deltas","title":"nodes_deltas     <code>instance-attribute</code>","text":"<pre><code>nodes_deltas: Dict[str, Dict[str, Union[torch.Tensor, np.ndarray]]] = {}\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.nodes_lr","title":"nodes_lr     <code>instance-attribute</code>","text":"<pre><code>nodes_lr: Dict[str, Dict[str, float]] = {}\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.nodes_states","title":"nodes_states     <code>instance-attribute</code>","text":"<pre><code>nodes_states: Dict[str, Dict[str, Union[torch.Tensor, np.ndarray]]] = {}\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.server_lr","title":"server_lr     <code>instance-attribute</code>","text":"<pre><code>server_lr: float = server_lr\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.Scaffold-functions","title":"Functions","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.aggregate","title":"<pre><code>aggregate(model_params, weights, global_model, training_plan, training_replies, n_updates=1, n_round=0, args, kwargs)\n</code></pre>","text":"<p>Aggregates local models coming from nodes into a global model, using SCAFFOLD algorithm (2nd option) [Scaffold: Stochastic Controlled Averaging for Federated Learning][https://arxiv.org/abs/1910.06378]</p>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.aggregate--performed-computations","title":"Performed computations:","text":"<ul> <li>Compute participating nodes' model update:<ul> <li>update_i = y_i - x</li> </ul> </li> <li>Compute aggregated model parameters:<ul> <li>x(+) = x - eta_g sum_S(update_i)</li> </ul> </li> <li>Update participating nodes' state:<ul> <li>c_i = delta_i + 1/(K*eta_i) * update_i</li> </ul> </li> <li>Update the global state and all nodes' correction state:<ul> <li>c = 1/N sum_{i=1}^n c_i</li> <li>delta_i = (c_i - c)</li> </ul> </li> </ul> <p>where, according to paper notations     c_i: local state variable for node <code>i</code>     c: global state variable     delta_i: (c_i - c), correction state for node <code>i</code>     eta_g: server's learning rate     eta_i: node i's learning rate     N: total number of node participating to federated learning     S: number of nodes considered during current round (S&lt;=N)     K: number of updates done during the round (ie number of data batches).     x: global model parameters     y_i: node i 's local model parameters at the end of the round</p> <p>Parameters:</p> Name Type Description Default <code>model_params</code> <code>Dict</code> <p>list of models parameters received from nodes</p> required <code>weights</code> <code>Dict[str, float]</code> <p>weights depicting sample proportions available on each node. Unused for Scaffold.</p> required <code>global_model</code> <code>Dict[str, Union[torch.Tensor, np.ndarray]]</code> <p>global model, ie aggregated model</p> required <code>training_plan</code> <code>BaseTrainingPlan</code> <p>instance of TrainingPlan</p> required <code>training_replies</code> <code>Responses</code> <p>Training replies from each node that participates in the current round</p> required <code>n_updates</code> <code>int</code> <p>number of updates (number of batch performed). Defaults to 1.</p> <code>1</code> <code>n_round</code> <code>int</code> <p>current round. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Aggregated parameters, as a dict mapping weigth names and values.</p> <p>Raises:</p> Type Description <code>FedbiomedAggregatorError</code> <p>If no FederatedDataset is attached to this Scaffold instance, or if <code>node_ids</code> do not belong to the dataset attached to it.</p> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def aggregate(self,\nmodel_params: Dict,\nweights: Dict[str, float],\nglobal_model: Dict[str, Union[torch.Tensor, np.ndarray]],\ntraining_plan: BaseTrainingPlan,\ntraining_replies: Responses,\nn_updates: int = 1,\nn_round: int = 0,\n*args, **kwargs) -&gt; Dict:\n\"\"\"\n    Aggregates local models coming from nodes into a global model, using SCAFFOLD algorithm (2nd option)\n    [Scaffold: Stochastic Controlled Averaging for Federated Learning][https://arxiv.org/abs/1910.06378]\n    Performed computations:\n    -----------------------\n    - Compute participating nodes' model update:\n        * update_i = y_i - x\n    - Compute aggregated model parameters:\n        * x(+) = x - eta_g sum_S(update_i)\n    - Update participating nodes' state:\n        * c_i = delta_i + 1/(K*eta_i) * update_i\n    - Update the global state and all nodes' correction state:\n        * c = 1/N sum_{i=1}^n c_i\n        * delta_i = (c_i - c)\n    where, according to paper notations\n        c_i: local state variable for node `i`\n        c: global state variable\n        delta_i: (c_i - c), correction state for node `i`\n        eta_g: server's learning rate\n        eta_i: node i's learning rate\n        N: total number of node participating to federated learning\n        S: number of nodes considered during current round (S&lt;=N)\n        K: number of updates done during the round (ie number of data batches).\n        x: global model parameters\n        y_i: node i 's local model parameters at the end of the round\n    Args:\n        model_params: list of models parameters received from nodes\n        weights: weights depicting sample proportions available\n            on each node. Unused for Scaffold.\n        global_model: global model, ie aggregated model\n        training_plan (BaseTrainingPlan): instance of TrainingPlan\n        training_replies: Training replies from each node that participates in the current round\n        n_updates: number of updates (number of batch performed). Defaults to 1.\n        n_round: current round. Defaults to 0.\n    Returns:\n        Aggregated parameters, as a dict mapping weigth names and values.\n    Raises:\n        FedbiomedAggregatorError: If no FederatedDataset is attached to this\n            Scaffold instance, or if `node_ids` do not belong to the dataset\n            attached to it.\n    \"\"\"\n# Gather the learning rates used by nodes, updating `self.nodes_lr`.\nself.set_nodes_learning_rate_after_training(training_plan, training_replies, n_round)\n# At round 0, initialize zero-valued correction states.\nif n_round == 0:\nself.init_correction_states(global_model)\n# Check that the input node_ids match known ones.\nif not set(model_params).issubset(self._fds.node_ids()):\nraise FedbiomedAggregatorError(\n\"Received updates from nodes that are unknown to this aggregator.\"\n)\n# Compute the node-wise model update: (x^t - y_i^t).\nmodel_updates = {\nnode_id: {\nkey: (global_model[key] - local_value)\nfor key, local_value in params.items()\n}\nfor node_id, params in model_params.items()\n}\n# Update all Scaffold state variables.\nself.update_correction_states(model_updates, n_updates)\n# Compute and return the aggregated model parameters.\nglobal_new = {}  # type: Dict[str, Union[torch.Tensor, np.ndarray]]\nfor key, val in global_model.items():\nupd = sum(model_updates[node_id][key] for node_id in model_params)\nglobal_new[key] = val - upd * (self.server_lr / len(model_params))\nreturn global_new\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.check_values","title":"<pre><code>check_values(n_updates, training_plan)\n</code></pre>","text":"<p>Check if all values/parameters are correct and have been set before using aggregator.</p> <p>Raise an error otherwise.</p> <p>This can prove useful if user has set wrong hyperparameter values, so that user will have errors before performing first round of training</p> <p>Parameters:</p> Name Type Description Default <code>n_updates</code> <code>int</code> <p>number of updates. Must be non-zero and an integer.</p> required <code>training_plan</code> <code>BaseTrainingPlan</code> <p>training plan. used for checking if optimizer is SGD, otherwise, triggers warning.</p> required <p>Raises:</p> Type Description <code>FedbiomedAggregatorError</code> <p>triggered if <code>num_updates</code> entry is missing (needed for Scaffold aggregator)</p> <code>FedbiomedAggregatorError</code> <p>triggered if any of the learning rate(s) equals 0</p> <code>FedbiomedAggregatorError</code> <p>triggered if number of updates equals 0 or is not an integer</p> <code>FedbiomedAggregatorError</code> <p>triggered if FederatedDataset has not been set.</p> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def check_values(self, n_updates: int, training_plan: BaseTrainingPlan) -&gt; True:\n\"\"\"Check if all values/parameters are correct and have been set before using aggregator.\n    Raise an error otherwise.\n    This can prove useful if user has set wrong hyperparameter values, so that user will\n    have errors before performing first round of training\n    Args:\n        n_updates: number of updates. Must be non-zero and an integer.\n        training_plan: training plan. used for checking if optimizer is SGD, otherwise,\n            triggers warning.\n    Raises:\n        FedbiomedAggregatorError: triggered if `num_updates` entry is missing (needed for Scaffold aggregator)\n        FedbiomedAggregatorError: triggered if any of the learning rate(s) equals 0\n        FedbiomedAggregatorError: triggered if number of updates equals 0 or is not an integer\n        FedbiomedAggregatorError: triggered if [FederatedDataset][fedbiomed.researcher.datasets.FederatedDataset]\n            has not been set.\n    \"\"\"\nif n_updates is None:\nraise FedbiomedAggregatorError(\"Cannot perform Scaffold: missing 'num_updates' entry in the training_args\")\nelif n_updates &lt;= 0 or int(n_updates) != float(n_updates):\nraise FedbiomedAggregatorError(\n\"n_updates should be a positive non zero integer, but got \"\nf\"n_updates: {n_updates} in SCAFFOLD aggregator\"\n)\nif self._fds is None:\nraise FedbiomedAggregatorError(\n\"Federated Dataset not provided, but needed for Scaffold. Please use setter `set_fds()`.\"\n)\nif hasattr(training_plan, \"_optimizer\") and training_plan.type() is TrainingPlans.TorchTrainingPlan:\nif not isinstance(training_plan._optimizer.optimizer, torch.optim.SGD):\nlogger.warning(f\"Found optimizer {training_plan._optimizer.optimizer}, but SCAFFOLD requieres SGD optimizer. Results may be inconsistants\")\nreturn True\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.create_aggregator_args","title":"<pre><code>create_aggregator_args(global_model, node_ids)\n</code></pre>","text":"<p>Return correction states that are to be sent to the nodes.</p> <p>Parameters:</p> Name Type Description Default <code>global_model</code> <code>Dict[str, Union[torch.Tensor, np.ndarray]]</code> <p>parameters of the global model, formatted as a dict mapping weight tensors to their names.</p> required <code>node_ids</code> <code>Collection[str]</code> <p>identifiers of the nodes that are to receive messages.</p> required <p>Returns:</p> Name Type Description <code>aggregator_msg</code> <code>Dict[str, Dict[str, Any]]</code> <p>Dict associating MQTT-transmitted messages to node identifiers.</p> <code>aggregator_dat</code> <code>Dict[str, Dict[str, Any]]</code> <p>Dict associating file-exchange-transmitted messages to node identifiers. The Scaffold correction states are part of this dict.</p> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def create_aggregator_args(\nself,\nglobal_model: Dict[str, Union[torch.Tensor, np.ndarray]],\nnode_ids: Collection[str]\n) -&gt; Tuple[Dict[str, Dict[str, Any]], Dict[str, Dict[str, Any]]]:\n\"\"\"Return correction states that are to be sent to the nodes.\n    Args:\n        global_model: parameters of the global model, formatted as a dict\n            mapping weight tensors to their names.\n        node_ids: identifiers of the nodes that are to receive messages.\n    Returns:\n        aggregator_msg: Dict associating MQTT-transmitted messages to node\n            identifiers.\n        aggregator_dat: Dict associating file-exchange-transmitted messages\n            to node identifiers. The Scaffold correction states are part of\n            this dict.\n    \"\"\"\n# Optionally initialize states, and verify that nodes are known.\nif not self.nodes_deltas:\nself.init_correction_states(global_model)\nif not set(node_ids).issubset(self._fds.node_ids()):\nraise FedbiomedAggregatorError(\n\"Scaffold cannot create aggregator args for nodes that are not\"\n\"covered by its attached FederatedDataset.\"\n)\n# Pack node-wise messages, for the MQTT and file exchange channels.\naggregator_msg = {}\naggregator_dat = {}\nfor node_id in node_ids:\n# If a node was late-added to the FederatedDataset, create states.\nif node_id not in self.nodes_deltas:\nzeros = {key: initialize(val)[1] for key, val in self.global_state.items()}\nself.nodes_deltas[node_id] = zeros\nself.nodes_states[node_id] = copy.deepcopy(zeros)\n# Add information for the current node to the message dicts.\naggregator_dat[node_id] = {\n'aggregator_name': self.aggregator_name,\n'aggregator_correction': self.nodes_deltas[node_id]\n}\naggregator_msg[node_id] = {\n'aggregator_name': self.aggregator_name\n}\nreturn aggregator_msg, aggregator_dat\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.init_correction_states","title":"<pre><code>init_correction_states(global_model)\n</code></pre>","text":"<p>Initialize Scaffold state variables.</p> <p>Parameters:</p> Name Type Description Default <code>global_model</code> <code>Dict[str, Union[torch.Tensor, np.ndarray]]</code> <p>parameters of the global model, formatted as a dict mapping weight tensors to their names.</p> required <p>Raises:</p> Type Description <code>FedbiomedAggregatorError</code> <p>if no FederatedDataset is attached to this aggregator.</p> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def init_correction_states(\nself,\nglobal_model: Dict[str, Union[torch.Tensor, np.ndarray]],\n) -&gt; None:\n\"\"\"Initialize Scaffold state variables.\n    Args:\n        global_model: parameters of the global model, formatted as a dict\n            mapping weight tensors to their names.\n    Raises:\n        FedbiomedAggregatorError: if no FederatedDataset is attached to\n            this aggregator.\n    \"\"\"\n# Gather node ids from the attached FederatedDataset.\nif self._fds is None:\nraise FedbiomedAggregatorError(\n\"Cannot initialize correction states: Scaffold aggregator does \"\n\"not have a FederatedDataset attached.\"\n)\nnode_ids = self._fds.node_ids()\n# Initialize nodes states with zero scalars, that will be summed into actual tensors.\ninit_params = {key: initialize(tensor)[1] for key, tensor in global_model.items()}\nself.nodes_deltas = {node_id: copy.deepcopy(init_params) for node_id in node_ids}\nself.nodes_states = copy.deepcopy(self.nodes_deltas)\nself.global_state = init_params\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.load_state","title":"<pre><code>load_state(state=None)\n</code></pre>","text":"Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def load_state(self, state: Dict[str, Any] = None):\nsuper().load_state(state)\nself.server_lr = self._aggregator_args['server_lr']\n# loading global state\nglobal_state_filename = self._aggregator_args['global_state_filename']\nself.global_state = Serializer.load(global_state_filename)\nfor node_id in self._aggregator_args['aggregator_correction']:\narg_filename = self._aggregator_args['aggregator_correction'][node_id]\nself.nodes_deltas[node_id] = Serializer.load(arg_filename)\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.save_state","title":"<pre><code>save_state(breakpoint_path, global_model)\n</code></pre>","text":"Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def save_state(\nself,\nbreakpoint_path: str,\nglobal_model: Mapping[str, Union[torch.Tensor, np.ndarray]]\n) -&gt; Dict[str, Any]:\n# adding aggregator parameters to the breakpoint that wont be sent to nodes\nself._aggregator_args['server_lr'] = self.server_lr\n# saving global state variable into a file\nfilename = os.path.join(breakpoint_path, f\"global_state_{uuid.uuid4()}.mpk\")\nSerializer.dump(self.global_state, filename)\nself._aggregator_args['global_state_filename'] = filename\n# adding aggregator parameters that will be sent to nodes afterwards\nreturn super().save_state(\nbreakpoint_path, global_model=global_model, node_ids=self._fds.node_ids()\n)\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.set_nodes_learning_rate_after_training","title":"<pre><code>set_nodes_learning_rate_after_training(training_plan, training_replies, n_round)\n</code></pre>","text":"<p>Gets back learning rate of optimizer from Node (if learning rate scheduler is used)</p> <p>Parameters:</p> Name Type Description Default <code>training_plan</code> <code>BaseTrainingPlan</code> <p>training plan instance</p> required <code>training_replies</code> <code>List[Responses]</code> <p>training replies that must contain am <code>optimizer_args</code> entry and a learning rate</p> required <code>n_round</code> <code>int</code> <p>number of rounds already performed</p> required <p>Raises:</p> Type Description <code>FedbiomedAggregatorError</code> <p>raised when setting learning rate has been unsuccessful</p> <p>Returns:</p> Type Description <code>Dict[str, List[float]]</code> <p>Dict[str, List[float]]: dictionary mapping node_id and a list of float, as many as the number of layers contained in the model (in Pytroch, each layer can have a specific learning rate).</p> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def set_nodes_learning_rate_after_training(\nself,\ntraining_plan: BaseTrainingPlan,\ntraining_replies: Responses,\nn_round: int\n) -&gt; Dict[str, List[float]]:\n\"\"\"Gets back learning rate of optimizer from Node (if learning rate scheduler is used)\n    Args:\n        training_plan (BaseTrainingPlan): training plan instance\n        training_replies (List[Responses]): training replies that must contain am `optimizer_args`\n            entry and a learning rate\n        n_round (int): number of rounds already performed\n    Raises:\n        FedbiomedAggregatorError: raised when setting learning rate has been unsuccessful\n    Returns:\n        Dict[str, List[float]]: dictionary mapping node_id and a list of float, as many as\n            the number of layers contained in the model (in Pytroch, each layer can have a specific learning rate).\n    \"\"\"\nn_model_layers = len(training_plan.get_model_params())\nfor node_id in self._fds.node_ids():\nlrs: Dict[str, float] = {}\n# retrieve node learning rate from training replies\nnode_idx: int = training_replies[n_round].get_index_from_node_id(node_id)\nif node_idx is not None:\n# case where node provided lr information\nlrs = training_replies[n_round][node_idx]['optimizer_args'].get('lr')\nif node_idx is None or lrs is None:\n# fall back to default value if no lr information was provided\nlrs = training_plan.optimizer().get_learning_rate()\nif len(lrs) != n_model_layers:\nraise FedbiomedAggregatorError(\n\"Error when setting node learning rate for SCAFFOLD: cannot extract node learning rate.\"\n)\nself.nodes_lr[node_id] = lrs\nreturn self.nodes_lr\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.set_training_plan_type","title":"<pre><code>set_training_plan_type(training_plan_type)\n</code></pre>","text":"<p>Overrides <code>set_training_plan_type</code> from parent class. Checks the training plan type, and if it is SKlearnTrainingPlan, raises an error. Otherwise, calls parent method.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_type</code> <code>TrainingPlans</code> <p>training_plan type</p> required <p>Raises:</p> Type Description <code>FedbiomedAggregatorError</code> <p>raised if training_plan type has been set to SKLearn training plan</p> <p>Returns:</p> Name Type Description <code>TrainingPlans</code> <code>TrainingPlans</code> <p>training plan type</p> Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def set_training_plan_type(self, training_plan_type: TrainingPlans) -&gt; TrainingPlans:\n\"\"\"\n    Overrides `set_training_plan_type` from parent class.\n    Checks the training plan type, and if it is SKlearnTrainingPlan,\n    raises an error. Otherwise, calls parent method.\n    Args:\n        training_plan_type (TrainingPlans): training_plan type\n    Raises:\n        FedbiomedAggregatorError: raised if training_plan type has been set to SKLearn training plan\n    Returns:\n        TrainingPlans: training plan type\n    \"\"\"\nif training_plan_type == TrainingPlans.SkLearnTrainingPlan:\nraise FedbiomedAggregatorError(\"Aggregator SCAFFOLD not implemented for SKlearn\")\ntraining_plan_type = super().set_training_plan_type(training_plan_type)\n# TODO: trigger a warning if user is trying to use scaffold with something else than SGD\nreturn training_plan_type\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.update_correction_states","title":"<pre><code>update_correction_states(model_updates, n_updates)\n</code></pre>","text":"<p>Update all Scaffold state variables based on node-wise model updates.</p>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.scaffold.Scaffold.update_correction_states--performed-computations","title":"Performed computations:","text":"<ul> <li>Update participating nodes' state:<ul> <li>c_i = delta_i + 1/(K*eta_i) * update_i</li> </ul> </li> <li>Update the global state and all nodes' correction state:<ul> <li>c = 1/N sum_{i=1}^n c_i</li> <li>delta_i = (c_i - c)</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>model_updates</code> <code>Dict[str, Dict[str, Union[np.ndarray, torch.Tensor]]]</code> <p>node-wise model weight updates.</p> required <code>n_updates</code> <code>int</code> <p>number of local optimization steps.</p> required Source code in <code>fedbiomed/researcher/aggregators/scaffold.py</code> <pre><code>def update_correction_states(\nself,\nmodel_updates: Dict[str, Dict[str, Union[np.ndarray, torch.Tensor]]],\nn_updates: int,\n) -&gt; None:\n\"\"\"Update all Scaffold state variables based on node-wise model updates.\n    Performed computations:\n    ----------------------\n    - Update participating nodes' state:\n        * c_i = delta_i + 1/(K*eta_i) * update_i\n    - Update the global state and all nodes' correction state:\n        * c = 1/N sum_{i=1}^n c_i\n        * delta_i = (c_i - c)\n    Args:\n        model_updates: node-wise model weight updates.\n        n_updates: number of local optimization steps.\n    \"\"\"\n# Update the node-wise states for participating nodes:\n# c_i^{t+1} = delta_i^t + (x^t - y_i^t) / (M * eta)\nfor node_id, updates in model_updates.items():\nd_i = self.nodes_deltas[node_id]\nfor (key, val) in updates.items():\nif self.nodes_lr[node_id].get(key) is not None:\nself.nodes_states[node_id].update(\n{\nkey: d_i[key] + val / (self.nodes_lr[node_id][key] * n_updates)\n}\n)\n# Update the global state: c^{t+1} = average(c_i^{t+1})\nfor key in self.global_state:\nself.global_state[key] = 0\nfor state in self.nodes_states.values():\nif state.get(key) is not None:\nself.global_state[key] = (\nsum(state[key] for state in self.nodes_states.values())\n/ len(self.nodes_states)\n)\n# Compute the new node-wise correction states:\n# delta_i^{t+1} = c_i^{t+1} - c^{t+1}\nself.nodes_deltas = {\nnode_id: {\nkey: val - self.global_state[key] for key, val in state.items()\n}\nfor node_id, state in self.nodes_states.items()\n}\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators-functions","title":"Functions","text":""},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.federated_averaging","title":"<pre><code>federated_averaging(model_params, weights)\n</code></pre>","text":"<p>Defines Federated Averaging (FedAvg) strategy for model aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>model_params</code> <code>List[Dict[str, Union[torch.Tensor, np.ndarray]]]</code> <p>list that contains nodes' model parameters; each model is stored as an OrderedDict (maps model layer name to the model weights)</p> required <code>weights</code> <code>List[float]</code> <p>weights for performing weighted sum in FedAvg strategy (depending on the dataset size of each node). Items in the list must always sum up to 1</p> required <p>Returns:</p> Type Description <code>Mapping[str, Union[torch.Tensor, np.ndarray]]</code> <p>Final model with aggregated layers, as an OrderedDict object.</p> Source code in <code>fedbiomed/researcher/aggregators/functional.py</code> <pre><code>def federated_averaging(model_params: List[Dict[str, Union[torch.Tensor, np.ndarray]]],\nweights: List[float]) -&gt; Mapping[str, Union[torch.Tensor, np.ndarray]]:\n\"\"\"Defines Federated Averaging (FedAvg) strategy for model aggregation.\n    Args:\n        model_params: list that contains nodes' model parameters; each model is stored as an OrderedDict (maps\n            model layer name to the model weights)\n        weights: weights for performing weighted sum in FedAvg strategy (depending on the dataset size of each node).\n            Items in the list must always sum up to 1\n    Returns:\n        Final model with aggregated layers, as an OrderedDict object.\n    \"\"\"\nassert len(model_params) &gt; 0, 'An empty list of models was passed.'\nassert len(weights) == len(model_params), 'List with number of observations must have ' \\\n                                              'the same number of elements that list of models.'\n# Compute proportions\nproportions = [n_k / sum(weights) for n_k in weights]\nreturn weighted_sum(model_params, proportions)\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.initialize","title":"<pre><code>initialize(val)\n</code></pre>","text":"<p>Initialize tensor or array vector.</p> Source code in <code>fedbiomed/researcher/aggregators/functional.py</code> <pre><code>def initialize(val: Union[torch.Tensor, np.ndarray]) -&gt; Tuple[str, Union[torch.Tensor, np.ndarray]]:\n\"\"\"Initialize tensor or array vector. \"\"\"\nif isinstance(val, torch.Tensor):\nreturn 'tensor', torch.zeros_like(val).float()\nif isinstance(val, (list, np.ndarray)):\nval = np.array(val)\nreturn 'array', np.zeros(val.shape, dtype = float)\n</code></pre>"},{"location":"developer/api/researcher/aggregators/#fedbiomed.researcher.aggregators.weighted_sum","title":"<pre><code>weighted_sum(model_params, proportions)\n</code></pre>","text":"<p>Performs weighted sum operation</p> <p>Parameters:</p> Name Type Description Default <code>model_params</code> <code>List[Dict[str, Union[torch.Tensor, np.ndarray]]]</code> <p>list that contains nodes' model parameters; each model is stored as an OrderedDict (maps model layer name to the model weights)</p> required <code>proportions</code> <code>List[float]</code> <p>weights of all items whithin model_params's list</p> required <p>Returns:</p> Type Description <code>Mapping[str, Union[torch.Tensor, np.ndarray]]</code> <p>Mapping[str, Union[torch.Tensor, np.ndarray]]: model resulting from the weigthed sum                                             operation</p> Source code in <code>fedbiomed/researcher/aggregators/functional.py</code> <pre><code>def weighted_sum(model_params: List[Dict[str, Union[torch.Tensor, np.ndarray]]],\nproportions: List[float]) -&gt; Mapping[str, Union[torch.Tensor, np.ndarray]]:\n\"\"\"Performs weighted sum operation\n    Args:\n        model_params (List[Dict[str, Union[torch.Tensor, np.ndarray]]]): list that contains nodes'\n            model parameters; each model is stored as an OrderedDict (maps model layer name to the model weights)\n        proportions (List[float]): weights of all items whithin model_params's list\n    Returns:\n        Mapping[str, Union[torch.Tensor, np.ndarray]]: model resulting from the weigthed sum \n                                                       operation\n    \"\"\"\n# Empty model parameter dictionary\navg_params = copy.deepcopy(model_params[0])\nfor key, val in avg_params.items():\n(t, avg_params[key] ) = initialize(val)\nif t == 'tensor':\nfor model, weight in zip(model_params, proportions):\nfor key in avg_params.keys():\navg_params[key] += weight * model[key]\nif t == 'array':\nfor key in avg_params.keys():\nmatr = np.array([ d[key] for d in model_params ])\navg_params[key] = np.average(matr, weights=np.array(proportions), axis=0)\nreturn avg_params\n</code></pre>"},{"location":"developer/api/researcher/datasets/","title":"Datasets","text":""},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets","title":"fedbiomed.researcher.datasets","text":"Module: <code>fedbiomed.researcher.datasets</code> <p>Module includes the classes that allow researcher to interact with remote datasets (federated datasets).</p>"},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets-classes","title":"Classes","text":""},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets.FederatedDataSet","title":"FederatedDataSet","text":"CLASS  <pre><code>FederatedDataSet(data)\n</code></pre> <p>A class that allows researcher to interact with remote datasets (federated datasets).</p> <p>It contains details about remote datasets, such as client ids, data size that can be useful for aggregating or sampling strategies on researcher's side</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict</code> <p>Dictionary of datasets. Each key is a <code>str</code> representing a node's ID. Each value is a <code>dict</code> (or a <code>list</code> containing exactly one <code>dict</code>). Each <code>dict</code> contains the description of the dataset associated to this node in the federated dataset. </p> required <p>Raises:</p> Type Description <code>FedbiomedFederatedDataSetError</code> <p>bad <code>data</code> format</p> Source code in <code>fedbiomed/researcher/datasets.py</code> <pre><code>def __init__(self, data: Dict):\n\"\"\"Construct FederatedDataSet object.\n    Args:\n        data: Dictionary of datasets. Each key is a `str` representing a node's ID. Each value is\n            a `dict` (or a `list` containing exactly one `dict`). Each `dict` contains the description\n            of the dataset associated to this node in the federated dataset. \n    Raises:\n        FedbiomedFederatedDataSetError: bad `data` format\n    \"\"\"\n# check structure of data\nself._v = Validator()\nself._v.register(\"list_or_dict\", self._dataset_type, override=True)\ntry:\nself._v.validate(data, dict)\nfor node, ds in data.items():\nself._v.validate(node, str)\nself._v.validate(ds, \"list_or_dict\")\nif isinstance(ds, list):\nif len(ds) == 1:\nself._v.validate(ds[0], dict)\n# convert list of one dict to dict\ndata[node] = ds[0]\nelse:\nerrmess = f'{ErrorNumbers.FB416.value}: {node} must have one unique dataset ' \\\n                        f'but has {len(ds)} datasets.'\nlogger.error(errmess)\nraise FedbiomedFederatedDataSetError(errmess)\nexcept ValidatorError as e:\nerrmess = f'{ErrorNumbers.FB416.value}: bad parameter `data` must be a `dict` of ' \\\n            f'(`list` of one) `dict`: {e}'\nlogger.error(errmess)\nraise FedbiomedFederatedDataSetError(errmess)\nself._data = data\n</code></pre>"},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets.FederatedDataSet-functions","title":"Functions","text":""},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets.FederatedDataSet.data","title":"<pre><code>data()\n</code></pre>","text":"<p>Retrieve FederatedDataset as <code>dict</code>.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Federated datasets, keys as node ids</p> Source code in <code>fedbiomed/researcher/datasets.py</code> <pre><code>def data(self) -&gt; Dict:\n\"\"\"Retrieve FederatedDataset as [`dict`][dict].\n    Returns:\n       Federated datasets, keys as node ids\n    \"\"\"\nreturn self._data\n</code></pre>"},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets.FederatedDataSet.node_ids","title":"<pre><code>node_ids()\n</code></pre>","text":"<p>Retrieve Node ids from <code>FederatedDataSet</code>.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of node ids</p> Source code in <code>fedbiomed/researcher/datasets.py</code> <pre><code>def node_ids(self) -&gt; List[str]:\n\"\"\"Retrieve Node ids from `FederatedDataSet`.\n    Returns:\n        List of node ids\n    \"\"\"\nreturn list(self._data.keys())\n</code></pre>"},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets.FederatedDataSet.sample_sizes","title":"<pre><code>sample_sizes()\n</code></pre>","text":"<p>Retrieve list of sample sizes of node's dataset.</p> <p>Returns:</p> Type Description <code>List[int]</code> <p>List of sample sizes in federated datasets in the same order with node_ids</p> Source code in <code>fedbiomed/researcher/datasets.py</code> <pre><code>def sample_sizes(self) -&gt; List[int]:\n\"\"\"Retrieve list of sample sizes of node's dataset.\n    Returns:\n        List of sample sizes in federated datasets in the same order with\n            [node_ids][fedbiomed.researcher.datasets.FederatedDataSet.node_ids]\n    \"\"\"\nsample_sizes = []\nfor (key, val) in self._data.items():\nsample_sizes.append(val[\"shape\"][0])\nreturn sample_sizes\n</code></pre>"},{"location":"developer/api/researcher/datasets/#fedbiomed.researcher.datasets.FederatedDataSet.shapes","title":"<pre><code>shapes()\n</code></pre>","text":"<p>Get shape of FederatedDatasets by node ids.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Includes <code>sample_sizes</code> by node_ids.</p> Source code in <code>fedbiomed/researcher/datasets.py</code> <pre><code>def shapes(self) -&gt; Dict[str, int]:\n\"\"\"Get shape of FederatedDatasets by node ids.\n    Returns:\n        Includes [`sample_sizes`][fedbiomed.researcher.datasets.FederatedDataSet.sample_sizes] by node_ids.\n    \"\"\"\nshapes_dict = {}\nfor node_id, node_data_size in zip(self.node_ids(),\nself.sample_sizes()):\nshapes_dict[node_id] = node_data_size\nreturn shapes_dict\n</code></pre>"},{"location":"developer/api/researcher/experiment/","title":"Experiment","text":""},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment","title":"fedbiomed.researcher.experiment","text":"Module: <code>fedbiomed.researcher.experiment</code> <p>Code of the researcher. Implements the experiment orchestration</p>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.TExperiment","title":"TExperiment     <code>module-attribute</code>","text":"<pre><code>TExperiment = TypeVar('TExperiment', bound='Experiment')\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.TrainingPlan","title":"TrainingPlan     <code>module-attribute</code>","text":"<pre><code>TrainingPlan = TypeVar('TrainingPlan', TorchTrainingPlan, SKLearnTrainingPlan)\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Type_TrainingPlan","title":"Type_TrainingPlan     <code>module-attribute</code>","text":"<pre><code>Type_TrainingPlan = TypeVar('Type_TrainingPlan', Type[TorchTrainingPlan], Type[SKLearnTrainingPlan])\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.training_plans","title":"training_plans     <code>module-attribute</code>","text":"<pre><code>training_plans = (TorchTrainingPlan, SKLearnTrainingPlan)\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment-classes","title":"Classes","text":""},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment","title":"Experiment","text":"CLASS  <pre><code>Experiment(tags=None, nodes=None, training_data=None, aggregator=None, node_selection_strategy=None, round_limit=None, training_plan_class=None, training_plan_path=None, model_args={}, training_args=None, save_breakpoints=False, tensorboard=False, experimentation_folder=None, secagg=False)\n</code></pre> <p>This class represents the orchestrator managing the federated training</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Union[List[str], str, None]</code> <p>list of string with data tags or string with one data tag. Empty list of tags ([]) means any dataset is accepted, it is different from None (tags not set, cannot search for training_data yet).</p> <code>None</code> <code>nodes</code> <code>Union[List[str], None]</code> <p>list of node_ids to filter the nodes to be involved in the experiment. Defaults to None (no filtering).</p> <code>None</code> <code>training_data</code> <code>Union[FederatedDataSet, dict, None]</code> <ul> <li>If it is a FederatedDataSet object, use this value as training_data.</li> <li>else if it is a dict, create and use a FederatedDataSet object from the dict and use this value as     training_data. The dict should use node ids as keys, values being list of dicts (each dict     representing a dataset on a node).</li> <li>else if it is None (no training data provided)</li> <li>if <code>tags</code> is not None, set training_data by     searching for datasets with a query to the nodes using <code>tags</code> and <code>nodes</code></li> <li>if <code>tags</code> is None, set training_data to None (no training_data set yet,     experiment is not fully initialized and cannot be launched) Defaults to None (query nodes for dataset if <code>tags</code> is not None, set training_data to None else)</li> </ul> <code>None</code> <code>aggregator</code> <code>Union[Aggregator, Type[Aggregator], None]</code> <p>object or class defining the method for aggregating local updates. Default to None (use <code>FedAverage</code> for aggregation)</p> <code>None</code> <code>node_selection_strategy</code> <code>Union[Strategy, Type[Strategy], None]</code> <p>object or class defining how nodes are sampled at each round for training, and how non-responding nodes are managed.  Defaults to None: - use <code>DefaultStrategy</code> if training_data is     initialized - else strategy is None (cannot be initialized), experiment cannot be launched yet</p> <code>None</code> <code>round_limit</code> <code>Union[int, None]</code> <p>the maximum number of training rounds (nodes &lt;-&gt; central server) that should be executed for the experiment. <code>None</code> means that no limit is defined. Defaults to None.</p> <code>None</code> <code>training_plan_class</code> <code>Union[Type_TrainingPlan, str, None]</code> <p>name of the training plan class <code>str</code> or training plan class (<code>Type_TrainingPlan</code>) to use for training. For experiment to be properly and fully defined <code>training_plan_class</code> needs to be: - a <code>str</code> when <code>training_plan_class_path</code> is not None (training plan class comes from a file). - a <code>Type_TrainingPlan</code> when <code>training_plan_class_path</code> is None (training plan class passed     as argument). Defaults to None (no training plan class defined yet)</p> <code>None</code> <code>training_plan_path</code> <code>Union[str, None]</code> <p>path to a file containing training plan code <code>str</code> or None (no file containing training plan code, <code>training_plan</code> needs to be a class matching <code>Type_TrainingPlan</code>) Defaults to None.</p> <code>None</code> <code>model_args</code> <code>dict</code> <p>contains model arguments passed to the constructor of the training plan when instantiating it : output and input feature dimension, etc.</p> <code>{}</code> <code>training_args</code> <code>Union[TypeVar(TrainingArgs), dict, None]</code> <p>contains training arguments passed to the <code>training_routine</code> of the training plan when launching it: lr, epochs, batch_size...</p> <code>None</code> <code>save_breakpoints</code> <code>bool</code> <p>whether to save breakpoints or not after each training round. Breakpoints can be used for resuming a crashed experiment.</p> <code>False</code> <code>tensorboard</code> <code>bool</code> <p>whether to save scalar values  for displaying in Tensorboard during training for each node. Currently, it is only used for loss values. - If it is true, monitor instantiates a <code>Monitor</code> object that write scalar logs into <code>./runs</code> directory. - If it is False, it stops monitoring if it was active.</p> <code>False</code> <code>experimentation_folder</code> <code>Union[str, None]</code> <p>choose a specific name for the folder where experimentation result files and breakpoints are stored. This should just contain the name for the folder not a path. The name is used as a subdirectory of <code>environ[EXPERIMENTS_DIR])</code>. Defaults to None (auto-choose a folder name) - Caveat : if using a specific name this experimentation will not be automatically detected as the last experimentation by <code>load_breakpoint</code> - Caveat : do not use a <code>experimentation_folder</code> name finishing with numbers ([0-9]+) as this would confuse the last experimentation detection heuristic by <code>load_breakpoint</code>.</p> <code>None</code> <code>secagg</code> <code>Union[bool, SecureAggregation]</code> <p>whether to setup a secure aggregation context for this experiment, and use it to send encrypted updates from nodes to researcher. Defaults to <code>False</code></p> <code>False</code> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef __init__(self,\ntags: Union[List[str], str, None] = None,\nnodes: Union[List[str], None] = None,\ntraining_data: Union[FederatedDataSet, dict, None] = None,\naggregator: Union[Aggregator, Type[Aggregator], None] = None,\nnode_selection_strategy: Union[Strategy, Type[Strategy], None] = None,\nround_limit: Union[int, None] = None,\ntraining_plan_class: Union[Type_TrainingPlan, str, None] = None,\ntraining_plan_path: Union[str, None] = None,\nmodel_args: dict = {},\ntraining_args: Union[TypeVar(\"TrainingArgs\"), dict, None] = None,\nsave_breakpoints: bool = False,\ntensorboard: bool = False,\nexperimentation_folder: Union[str, None] = None,\nsecagg: Union[bool, SecureAggregation] = False,\n):\n\"\"\"Constructor of the class.\n    Args:\n        tags: list of string with data tags or string with one data tag. Empty list of tags ([]) means any dataset\n            is accepted, it is different from None (tags not set, cannot search for training_data yet).\n        nodes: list of node_ids to filter the nodes to be involved in the experiment. Defaults to None (no\n            filtering).\n        training_data:\n            * If it is a FederatedDataSet object, use this value as training_data.\n            * else if it is a dict, create and use a FederatedDataSet object from the dict and use this value as\n                training_data. The dict should use node ids as keys, values being list of dicts (each dict\n                representing a dataset on a node).\n            * else if it is None (no training data provided)\n              - if `tags` is not None, set training_data by\n                searching for datasets with a query to the nodes using `tags` and `nodes`\n              - if `tags` is None, set training_data to None (no training_data set yet,\n                experiment is not fully initialized and cannot be launched)\n            Defaults to None (query nodes for dataset if `tags` is not None, set training_data\n            to None else)\n        aggregator: object or class defining the method for aggregating local updates. Default to None (use\n            [`FedAverage`][fedbiomed.researcher.aggregators.FedAverage] for aggregation)\n        node_selection_strategy:object or class defining how nodes are sampled at each round for training, and how\n            non-responding nodes are managed.  Defaults to None:\n            - use [`DefaultStrategy`][fedbiomed.researcher.strategies.DefaultStrategy] if training_data is\n                initialized\n            - else strategy is None (cannot be initialized), experiment cannot be launched yet\n        round_limit: the maximum number of training rounds (nodes &lt;-&gt; central server) that should be executed for\n            the experiment. `None` means that no limit is defined. Defaults to None.\n        training_plan_class: name of the training plan class [`str`][str] or training plan class\n            (`Type_TrainingPlan`) to use for training.\n            For experiment to be properly and fully defined `training_plan_class` needs to be:\n            - a [`str`][str] when `training_plan_class_path` is not None (training plan class comes from a file).\n            - a `Type_TrainingPlan` when `training_plan_class_path` is None (training plan class passed\n                as argument).\n            Defaults to None (no training plan class defined yet)\n        training_plan_path: path to a file containing training plan code [`str`][str] or None (no file containing\n            training plan code, `training_plan` needs to be a class matching `Type_TrainingPlan`) Defaults to None.\n        model_args: contains model arguments passed to the constructor of the training plan when instantiating it :\n            output and input feature dimension, etc.\n        training_args: contains training arguments passed to the `training_routine` of the training plan when\n            launching it: lr, epochs, batch_size...\n        save_breakpoints: whether to save breakpoints or not after each training round. Breakpoints can be used for\n            resuming a crashed experiment.\n        tensorboard: whether to save scalar values  for displaying in Tensorboard during training for each node.\n            Currently, it is only used for loss values.\n            - If it is true, monitor instantiates a `Monitor` object that write scalar logs into `./runs` directory.\n            - If it is False, it stops monitoring if it was active.\n        experimentation_folder: choose a specific name for the folder where experimentation result files and\n            breakpoints are stored. This should just contain the name for the folder not a path. The name is used\n            as a subdirectory of `environ[EXPERIMENTS_DIR])`. Defaults to None (auto-choose a folder name)\n            - Caveat : if using a specific name this experimentation will not be automatically detected as the last\n            experimentation by `load_breakpoint`\n            - Caveat : do not use a `experimentation_folder` name finishing with numbers ([0-9]+) as this would\n            confuse the last experimentation detection heuristic by `load_breakpoint`.\n        secagg: whether to setup a secure aggregation context for this experiment, and use it\n            to send encrypted updates from nodes to researcher. Defaults to `False`\n    \"\"\"\n# predefine all class variables, so no need to write try/except\n# block each time we use it\nself._fds = None\nself._node_selection_strategy = None\nself._job = None\nself._round_limit = None\nself._training_plan_path = None\nself._reqs = None\nself._training_args = None\nself._node_selection_strategy = None\nself._tags = None\nself._monitor = None\nself._experimentation_folder = None\nself.aggregator_args = {}\nself._aggregator = None\nself._global_model = None\nself._client_correction_states_dict = {}\nself._client_states_dict = {}\nself._server_state = None\nself._secagg = None\n# set self._secagg\nself.set_secagg(secagg)\n# set self._tags and self._nodes\nself.set_tags(tags)\nself.set_nodes(nodes)\n# set self._model_args and self._training_args to dict\nself.set_model_args(model_args)\nself.set_training_args(training_args)\n# Useless to add a param and setter/getter for Requests() as it is a singleton ?\nself._reqs = Requests()\n# set self._fds: type Union[FederatedDataSet, None]\nself.set_training_data(training_data, True)\n# set self._aggregator : type Aggregator\nself.set_aggregator(aggregator)\n# set self._node_selection_strategy: type Union[Strategy, None]\nself.set_strategy(node_selection_strategy)\n# \"current\" means number of rounds already trained\nself._set_round_current(0)\nself.set_round_limit(round_limit)\n# set self._experimentation_folder: type str\nself.set_experimentation_folder(experimentation_folder)\n# Note: currently keep this parameter as it cannot be updated in Job()\n# without refactoring Job() first\n# sets self._training_plan_is_defined: bool == is the training plan properly defined ?\n# with current version of jobs, a correctly defined model requires:\n# - either training_plan_path to None + training_plan_class is the class a training plan\n# - or training_plan_path not None + training_plan_class is a name (str) of a training plan\n#\n# note: no need to set self._training_plan_is_defined before calling `set_training_plan_class`\nself.set_training_plan_class(training_plan_class)\nself.set_training_plan_path(training_plan_path)\n# set self._job to Union[Job, None]\nself.set_job()\n# TODO: rewrite after experiment results refactoring\nself._aggregated_params = {}\nself.set_save_breakpoints(save_breakpoints)\n# always create a monitoring process\nself._monitor = Monitor()\nself._reqs.add_monitor_callback(self._monitor.on_message_handler)\nself.set_tensorboard(tensorboard)\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.aggregator_args","title":"aggregator_args     <code>instance-attribute</code>","text":"<pre><code>aggregator_args = {}\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.secagg","title":"secagg     <code>property</code>","text":"<pre><code>secagg: SecureAggregation\n</code></pre> <p>Gets secagg object <code>SecureAggregation</code></p> <p>Returns:</p> Type Description <code>SecureAggregation</code> <p>Secure aggregation object.</p>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment-functions","title":"Functions","text":""},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.aggregated_params","title":"<pre><code>aggregated_params()\n</code></pre>","text":"<p>Retrieves all aggregated parameters of each round of training</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of aggregated parameters keys stand for each round of training</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef aggregated_params(self) -&gt; dict:\n\"\"\"Retrieves all aggregated parameters of each round of training\n    Returns:\n        Dictionary of aggregated parameters keys stand for each round of training\n    \"\"\"\nreturn self._aggregated_params\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.aggregator","title":"<pre><code>aggregator()\n</code></pre>","text":"<p>Retrieves aggregator class that will be used for aggregating model parameters.</p> <p>To set or update aggregator: <code>set_aggregator</code>.</p> <p>Returns:</p> Type Description <code>Aggregator</code> <p>A class or an object that is an instance of Aggregator</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef aggregator(self) -&gt; Aggregator:\n\"\"\" Retrieves aggregator class that will be used for aggregating model parameters.\n    To set or update aggregator: [`set_aggregator`][fedbiomed.researcher.experiment.Experiment.set_aggregator].\n    Returns:\n        A class or an object that is an instance of [Aggregator][fedbiomed.researcher.aggregators.Aggregator]\n    \"\"\"\nreturn self._aggregator\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.breakpoint","title":"<pre><code>breakpoint()\n</code></pre>","text":"<p>Saves breakpoint with the state of the training at a current round. The following Experiment attributes will</p> be saved <ul> <li>round_current</li> <li>round_limit</li> <li>tags</li> <li>experimentation_folder</li> <li>aggregator</li> <li>node_selection_strategy</li> <li>training_data</li> <li>training_args</li> <li>model_args</li> <li>training_plan_path</li> <li>training_plan_class</li> <li>aggregated_params</li> <li>job (attributes returned by the Job, aka job state)</li> <li>secagg</li> </ul> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>experiment not fully defined, experiment did not run any round yet, or error when saving breakpoint</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef breakpoint(self) -&gt; None:\n\"\"\"\n    Saves breakpoint with the state of the training at a current round. The following Experiment attributes will\n    be saved:\n      - round_current\n      - round_limit\n      - tags\n      - experimentation_folder\n      - aggregator\n      - node_selection_strategy\n      - training_data\n      - training_args\n      - model_args\n      - training_plan_path\n      - training_plan_class\n      - aggregated_params\n      - job (attributes returned by the Job, aka job state)\n      - secagg\n    Raises:\n        FedbiomedExperimentError: experiment not fully defined, experiment did not run any round yet, or error when\n            saving breakpoint\n    \"\"\"\n# at this point, we run the constructor so all object variables are defined\n# check pre-requisistes for saving a breakpoint\n#\n# need to have run at least 1 round to save a breakpoint\nif self._round_current &lt; 1:\nmsg = ErrorNumbers.FB413.value + \\\n            ' - need to run at least 1 before saving a breakpoint'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nelif self._fds is None:\nmsg = ErrorNumbers.FB413.value + \\\n            ' - need to define `training_data` for saving a breakpoint'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nelif self._node_selection_strategy is None:\nmsg = ErrorNumbers.FB413.value + \\\n            ' - need to define `strategy` for saving a breakpoint'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nelif self._job is None:\nmsg = ErrorNumbers.FB413.value + \\\n            ' - need to define `job` for saving a breakpoint'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# conditions are met, save breakpoint\nbreakpoint_path, breakpoint_file_name = \\\n        choose_bkpt_file(self._experimentation_folder, self._round_current - 1)\nstate = {\n'breakpoint_version': str(__breakpoints_version__),\n'training_data': self._fds.data(),\n'training_args': self._training_args.dict(),\n'model_args': self._model_args,\n'training_plan_path': self._job.training_plan_file,  # only in Job we always model saved to a file\n# with current version\n'training_plan_class': self._job.training_plan_name,  # not always available properly\n# formatted in Experiment with current version\n'round_current': self._round_current,\n'round_limit': self._round_limit,\n'experimentation_folder': self._experimentation_folder,\n'aggregator': self._aggregator.save_state(breakpoint_path, global_model=self._global_model),  # aggregator state\n'node_selection_strategy': self._node_selection_strategy.save_state(),\n# strategy state\n'tags': self._tags,\n'aggregated_params': self._save_aggregated_params(\nself._aggregated_params, breakpoint_path),\n'job': self._job.save_state(breakpoint_path),  # job state\n'secagg': self._secagg.save_state()\n}\n# rewrite paths in breakpoint : use the links in breakpoint directory\nstate['training_plan_path'] = create_unique_link(\nbreakpoint_path,\n# - Need a file with a restricted characters set in name to be able to import as module\n'model_' + str(\"{:04d}\".format(self._round_current - 1)), '.py',\n# - Prefer relative path, eg for using experiment result after\n# experiment in a different tree\nos.path.join('..', os.path.basename(state[\"training_plan_path\"]))\n)\n# save state into a json file.\nbreakpoint_file_path = os.path.join(breakpoint_path, breakpoint_file_name)\ntry:\nwith open(breakpoint_file_path, 'w') as bkpt:\njson.dump(state, bkpt)\nlogger.info(f\"breakpoint for round {self._round_current - 1} saved at \" +\nos.path.dirname(breakpoint_file_path))\nexcept (OSError, ValueError, TypeError, RecursionError) as e:\n# - OSError: heuristic for catching open() and write() errors\n# - see json.dump() documentation for documented errors for this call\nmsg = ErrorNumbers.FB413.value + f' - save failed with message {str(e)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.check_training_plan_status","title":"<pre><code>check_training_plan_status()\n</code></pre>","text":"<p>Method for checking training plan status, ie whether it is approved or not by the nodes</p> <p>Returns:</p> Type Description <code>Responses</code> <p>Training plan status for answering nodes</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad argument type</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef check_training_plan_status(self) -&gt; Responses:\n\"\"\" Method for checking training plan status, ie whether it is approved or not by the nodes\n    Returns:\n        Training plan status for answering nodes\n    Raises:\n        FedbiomedExperimentError: bad argument type\n    \"\"\"\n# at this point, self._job exists (initialized in constructor)\nif self._job is None:\n# cannot check training plan status if job not defined\nmsg = ErrorNumbers.FB412.value + \\\n              ', in method `check_training_plan_status` : no `job` defined for experiment'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# always returns a `Responses()` object\nresponses = self._job.check_training_plan_is_approved_by_nodes()\nreturn responses\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.experimentation_folder","title":"<pre><code>experimentation_folder()\n</code></pre>","text":"<p>Retrieves the folder name where experiment data/result are saved.</p> <p>Please see also<code>set_experimentation_folder</code></p> <p>Returns:</p> Type Description <code>str</code> <p>File name where experiment related files are saved</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef experimentation_folder(self) -&gt; str:\n\"\"\"Retrieves the folder name where experiment data/result are saved.\n    Please see also[`set_experimentation_folder`]\n    [fedbiomed.researcher.experiment.Experiment.set_experimentation_folder]\n    Returns:\n        File name where experiment related files are saved\n    \"\"\"\nreturn self._experimentation_folder\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.experimentation_path","title":"<pre><code>experimentation_path()\n</code></pre>","text":"<p>Retrieves the file path where experimentation folder is located and experiment related files are saved.</p> <p>Returns:</p> Type Description <code>str</code> <p>Experiment directory where all experiment related files are saved</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef experimentation_path(self) -&gt; str:\n\"\"\"Retrieves the file path where experimentation folder is located and experiment related files are saved.\n    Returns:\n        Experiment directory where all experiment related files are saved\n    \"\"\"\nreturn os.path.join(environ['EXPERIMENTS_DIR'], self._experimentation_folder)\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.info","title":"<pre><code>info()\n</code></pre>","text":"<p>Prints out the information about the current status of the experiment.</p> <p>Lists  all the parameters/arguments of the experiment and informs whether the experiment can be run.</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>Inconsistent experiment due to missing variables</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef info(self) -&gt; None:\n\"\"\"Prints out the information about the current status of the experiment.\n    Lists  all the parameters/arguments of the experiment and informs whether the experiment can be run.\n    Raises:\n        FedbiomedExperimentError: Inconsistent experiment due to missing variables\n    \"\"\"\n# at this point all attributes are initialized (in constructor)\ninfo = {\n'Arguments': [\n'Tags',\n'Nodes filter',\n'Training Data',\n'Aggregator',\n'Strategy',\n'Job',\n'Training Plan Path',\n'Training Plan Class',\n'Model Arguments',\n'Training Arguments',\n'Rounds already run',\n'Rounds total',\n'Experiment folder',\n'Experiment Path',\n'Breakpoint State',\n'Secure Aggregation'\n],\n# max 60 characters per column for values - can we do that with tabulate() ?\n'Values': ['\\n'.join(findall('.{1,60}',\nstr(e))) for e in [\nself._tags,\nself._nodes,\nself._fds,\nself._aggregator.aggregator_name if self._aggregator is not None else None,\nself._node_selection_strategy,\nself._job,\nself._training_plan_path,\nself._training_plan_class,\nself._model_args,\nself._training_args,\nself._round_current,\nself._round_limit,\nself._experimentation_folder,\nself.experimentation_path(),\nself._save_breakpoints,\nf'- Using: {self._secagg}\\n- Active: {self._secagg.active}'\n]\n]\n}\nprint(tabulate(info, headers='keys'))\n# definitions that may be missing for running the experiment\n# (value None == not defined yet for _fds et _job,\n# False == no valid model for _training_plan_is_defined )\nmay_be_missing = {\n'_fds': 'Training Data',\n'_node_selection_strategy': 'Strategy',\n'_training_plan_is_defined': 'Training Plan',\n'_job': 'Job'\n}\n# definitions found missing\nmissing = ''\nfor key, value in may_be_missing.items():\ntry:\nif eval('self.' + key) is None or eval('self.' + key) is False:\nmissing += f'- {value}\\n'\nexcept Exception:\n# should not happen, all eval variables should be defined\nmsg = ErrorNumbers.FB400.value + \\\n                f', in method `info` : self.{key} not defined for experiment'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nif missing:\nprint(f'\\nExperiment cannot be run (not fully defined), missing :\\n{missing}')\nelse:\nprint('\\nExperiment can be run now (fully defined)')\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.job","title":"<pre><code>job()\n</code></pre>","text":"<p>Retrieves the <code>Job</code> that manages training rounds.</p> <p>Returns:</p> Type Description <code>Union[Job, None]</code> <p>Initialized <code>Job</code> object. None, if it isn't declared yet or not information to set to job. Please see <code>set_job</code>.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef job(self) -&gt; Union[Job, None]:\n\"\"\"Retrieves the [`Job`][fedbiomed.researcher.job] that manages training rounds.\n    Returns:\n        Initialized `Job` object. None, if it isn't declared yet or not information to set to job. Please see\n            [`set_job`][fedbiomed.researcher.experiment.Experiment.set_job].\n    \"\"\"\nreturn self._job\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.load_breakpoint","title":"<pre><code>load_breakpoint(breakpoint_folder_path=None)\n</code></pre>  <code>classmethod</code>","text":"<p>Loads breakpoint (provided a breakpoint has been saved) so experience can be resumed. Useful if training has crashed researcher side or if user wants to resume experiment.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Type[TExperiment]</code> <p>Experiment class</p> required <code>breakpoint_folder_path</code> <code>Union[str, None]</code> <p>path of the breakpoint folder. Path can be absolute or relative eg: \"var/experiments/Experiment_xxxx/breakpoints_xxxx\". If None, loads latest breakpoint of the latest experiment. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>TExperiment</code> <p>Reinitialized experiment object. With given object-0.2119,  0.0796, -0.0759, user can then use <code>.run()</code> method to pursue model training.</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad argument type, error when reading breakpoint or bad loaded breakpoint content (corrupted)</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@classmethod\n@exp_exceptions\ndef load_breakpoint(cls: Type[TExperiment],\nbreakpoint_folder_path: Union[str, None] = None) -&gt; TExperiment:\n\"\"\"\n    Loads breakpoint (provided a breakpoint has been saved)\n    so experience can be resumed. Useful if training has crashed\n    researcher side or if user wants to resume experiment.\n    Args:\n      cls: Experiment class\n      breakpoint_folder_path: path of the breakpoint folder. Path can be absolute or relative eg:\n        \"var/experiments/Experiment_xxxx/breakpoints_xxxx\". If None, loads latest breakpoint of the latest\n        experiment. Defaults to None.\n    Returns:\n        Reinitialized experiment object. With given object-0.2119,  0.0796, -0.0759, user can then use `.run()`\n            method to pursue model training.\n    Raises:\n        FedbiomedExperimentError: bad argument type, error when reading breakpoint or bad loaded breakpoint\n            content (corrupted)\n    \"\"\"\n# check parameters type\nif not isinstance(breakpoint_folder_path, str) and breakpoint_folder_path is not None:\nmsg = (\nf\"{ErrorNumbers.FB413.value}: load failed, `breakpoint_folder_path`\"\nf\" has bad type {type(breakpoint_folder_path)}\"\n)\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# get breakpoint folder path (if it is None) and state file\nbreakpoint_folder_path, state_file = find_breakpoint_path(breakpoint_folder_path)\nbreakpoint_folder_path = os.path.abspath(breakpoint_folder_path)\ntry:\npath = os.path.join(breakpoint_folder_path, state_file)\nwith open(path, \"r\", encoding=\"utf-8\") as file:\nsaved_state = json.load(file)\nexcept (json.JSONDecodeError, OSError) as exc:\n# OSError: heuristic for catching file access issues\nmsg = (\nf\"{ErrorNumbers.FB413.value}: load failed,\"\nf\" reading breakpoint file failed with message {exc}\"\n)\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg) from exc\nif not isinstance(saved_state, dict):\nmsg = (\nf\"{ErrorNumbers.FB413.value}: load failed, breakpoint file seems\"\nf\" corrupted. Type should be `dict` not {type(saved_state)}\"\n)\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# First, check version of breakpoints\nbkpt_version = saved_state.get('breakpoint_version', __default_version__)\nraise_for_version_compatibility(bkpt_version, __breakpoints_version__,\nf\"{ErrorNumbers.FB413.value}: Breakpoint file was generated with version %s \"\nf\"which is incompatible with the current version %s.\")\n# retrieve breakpoint training data\nbkpt_fds = saved_state.get('training_data')\nbkpt_fds = FederatedDataSet(bkpt_fds)\n# retrieve breakpoint sampling strategy\nbkpt_sampling_strategy_args = saved_state.get(\"node_selection_strategy\")\nbkpt_sampling_strategy = cls._create_object(bkpt_sampling_strategy_args, data=bkpt_fds)\n# initializing experiment\nloaded_exp = cls(tags=saved_state.get('tags'),\nnodes=None,  # list of previous nodes is contained in training_data\ntraining_data=bkpt_fds,\nnode_selection_strategy=bkpt_sampling_strategy,\nround_limit=saved_state.get(\"round_limit\"),\ntraining_plan_class=saved_state.get(\"training_plan_class\"),\ntraining_plan_path=saved_state.get(\"training_plan_path\"),\nmodel_args=saved_state.get(\"model_args\"),\ntraining_args=saved_state.get(\"training_args\"),\nsave_breakpoints=True,\nexperimentation_folder=saved_state.get('experimentation_folder'),\nsecagg=SecureAggregation.load_state(saved_state.get('secagg')))\n# nota: we are initializing experiment with no aggregator: hence, by default,\n# `loaded_exp` will be loaded with FedAverage.\n# changing `Experiment` attributes\nloaded_exp._set_round_current(saved_state.get('round_current'))\n# TODO: checks when loading parameters\ntraining_plan = loaded_exp.training_plan()\nif training_plan is None:\nmsg = ErrorNumbers.FB413.value + ' - load failed, ' + \\\n            'breakpoint file seems corrupted, `training_plan` is None'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nelse:\nloaded_exp._aggregated_params = loaded_exp._load_aggregated_params(\nsaved_state.get('aggregated_params')\n)\n# retrieve and change federator\nbkpt_aggregator_args = saved_state.get(\"aggregator\")\nbkpt_aggregator = loaded_exp._create_object(bkpt_aggregator_args, training_plan=training_plan)\nloaded_exp.set_aggregator(bkpt_aggregator)\n# changing `Job` attributes\nloaded_exp._job.load_state(saved_state.get('job'))\nlogger.info(f\"Experimentation reload from {breakpoint_folder_path} successful!\")\nreturn loaded_exp\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.model_args","title":"<pre><code>model_args()\n</code></pre>","text":"<p>Retrieves model arguments.</p> <p>Please see also <code>set_model_args</code></p> <p>Returns:</p> Type Description <code>dict</code> <p>The arguments that are going to be passed to <code>training_plans</code> classes in built time on the node side.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef model_args(self) -&gt; dict:\n\"\"\"Retrieves model arguments.\n    Please see also [`set_model_args`][fedbiomed.researcher.experiment.Experiment.set_model_args]\n    Returns:\n        The arguments that are going to be passed to [`training_plans`][fedbiomed.common.training_plans]\n            classes in built time on the node side.\n    \"\"\"\nreturn self._model_args\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.monitor","title":"<pre><code>monitor()\n</code></pre>","text":"<p>Retrieves the monitor object</p> <p>Monitor is responsible for receiving and parsing real-time training and validation feed-back from each node participate to federated training. See <code>Monitor</code></p> <p>Returns:</p> Type Description <code>Monitor</code> <p>Monitor object that will always exist with experiment to retrieve feed-back from the nodes.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef monitor(self) -&gt; Monitor:\n\"\"\"Retrieves the monitor object\n    Monitor is responsible for receiving and parsing real-time training and validation feed-back from each node\n    participate to federated training. See [`Monitor`][fedbiomed.researcher.monitor.Monitor]\n    Returns:\n        Monitor object that will always exist with experiment to retrieve feed-back from the nodes.\n    \"\"\"\nreturn self._monitor\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.nodes","title":"<pre><code>nodes()\n</code></pre>","text":"<p>Retrieves the <code>nodes</code> that are chosen for federated training.</p> <p>Please see <code>set_nodes</code> to set <code>nodes</code>.</p> <p>Returns:</p> Type Description <code>Union[List[str], None]</code> <p>Object that contains meta-data for the datasets of each node. <code>None</code> if nodes are not set.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef nodes(self) -&gt; Union[List[str], None]:\n\"\"\"Retrieves the `nodes` that are chosen for federated training.\n    Please see [`set_nodes`][fedbiomed.researcher.experiment.Experiment.set_nodes] to set `nodes`.\n    Returns:\n        Object that contains meta-data for the datasets of each node. `None` if nodes are not set.\n    \"\"\"\nreturn self._nodes\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.round_current","title":"<pre><code>round_current()\n</code></pre>","text":"<p>Retrieves the round where the experiment is at.</p> <p>Returns:</p> Type Description <code>int</code> <p>Indicates the round number that the experiment will perform next.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef round_current(self) -&gt; int:\n\"\"\"Retrieves the round where the experiment is at.\n    Returns:\n        Indicates the round number that the experiment will perform next.\n    \"\"\"\nreturn self._round_current\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.round_limit","title":"<pre><code>round_limit()\n</code></pre>","text":"<p>Retrieves the round limit from the experiment object.</p> <p>Please see  also <code>set_round_limit</code> to change or set round limit.</p> <p>Returns:</p> Type Description <code>Union[int, None]</code> <p>Round limit that shows maximum number of rounds that can be performed. <code>None</code> if it isn't declared yet.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef round_limit(self) -&gt; Union[int, None]:\n\"\"\"Retrieves the round limit from the experiment object.\n    Please see  also [`set_round_limit`][fedbiomed.researcher.experiment.Experiment.set_training_data] to change\n    or set round limit.\n    Returns:\n        Round limit that shows maximum number of rounds that can be performed. `None` if it isn't declared yet.\n    \"\"\"\nreturn self._round_limit\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.run","title":"<pre><code>run(rounds=None, increase=False)\n</code></pre>","text":"<p>Run one or more rounds of an experiment, continuing from the point the experiment had reached.</p> <p>Parameters:</p> Name Type Description Default <code>rounds</code> <code>Union[int, None]</code> <p>Number of experiment rounds to run in this call. * <code>None</code> means \"run all the rounds remaining in the experiment\" computed as     maximum rounds (<code>round_limit</code> for this experiment) minus the number of     rounds already run rounds (<code>round_current</code> for this experiment).     It does nothing and issues a warning if <code>round_limit</code> is <code>None</code> (no     round limit defined for the experiment) * <code>int</code> &gt;= 1 means \"run at most <code>rounds</code> rounds\".     If <code>round_limit</code> is <code>None</code> for the experiment, run exactly <code>rounds</code> rounds.     If a <code>round_limit</code> is set for the experiment and the number or rounds would increase beyond the <code>round_limit</code> of the experiment: - if <code>increase</code> is True, increase the <code>round_limit</code> to   (<code>round_current</code> + <code>rounds</code>) and run <code>rounds</code> rounds - if <code>increase</code> is False, run (<code>round_limit</code> - <code>round_current</code>)   rounds, don't modify the maximum <code>round_limit</code> of the experiment   and issue a warning.</p> <code>None</code> <code>increase</code> <code>bool</code> <p>automatically increase the <code>round_limit</code> of the experiment for executing the specified number of <code>rounds</code>. Does nothing if <code>round_limit</code> is <code>None</code> or <code>rounds</code> is None. Defaults to False</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of rounds have been run</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad argument type or value</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef run(self, rounds: Union[int, None] = None, increase: bool = False) -&gt; int:\n\"\"\"Run one or more rounds of an experiment, continuing from the point the\n    experiment had reached.\n    Args:\n        rounds: Number of experiment rounds to run in this call.\n            * `None` means \"run all the rounds remaining in the experiment\" computed as\n                maximum rounds (`round_limit` for this experiment) minus the number of\n                rounds already run rounds (`round_current` for this experiment).\n                It does nothing and issues a warning if `round_limit` is `None` (no\n                round limit defined for the experiment)\n            * `int` &gt;= 1 means \"run at most `rounds` rounds\".\n                If `round_limit` is `None` for the experiment, run exactly `rounds` rounds.\n                If a `round_limit` is set for the experiment and the number or rounds would\n            increase beyond the `round_limit` of the experiment:\n            - if `increase` is True, increase the `round_limit` to\n              (`round_current` + `rounds`) and run `rounds` rounds\n            - if `increase` is False, run (`round_limit` - `round_current`)\n              rounds, don't modify the maximum `round_limit` of the experiment\n              and issue a warning.\n        increase: automatically increase the `round_limit`\n            of the experiment for executing the specified number of `rounds`.\n            Does nothing if `round_limit` is `None` or `rounds` is None.\n            Defaults to False\n    Returns:\n        Number of rounds have been run\n    Raises:\n        FedbiomedExperimentError: bad argument type or value\n    \"\"\"\n# check rounds is a &gt;=1 integer or None\nif rounds is None:\npass\nelif isinstance(rounds, int):\nif rounds &lt; 1:\nmsg = ErrorNumbers.FB410.value + \\\n                f', in method `run` param `rounds` : value {rounds}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nelse:\n# bad type\nmsg = ErrorNumbers.FB410.value + \\\n            f', in method `run` param `rounds` : type {type(rounds)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# check increase is a boolean\nif not isinstance(increase, bool):\nmsg = ErrorNumbers.FB410.value + \\\n            f', in method `run` param `increase` : type {type(increase)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# compute number of rounds to run + updated rounds limit\nif rounds is None:\nif isinstance(self._round_limit, int):\n# run all remaining rounds in the experiment\nrounds = self._round_limit - self._round_current\nif rounds == 0:\n# limit already reached\nlogger.warning(f'Round limit of {self._round_limit} already reached '\n'for this experiment, do nothing.')\nreturn 0\nelse:\n# cannot run if no number of rounds given and no round limit exists\nlogger.warning('Cannot run, please specify a number of `rounds` to run or '\n'set a `round_limit` to the experiment')\nreturn 0\nelse:\n# at this point, rounds is an int &gt;= 1\nif isinstance(self._round_limit, int):\nif (self._round_current + rounds) &gt; self._round_limit:\nif increase:\n# dont change rounds, but extend self._round_limit as necessary\nlogger.debug(f'Auto increasing total rounds for experiment from {self._round_limit} '\nf'to {self._round_current + rounds}')\nself._round_limit = self._round_current + rounds\nelse:\nnew_rounds = self._round_limit - self._round_current\nif new_rounds == 0:\n# limit already reached\nlogger.warning(f'Round limit of {self._round_limit} already reached '\n'for this experiment, do nothing.')\nreturn 0\nelse:\n# reduce the number of rounds to run in the experiment\nlogger.warning(f'Limit of {self._round_limit} rounds for the experiment '\nf'will be reached, reducing the number of rounds for this '\nf'run from {rounds} to {new_rounds}')\nrounds = new_rounds\n# At this point `rounds` is an int &gt; 0 (not None)\n# run the rounds\nfor _ in range(rounds):\nif isinstance(self._round_limit, int) and self._round_current == (self._round_limit - 1) \\\n                and self._training_args['test_on_global_updates'] is True:\n# Do \"validation after a round\" only if this a round limit is defined and we reached it\n# and validation is active on global params\n# When this condition is met, it also means we are running the last of\n# the `rounds` rounds in this function\ntest_after = True\nelse:\ntest_after = False\nincrement = self.run_once(increase=False, test_after=test_after)\nif increment == 0:\n# should not happen\nmsg = ErrorNumbers.FB400.value + \\\n                f', in method `run` method `run_once` returns {increment}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nreturn rounds\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.run_once","title":"<pre><code>run_once(increase=False, test_after=False)\n</code></pre>","text":"<p>Run at most one round of an experiment, continuing from the point the experiment had reached.</p> <p>If <code>round_limit</code> is <code>None</code> for the experiment (no round limit defined), run one round. If <code>round_limit</code> is not <code>None</code> and the <code>round_limit</code> of the experiment is already reached: * if <code>increase</code> is False, do nothing and issue a warning * if <code>increase</code> is True, increment total number of round <code>round_limit</code> and run one round</p> <p>Parameters:</p> Name Type Description Default <code>increase</code> <code>bool</code> <p>automatically increase the <code>round_limit</code> of the experiment if needed. Does nothing if <code>round_limit</code> is <code>None</code>. Defaults to False</p> <code>False</code> <code>test_after</code> <code>bool</code> <p>if True, do a second request to the nodes after the round, only for validation on aggregated params. Intended to be used after the last training round of an experiment. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of rounds really run</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad argument type or value</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef run_once(self, increase: bool = False, test_after: bool = False) -&gt; int:\n\"\"\"Run at most one round of an experiment, continuing from the point the\n    experiment had reached.\n    If `round_limit` is `None` for the experiment (no round limit defined), run one round.\n    If `round_limit` is not `None` and the `round_limit` of the experiment is already reached:\n    * if `increase` is False, do nothing and issue a warning\n    * if `increase` is True, increment total number of round `round_limit` and run one round\n    Args:\n        increase: automatically increase the `round_limit` of the experiment if needed. Does nothing if\n            `round_limit` is `None`. Defaults to False\n        test_after: if True, do a second request to the nodes after the round, only for validation on aggregated\n            params. Intended to be used after the last training round of an experiment. Defaults to False.\n    Returns:\n        Number of rounds really run\n    Raises:\n        FedbiomedExperimentError: bad argument type or value\n    \"\"\"\n# check increase is a boolean\nif not isinstance(increase, bool):\nmsg = ErrorNumbers.FB410.value + \\\n              f', in method `run_once` param `increase` : type {type(increase)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# nota:  we should never have self._round_current &gt; self._round_limit, only ==\nif self._round_limit is not None and self._round_current &gt;= self._round_limit:\nif increase is True:\nlogger.debug(f'Auto increasing total rounds for experiment from {self._round_limit} '\nf'to {self._round_current + 1}')\nself._round_limit = self._round_current + 1\nelse:\nlogger.warning(f'Round limit of {self._round_limit} was reached, do nothing')\nreturn 0\n# at this point, self._aggregator always exists and is not None\n# self.{_node_selection_strategy,_job} exist but may be None\n# check pre-requisites are met for running a round\n# for component in (self._node_selection_strategy, self._job):\nif self._node_selection_strategy is None:\nmsg = ErrorNumbers.FB411.value + ', missing `node_selection_strategy`'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nelif self._job is None:\nmsg = ErrorNumbers.FB411.value + ', missing `job`'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# Ready to execute a training round using the job, strategy and aggregator\nif self._global_model is None:\nself._global_model = self._job.training_plan.after_training_params()\n# initial server state, before optimization/aggregation\nself._aggregator.set_training_plan_type(self._job.training_plan.type())\n# Sample nodes using strategy (if given)\nself._job.nodes = self._node_selection_strategy.sample_nodes(self._round_current)\n# If secure aggregation is activated ---------------------------------------------------------------------\nsecagg_arguments = None\nif self._secagg.active:\nself._secagg.setup(parties=[environ[\"ID\"]] + self._job.nodes,\njob_id=self._job.id)\nsecagg_arguments = self._secagg.train_arguments()\n# --------------------------------------------------------------------------------------------------------\n# Check aggregator parameter(s) before starting a round\nself._aggregator.check_values(n_updates=self._training_args.get('num_updates'),\ntraining_plan=self._job.training_plan)\nlogger.info('Sampled nodes in round ' + str(self._round_current) + ' ' + str(self._job.nodes))\naggr_args_thr_msg, aggr_args_thr_file = self._aggregator.create_aggregator_args(self._global_model,\nself._job.nodes)\n# Trigger training round on sampled nodes\n_ = self._job.start_nodes_training_round(round_=self._round_current,\naggregator_args_thr_msg=aggr_args_thr_msg,\naggregator_args_thr_files=aggr_args_thr_file,\ndo_training=True,\nsecagg_arguments=secagg_arguments)\n# refining/normalizing model weights received from nodes\nmodel_params, weights, total_sample_size, encryption_factors = self._node_selection_strategy.refine(\nself._job.training_replies[self._round_current], self._round_current)\nself._aggregator.set_fds(self._fds)\nif self._secagg.active:\nflatten_params = self._secagg.aggregate(\nround_=self._round_current,\nencryption_factors=encryption_factors,\ntotal_sample_size=total_sample_size,\nmodel_params=model_params\n)\n# FIXME: Access TorchModel through non-private getter once it is implemented\naggregated_params: Dict[str, Union['torch.tensor', 'nd.ndarray']] = \\\n            self._job.training_plan._model.unflatten(flatten_params)\nelse:\n# aggregate models from nodes to a global model\naggregated_params = self._aggregator.aggregate(model_params,\nweights,\nglobal_model=self._global_model,\ntraining_plan=self._job.training_plan,\ntraining_replies=self._job.training_replies,\nnode_ids=self._job.nodes,\nn_updates=self._training_args.get('num_updates'),\nn_round=self._round_current)\n# write results of the aggregated model in a temp file\n# Export aggregated parameters to a local file and upload it.\n# Also assign the new values to the job's training plan's model.\nself._global_model = aggregated_params  # update global model\naggregated_params_path, _ = self._job.update_parameters(aggregated_params)\nlogger.info(f'Saved aggregated params for round {self._round_current} '\nf'in {aggregated_params_path}')\nself._aggregated_params[self._round_current] = {'params': aggregated_params,\n'params_path': aggregated_params_path}\nself._round_current += 1\n# Update round in monitor for the next round\nself._monitor.set_round(round_=self._round_current + 1)\nif self._save_breakpoints:\nself.breakpoint()\n# do final validation after saving breakpoint :\n# not saved in breakpoint for current round, but more simple\nif test_after:\n# FIXME: should we sample nodes here too?\naggr_args_thr_msg, aggr_args_thr_file = self._aggregator.create_aggregator_args(self._global_model,\nself._job.nodes)\nself._job.start_nodes_training_round(round_=self._round_current,\naggregator_args_thr_msg=aggr_args_thr_msg,\naggregator_args_thr_files=aggr_args_thr_file,\ndo_training=False)\nreturn 1\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.save_breakpoints","title":"<pre><code>save_breakpoints()\n</code></pre>","text":"<p>Retrieves the status of saving breakpoint after each round of training.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code>, If saving breakpoint is active. <code>False</code>, vice versa.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef save_breakpoints(self) -&gt; bool:\n\"\"\"Retrieves the status of saving breakpoint after each round of training.\n    Returns:\n        `True`, If saving breakpoint is active. `False`, vice versa.\n    \"\"\"\nreturn self._save_breakpoints\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_aggregator","title":"<pre><code>set_aggregator(aggregator)\n</code></pre>","text":"<p>Sets aggregator + verification on arguments type</p> <p>Parameters:</p> Name Type Description Default <code>aggregator</code> <code>Union[Aggregator, Type[Aggregator], None]</code> <p>Object or class defining the method for aggregating local updates. Default to None (use <code>FedAverage</code> for aggregation)</p> required <p>Returns:</p> Type Description <code>Aggregator</code> <p>aggregator (Aggregator)</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad aggregator type</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_aggregator(self, aggregator: Union[Aggregator, Type[Aggregator], None]) -&gt; \\\n        Aggregator:\n\"\"\"Sets aggregator + verification on arguments type\n    Args:\n        aggregator: Object or class defining the method for aggregating local updates. Default to None\n            (use `FedAverage` for aggregation)\n    Returns:\n        aggregator (Aggregator)\n    Raises:\n        FedbiomedExperimentError : bad aggregator type\n    \"\"\"\nif aggregator is None:\n# default aggregator\nself._aggregator = FedAverage()\nelif inspect.isclass(aggregator):\n# a class is provided, need to instantiate an object\nif issubclass(aggregator, Aggregator):\nself._aggregator = aggregator()\nelse:\n# bad argument\nmsg = ErrorNumbers.FB410.value + f' `aggregator` : {aggregator} class'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nelif isinstance(aggregator, Aggregator):\n# an object of a proper class is provided, nothing to do\nself._aggregator = aggregator\nelse:\n# other bad type or object\nmsg = ErrorNumbers.FB410.value + f' `aggregator` : {type(aggregator)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# at this point self._aggregator is (non-None) aggregator object\nself.aggregator_args[\"aggregator_name\"] = self._aggregator.aggregator_name\nif self._fds is not None:\nself._aggregator.set_fds(self._fds)\nreturn self._aggregator\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_experimentation_folder","title":"<pre><code>set_experimentation_folder(experimentation_folder)\n</code></pre>","text":"<p>Sets <code>experimentation_folder</code>, the folder name where experiment data/result are saved.</p> <p>Parameters:</p> Name Type Description Default <code>experimentation_folder</code> <code>Union[str, None]</code> <p>File name where experiment related files are saved</p> required <p>Returns:</p> Type Description <code>str</code> <p>experimentation_folder (str)</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad <code>experimentation_folder</code> type</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_experimentation_folder(self, experimentation_folder: Union[str, None]) -&gt; str:\n\"\"\"Sets `experimentation_folder`, the folder name where experiment data/result are saved.\n    Args:\n        experimentation_folder: File name where experiment related files are saved\n    Returns:\n        experimentation_folder (str)\n    Raises:\n        FedbiomedExperimentError : bad `experimentation_folder` type\n    \"\"\"\nif experimentation_folder is None:\nself._experimentation_folder = create_exp_folder()\nelif isinstance(experimentation_folder, str):\nsanitized_folder = sanitize_filename(experimentation_folder, platform='auto')\nself._experimentation_folder = create_exp_folder(sanitized_folder)\nif (sanitized_folder != experimentation_folder):\nlogger.warning(f'`experimentation_folder` was sanitized from '\nf'{experimentation_folder} to {sanitized_folder}')\nelse:\nmsg = ErrorNumbers.FB410.value + \\\n            f' `experimentation_folder` : {type(experimentation_folder)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# at this point self._experimentation_folder is a str valid for a foldername\n# _job doesn't always exist at this point\nif self._job is not None:\nlogger.debug('Experimentation folder changed, you may need to update `job`')\nreturn self._experimentation_folder\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_job","title":"<pre><code>set_job()\n</code></pre>","text":"<p>Setter for job, it verifies pre-requisites are met for creating a job attached to this experiment. If yes, instantiate a job ; if no, return None.</p> <p>Returns:</p> Type Description <code>Union[Job, None]</code> <p>The object that is initialized for creating round jobs.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_job(self) -&gt; Union[Job, None]:\n\"\"\"Setter for job, it verifies pre-requisites are met for creating a job\n    attached to this experiment. If yes, instantiate a job ; if no, return None.\n    Returns:\n        The object that is initialized for creating round jobs.\n    \"\"\"\n# at this point all are defined among:\n# self.{_reqs,_fds,_training_plan_is_defined,_training_plan,_training_plan_path,_model_args,_training_args}\n# self._experimentation_folder =&gt; self.experimentation_path()\n# self._round_current\nif self._job is not None:\n# a job is already defined, and it may also have run some rounds\nlogger.debug('Experimentation `job` changed after running '\n'{self._round_current} rounds, may give inconsistent results')\nif self._training_plan_is_defined is not True:\n# training plan not properly defined yet\nself._job = None\nlogger.debug('Experiment not fully configured yet: no job. Missing proper training plan '\nf'definition (training_plan={self._training_plan_class} '\nf'training_plan_path={self._training_plan_path})')\nelif self._fds is None:\n# not training data yet\nself._job = None\nlogger.debug('Experiment not fully configured yet: no job. Missing training data')\nelse:\n# meeting requisites for instantiating a job\nself._job = Job(reqs=self._reqs,\ntraining_plan_class=self._training_plan_class,\ntraining_plan_path=self._training_plan_path,\nmodel_args=self._model_args,\ntraining_args=self._training_args,\ndata=self._fds,\nkeep_files_dir=self.experimentation_path())\nreturn self._job\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_model_args","title":"<pre><code>set_model_args(model_args)\n</code></pre>","text":"<p>Sets <code>model_args</code> + verification on arguments type</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>dict</code> <p>contains model arguments passed to the constructor of the training plan when instantiating it : output and input feature dimension, etc.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Model arguments that have been set.</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad model_args type</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_model_args(self, model_args: dict) -&gt; dict:\n\"\"\"Sets `model_args` + verification on arguments type\n    Args:\n        model_args (dict): contains model arguments passed to the constructor\n            of the training plan when instantiating it : output and input feature\n            dimension, etc.\n    Returns:\n        Model arguments that have been set.\n    Raises:\n        FedbiomedExperimentError : bad model_args type\n    \"\"\"\nif isinstance(model_args, dict):\nself._model_args = model_args\nelse:\n# bad type\nmsg = ErrorNumbers.FB410.value + f' `model_args` : {type(model_args)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# self._model_args always exist at this point\nif self._job is not None:\nlogger.debug('Experimentation model_args changed, you may need to update `job`')\nreturn self._model_args\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_nodes","title":"<pre><code>set_nodes(nodes)\n</code></pre>","text":"<p>Sets for nodes + verifications on argument type</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Union[List[str], None]</code> <p>List of node_ids to filter the nodes to be involved in the experiment.</p> required <p>Returns:</p> Type Description <code>Union[List[str], None]</code> <p>List of tags that are set. None, if the argument <code>nodes</code> is None.</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>Bad nodes type</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_nodes(self, nodes: Union[List[str], None]) -&gt; Union[List[str], None]:\n\"\"\"Sets for nodes + verifications on argument type\n    Args:\n        nodes: List of node_ids to filter the nodes to be involved in the experiment.\n    Returns:\n        List of tags that are set. None, if the argument `nodes` is None.\n    Raises:\n        FedbiomedExperimentError : Bad nodes type\n    \"\"\"\nif isinstance(nodes, list):\nfor node in nodes:\nif not isinstance(node, str):\nmsg = ErrorNumbers.FB410.value + f' `nodes` : list of {type(node)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nself._nodes = nodes\nelif nodes is None:\nself._nodes = nodes\nelse:\nmsg = ErrorNumbers.FB410.value + f' `nodes` : {type(nodes)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# self._nodes always exist at this point\nif self._fds is not None:\nlogger.debug('Experimentation nodes filter changed, you may need to update `training_data`')\nreturn self._nodes\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_round_limit","title":"<pre><code>set_round_limit(round_limit)\n</code></pre>","text":"<p>Sets <code>round_limit</code> + verification on arguments type</p> <p>Parameters:</p> Name Type Description Default <code>round_limit</code> <code>Union[int, None]</code> <p>the maximum number of training rounds (nodes &lt;-&gt; central server) that should be executed for the experiment. <code>None</code> means that no limit is defined.</p> required <p>Returns:</p> Type Description <code>Union[int, None]</code> <p>Round limit for experiment of federated learning</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad rounds type or value</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_round_limit(self, round_limit: Union[int, None]) -&gt; Union[int, None]:\n\"\"\"Sets `round_limit` + verification on arguments type\n    Args:\n        round_limit: the maximum number of training rounds (nodes &lt;-&gt; central server) that should be executed\n            for the experiment. `None` means that no limit is defined.\n    Returns:\n        Round limit for experiment of federated learning\n    Raises:\n        FedbiomedExperimentError : bad rounds type or value\n    \"\"\"\n# at this point round_current exists and is an int &gt;= 0\nif round_limit is None:\n# no limit for training rounds\nself._round_limit = None\nelif isinstance(round_limit, int):\n# at this point round_limit is an int\nif round_limit &lt; 0:\nmsg = ErrorNumbers.FB410.value + f' `round_limit` can not be negative: {round_limit}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nelif round_limit &lt; self._round_current:\n# self._round_limit can't be less than current round\nlogger.error(f'cannot set `round_limit` to less than the number of already run rounds '\nf'({self._round_current})')\nelse:\nself._round_limit = round_limit\nelse:\nmsg = ErrorNumbers.FB410.value + f' `round_limit` : {type(round_limit)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# at this point self._round_limit is a Union[int, None]\nreturn self._round_limit\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_save_breakpoints","title":"<pre><code>set_save_breakpoints(save_breakpoints)\n</code></pre>","text":"<p>Setter for save_breakpoints + verification on arguments type</p> <p>Parameters:</p> Name Type Description Default <code>save_breakpoints</code> <code>bool</code> <p>whether to save breakpoints or not after each training round. Breakpoints can be used for resuming a crashed experiment.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Status of saving breakpoints</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad save_breakpoints type</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_save_breakpoints(self, save_breakpoints: bool) -&gt; bool:\n\"\"\" Setter for save_breakpoints + verification on arguments type\n    Args:\n        save_breakpoints (bool): whether to save breakpoints or\n            not after each training round. Breakpoints can be used for resuming\n            a crashed experiment.\n    Returns:\n        Status of saving breakpoints\n    Raises:\n        FedbiomedExperimentError: bad save_breakpoints type\n    \"\"\"\nif isinstance(save_breakpoints, bool):\nself._save_breakpoints = save_breakpoints\n# no warning if done during experiment, we may change breakpoint policy at any time\nelse:\nmsg = ErrorNumbers.FB410.value + f' `save_breakpoints` : {type(save_breakpoints)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nreturn self._save_breakpoints\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_secagg","title":"<pre><code>set_secagg(secagg)\n</code></pre>","text":"Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_secagg(self, secagg: Union[bool, SecureAggregation]):\nif isinstance(secagg, bool):\nself._secagg = SecureAggregation(active=secagg, timeout=10)\nelif isinstance(secagg, SecureAggregation):\nself._secagg = secagg\nelse:\nmsg = f\"{ErrorNumbers.FB410.value}: Expected `secagg` argument bool or `SecureAggregation`, \" \\\n              f\"but got {type(secagg)}\"\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nreturn self._secagg\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_strategy","title":"<pre><code>set_strategy(node_selection_strategy)\n</code></pre>","text":"<p>Sets for <code>node_selection_strategy</code> + verification on arguments type</p> <p>Parameters:</p> Name Type Description Default <code>node_selection_strategy</code> <code>Union[Strategy, Type[Strategy], None]</code> <p>object or class defining how nodes are sampled at each round for training, and how non-responding nodes are managed. Defaults to None: - use <code>DefaultStrategy</code> if training_data is initialized - else strategy is None (cannot be initialized), experiment cannot   be launched yet</p> required <p>Returns:</p> Type Description <code>Union[Strategy, None]</code> <p>node selection strategy class</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad strategy type</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_strategy(self, node_selection_strategy: Union[Strategy, Type[Strategy], None]) -&gt; \\\n        Union[Strategy, None]:\n\"\"\"Sets for `node_selection_strategy` + verification on arguments type\n    Args:\n        node_selection_strategy: object or class defining how nodes are sampled at each round for training, and\n            how non-responding nodes are managed. Defaults to None:\n            - use `DefaultStrategy` if training_data is initialized\n            - else strategy is None (cannot be initialized), experiment cannot\n              be launched yet\n    Returns:\n        node selection strategy class\n    Raises:\n        FedbiomedExperimentError : bad strategy type\n    \"\"\"\nif self._fds is not None:\nif node_selection_strategy is None:\n# default node_selection_strategy\nself._node_selection_strategy = DefaultStrategy(self._fds)\nelif inspect.isclass(node_selection_strategy):\n# a class is provided, need to instantiate an object\nif issubclass(node_selection_strategy, Strategy):\nself._node_selection_strategy = node_selection_strategy(self._fds)\nelse:\n# bad argument\nmsg = ErrorNumbers.FB410.value + \\\n                    f' `node_selection_strategy` : {node_selection_strategy} class'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nelif isinstance(node_selection_strategy, Strategy):\n# an object of a proper class is provided, nothing to do\nself._node_selection_strategy = node_selection_strategy\nelse:\n# other bad type or object\nmsg = ErrorNumbers.FB410.value + \\\n                f' `node_selection_strategy` : {type(node_selection_strategy)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nelse:\n# cannot initialize strategy if not FederatedDataSet yet\nself._node_selection_strategy = None\nlogger.debug('Experiment not fully configured yet: no node selection strategy')\n# at this point self._node_selection_strategy is a Union[Strategy, None]\nreturn self._node_selection_strategy\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_tags","title":"<pre><code>set_tags(tags)\n</code></pre>","text":"<p>Sets tags + verifications on argument type</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Union[List[str], str, None]</code> <p>List of string with data tags or string with one data tag. Empty list of tags ([]) means any dataset is accepted, it is different from None (tags not set, cannot search for training_data yet).</p> required <p>Returns:</p> Type Description <code>Union[List[str], None]</code> <p>List of tags that are set. None, if the argument <code>tags</code> is None.</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>Bad tags type</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_tags(self, tags: Union[List[str], str, None]) -&gt; Union[List[str], None]:\n\"\"\"Sets tags + verifications on argument type\n    Args:\n        tags: List of string with data tags or string with one data tag. Empty list\n            of tags ([]) means any dataset is accepted, it is different from None (tags not set, cannot search\n            for training_data yet).\n    Returns:\n        List of tags that are set. None, if the argument `tags` is None.\n    Raises:\n        FedbiomedExperimentError : Bad tags type\n    \"\"\"\nif isinstance(tags, list):\nfor tag in tags:\nif not isinstance(tag, str):\nmsg = ErrorNumbers.FB410.value + f' `tags` : list of {type(tag)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nself._tags = tags\nelif isinstance(tags, str):\nself._tags = [tags]\nelif tags is None:\nself._tags = tags\nelse:\nmsg = ErrorNumbers.FB410.value + f' `tags` : {type(tags)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# self._tags always exist at this point\nif self._fds is not None:\nlogger.debug('Experimentation tags changed, you may need to update `training_data`')\nreturn self._tags\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_tensorboard","title":"<pre><code>set_tensorboard(tensorboard)\n</code></pre>","text":"<p>Sets the tensorboard flag</p> <p>Parameters:</p> Name Type Description Default <code>tensorboard</code> <code>bool</code> <p>If <code>True</code> tensorboard log files will be writen after receiving training feedbacks</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Status of tensorboard</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_tensorboard(self, tensorboard: bool) -&gt; bool:\n\"\"\"\n    Sets the tensorboard flag\n    Args:\n        tensorboard: If `True` tensorboard log files will be writen after receiving training feedbacks\n    Returns:\n        Status of tensorboard\n    \"\"\"\nif isinstance(tensorboard, bool):\nself._tensorboard = tensorboard\nself._monitor.set_tensorboard(tensorboard)\nelse:\nmsg = ErrorNumbers.FB410.value + f' `tensorboard` : {type(tensorboard)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nreturn self._tensorboard\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_test_metric","title":"<pre><code>set_test_metric(metric, metric_args)\n</code></pre>","text":"<p>Sets a metric for federated model validation</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Union[MetricTypes, str, None]</code> <p>A class as an instance of <code>MetricTypes</code>. <code>str</code> for referring one of  metric which provided as attributes in <code>MetricTypes</code>. None, if it isn't declared yet.</p> required <code>**metric_args</code> <code>dict</code> <p>A dictionary that contains arguments for metric function. Arguments should be compatible with corresponding metrics in <code>sklearn.metrics</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Union[str, None], Dict[str, Any]]</code> <p>Metric and  metric args as tuple</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>Invalid type for <code>metric</code> argument</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_test_metric(self, metric: Union[MetricTypes, str, None], **metric_args: dict) -&gt; \\\n        Tuple[Union[str, None], Dict[str, Any]]:\n\"\"\" Sets a metric for federated model validation\n    Args:\n        metric: A class as an instance of [`MetricTypes`][fedbiomed.common.metrics.MetricTypes]. [`str`][str] for\n            referring one of  metric which provided as attributes in [`MetricTypes`]\n            [fedbiomed.common.metrics.MetricTypes]. None, if it isn't declared yet.\n        **metric_args: A dictionary that contains arguments for metric function. Arguments\n            should be compatible with corresponding metrics in [`sklearn.metrics`][sklearn.metrics].\n    Returns:\n        Metric and  metric args as tuple\n    Raises:\n        FedbiomedExperimentError: Invalid type for `metric` argument\n    \"\"\"\nself._training_args['test_metric'] = metric\n# using **metric_args, we know `test_metric_args` is a Dict[str, Any]\nself._training_args['test_metric_args'] = metric_args\nif self._job is not None:\n# job setter function exists, use it\nself._job.training_args = self._training_args\nlogger.debug('Experimentation training_args updated for `job`')\nreturn metric, metric_args\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_test_on_global_updates","title":"<pre><code>set_test_on_global_updates(flag=True)\n</code></pre>","text":"<p>Setter for test_on_global_updates, that indicates whether to  perform a validation on the federated model updates on the node side before training model locally where aggregated model parameters are received.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>bool</code> <p>whether to perform model validation on global updates. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>Value of the flag <code>test_on_global_updates</code>.</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad flag type</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_test_on_global_updates(self, flag: bool = True) -&gt; bool:\n\"\"\"\n    Setter for test_on_global_updates, that indicates whether to  perform a validation on the federated model\n    updates on the node side before training model locally where aggregated model parameters are received.\n    Args:\n        flag (bool, optional): whether to perform model validation on global updates. Defaults to True.\n    Returns:\n        Value of the flag `test_on_global_updates`.\n    Raises:\n        FedbiomedExperimentError : bad flag type\n    \"\"\"\nself._training_args['test_on_global_updates'] = flag\nif self._job is not None:\n# job setter function exists, use it\nself._job.training_args = self._training_args\nlogger.debug('Experimentation training_args updated for `job`')\nreturn self._training_args['test_on_global_updates']\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_test_on_local_updates","title":"<pre><code>set_test_on_local_updates(flag=True)\n</code></pre>","text":"<p>Setter for <code>test_on_local_updates</code>, that indicates whether to perform a validation on the federated model on the node side where model parameters are updated locally after training in each node.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>bool</code> <p>whether to perform model validation on local updates. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>value of the flag <code>test_on_local_updates</code></p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad flag type</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_test_on_local_updates(self, flag: bool = True) -&gt; bool:\n\"\"\"\n    Setter for `test_on_local_updates`, that indicates whether to perform a validation on the federated model on the\n    node side where model parameters are updated locally after training in each node.\n    Args:\n        flag (bool, optional): whether to perform model validation on local updates. Defaults to True.\n    Returns:\n        value of the flag `test_on_local_updates`\n    Raises:\n        FedbiomedExperimentError: bad flag type\n    \"\"\"\nself._training_args['test_on_local_updates'] = flag\nif self._job is not None:\n# job setter function exists, use it\nself._job.training_args = self._training_args\nlogger.debug('Experimentation training_args updated for `job`')\nreturn self._training_args['test_on_local_updates']\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_test_ratio","title":"<pre><code>set_test_ratio(ratio)\n</code></pre>","text":"<p>Sets validation ratio for model validation.</p> <p>When setting test_ratio, nodes will allocate (1 - <code>test_ratio</code>) fraction of data for training and the remaining for validating model. This could be useful for validating the model, once every round, as well as controlling overfitting, doing early stopping, ....</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> <code>float</code> <p>validation ratio. Must be within interval [0,1].</p> required <p>Returns:</p> Type Description <code>float</code> <p>Validation ratio that is set</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad data type</p> <code>FedbiomedExperimentError</code> <p>ratio is not within interval [0, 1]</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_test_ratio(self, ratio: float) -&gt; float:\n\"\"\" Sets validation ratio for model validation.\n    When setting test_ratio, nodes will allocate (1 - `test_ratio`) fraction of data for training and the\n    remaining for validating model. This could be useful for validating the model, once every round, as well as\n    controlling overfitting, doing early stopping, ....\n    Args:\n        ratio: validation ratio. Must be within interval [0,1].\n    Returns:\n        Validation ratio that is set\n    Raises:\n        FedbiomedExperimentError: bad data type\n        FedbiomedExperimentError: ratio is not within interval [0, 1]\n    \"\"\"\nself._training_args['test_ratio'] = ratio\nif self._job is not None:\n# job setter function exists, use it\nself._job.training_args = self._training_args\nlogger.debug('Experimentation training_args updated for `job`')\nreturn ratio\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_training_args","title":"<pre><code>set_training_args(training_args, reset=True)\n</code></pre>","text":"<p>Sets <code>training_args</code> + verification on arguments type</p> <p>Parameters:</p> Name Type Description Default <code>training_args</code> <code>dict</code> <p>contains training arguments passed to the <code>training_routine</code> of the <code>fedbiomed.common.training_plans</code> when launching it: lr, epochs, batch_size...</p> required <code>reset</code> <code>bool</code> <p>whether to reset the training_args (if previous training_args has already been set), or to update them with training_args. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Training arguments</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad training_args type</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_training_args(self, training_args: dict, reset: bool = True) -&gt; dict:\n\"\"\" Sets `training_args` + verification on arguments type\n    Args:\n        training_args (dict): contains training arguments passed to the `training_routine` of the\n            [`fedbiomed.common.training_plans`][fedbiomed.common.training_plans] when launching it:\n            lr, epochs, batch_size...\n        reset (bool, optional): whether to reset the training_args (if previous training_args has already been\n            set), or to update them with training_args. Defaults to True.\n    Returns:\n        Training arguments\n    Raises:\n        FedbiomedExperimentError : bad training_args type\n    \"\"\"\nif isinstance(training_args, TrainingArgs):\nself._training_args = deepcopy(training_args)\nelse:\nself._training_args = TrainingArgs(training_args, only_required=False)\n# Propagate training arguments to job\nif self._job is not None:\nself._job.training_args = self._training_args\nreturn self._training_args.dict()\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_training_data","title":"<pre><code>set_training_data(training_data, from_tags=False)\n</code></pre>","text":"<p>Sets training data for federated training + verification on arguments type</p> <p>Parameters:</p> Name Type Description Default <code>training_data</code> <code>Union[FederatedDataSet, dict, None]</code> <ul> <li>If it is a FederatedDataSet object, use this value as training_data.</li> <li>else if it is a dict, create and use a FederatedDataSet object from the dict   and use this value as training_data. The dict should use node ids as keys,   values being list of dicts (each dict representing a dataset on a node).</li> <li>else if it is None (no training data provided)</li> <li>if <code>from_tags</code> is True and <code>tags</code> is not None, set training_data by     searching for datasets with a query to the nodes using <code>tags</code> and <code>nodes</code></li> <li>if <code>from_tags</code> is False or <code>tags</code> is None, set training_data to None (no training_data set yet,     experiment is not fully initialized and cannot be launched)</li> </ul> required <code>from_tags</code> <code>bool</code> <p>If True, query nodes for datasets when no <code>training_data</code> is provided. Not used when <code>training_data</code> is provided.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[FederatedDataSet, None]</code> <p>Nodes and dataset with meta-data</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad training_data type</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_training_data(\nself,\ntraining_data: Union[FederatedDataSet, dict, None],\nfrom_tags: bool = False) -&gt; \\\n        Union[FederatedDataSet, None]:\n\"\"\"Sets training data for federated training + verification on arguments type\n    Args:\n        training_data:\n            * If it is a FederatedDataSet object, use this value as training_data.\n            * else if it is a dict, create and use a FederatedDataSet object from the dict\n              and use this value as training_data. The dict should use node ids as keys,\n              values being list of dicts (each dict representing a dataset on a node).\n            * else if it is None (no training data provided)\n              - if `from_tags` is True and `tags` is not None, set training_data by\n                searching for datasets with a query to the nodes using `tags` and `nodes`\n              - if `from_tags` is False or `tags` is None, set training_data to None (no training_data set yet,\n                experiment is not fully initialized and cannot be launched)\n        from_tags: If True, query nodes for datasets when no `training_data` is provided.\n            Not used when `training_data` is provided.\n    Returns:\n        Nodes and dataset with meta-data\n    Raises:\n        FedbiomedExperimentError : bad training_data type\n    \"\"\"\n# we can trust _reqs _tags _nodes are existing and properly typed/formatted\nif not isinstance(from_tags, bool):\nmsg = ErrorNumbers.FB410.value + f' `from_tags` : got {type(from_tags)} but expected a boolean'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# case where no training data are passed\nif training_data is None and from_tags is True:\n# cannot search for training_data if tags not initialized;\n# nodes can be None (no filtering on nodes by default)\nif self._tags is not None:\ntraining_data = self._reqs.search(self._tags, self._nodes)\nif isinstance(training_data, FederatedDataSet):\nself._fds = training_data\nelif isinstance(training_data, dict):\nself._fds = FederatedDataSet(training_data)\nelif training_data is not None:\nmsg = ErrorNumbers.FB410.value + f' `training_data` has incorrect type: {type(training_data)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nelse:\nself._fds = None\nlogger.debug('Experiment not fully configured yet: no training data')\n# at this point, self._fds is either None or a FederatedDataSet object\nif self._node_selection_strategy is not None:\nlogger.debug('Training data changed, '\n'you may need to update `node_selection_strategy`')\nif self._job is not None:\nlogger.debug('Training data changed, you may need to update `job`')\nif self._aggregator is not None:\nlogger.debug('Training data changed, you may need to update `aggregator`')\nreturn self._fds\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_training_plan_class","title":"<pre><code>set_training_plan_class(training_plan_class)\n</code></pre>","text":"<p>Sets  <code>training_plan</code> + verification on arguments type</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_class</code> <code>Union[Type_TrainingPlan, str, None]</code> <p>name of the training plan class (<code>str</code>) or training plan class as one of <code>TrainingPlans</code> to use for training. For experiment to be properly and fully defined <code>training_plan_class</code> needs to be:     - a <code>str</code> when <code>training_plan_path</code> is not None (training plan class comes from a file).     - a <code>Type_TrainingPlan</code> when <code>training_plan_path</code> is None (training plan class passed     as argument).</p> required <p>Returns:</p> Type Description <code>Union[Type_TrainingPlan, str, None]</code> <p><code>training_plan_class</code> that is set for experiment</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad training_plan_class type</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_training_plan_class(self, training_plan_class: Union[Type_TrainingPlan, str, None]) -&gt; \\\n        Union[Type_TrainingPlan, str, None]:\n\"\"\"Sets  `training_plan` + verification on arguments type\n    Args:\n        training_plan_class: name of the training plan class (`str`) or training plan class as one\n            of [`TrainingPlans`] [fedbiomed.common.training_plans] to use for training.\n            For experiment to be properly and fully defined `training_plan_class` needs to be:\n                - a `str` when `training_plan_path` is not None (training plan class comes from a file).\n                - a `Type_TrainingPlan` when `training_plan_path` is None (training plan class passed\n                as argument).\n    Returns:\n        `training_plan_class` that is set for experiment\n    Raises:\n        FedbiomedExperimentError : bad training_plan_class type\n    \"\"\"\nif training_plan_class is None:\nself._training_plan_class = None\nself._training_plan_is_defined = False\nelif isinstance(training_plan_class, str):\nif str.isidentifier(training_plan_class):\n# correct python identifier\nself._training_plan_class = training_plan_class\n# training_plan_class_path may not be defined at this point\nself._training_plan_is_defined = isinstance(self._training_plan_path, str)\nelse:\n# bad identifier\nmsg = ErrorNumbers.FB410.value + f' `training_plan_class` : {training_plan_class} bad identifier'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nelif inspect.isclass(training_plan_class):\n# training_plan_class must be a subclass of a valid training plan\nif issubclass(training_plan_class, training_plans):\n# valid class\nself._training_plan_class = training_plan_class\n# training_plan_class_path may not be defined at this point\nself._training_plan_is_defined = self._training_plan_path is None\nelse:\n# bad class\nmsg = ErrorNumbers.FB410.value + f' `training_plan_class` : {training_plan_class} class'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nelse:\n# bad type\nmsg = ErrorNumbers.FB410.value + f' `training_plan_class` of type: {type(training_plan_class)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# self._training_plan_is_defined and self._training_plan_class always exist at this point\nif not self._training_plan_is_defined:\nlogger.debug(f'Experiment not fully configured yet: no valid training plan, '\nf'training_plan_class={self._training_plan_class} '\nf'training_plan_class_path={self._training_plan_path}')\nif self._job is not None:\nlogger.debug('Experimentation training_plan changed, you may need to update `job`')\nreturn self._training_plan_class\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.set_training_plan_path","title":"<pre><code>set_training_plan_path(training_plan_path)\n</code></pre>","text":"<p>Sets <code>training_plan_path</code> + verification on arguments type.</p> <p>Training plan path is the path where training plan class is saved as python script/module externally.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan_path</code> <code>Union[str, None]) </code> <p>path to a file containing  training plan code (<code>str</code>) or None (no file containing training plan code, <code>training_plan</code> needs to be a class matching one of <code>training_plans</code></p> required <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>The path that is set for retrieving module where training plan class is defined</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad training_plan_path type</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef set_training_plan_path(self, training_plan_path: Union[str, None]) -&gt; Union[str, None]:\n\"\"\"Sets `training_plan_path` + verification on arguments type.\n    Training plan path is the path where training plan class is saved as python script/module externally.\n    Args:\n        training_plan_path (Union[str, None]) : path to a file containing  training plan code (`str`) or None\n            (no file containing training plan code, `training_plan` needs to be a class matching one\n            of [`training_plans`][fedbiomed.common.training_plans]\n    Returns:\n        The path that is set for retrieving module where training plan class is defined\n    Raises:\n        FedbiomedExperimentError : bad training_plan_path type\n    \"\"\"\n# self._training_plan and self._training_plan_is_defined already exist when entering this function\nif training_plan_path is None:\nself._training_plan_path = None\n# .. so training plan is defined if it is a class (+ then, it has been tested as valid)\nself._training_plan_is_defined = inspect.isclass(self._training_plan_class)\nelif isinstance(training_plan_path, str):\nif sanitize_filepath(training_plan_path, platform='auto') == training_plan_path \\\n                and os.path.isfile(training_plan_path):\n# provided training plan path is a sane path to an existing file\nself._training_plan_path = training_plan_path\n# if providing a training plan path, we expect a training plan class name (not a class)\nself._training_plan_is_defined = isinstance(self._training_plan_class, str)\nelse:\n# bad filepath\nmsg = ErrorNumbers.FB410.value + \\\n                f' `training_plan_path` : {training_plan_path} is not a same path to an existing file'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nelse:\n# bad type\nmsg = ErrorNumbers.FB410.value + ' `training_plan_path` must be string, ' \\\n                                         f'but got type: {type(training_plan_path)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# self._training_plan_path is also defined at this point\nif not self._training_plan_is_defined:\nlogger.debug(f'Experiment not fully configured yet: no valid training plan, '\nf'training_plan={self._training_plan_class} training_plan_path={self._training_plan_path}')\nif self._job is not None:\nlogger.debug('Experimentation training_plan_path changed, you may need to update `job`')\nreturn self._training_plan_path\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.strategy","title":"<pre><code>strategy()\n</code></pre>","text":"<p>Retrieves the class that represents the node selection strategy.</p> <p>Please see also <code>set_strategy</code> to set or update node selection strategy.</p> <p>Returns:</p> Type Description <code>Union[Strategy, None]</code> <p>A class or object as an instance of <code>Strategy</code>. <code>None</code> if it is not declared yet. It means that node selection strategy will be <code>DefaultStrategy</code>.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef strategy(self) -&gt; Union[Strategy, None]:\n\"\"\"Retrieves the class that represents the node selection strategy.\n    Please see also [`set_strategy`][fedbiomed.researcher.experiment.Experiment.set_strategy] to set or update\n    node selection strategy.\n    Returns:\n        A class or object as an instance of [`Strategy`][fedbiomed.researcher.strategies.Strategy]. `None` if\n            it is not declared yet. It means that node selection strategy will be\n            [`DefaultStrategy`][fedbiomed.researcher.strategies.DefaultStrategy].\n    \"\"\"\nreturn self._node_selection_strategy\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.tags","title":"<pre><code>tags()\n</code></pre>","text":"<p>Retrieves the tags from the experiment object.</p> <p>Please see <code>set_tags</code> to set tags.</p> <p>Returns:</p> Type Description <code>Union[List[str], None]</code> <p>List of tags that has been set. <code>None</code> if it isn't declare yet.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef tags(self) -&gt; Union[List[str], None]:\n\"\"\"Retrieves the tags from the experiment object.\n    Please see [`set_tags`][fedbiomed.researcher.experiment.Experiment.set_tags] to set tags.\n    Returns:\n        List of tags that has been set. `None` if it isn't declare yet.\n    \"\"\"\nreturn self._tags\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.test_metric","title":"<pre><code>test_metric()\n</code></pre>","text":"<p>Retrieves the metric for validation routine.</p> <p>Please see also <code>set_test_metric</code>     to change/set <code>test_metric</code></p> <p>Returns:</p> Type Description <code>Union[MetricTypes, str, None]</code> <p>A class as an instance of <code>MetricTypes</code>. <code>str</code> for referring one of  metric which provided as attributes in <code>MetricTypes</code>. None, if it isn't declared yet.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef test_metric(self) -&gt; Union[MetricTypes, str, None]:\n\"\"\"Retrieves the metric for validation routine.\n    Please see also [`set_test_metric`][fedbiomed.researcher.experiment.Experiment.set_test_metric]\n        to change/set `test_metric`\n    Returns:\n        A class as an instance of [`MetricTypes`][fedbiomed.common.metrics.MetricTypes]. [`str`][str] for referring\n            one of  metric which provided as attributes in [`MetricTypes`][fedbiomed.common.metrics.MetricTypes].\n            None, if it isn't declared yet.\n    \"\"\"\nreturn self._training_args['test_metric']\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.test_metric_args","title":"<pre><code>test_metric_args()\n</code></pre>","text":"<p>Retrieves the metric argument for the metric function that is going to be used.</p> <p>Please see also <code>set_test_metric</code> to change/set <code>test_metric</code> and get more information on the arguments can be used.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary that contains arguments for metric function. See <code>set_test_metric</code></p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef test_metric_args(self) -&gt; Dict[str, Any]:\n\"\"\"Retrieves the metric argument for the metric function that is going to be used.\n    Please see also [`set_test_metric`][fedbiomed.researcher.experiment.Experiment.set_test_metric] to change/set\n    `test_metric` and get more information on the arguments can be used.\n    Returns:\n        A dictionary that contains arguments for metric function. See [`set_test_metric`]\n            [fedbiomed.researcher.experiment.Experiment.set_test_metric]\n    \"\"\"\nreturn self._training_args['test_metric_args']\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.test_on_global_updates","title":"<pre><code>test_on_global_updates()\n</code></pre>","text":"<p>Retrieves the status of whether validation will be performed on globally updated (aggregated) parameters by the nodes at the beginning of each round.</p> <p>Please see also <code>set_test_on_global_updates</code>.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True, if validation is active on globally updated (aggregated) parameters. False for vice versa.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef test_on_global_updates(self) -&gt; bool:\n\"\"\" Retrieves the status of whether validation will be performed on globally updated (aggregated)\n    parameters by the nodes at the beginning of each round.\n    Please see also [`set_test_on_global_updates`]\n    [fedbiomed.researcher.experiment.Experiment.set_test_on_global_updates].\n    Returns:\n        True, if validation is active on globally updated (aggregated) parameters. False for vice versa.\n    \"\"\"\nreturn self._training_args['test_on_global_updates']\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.test_on_local_updates","title":"<pre><code>test_on_local_updates()\n</code></pre>","text":"<p>Retrieves the status of whether validation will be performed on locally updated parameters by the nodes at the end of each round.</p> <p>Please see also     <code>set_test_on_local_updates</code>.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True, if validation is active on locally updated parameters. False for vice versa.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef test_on_local_updates(self) -&gt; bool:\n\"\"\"Retrieves the status of whether validation will be performed on locally updated parameters by\n    the nodes at the end of each round.\n    Please see also\n        [`set_test_on_local_updates`][fedbiomed.researcher.experiment.Experiment.set_test_on_local_updates].\n    Returns:\n        True, if validation is active on locally updated parameters. False for vice versa.\n    \"\"\"\nreturn self._training_args['test_on_local_updates']\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.test_ratio","title":"<pre><code>test_ratio()\n</code></pre>","text":"<p>Retrieves the ratio for validation partition of entire dataset.</p> <p>Please see also <code>set_test_ratio</code> to     change/set <code>test_ratio</code></p> <p>Returns:</p> Type Description <code>float</code> <p>The ratio for validation part, <code>1 - test_ratio</code> is ratio for training set.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef test_ratio(self) -&gt; float:\n\"\"\"Retrieves the ratio for validation partition of entire dataset.\n    Please see also [`set_test_ratio`][fedbiomed.researcher.experiment.Experiment.set_test_ratio] to\n        change/set `test_ratio`\n    Returns:\n        The ratio for validation part, `1 - test_ratio` is ratio for training set.\n    \"\"\"\nreturn self._training_args['test_ratio']\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.training_args","title":"<pre><code>training_args()\n</code></pre>","text":"<p>Retrieves training arguments.</p> <p>Please see also <code>set_training_args</code></p> <p>Returns:</p> Type Description <code>dict</code> <p>The arguments that are going to be passed to <code>training_routine</code> of <code>training_plans</code> classes to perfom training on the node side. An example training routine: <code>TorchTrainingPlan.training_routine</code></p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef training_args(self) -&gt; dict:\n\"\"\"Retrieves training arguments.\n    Please see also [`set_training_args`][fedbiomed.researcher.experiment.Experiment.set_training_args]\n    Returns:\n        The arguments that are going to be passed to `training_routine` of [`training_plans`]\n            [fedbiomed.common.training_plans] classes to perfom training on the node side.\n            An example training routine: [`TorchTrainingPlan.training_routine`]\n            [fedbiomed.common.training_plans.TorchTrainingPlan.training_routine]\n    \"\"\"\nreturn self._training_args.dict()\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.training_data","title":"<pre><code>training_data()\n</code></pre>","text":"<p>Retrieves the training data which is an instance of <code>FederatedDataset</code></p> <p>Please see <code>set_training_data</code> to set or update training data.</p> <p>Returns:</p> Type Description <code>Union[FederatedDataSet, None]</code> <p>Object that contains meta-data for the datasets of each node. <code>None</code> if it isn't set yet.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef training_data(self) -&gt; Union[FederatedDataSet, None]:\n\"\"\"Retrieves the training data which is an instance of [`FederatedDataset`]\n    [fedbiomed.researcher.datasets.FederatedDataSet]\n    Please see [`set_training_data`][fedbiomed.researcher.experiment.Experiment.set_training_data] to set or\n    update training data.\n    Returns:\n        Object that contains meta-data for the datasets of each node. `None` if it isn't set yet.\n    \"\"\"\nreturn self._fds\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.training_plan","title":"<pre><code>training_plan()\n</code></pre>","text":"<p>Retrieves training plan instance that has been built and send the nodes through HTTP restfull service for each round of training.</p> <p>Loading aggregated parameters</p> <p>After retrieving the training plan instance aggregated parameters should be loaded. Example: <pre><code>training_plan = exp.training_plan()\ntraining_plan.model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n</code></pre></p> <p>Returns:</p> Type Description <code>Union[TrainingPlan, None]</code> <p>Training plan object which is an instance one of training_plans.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef training_plan(self) -&gt; Union[TrainingPlan, None]:\n\"\"\" Retrieves training plan instance that has been built and send the nodes through HTTP restfull service\n    for each round of training.\n    !!! info \"Loading aggregated parameters\"\n        After retrieving the training plan instance aggregated parameters should be loaded.\n        Example:\n        ```python\n        training_plan = exp.training_plan()\n        training_plan.model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n        ```\n    Returns:\n        Training plan object which is an instance one of [training_plans][fedbiomed.common.training_plans].\n    \"\"\"\n# at this point `job` is defined but may be None\nif self._job is None:\nlogger.error('No `job` defined for experiment, cannot get `training_plan`')\nreturn None\nelse:\nreturn self._job.training_plan\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.training_plan_approve","title":"<pre><code>training_plan_approve(training_plan, description='no description provided', nodes=[], timeout=5)\n</code></pre>","text":"<p>Send a training plan and a ApprovalRequest message to node(s).</p> <p>This is a simple redirect to the Requests.training_plan_approve() method.</p> <p>If a list of node id(s) is provided, the message will be individually sent to all nodes of the list. If the node id(s) list is None (default), the message is broadcast to all nodes.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan</code> <code>BaseTrainingPlan</code> <p>the training plan to upload and send to the nodes for approval.    It can be:    - a path_name (str)    - a training_plan (class)    - an instance of a training plan</p> required <code>nodes</code> <code>list</code> <p>list of nodes (specified by their UUID)</p> <code>[]</code> <code>description</code> <code>str</code> <p>Description for training plan approve request</p> <code>'no description provided'</code> <code>timeout</code> <code>int</code> <p>maximum waiting time for the answers</p> <code>5</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>a dictionary of pairs (node_id: status), where status indicates to the researcher</p> <code>dict</code> <p>that the training plan has been correctly downloaded on the node side.</p> <code>Warning</code> <code>dict</code> <p>status does not mean that the training plan is approved, only that it has been added</p> <code>dict</code> <p>to the \"approval queue\" on the node side.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef training_plan_approve(self,\ntraining_plan: 'BaseTrainingPlan',\ndescription: str = \"no description provided\",\nnodes: list = [],\ntimeout: int = 5) -&gt; dict:\n\"\"\"Send a training plan and a ApprovalRequest message to node(s).\n    This is a simple redirect to the Requests.training_plan_approve() method.\n    If a list of node id(s) is provided, the message will be individually sent\n    to all nodes of the list.\n    If the node id(s) list is None (default), the message is broadcast to all nodes.\n    Args:\n        training_plan: the training plan to upload and send to the nodes for approval.\n               It can be:\n               - a path_name (str)\n               - a training_plan (class)\n               - an instance of a training plan\n        nodes: list of nodes (specified by their UUID)\n        description: Description for training plan approve request\n        timeout: maximum waiting time for the answers\n    Returns:\n        a dictionary of pairs (node_id: status), where status indicates to the researcher\n        that the training plan has been correctly downloaded on the node side.\n        Warning: status does not mean that the training plan is approved, only that it has been added\n        to the \"approval queue\" on the node side.\n    \"\"\"\nreturn self._reqs.training_plan_approve(training_plan,\ndescription,\nnodes,\ntimeout)\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.training_plan_class","title":"<pre><code>training_plan_class()\n</code></pre>","text":"<p>Retrieves the training plan (training plan class) that is created for training.</p> <p>Please see also <code>set_training_plan_class</code>.</p> <p>Returns:</p> Type Description <code>Union[Type_TrainingPlan, str, None]</code> <p>Training plan class as one of <code>Type_TrainingPlan</code>. None if it isn't declared yet. <code>str</code> if <code>training_plan_path</code>that represents training plan class created externally is provided.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef training_plan_class(self) -&gt; Union[Type_TrainingPlan, str, None]:\n\"\"\"Retrieves the training plan (training plan class) that is created for training.\n    Please see also [`set_training_plan_class`][fedbiomed.researcher.experiment.Experiment.set_training_plan_class].\n    Returns:\n        Training plan class as one of [`Type_TrainingPlan`][fedbiomed.researcher.experiment.Type_TrainingPlan]. None\n            if it isn't declared yet. [`str`][str] if [`training_plan_path`]\n            [fedbiomed.researcher.experiment.Experiment.training_plan_path]that represents training plan class\n            created externally is provided.\n    \"\"\"\nreturn self._training_plan_class\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.training_plan_file","title":"<pre><code>training_plan_file(display=True)\n</code></pre>","text":"<p>This method displays saved final training plan for the experiment     that will be sent to the nodes for training.</p> <p>Parameters:</p> Name Type Description Default <code>display</code> <code>bool</code> <p>If <code>True</code>, prints content of the training plan file. Default is <code>True</code></p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to training plan file</p> <p>Raises:</p> Type Description <code>FedbiomedExperimentError</code> <p>bad argument type, or cannot read training plan file content</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef training_plan_file(self, display: bool = True) -&gt; str:\n\"\"\" This method displays saved final training plan for the experiment\n        that will be sent to the nodes for training.\n    Args:\n        display: If `True`, prints content of the training plan file. Default is `True`\n    Returns:\n        Path to training plan file\n    Raises:\n        FedbiomedExperimentError: bad argument type, or cannot read training plan file content\n    \"\"\"\nif not isinstance(display, bool):\n# bad type\nmsg = ErrorNumbers.FB410.value + \\\n            f', in method `training_plan_file` param `display` : type {type(display)}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\n# at this point, self._job exists (initialized in constructor)\nif self._job is None:\n# cannot check training plan file if job not defined\nmsg = ErrorNumbers.FB412.value + \\\n            ', in method `training_plan_file` : no `job` defined for experiment'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\ntraining_plan_file = self._job.training_plan_file\n# Display content so researcher can copy\ntry:\nif display:\nwith open(training_plan_file) as file:\ncontent = file.read()\nfile.close()\nprint(content)\nexcept OSError as e:\n# cannot read training plan file content\nmsg = ErrorNumbers.FB412.value + \\\n            f', in method `training_plan_file` : error when reading training plan file - {e}'\nlogger.critical(msg)\nraise FedbiomedExperimentError(msg)\nreturn self._job.training_plan_file\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.training_plan_path","title":"<pre><code>training_plan_path()\n</code></pre>","text":"<p>Retrieves training plan path where training plan class is saved as python script externally.</p> <p>Please see also <code>set_training_plan_path</code>.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>Path to python script (<code>.py</code>) where training plan class (training plan) is created. None if it isn't declared yet.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef training_plan_path(self) -&gt; Union[str, None]:\n\"\"\"Retrieves training plan path where training plan class is saved as python script externally.\n    Please see also [`set_training_plan_path`][fedbiomed.researcher.experiment.Experiment.set_training_plan_path].\n    Returns:\n        Path to python script (`.py`) where training plan class (training plan) is created. None if it isn't\n            declared yet.\n    \"\"\"\nreturn self._training_plan_path\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.Experiment.training_replies","title":"<pre><code>training_replies()\n</code></pre>","text":"<p>Retrieves training replies of each round of training.</p> <p>Training replies contains timing statistics and the files parth/URLs that has been received after each round.</p> <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>Dictionary of training replies keys stand for each round of training. None, if Job isn't declared or empty dict if there is no training round has been run.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>@exp_exceptions\ndef training_replies(self) -&gt; Union[dict, None]:\n\"\"\"Retrieves training replies of each round of training.\n    Training replies contains timing statistics and the files parth/URLs that has been received after each round.\n    Returns:\n        Dictionary of training replies keys stand for each round of training. None, if\n            [Job][fedbiomed.researcher.job] isn't declared or empty dict if there is no training round has been run.\n    \"\"\"\n# at this point `job` is defined but may be None\nif self._job is None:\nlogger.error('No `job` defined for experiment, cannot get `training_replies`')\nreturn None\nelse:\nreturn self._job.training_replies\n</code></pre>"},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment-functions","title":"Functions","text":""},{"location":"developer/api/researcher/experiment/#fedbiomed.researcher.experiment.exp_exceptions","title":"<pre><code>exp_exceptions(function)\n</code></pre>","text":"<p>Decorator for handling all exceptions in the Experiment class() : pretty print a message for the user, quit Experiment.</p> Source code in <code>fedbiomed/researcher/experiment.py</code> <pre><code>def exp_exceptions(function):\n\"\"\"\n    Decorator for handling all exceptions in the Experiment class() :\n    pretty print a message for the user, quit Experiment.\n    \"\"\"\n# wrap the original function catching the exceptions\n@functools.wraps(function)\ndef payload(*args, **kwargs):\ncode = 0\ntry:\nret = function(*args, **kwargs)\nexcept FedbiomedSilentTerminationError:\n# handle the case of nested calls will exception decorator\nraise\nexcept SystemExit as e:\n# handle the sys.exit() from other clauses\nsys.exit(e)\nexcept KeyboardInterrupt:\ncode = 1\nprint(\n'\\n--------------------',\n'Fed-BioMed researcher stopped due to keyboard interrupt',\n'--------------------',\nsep=os.linesep)\nlogger.critical('Fed-BioMed researcher stopped due to keyboard interrupt')\nexcept FedbiomedError as e:\ncode = 1\nprint(\n'\\n--------------------',\nf'Fed-BioMed researcher stopped due to exception:\\n{str(e)}',\n'--------------------',\nsep=os.linesep)\n# redundant, should be already logged when raising exception\n# logger.critical(f'Fed-BioMed researcher stopped due to exception:\\n{str(e)}')\nexcept BaseException as e:\ncode = 3\nprint(\n'\\n--------------------',\nf'Fed-BioMed researcher stopped due to unknown error:\\n{str(e)}',\n'More details in the backtrace extract below',\n'--------------------',\nsep=os.linesep)\n# at most 5 backtrace entries to avoid too long output\ntraceback.print_exc(limit=5, file=sys.stdout)\nprint('--------------------')\nlogger.critical(f'Fed-BioMed stopped due to unknown error:\\n{str(e)}')\nif code != 0:\nif is_ipython():\n# raise a silent specific exception, don't exit the interactive kernel\nraise FedbiomedSilentTerminationError\nelse:\n# exit the process\nsys.exit(code)\nreturn ret\nreturn payload\n</code></pre>"},{"location":"developer/api/researcher/filetools/","title":"Filetools","text":""},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools","title":"fedbiomed.researcher.filetools","text":"Module: <code>fedbiomed.researcher.filetools</code> <p>Functions for managing Job/Experiment files.</p>"},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools-functions","title":"Functions","text":""},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools.choose_bkpt_file","title":"<pre><code>choose_bkpt_file(experimentation_folder, round_=0)\n</code></pre>","text":"<p>It creates a breakpoint folder and chooses a breakpoint file name for each round.</p> <p>Parameters:</p> Name Type Description Default <code>experimentation_folder</code> <code>str</code> <p>indicates the experimentation folder name. This should just contain the name of the folder not a full path.</p> required <code>round_</code> <code>int</code> <p>the current number of already run rounds minus one. Starts from 0. Defaults to 0.</p> <code>0</code> <p>Raises:</p> Type Description <code>PermissionError</code> <p>cannot create experimentation folder</p> <code>OSError</code> <p>cannot create experimentation folder</p> <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>A tuple that contains following instacens breakpoint_folder_path: name of the created folder that will contain all data for the current round breakpoint_file: name of the file that will contain the state of an experiment.</p> Source code in <code>fedbiomed/researcher/filetools.py</code> <pre><code>def choose_bkpt_file(experimentation_folder: str, round_: int = 0) -&gt; Tuple[str, str]:\n\"\"\"\n    It creates a breakpoint folder and chooses a breakpoint file name for each round.\n    Args:\n        experimentation_folder (str): indicates the experimentation folder name. This should just contain the name\n            of the folder not a full path.\n        round_: the current number of already run rounds minus one. Starts from 0. Defaults to 0.\n    Raises:\n        PermissionError: cannot create experimentation folder\n        OSError: cannot create experimentation folder\n    Returns:\n        A tuple that contains following instacens\n            breakpoint_folder_path: name of the created folder that will contain all data for the current round\n            breakpoint_file: name of the file that will contain the state of an experiment.\n    \"\"\"\nbreakpoint_folder = \"breakpoint_\" + str(\"{:04d}\".format(round_))\nbreakpoint_folder_path = os.path.join(environ['EXPERIMENTS_DIR'],\nexperimentation_folder,\nbreakpoint_folder)\ntry:\nos.makedirs(breakpoint_folder_path, exist_ok=True)\nexcept (PermissionError, OSError) as err:\nlogger.error(\"Can not save breakpoint folder at \" +\nf\"{breakpoint_folder_path} due to some error {err}\")\nraise\nbreakpoint_file = breakpoint_folder + \".json\"\nreturn breakpoint_folder_path, breakpoint_file\n</code></pre>"},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools.copy_file","title":"<pre><code>copy_file(filepath, breakpoint_path)\n</code></pre>","text":"Source code in <code>fedbiomed/researcher/filetools.py</code> <pre><code>def copy_file(filepath: str, breakpoint_path: str) -&gt; str:\nfilename = os.path.dirname(filepath)\nfile_copy_path = os.path.join( breakpoint_path, filename)\nshutil.copy2(filepath, file_copy_path )\nreturn file_copy_path\n</code></pre>"},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools.create_exp_folder","title":"<pre><code>create_exp_folder(experimentation_folder=None)\n</code></pre>","text":"<p>Creates a folder for the current experiment (ie the current run of the model). Experiment files to keep are stored here: model file, all versions of node parameters, all versions of aggregated parameters, breakpoints. The created folder is a subdirectory of environ[EXPERIMENTS_DIR]</p> <p>Parameters:</p> Name Type Description Default <code>experimentation_folder</code> <code>str</code> <p>optionaly provide an experimentation folder name. This should just contain the name of the folder not a path. default; if no folder name is given, generate a <code>Experiment_x</code> name where <code>x-1</code> is the number of experiments already run (<code>x</code>=0 for the first experiment)</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Experimentation folder</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>cannot create experimentation folder</p> <code>OSError</code> <p>cannot create experimentation folder</p> <code>ValueError</code> <p>bad <code>experimentation_folder</code> argument</p> Source code in <code>fedbiomed/researcher/filetools.py</code> <pre><code>def create_exp_folder(experimentation_folder: str = None) -&gt; str:\n\"\"\" Creates a folder for the current experiment (ie the current run of the model). Experiment files to keep\n    are stored here: model file, all versions of node parameters, all versions of aggregated parameters, breakpoints.\n    The created folder is a subdirectory of environ[EXPERIMENTS_DIR]\n    Args:\n        experimentation_folder (str, optional): optionaly provide an experimentation\n            folder name. This should just contain the name of the folder not a path.\n            default; if no folder name is given, generate a `Experiment_x` name where `x-1`\n            is the number of experiments already run (`x`=0 for the first experiment)\n    Returns:\n        Experimentation folder\n    Raises:\n        PermissionError: cannot create experimentation folder\n        OSError: cannot create experimentation folder\n        ValueError: bad `experimentation_folder` argument\n    \"\"\"\nif not os.path.isdir(environ['EXPERIMENTS_DIR']):\ntry:\nos.makedirs(environ['EXPERIMENTS_DIR'], exist_ok=True)\nexcept (PermissionError, OSError) as err:\nlogger.error(\"Can not save experiment files because \" +\nf\"{environ['EXPERIMENTS_DIR']} folder could not be created due to {err}\")\nraise\n# if no name is given for the experiment folder we choose one\nif not experimentation_folder:\n# FIXME: improve method robustness (here nb of exp equals nb of files\n# in directory)\nall_files = os.listdir(environ['EXPERIMENTS_DIR'])\nexperimentation_folder = \"Experiment_\" + str(\"{:04d}\".format(len(all_files)))\nelse:\nif os.path.basename(experimentation_folder) != experimentation_folder:\n# experimentation folder cannot be a path\nraise ValueError(f\"Bad experimentation folder {experimentation_folder} - \" +\n\"it cannot be a path\")\ntry:\nos.makedirs(os.path.join(environ['EXPERIMENTS_DIR'], experimentation_folder),\nexist_ok=True)\nexcept (PermissionError, OSError) as err:\nlogger.error(\"Can not save experiment files because \" +\nf\"{environ['EXPERIMENTS_DIR']}/{experimentation_folder} \" +\nf\"folder could not be created due to {err}\")\nraise\nreturn experimentation_folder\n</code></pre>"},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools.create_unique_file_link","title":"<pre><code>create_unique_file_link(breakpoint_folder_path, file_path)\n</code></pre>","text":"<p>Create a symbolic link in <code>breakpoint_folder_path</code> with a non-existing name derived from basename of <code>file_path</code>. The symbolic link points to the real file targeted by <code>file_path</code></p> <p>Parameters:</p> Name Type Description Default <code>breakpoint_folder_path</code> <code>str</code> <p>directory for the source link</p> required <code>file_path</code> <code>str</code> <p>path to the target of the link</p> required <p>Returns:</p> Type Description <code>str</code> <p>Path of the created link</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>bad name for link source or destination</p> Source code in <code>fedbiomed/researcher/filetools.py</code> <pre><code>def create_unique_file_link(breakpoint_folder_path: str, file_path: str) -&gt; str:\n\"\"\"\n    Create a symbolic link in `breakpoint_folder_path` with a non-existing name derived from basename of\n    `file_path`. The symbolic link points to the real file targeted by `file_path`\n    Args:\n        breakpoint_folder_path: directory for the source link\n        file_path: path to the target of the link\n    Returns:\n        Path of the created link\n    Raises:\n        ValueError: bad name for link source or destination\n    \"\"\"\ntry:\nreal_file_path = os.path.realpath(file_path)\nreal_bkpt_folder_path = os.path.realpath(breakpoint_folder_path)\nif not os.path.isdir(real_bkpt_folder_path) \\\n                or not os.path.isdir(os.path.dirname(real_file_path)):\nraise ValueError\n# - use relative path for link target for portability\n# - link to the real file, not to a link-to-the-file\nlink_target = os.path.relpath(real_file_path, start=real_bkpt_folder_path)\nexcept ValueError as err:\nmess = 'Saving breakpoint error, ' + \\\n            f'cannot get relative path to {file_path} from {breakpoint_folder_path}, ' + \\\n            f'due to error {err}'\nlogger.error(mess)\nraise\n# heuristic : assume limited set of characters in filename postfix\nre_src_prefix = re.search(\"(.+)\\.[a-zA-Z]+$\",\nos.path.basename(file_path))\nre_src_postfix = re.search(\".+(\\.[a-zA-Z]+)$\",\nos.path.basename(file_path))\nif not re_src_prefix or not re_src_postfix:\nerror_message = f'Saving breakpoint error, bad filename {file_path} gives ' + \\\n            f'prefix {re_src_prefix} and postfix {re_src_postfix}'\nlogger.error(error_message)\nraise ValueError(error_message)\nlink_src_prefix = re_src_prefix.group(1)\nlink_src_postfix = re_src_postfix.group(1)\nreturn create_unique_link(breakpoint_folder_path,\nlink_src_prefix, link_src_postfix,\nlink_target)\n</code></pre>"},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools.create_unique_link","title":"<pre><code>create_unique_link(breakpoint_folder_path, link_src_prefix, link_src_postfix, link_target_path)\n</code></pre>","text":"<p>Find a non-existing name in <code>breakpoint_folder_path</code> and create a symbolic link to a given target name.</p> <p>Parameters:</p> Name Type Description Default <code>breakpoint_folder_path</code> <code>str</code> <p>directory for the source link</p> required <code>link_src_prefix</code> <code>str</code> <p>beginning of the name for the source link (before unique id)</p> required <code>link_src_postfix</code> <code>str</code> <p>end of the name for the source link (after unique id)</p> required <code>link_target_path</code> <code>str</code> <p>target for the symbolic link</p> required <p>Returns:</p> Type Description <code>str</code> <p>Path of the created link</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>cannot create symlink</p> <code>OSError</code> <p>cannot create symlink</p> <code>FileExistsError</code> <p>cannot create symlink</p> <code>FileNotFoundError</code> <p>non-existent directory</p> Source code in <code>fedbiomed/researcher/filetools.py</code> <pre><code>def create_unique_link(breakpoint_folder_path: str,\nlink_src_prefix: str,\nlink_src_postfix: str,\nlink_target_path: str) -&gt; str:\n\"\"\" Find a non-existing name in `breakpoint_folder_path` and create a symbolic link to a given target name.\n    Args:\n        breakpoint_folder_path: directory for the source link\n        link_src_prefix: beginning of the name for the source link (before unique id)\n        link_src_postfix: end of the name for the source link (after unique id)\n        link_target_path: target for the symbolic link\n    Returns:\n        Path of the created link\n    Raises:\n        PermissionError: cannot create symlink\n        OSError: cannot create symlink\n        FileExistsError: cannot create symlink\n        FileNotFoundError : non-existent directory\n    \"\"\"\nstub = 0\nlink_src_path = os.path.join(breakpoint_folder_path,\nlink_src_prefix + link_src_postfix)\n# Need to ensure unique name for link (e.g. when replaying from non-last breakpoint)\nwhile os.path.exists(link_src_path) or os.path.islink(link_src_path):\nstub += 1\nlink_src_path = os.path.join(breakpoint_folder_path,\nlink_src_prefix + '_' + str(\"{:02}\".format(stub)) + link_src_postfix)\ntry:\nos.symlink(link_target_path, link_src_path)\nexcept(FileExistsError, PermissionError, OSError, FileNotFoundError) as err:\nlogger.error(f\"Can not create link to experiment file {link_target_path} \" +\nf\"from {link_src_path} due to error {err}\")\nraise\nreturn link_src_path\n</code></pre>"},{"location":"developer/api/researcher/filetools/#fedbiomed.researcher.filetools.find_breakpoint_path","title":"<pre><code>find_breakpoint_path(breakpoint_folder_path=None)\n</code></pre>","text":"<p>Finds breakpoint folder path and file, depending on if user specifies a specific breakpoint path (unchanged in this case) or not (try to guess the latest breakpoint).</p> <p>Parameters:</p> Name Type Description Default <code>breakpoint_folder_path</code> <code>str</code> <p>path towards breakpoint. If None, (default), consider the latest breakpoint saved on default path (provided at least one breakpoint exists). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>With length of two that represents respectively:</p> <ul> <li>path to breakpoint folder (unchanged if specified by user)</li> <li>breakpoint file.</li> </ul> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>triggered either if breakpoint cannot be found, folder is empty or file cannot be parsed</p> Source code in <code>fedbiomed/researcher/filetools.py</code> <pre><code>def find_breakpoint_path(breakpoint_folder_path: str = None) -&gt; Tuple[str, str]:\n\"\"\" Finds breakpoint folder path and file, depending on if user specifies a specific breakpoint path (unchanged in\n    this case) or not (try to guess the latest breakpoint).\n    Args:\n        breakpoint_folder_path: path towards breakpoint. If None, (default), consider the latest breakpoint saved on\n            default path (provided at least one breakpoint exists). Defaults to None.\n    Returns:\n        With length of two that represents respectively:\n            - path to breakpoint folder (unchanged if specified by user)\n            - breakpoint file.\n    Raises:\n        FileNotFoundError: triggered either if breakpoint cannot be found, folder is empty or file cannot be parsed\n    \"\"\"\n# First, let's test if folder is a real folder path\nif breakpoint_folder_path is None:\ntry:\n# retrieve latest experiment\n# for error message\nlatest_exp_folder = environ['EXPERIMENTS_DIR'] + \"/NO_FOLDER_FOUND\"\n# content of breakpoint folder\nexperiment_folders = os.listdir(environ['EXPERIMENTS_DIR'])\nlatest_exp_folder = _get_latest_file(\nenviron['EXPERIMENTS_DIR'],\nexperiment_folders,\nonly_folder=True)\nlatest_exp_folder = os.path.join(environ['EXPERIMENTS_DIR'],\nlatest_exp_folder)\nbkpt_folders = os.listdir(latest_exp_folder)\nbreakpoint_folder_path = _get_latest_file(\nlatest_exp_folder,\nbkpt_folders,\nonly_folder=True)\nbreakpoint_folder_path = os.path.join(latest_exp_folder,\nbreakpoint_folder_path)\nexcept FileNotFoundError as err:\nlogger.error(\"Cannot find latest breakpoint in \" + latest_exp_folder +\n\" Are you sure at least one breakpoint is saved there ? \" +\n\" - Error: \" + str(err))\nraise\nelse:\nif not os.path.isdir(breakpoint_folder_path):\nraise FileNotFoundError(\nf\"Breakpoint folder {breakpoint_folder_path} is not a directory\")\n# check if folder is a valid breakpoint\n#\n# verify the validity of the breakpoint content\n# TODO: be more robust\nall_breakpoint_materials = os.listdir(breakpoint_folder_path)\nif len(all_breakpoint_materials) == 0:\nraise FileNotFoundError(f'Breakpoint folder {breakpoint_folder_path} is empty !')\nstate_file = None\nfor breakpoint_material in all_breakpoint_materials:\n# look for the json file containing experiment state\n# (it should be named `breakpoint_xx.json`)\njson_match = re.fullmatch(r'breakpoint_\\d*\\.json',\nbreakpoint_material)\n# there should be at most one - TODO: verify\nif json_match is not None:\nlogger.debug(f\"found json file containing states at\\\n{breakpoint_material}\")\nstate_file = breakpoint_material\nif state_file is None:\nmessage = \"Cannot find JSON file containing \" + \\\n            f\"model state at {breakpoint_folder_path}. Aborting\"\nlogger.error(message)\nraise FileNotFoundError(message)\nreturn breakpoint_folder_path, state_file\n</code></pre>"},{"location":"developer/api/researcher/job/","title":"Job","text":""},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job","title":"fedbiomed.researcher.job","text":"Module: <code>fedbiomed.researcher.job</code> <p>Manage the training part of the experiment.</p>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job-classes","title":"Classes","text":""},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job","title":"Job","text":"CLASS  <pre><code>Job(reqs=None, nodes=None, training_plan_class=None, training_plan_path=None, training_args=None, model_args=None, data=None, keep_files_dir=None)\n</code></pre> <p>Represents the entity that manage the training part at  the nodes level</p> <p>Starts a message queue, loads python model file created by researcher (through <code>training_plans</code>) and saves the loaded model in a temporary file (under the filename '/my_model_.py'). <p>Parameters:</p> Name Type Description Default <code>reqs</code> <code>Requests</code> <p>Researcher's requests assigned to nodes. Defaults to None.</p> <code>None</code> <code>nodes</code> <code>dict</code> <p>A dict of node_id containing the nodes used for training</p> <code>None</code> <code>training_plan_class</code> <code>Union[Type[Callable], str]</code> <p>instance or class of the TrainingPlan.</p> <code>None</code> <code>training_plan_path</code> <code>str</code> <p>Path to file containing model class code</p> <code>None</code> <code>training_args</code> <code>TrainingArgs</code> <p>Contains training parameters; lr, epochs, batch_size.</p> <code>None</code> <code>model_args</code> <code>dict</code> <p>Contains output and input feature dimension</p> <code>None</code> <code>data</code> <code>FederatedDataSet</code> <p>Federated datasets</p> <code>None</code> <code>keep_files_dir</code> <code>str</code> <p>Directory for storing files created by the job that we want to keep beyond the execution of the job. Defaults to None, files are not kept after the end of the job.</p> <code>None</code> <p>Raises:</p> Type Description <code>NameError</code> <p>If model is not defined or if the class can not to be inspected</p> Source code in <code>fedbiomed/researcher/job.py</code> <pre><code>def __init__(self,\nreqs: Requests = None,\nnodes: dict = None,\ntraining_plan_class: Union[Type[Callable], str] = None,\ntraining_plan_path: str = None,\ntraining_args: TrainingArgs = None,\nmodel_args: dict = None,\ndata: FederatedDataSet = None,\nkeep_files_dir: str = None):\n\"\"\" Constructor of the class\n    Args:\n        reqs: Researcher's requests assigned to nodes. Defaults to None.\n        nodes: A dict of node_id containing the nodes used for training\n        training_plan_class: instance or class of the TrainingPlan.\n        training_plan_path: Path to file containing model class code\n        training_args: Contains training parameters; lr, epochs, batch_size.\n        model_args: Contains output and input feature dimension\n        data: Federated datasets\n        keep_files_dir: Directory for storing files created by the job that we want to keep beyond the execution\n            of the job. Defaults to None, files are not kept after the end of the job.\n    Raises:\n        NameError: If model is not defined or if the class can not to be inspected\n    \"\"\"\nself._id = str(uuid.uuid4())  # creating a unique job id\nself._researcher_id = environ['RESEARCHER_ID']\nself._repository_args = {}\nself._training_args = training_args\nself._model_args = model_args\nself._nodes = nodes\nself._training_replies = {}  # will contain all node replies for every round\nself._model_file = None  # path to local file containing model code\nself._model_params_file = \"\"  # path to local file containing current version of aggregated params\nself._training_plan_class = training_plan_class\n# self._training_plan = None  # declared below, as a TrainingPlan instance\nself._aggregator_args = None\nif keep_files_dir:\nself._keep_files_dir = keep_files_dir\nelse:\nself._keep_files_dir = tempfile.mkdtemp(prefix=environ['TMP_DIR'])\natexit.register(lambda: shutil.rmtree(self._keep_files_dir))  # remove directory\n# when script ends running (replace\n# `with tempfile.TemporaryDirectory(dir=environ['TMP_DIR']) as self._keep_files_dir: `)\nif reqs is None:\nself._reqs = Requests()\nelse:\nself._reqs = reqs\nself.last_msg = None\nself._data = data\n# Check dataset quality\nif self._data is not None:\nself.check_data_quality()\n# Model is mandatory\nif self._training_plan_class is None:\nmess = \"Missing training plan class name or instance in Job arguments\"\nlogger.critical(mess)\nraise NameError(mess)\n# handle case when model is in a file\nif training_plan_path is not None:\ntry:\n# import model from python file\nmodel_module = os.path.basename(training_plan_path)\nmodel_module = re.search(\"(.*)\\.py$\", model_module).group(1)\nsys.path.insert(0, os.path.dirname(training_plan_path))\nmodule = importlib.import_module(model_module)\ntr_class = getattr(module, self._training_plan_class)\nself._training_plan_class = tr_class\nsys.path.pop(0)\nexcept Exception as e:\ne = sys.exc_info()\nlogger.critical(f\"Cannot import class {self._training_plan_class} from \"\nf\"path {training_plan_path} - Error: {str(e)}\")\nsys.exit(-1)\n# check class is defined\ntry:\n_ = inspect.isclass(self._training_plan_class)\nexcept NameError:\nmess = f\"Cannot find training plan for Job, training plan class {self._training_plan_class} is not defined\"\nlogger.critical(mess)\nraise NameError(mess)\n# create/save TrainingPlan instance\nif inspect.isclass(self._training_plan_class):\nself._training_plan = self._training_plan_class()  # contains TrainingPlan\nelse:\nself._training_plan = self._training_plan_class\nself._training_plan.post_init(model_args={} if self._model_args is None else self._model_args,\ntraining_args=self._training_args)\n# find the name of the class in any case\n# (it is `model` only in the case where `model` is not an instance)\nself._training_plan_name = self._training_plan.__class__.__name__\nself.repo = Repository(environ['UPLOADS_URL'], self._keep_files_dir, environ['CACHE_DIR'])\nself._training_plan_file = os.path.join(self._keep_files_dir, 'my_model_' + str(uuid.uuid4()) + '.py')\ntry:\nself._training_plan.save_code(self._training_plan_file)\nexcept Exception as e:\nlogger.error(\"Cannot save the training plan to a local tmp dir : \" + str(e))\nreturn\n# upload my_model_xxx.py on repository server (contains model definition)\nrepo_response = self.repo.upload_file(self._training_plan_file)\nself._repository_args['training_plan_url'] = repo_response['file']\n# Save model parameters to a local file and upload it to the remote repository.\n# The filename and remote url are assigned to attributes through this call.\ntry:\nself.update_parameters()\nexcept SystemExit:\nreturn\n# (below) regex: matches a character not present among \"^\", \"\\\", \".\"\n# characters at the end of string.\nself._repository_args['training_plan_class'] = self._training_plan_name\n# Validate fields in each argument\nself.validate_minimal_arguments(self._repository_args,\n['training_plan_url', 'training_plan_class', 'params_url'])\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.aggregator_args","title":"aggregator_args     <code>property</code>","text":"<pre><code>aggregator_args\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.id","title":"id     <code>property</code>","text":"<pre><code>id\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.last_msg","title":"last_msg     <code>instance-attribute</code>","text":"<pre><code>last_msg = None\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.nodes","title":"nodes     <code>property</code> <code>writable</code>","text":"<pre><code>nodes\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.repo","title":"repo     <code>instance-attribute</code>","text":"<pre><code>repo = Repository(environ['UPLOADS_URL'], self._keep_files_dir, environ['CACHE_DIR'])\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.requests","title":"requests     <code>property</code>","text":"<pre><code>requests\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.training_args","title":"training_args     <code>property</code> <code>writable</code>","text":"<pre><code>training_args\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.training_plan","title":"training_plan     <code>property</code>","text":"<pre><code>training_plan\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.training_plan_file","title":"training_plan_file     <code>property</code>","text":"<pre><code>training_plan_file\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.training_plan_name","title":"training_plan_name     <code>property</code>","text":"<pre><code>training_plan_name\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.training_replies","title":"training_replies     <code>property</code>","text":"<pre><code>training_replies\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job-functions","title":"Functions","text":""},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.check_data_quality","title":"<pre><code>check_data_quality()\n</code></pre>","text":"<p>Does quality check by comparing datasets that have been found in different nodes.</p> Source code in <code>fedbiomed/researcher/job.py</code> <pre><code>def check_data_quality(self):\n\"\"\"Does quality check by comparing datasets that have been found in different nodes. \"\"\"\ndata = self._data.data()\n# If there are more than two nodes ready for the job\nif len(data.keys()) &gt; 1:\n# First check data types are same based on searched tags\nlogger.info('Checking data quality of federated datasets...')\ndata_types = []  # CSV, Image or default\nshapes = []  # dimensions\ndtypes = []  # variable types for CSV datasets\n# Extract features into arrays for comparison\nfor feature in data.values():\ndata_types.append(feature[\"data_type\"])\ndtypes.append(feature[\"dtypes\"])\nshapes.append(feature[\"shape\"])\nif len(set(data_types)) &gt; 1:\nraise FedbiomedDataQualityCheckError(\nf'Different type of datasets has been loaded with same tag: {data_types}'\n)\nif data_types[0] == 'csv':\nif len(set([s[1] for s in shapes])) &gt; 1:\nraise FedbiomedDataQualityCheckError(\nf'Number of columns of federated datasets do not match {shapes}.'\n)\ndtypes_t = list(map(list, zip(*dtypes)))\nfor t in dtypes_t:\nif len(set(t)) &gt; 1:\n# FIXME: specifying a specific use case (in the condition above) should be avoided\nraise FedbiomedDataQualityCheckError(\nf'Variable data types do not match in federated datasets {dtypes}'\n)\nelif data_types[0] == 'images':\nshapes_t = list(map(list, zip(*[s[2:] for s in shapes])))\ndim_state = True\nfor s in shapes_t:\nif len(set(s)) != 1:\ndim_state = False\nif not dim_state:\nlogger.error(f'Dimensions of the images in federated datasets \\\n                             do not match. Please consider using resize. {shapes} ')\nif len(set([k[1] for k in shapes])) != 1:\nlogger.error(f'Color channels of the images in federated \\\n                                datasets do not match. {shapes}')\n# If it is default MNIST dataset pass\nelse:\npass\npass\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.check_training_plan_is_approved_by_nodes","title":"<pre><code>check_training_plan_is_approved_by_nodes()\n</code></pre>","text":"<p>Checks whether model is approved or not.</p> <p>This method sends <code>training-plan-status</code> request to the nodes. It should be run before running experiment. So, researchers can find out if their model has been approved</p> Source code in <code>fedbiomed/researcher/job.py</code> <pre><code>def check_training_plan_is_approved_by_nodes(self) -&gt; List:\n\"\"\" Checks whether model is approved or not.\n    This method sends `training-plan-status` request to the nodes. It should be run before running experiment.\n    So, researchers can find out if their model has been approved\n    \"\"\"\nmessage = {\n'researcher_id': self._researcher_id,\n'job_id': self._id,\n'training_plan_url': self._repository_args['training_plan_url'],\n'command': 'training-plan-status'\n}\nresponses = Responses([])\nreplied_nodes = []\nnode_ids = self._data.node_ids()\n# Send message to each node that has been found after dataset search request\nfor cli in node_ids:\nlogger.info('Sending request to node ' +\nstr(cli) + \" to check model is approved or not\")\nself._reqs.send_message(\nmessage,\ncli)\n# Wait for responses\nfor resp in self._reqs.get_responses(look_for_commands=['training-plan-status'], only_successful=False):\nresponses.append(resp)\nreplied_nodes.append(resp.get('node_id'))\nif resp.get('success') is True:\nif resp.get('approval_obligation') is True:\nif resp.get('status') == TrainingPlanApprovalStatus.APPROVED.value:\nlogger.info(f'Training plan has been approved by the node: {resp.get(\"node_id\")}')\nelse:\nlogger.warning(f'Training plan has NOT been approved by the node: {resp.get(\"node_id\")}.' +\nf'Training plan status : {resp.get(\"status\")}')\nelse:\nlogger.info(f'Training plan approval is not required by the node: {resp.get(\"node_id\")}')\nelse:\nlogger.warning(f\"Node : {resp.get('node_id')} : {resp.get('msg')}\")\n# Get the nodes that haven't replied training-plan-status request\nnon_replied_nodes = list(set(node_ids) - set(replied_nodes))\nif non_replied_nodes:\nlogger.warning(f\"Request for checking training plan status hasn't been replied \\\n                         by the nodes: {non_replied_nodes}. You might get error \\\n                             while running your experiment. \")\nreturn responses\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.load_state","title":"<pre><code>load_state(saved_state)\n</code></pre>","text":"<p>Load breakpoints state for a Job from a saved state</p> <p>Parameters:</p> Name Type Description Default <code>saved_state</code> <code>Dict[str, Any]</code> <p>breakpoint content</p> required Source code in <code>fedbiomed/researcher/job.py</code> <pre><code>def load_state(self, saved_state: Dict[str, Any]) -&gt; None:\n\"\"\"Load breakpoints state for a Job from a saved state\n    Args:\n        saved_state: breakpoint content\n    \"\"\"\n# Reload the job and researched ids.\nself._id = saved_state.get('job_id')\nself._researcher_id = saved_state.get('researcher_id')\n# Upload the latest model parameters. This records the filename and url.\nself.update_parameters(filename=saved_state.get(\"model_params_path\"))\n# Reloadthe latest training replies.\nself._training_replies = self._load_training_replies(\nsaved_state.get('training_replies')\n)\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.save_state","title":"<pre><code>save_state(breakpoint_path)\n</code></pre>","text":"<p>Creates current state of the job to be included in a breakpoint.</p> <p>Includes creating links to files included in the job state.</p> <p>Parameters:</p> Name Type Description Default <code>breakpoint_path</code> <code>str</code> <p>path to the existing breakpoint directory</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Job's current state for breakpoint</p> Source code in <code>fedbiomed/researcher/job.py</code> <pre><code>def save_state(self, breakpoint_path: str) -&gt; dict:\n\"\"\"Creates current state of the job to be included in a breakpoint.\n    Includes creating links to files included in the job state.\n    Args:\n        breakpoint_path: path to the existing breakpoint directory\n    Returns:\n        Job's current state for breakpoint\n    \"\"\"\n# Note: some state is passed to __init__() thus is not managed\n# as job state but as experiment state in current version\nstate = {\n'researcher_id': self._researcher_id,\n'job_id': self._id,\n'model_params_path': self._model_params_file,\n'training_replies': self._save_training_replies(self._training_replies)\n}\nstate['model_params_path'] = create_unique_link(\nbreakpoint_path, 'aggregated_params_current', '.mpk',\nos.path.join('..', os.path.basename(state[\"model_params_path\"]))\n)\nfor round_replies in state['training_replies']:\nfor response in round_replies:\nnode_params_path = create_unique_file_link(\nbreakpoint_path, response['params_path']\n)\nresponse['params_path'] = node_params_path\nreturn state\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.start_nodes_training_round","title":"<pre><code>start_nodes_training_round(round_, aggregator_args_thr_msg, aggregator_args_thr_files, secagg_arguments=None, do_training=True)\n</code></pre>","text":"<p>Sends training request to nodes and waits for the responses</p> <p>Parameters:</p> Name Type Description Default <code>round_</code> <code>int</code> <p>current number of round the algorithm is performing (a round is considered to be all the training steps of a federated model between 2 aggregations).</p> required <code>aggregator_args_thr_msg</code> <code>Dict[str, Dict[str, Any]]</code> <p>dictionary containing some metadata about the aggregation strategy, useful to transfer some data when it's required by am aggregator. First key should be the node_id, and sub-dictionary sould be parameters to be sent through MQTT messaging system</p> required <code>aggregator_args_thr_files</code> <code>Dict[str, Dict[str, Any]]</code> <p>dictionary containing metadata about aggregation strategy, to be transferred via the Repository's HTTP API, as opposed to the mqtt system. Format is the same as aggregator_args_thr_msg .</p> required <code>secagg_arguments</code> <code>Union[Dict, None]</code> <p>Secure aggregation ServerKey context id</p> <code>None</code> <code>do_training</code> <code>bool</code> <p>if False, skip training in this round (do only validation). Defaults to True.</p> <code>True</code> Source code in <code>fedbiomed/researcher/job.py</code> <pre><code>def start_nodes_training_round(self,\nround_: int,\naggregator_args_thr_msg: Dict[str, Dict[str, Any]],\naggregator_args_thr_files: Dict[str, Dict[str, Any]],\nsecagg_arguments: Union[Dict, None] = None,\ndo_training: bool = True):\n\"\"\" Sends training request to nodes and waits for the responses\n    Args:\n        round_: current number of round the algorithm is performing (a round is considered to be all the\n            training steps of a federated model between 2 aggregations).\n        aggregator_args_thr_msg: dictionary containing some metadata about the aggregation\n            strategy, useful to transfer some data when it's required by am aggregator. First key should be the\n            node_id, and sub-dictionary sould be parameters to be sent through MQTT messaging system\n        aggregator_args_thr_files: dictionary containing metadata about aggregation strategy, to be transferred\n            via the Repository's HTTP API, as opposed to the mqtt system. Format is the same as\n            aggregator_args_thr_msg .\n        secagg_arguments: Secure aggregation ServerKey context id\n        do_training: if False, skip training in this round (do only validation). Defaults to True.\n    \"\"\"\n# Assign empty dict to secagg arguments if it is None\nif secagg_arguments is None:\nsecagg_arguments = {}\nheaders = {'researcher_id': self._researcher_id,\n'job_id': self._id,\n'training_args': self._training_args.dict(),\n'training': do_training,\n'model_args': self._model_args,\n'round': round_,\n'secagg_servkey_id': secagg_arguments.get('secagg_servkey_id'),\n'secagg_biprime_id': secagg_arguments.get('secagg_biprime_id'),\n'secagg_random': secagg_arguments.get('secagg_random'),\n'secagg_clipping_range': secagg_arguments.get('secagg_clipping_range'),\n'command': 'train',\n'aggregator_args': {}}\nmsg = {**headers, **self._repository_args}\ntime_start = {}\n# pass heavy aggregator params through file exchange system\nself.upload_aggregator_args(aggregator_args_thr_msg, aggregator_args_thr_files)\nfor cli in self._nodes:\nmsg['dataset_id'] = self._data.data()[cli]['dataset_id']\nif aggregator_args_thr_msg:\n# add aggregator parameters to message header\nmsg['aggregator_args'] = aggregator_args_thr_msg[cli]\nif not do_training:\nlogger.info(f'\\033[1mSending request\\033[0m \\n'\nf'\\t\\t\\t\\t\\t\\033[1m To\\033[0m: {str(cli)} \\n'\nf'\\t\\t\\t\\t\\t\\033[1m Request: \\033[0m:Perform final validation on '\nf'aggregated parameters \\n {5 * \"-------------\"}')\nelse:\nmsg_print = {key: value for key, value in msg.items()\nif key != 'aggregator_args' and logger.level != \"DEBUG\" }\nlogger.info(f'\\033[1mSending request\\033[0m \\n'\nf'\\t\\t\\t\\t\\t\\033[1m To\\033[0m: {str(cli)} \\n'\nf'\\t\\t\\t\\t\\t\\033[1m Request: \\033[0m: Perform training with the arguments: '\nf'{str(msg_print)} '\nf'\\n {5 * \"-------------\"}')\ntime_start[cli] = time.perf_counter()\nself._reqs.send_message(msg, cli)  # send request to node\n# Recollect models trained\nself._training_replies[round_] = Responses([])\nwhile self.waiting_for_nodes(self._training_replies[round_]):\n# collect nodes responses from researcher request 'train'\n# (wait for all nodes with a ` while true` loop)\n# models_done = self._reqs.get_responses(look_for_commands=['train'])\nmodels_done = self._reqs.get_responses(look_for_commands=['train', 'error'], only_successful=False)\nfor m in models_done.data():  # retrieve all models\n# (there should have as many models done as nodes)\n# manage error messages during training\nif m['command'] == 'error':\nif m['extra_msg']:\nlogger.info(f\"Error message received during training: {str(m['errnum'].value)} \"\nf\"- {str(m['extra_msg'])}\")\nelse:\nlogger.info(f\"Error message received during training: {str(m['errnum'].value)}\")\nfaulty_node = m['node_id']  # remove the faulty node from the list\nif faulty_node not in list(self._nodes):\nlogger.warning(f\"Error message from {faulty_node} ignored, since this node is not part ot \"\nf\"the training any mode\")\ncontinue\nself._nodes.remove(faulty_node)\ncontinue\n# only consider replies for our request\nif m['researcher_id'] != environ['RESEARCHER_ID'] or \\\n                    m['job_id'] != self._id or m['node_id'] not in list(self._nodes):\ncontinue\n# manage training failure for this job\nif not m['success']:\nlogger.error(f\"Training failed for node {m['node_id']}: {m['msg']}\")\nself._nodes.remove(m['node_id'])  # remove the faulty node from the list\ncontinue\nrtime_total = time.perf_counter() - time_start[m['node_id']]\nif do_training:\nlogger.info(f\"Downloading model params after training on {m['node_id']} - from {m['params_url']}\")\ntry:\n_, params_path = self.repo.download_file(m[\"params_url\"], f\"node_params_{uuid.uuid4()}.mpk\")\nexcept FedbiomedRepositoryError as err:\nlogger.error(f\"Cannot download model parameter from node {m['node_id']}, probably because Node\"\nf\" stops working (details: {err})\")\nreturn\nresults = Serializer.load(params_path)\nparams = results[\"model_weights\"]\noptimizer_args = results.get(\"optimizer_args\")\nencryption_factor = results.get('encryption_factor', None)\nelse:\nparams_path = None\nparams = None\noptimizer_args = None\nencryption_factor = None\n# TODO: could choose completely different name/structure for\ntiming = m['timing']\ntiming['rtime_total'] = rtime_total\nr = Responses({'success': m['success'],\n'msg': m['msg'],\n'dataset_id': m['dataset_id'],\n'node_id': m['node_id'],\n'params_path': params_path,\n'params': params,\n'optimizer_args': optimizer_args,\n'sample_size': m[\"sample_size\"],\n'encryption_factor': encryption_factor,\n'timing': timing})\nself._training_replies[round_].append(r)\n# return the list of nodes which answered because nodes in error have been removed\nreturn self._nodes\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.update_parameters","title":"<pre><code>update_parameters(params=None, filename=None)\n</code></pre>","text":"<p>Save and upload global model parameters, optionally after updating them.</p> <p>This method is designed to save and upload the parameters of the wrapped training plan instance. It may also be used to update these parameters prior to their upload, whether based on provided in-memory values or on a pre-exported dump file.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>data structure containing the new version of the aggregated parameters for this job,</p> <code>None</code> <code>filename</code> <code>Optional[str]</code> <p>path to the file containing the new version of the aggregated parameters for this job,</p> <code>None</code> <code>is_model_params</code> <p>whether params are models parameters or another value that must be sent</p> required <code>variable_name</code> <p>name the filename with variable_name. Defaults to 'aggregated_prams'.</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dict storing new aggregated parameters that are to be assigned to this job's training plan's model. If None, export and upload the current model parameters.</p> <code>None</code> <code>filename</code> <code>Optional[str]</code> <p>Optional path to a pre-existing file containing the aggregated parameters to load an upload. If <code>params</code> is not None, <code>filename</code> has to be None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>filename</code> <code>str</code> <p>path to the local parameters file</p> <code>url</code> <code>str</code> <p>url at which the file was uploaded</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if both <code>params</code> and <code>filename</code> are provoided: these parameters are mutually-exclusive.</p> <p>!!! info \"Notes\":     * The path to the created and/or uploaded file is stored under the <code>_model_params_file</code> attribute,       that is updated by this method.     * The url of the uploaded file is stored under the <code>_repository_args[\"params_url\"]</code> attribute,       that is also updated by this method.</p> <p>!!! warning \"Warning\":     * The <code>params</code> and <code>filename</code> parameters are mutually-exclusive.</p> Source code in <code>fedbiomed/researcher/job.py</code> <pre><code>def update_parameters(\nself,\nparams: Optional[Dict[str, Any]] = None,\nfilename: Optional[str] = None,\n) -&gt; Tuple[str, str]:\n\"\"\"Save and upload global model parameters, optionally after updating them.\n    This method is designed to save and upload the parameters of the wrapped\n    training plan instance. It may also be used to update these parameters\n    prior to their upload, whether based on provided in-memory values or on\n    a pre-exported dump file.\n    Args:\n        params: data structure containing the new version of the aggregated parameters for this job,\n        defaults to empty dictionary {}\n        filename: path to the file containing the new version of the aggregated parameters for this job,\n        defaults to None.\n        is_model_params: whether params are models parameters or another value that must be sent\n        through file exchange system. Defaults to True (argument are model parameters).\n        variable_name:  name the filename with variable_name. Defaults to 'aggregated_prams'.\n        params: Optional dict storing new aggregated parameters that are to\n            be assigned to this job's training plan's model.\n            If None, export and upload the current model parameters.\n        filename: Optional path to a pre-existing file containing the\n            aggregated parameters to load an upload.\n            If `params` is not None, `filename` has to be None.\n    Returns:\n        filename: path to the local parameters file\n        url: url at which the file was uploaded\n    Raises:\n        ValueError: if both `params` and `filename` are provoided: these parameters are mutually-exclusive.\n    !!! info \"Notes\":\n        * The path to the created and/or uploaded file is stored under the `_model_params_file` attribute,\n          that is updated by this method.\n        * The url of the uploaded file is stored under the `_repository_args[\"params_url\"]` attribute,\n          that is also updated by this method.\n    !!! warning \"Warning\":\n        * The `params` and `filename` parameters are mutually-exclusive.\n    \"\"\"\ntry:\nif params and filename:\nraise ValueError(\"'update_parameters' received both filename and params: only one may be used.\")\n# Case when uploading a pre-existing file: load the parameters.\nif filename:\nparams = Serializer.load(filename)[\"model_weights\"]\nself._training_plan.set_model_params(params)\n# Case when exporting current parameters: create a local dump file.\nelse:\n# Case when uploading the current parameters: gather them.\nif params is None:\nparams = self._training_plan.get_model_params()\n# Case when uploading a new set of parameters: assign them.\nelse:\nself._training_plan.set_model_params(params)\n# At any rate, create a local dump file.\nfilename = os.path.join(self._keep_files_dir, f\"aggregated_params_{uuid.uuid4()}.mpk\")\nparams_dump = {\n\"researcher_id\": self._researcher_id,\n\"model_weights\": params,\n}\nSerializer.dump(params_dump, filename)\n# Upload the file and record its local and remote locations.\nself._model_params_file = filename\nrepo_response = self.repo.upload_file(filename)\nself._repository_args[\"params_url\"] = url = repo_response[\"file\"]\n# Return the local path and remote url to the file.\nreturn filename, url\n# Log exceptions and trigger a system exit if one is raised.\nexcept Exception:\nexc = sys.exc_info()\nlogger.error(\"'Job.update_parameters' failed with error: %s\", exc)\nsys.exit(-1)\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.upload_aggregator_args","title":"<pre><code>upload_aggregator_args(args_thr_msg, args_thr_files)\n</code></pre>","text":"<p>Uploads aggregator metadata to the Repository and updates the mqtt message accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>args_thr_msg</code> <code>Union[Dict[str, Dict[str, Any]], dict]</code> <p>dictionary containing metadata about the aggregation strategy, useful to transfer some data when it's required by am aggregator. First key should be the node_id, and sub-dictionary should be parameters to be sent through MQTT messaging system. This dictionary may be modified by this function with additional metadata about other metadata transferred via the Repository.</p> required <code>args_thr_files</code> <code>Union[Dict[str, Dict[str, Any]], dict]</code> <p>dictionary containing metadata about aggregation strategy, to be transferred via the Repository's HTTP API, as opposed to the mqtt system. Format is the same as aggregator_args_thr_msg .</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>The updated dictionary with metadata to be introduced in the mqtt message.</p> Source code in <code>fedbiomed/researcher/job.py</code> <pre><code>def upload_aggregator_args(self,\nargs_thr_msg: Union[Dict[str, Dict[str, Any]], dict],\nargs_thr_files: Union[Dict[str, Dict[str, Any]], dict]) -&gt; Dict[str, Dict[str, Any]]:\n\"\"\"Uploads aggregator metadata to the Repository and updates the mqtt message accordingly.\n    Args:\n        args_thr_msg: dictionary containing metadata about the aggregation\n            strategy, useful to transfer some data when it's required by am aggregator. First key should be the\n            node_id, and sub-dictionary should be parameters to be sent through MQTT messaging system. This\n            dictionary may be modified by this function with additional metadata about other metadata\n            transferred via the Repository.\n        args_thr_files: dictionary containing metadata about aggregation strategy, to be transferred\n            via the Repository's HTTP API, as opposed to the mqtt system. Format is the same as\n            aggregator_args_thr_msg .\n    Returns:\n        The updated dictionary with metadata to be introduced in the mqtt message.\n    \"\"\"\nfor node_id, aggr_params in args_thr_files.items():\nfor arg_name, aggr_param in aggr_params.items():\nif arg_name == 'aggregator_name':\ncontinue\nargs_thr_msg[node_id][arg_name] = {}\nargs_thr_msg[node_id][arg_name]['arg_name'] = arg_name  # name of the argument to look at\ntry:\nfilename = os.path.join(self._keep_files_dir, f\"{arg_name}_{uuid.uuid4()}.mpk\")\nSerializer.dump(aggr_param, filename)\nurl = self.repo.upload_file(filename)[\"file\"]\nexcept Exception as exc:\nlogger.critical(\"Failed to export %s to local file and upload it: %s\", arg_name, exc)\nsys.exit(-1)\nargs_thr_msg[node_id][arg_name]['filename'] = filename  # path to the file with the parameters\nargs_thr_msg[node_id][arg_name]['url'] = url\nreturn args_thr_msg\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.validate_minimal_arguments","title":"<pre><code>validate_minimal_arguments(obj, fields)\n</code></pre>  <code>staticmethod</code>","text":"<p>Validates a given dictionary by given mandatory fields.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>dict</code> <p>Object to be validated</p> required <code>fields</code> <code>Union[tuple, list]</code> <p>List of fields that should be present on the obj</p> required Source code in <code>fedbiomed/researcher/job.py</code> <pre><code>@staticmethod\ndef validate_minimal_arguments(obj: dict, fields: Union[tuple, list]):\n\"\"\" Validates a given dictionary by given mandatory fields.\n    Args:\n        obj: Object to be validated\n        fields: List of fields that should be present on the obj\n    \"\"\"\nfor f in fields:\nassert f in obj.keys(), f'Field {f} is required in object {obj}. Was not found.'\nif 'url' in f:\nassert validators.url(obj[f]), f'Url not valid: {f}'\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.Job.waiting_for_nodes","title":"<pre><code>waiting_for_nodes(responses)\n</code></pre>","text":"<p>Verifies if all nodes involved in the job are present and Responding</p> <p>Parameters:</p> Name Type Description Default <code>responses</code> <code>Responses</code> <p>contains message answers</p> required <p>Returns:</p> Type Description <code>bool</code> <p>False if all nodes are present in the Responses object. True if waiting for at least one node.</p> Source code in <code>fedbiomed/researcher/job.py</code> <pre><code>def waiting_for_nodes(self, responses: Responses) -&gt; bool:\n\"\"\" Verifies if all nodes involved in the job are present and Responding\n    Args:\n        responses: contains message answers\n    Returns:\n        False if all nodes are present in the Responses object. True if waiting for at least one node.\n    \"\"\"\ntry:\nnodes_done = set(responses.dataframe()['node_id'])\nexcept KeyError:\nnodes_done = set()\nreturn not nodes_done == set(self._nodes)\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.localJob","title":"localJob","text":"CLASS  <pre><code>localJob(dataset_path=None, training_plan_class='MyTrainingPlan', training_plan_path=None, training_args=None, model_args=None)\n</code></pre> <p>Represents the entity that manage the training part. LocalJob is the version of Job but applied locally on a local dataset (thus not involving any network). It is only used to compare results to a Federated approach, using networks.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <p>The path where data is stored on local disk.</p> <code>None</code> <code>training_plan_class</code> <code>str</code> <p>Name of the model class to use for training or model class.</p> <code>'MyTrainingPlan'</code> <code>training_plan_path</code> <code>str</code> <p>path to file containing model code. Defaults to None.</p> <code>None</code> <code>training_args</code> <code>TrainingArgs</code> <p>contains training parameters: lr, epochs, batch_size...</p> <code>None</code> <code>model_args</code> <code>dict</code> <p>contains output and input feature dimension.</p> <code>None</code> Source code in <code>fedbiomed/researcher/job.py</code> <pre><code>def __init__(self, dataset_path: str = None,\ntraining_plan_class: str = 'MyTrainingPlan',\ntraining_plan_path: str = None,\ntraining_args: TrainingArgs = None,\nmodel_args: dict = None):\n\"\"\"\n    Constructor of the class\n    Args:\n        dataset_path : The path where data is stored on local disk.\n        training_plan_class: Name of the model class to use for training or model class.\n        training_plan_path: path to file containing model code. Defaults to None.\n        training_args: contains training parameters: lr, epochs, batch_size...\n        model_args: contains output and input feature dimension.\n    \"\"\"\nself._id = str(uuid.uuid4())\nself._repository_args = {}\nself._localjob_training_args = training_args\nself._model_args = model_args\nself._training_args = TrainingArgs(training_args, only_required=False)\nself.dataset_path = dataset_path\nif training_args is not None:\nif training_args.get('test_on_local_updates', False) \\\n                or training_args.get('test_on_global_updates', False):\n# if user wants to perform validation, display this message\nlogger.warning(\"Cannot perform validation, not supported for LocalJob\")\n# handle case when model is in a file\nif training_plan_path is not None:\ntry:\nmodel_module = os.path.basename(training_plan_path)\nmodel_module = re.search(\"(.*)\\.py$\", model_module).group(1)\nsys.path.insert(0, os.path.dirname(training_plan_path))\nmodule = importlib.import_module(model_module)\ntr_class = getattr(module, training_plan_class)\nself._training_plan = tr_class()\nsys.path.pop(0)\nexcept Exception as e:\ne = sys.exc_info()\nlogger.critical(\"Cannot import class \" + training_plan_class + \" from path \" +\ntraining_plan_path + \" - Error: \" + str(e))\nsys.exit(-1)\nelse:\n# create/save model instance\nif inspect.isclass(training_plan_class):\nself._training_plan = training_plan_class()\nelse:\nself._training_plan = training_plan_class\nself._training_plan.post_init(model_args=self._model_args,\ntraining_args=self._training_args)\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.localJob-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.localJob.dataset_path","title":"dataset_path     <code>instance-attribute</code>","text":"<pre><code>dataset_path = dataset_path\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.localJob.model","title":"model     <code>property</code>","text":"<pre><code>model\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.localJob.training_args","title":"training_args     <code>property</code> <code>writable</code>","text":"<pre><code>training_args\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.localJob.training_plan","title":"training_plan     <code>property</code>","text":"<pre><code>training_plan\n</code></pre>"},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.localJob-functions","title":"Functions","text":""},{"location":"developer/api/researcher/job/#fedbiomed.researcher.job.localJob.start_training","title":"<pre><code>start_training()\n</code></pre>","text":"<p>Sends training task to nodes and waits for the responses</p> Source code in <code>fedbiomed/researcher/job.py</code> <pre><code>def start_training(self):\n\"\"\"Sends training task to nodes and waits for the responses\"\"\"\n# Run import statements (very unsafely).\nfor i in self._training_plan._dependencies:\nexec(i, globals())\n# Run the training routine.\ntry:\nself._training_plan.set_dataset_path(self.dataset_path)\ndata_manager = self._training_plan.training_data()\ntp_type = self._training_plan.type()\ndata_manager.load(tp_type=tp_type)\ntrain_loader, test_loader = data_manager.split(test_ratio=0)\nself._training_plan.training_data_loader = train_loader\nself._training_plan.testing_data_loader = test_loader\nself._training_plan.training_routine()\nexcept Exception as exc:\nlogger.error(\"Cannot train model in job: %s\", repr(exc))\n# Save the current parameters.\nelse:\ntry:\n# TODO: should test status code but not yet returned by upload_file\npath = os.path.join(\nenviron[\"TMP_DIR\"], f\"local_params_{uuid.uuid4()}.mpk\"\n)\nSerializer.dump(self._training_plan.get_model_params(), path)\nexcept Exception as exc:\nlogger.error(\"Cannot write results: %s\", repr(exc))\n</code></pre>"},{"location":"developer/api/researcher/monitor/","title":"Monitor","text":""},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor","title":"fedbiomed.researcher.monitor","text":"Module: <code>fedbiomed.researcher.monitor</code> <p>monitor class to trap information sent during training and sned it to tensordboard</p>"},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor-classes","title":"Classes","text":""},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.MetricStore","title":"MetricStore","text":"<p>           Bases: <code>dict</code></p> <p>Storage facility, used for storing training loss and testing metric values, in order to display them on Tensorboard. Inheriting from a dictionary, providing methods to simplify queries and saving metric values.</p> <p>Storage architecture: <pre><code>{&lt;node&gt;:\n    {&lt;for_&gt;:\n        {&lt;metric_name&gt;:\n            {&lt;round_&gt;: { &lt;iterations/values&gt;: List[float] }\n            }\n        }\n    }\n}\n</code></pre> Where: - <code>node</code>: node id - <code>for_</code>: either testing_global_updates, testing_local_updates, or training - <code>metric_name</code>: metric 's name. Custom or Custom_xxx if testing_step has been defined in TrainingPlan (custom metric) - <code>round_</code>: round number - <code>iterations</code>: index of iterations stored - <code>values</code>: metric value</p>"},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.MetricStore-functions","title":"Functions","text":""},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.MetricStore.add_iteration","title":"<pre><code>add_iteration(node, train, test_on_global_updates, round_, metric, iter_)\n</code></pre>","text":"<p>Method adding iteration to MetricStore based on node, training/validation, round and metric.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>The node id that metric value received from</p> required <code>train</code> <code>bool</code> <p>Training status, If true metric value is for training, Otherwise for validation</p> required <code>test_on_global_updates</code> <code>bool</code> <p>If True metric value is for validation on global updates. Otherwise, for validation on local updates</p> required <code>round_</code> <code>int</code> <p>The round that metric value has received at</p> required <code>metric</code> <code>dict</code> <p>Dictionary that contains metric names and their values e.g {'':} required <code>iter_</code> <code>int</code> <p>Iteration number for validation/training.</p> required <p>Returns      List of cumulative iteration for each metric/validation result</p> Source code in <code>fedbiomed/researcher/monitor.py</code> <pre><code>def add_iteration(self,\nnode: str,\ntrain: bool,\ntest_on_global_updates: bool,\nround_: int,\nmetric: dict,\niter_: int) -&gt; list[int]:\n\"\"\"\n    Method adding iteration to MetricStore based on node, training/validation, round and metric.\n    Args:\n        node: The node id that metric value received from\n        train: Training status, If true metric value is for training, Otherwise for validation\n        test_on_global_updates: If True metric value is for validation on global updates. Otherwise,\n            for validation on local updates\n        round_: The round that metric value has received at\n        metric: Dictionary that contains metric names and their values e.g {'&lt;metric-name&gt;':&lt;value&gt;}\n        iter_: Iteration number for validation/training.\n    Returns\n         List of cumulative iteration for each metric/validation result\n    \"\"\"\nif node not in self:\nself._register_node(node=node)\ncum_iter = []\nfor metric_name, metric_value in metric.items():\nfor_ = 'training' if train is True else 'testing_global_updates' \\\n            if test_on_global_updates is True else 'testing_local_updates'\nif metric_name not in self[node][for_]:\nself._register_metric(node=node, for_=for_, metric_name=metric_name)\n# FIXME: for now, if validation is done on global updates (before model local update)\n# last testing metric value computed on global updates at last round is overwritten\n# by the first one computed at first round\nif round_ in self[node][for_][metric_name]:\n# Each duplication means a new epoch for training, and it is not expected for\n# validation part. Especially for `testing_on_global_updates`. If there is a duplication\n# last value should overwrite\nduplicate = self._iter_duplication_status(round_=self[node][for_][metric_name][round_],\nnext_iter=iter_)\nif duplicate and test_on_global_updates:\nself._add_new_iteration(node, for_, metric_name, round_, iter_, metric_value, True)\nelse:\nself._add_new_iteration(node, for_, metric_name, round_, iter_, metric_value)\nelse:\nself._add_new_iteration(node, for_, metric_name, round_, iter_, metric_value, True)\ncum_iter.append(self._cumulative_iteration(self[node][for_][metric_name]))\nreturn cum_iter\n</code></pre>"},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.Monitor","title":"Monitor","text":"CLASS  <pre><code>Monitor()\n</code></pre> <p>Monitors nodes scalar feed-backs during training</p> Source code in <code>fedbiomed/researcher/monitor.py</code> <pre><code>def __init__(self):\n\"\"\"Constructor of the class \"\"\"\nself._log_dir = environ['TENSORBOARD_RESULTS_DIR']\nself._round = 1\nself._metric_store = MetricStore()\nself._event_writers = {}\nself._round_state = 0\nself._tensorboard = False\nif os.listdir(self._log_dir):\nlogger.info('Removing tensorboard logs from previous experiment')\n# Clear logs' directory from the files from other experiments.\nself._remove_logs()\n</code></pre>"},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.Monitor-functions","title":"Functions","text":""},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.Monitor.close_writer","title":"<pre><code>close_writer()\n</code></pre>","text":"<p>Closes <code>SummaryWriter</code> for each node</p> Source code in <code>fedbiomed/researcher/monitor.py</code> <pre><code>def close_writer(self):\n\"\"\" Closes `SummaryWriter` for each node \"\"\"\n# Close each open SummaryWriter\nfor node in self._event_writers:\nself._event_writers[node].close()\n</code></pre>"},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.Monitor.on_message_handler","title":"<pre><code>on_message_handler(msg)\n</code></pre>","text":"<p>Handler for messages received through general/monitoring channel. This method is used as callback function in Requests class</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Dict[str, Any]</code> <p>incoming message from Node. Must contain key named <code>command</code>, describing the nature of the command (currently the command is only add_scalar).</p> required Source code in <code>fedbiomed/researcher/monitor.py</code> <pre><code>def on_message_handler(self, msg: Dict[str, Any]):\n\"\"\" Handler for messages received through general/monitoring channel. This method is used as callback function\n    in Requests class\n    Args:\n        msg: incoming message from Node. Must contain key named `command`, describing the nature\n            of the command (currently the command is only add_scalar).\n    \"\"\"\n# For now monitor can only handle add_scalar messages\nif msg['command'] == 'add_scalar':\n# Save iteration value\ncumulative_iter, *_ = self._metric_store.add_iteration(\nnode=msg['node_id'],\ntrain=msg['train'],\ntest_on_global_updates=msg['test_on_global_updates'],\nmetric=msg['metric'],\nround_=self._round,\niter_=msg['iteration'])\n# Log metric result\nself._log_metric_result(message=msg, cum_iter=cumulative_iter)\n</code></pre>"},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.Monitor.set_round","title":"<pre><code>set_round(round_)\n</code></pre>","text":"<p>Setts round number that metric results will be received for.</p> <p>By default, at the beginning round is equal to 1 which stands for the first round. T his method should be called by experiment <code>run_once</code> after each round completed, and round should be set to current round + 1. This will inform monitor about the current round where the metric values are getting received.</p> <p>Parameters:</p> Name Type Description Default <code>round_</code> <p>The round that metric value will be saved at they are received</p> required Source code in <code>fedbiomed/researcher/monitor.py</code> <pre><code>def set_round(self, round_: int) -&gt; int:\n\"\"\" Setts round number that metric results will be received for.\n    By default, at the beginning round is equal to 1 which stands for the first round. T\n    his method should be called by experiment `run_once` after each round completed, and round should be set\n    to current round + 1. This will inform monitor about the current round where the metric values are getting\n    received.\n    Args:\n        round_ : The round that metric value will be saved at they are received\n    \"\"\"\nself._round = round_\nreturn self._round\n</code></pre>"},{"location":"developer/api/researcher/monitor/#fedbiomed.researcher.monitor.Monitor.set_tensorboard","title":"<pre><code>set_tensorboard(tensorboard)\n</code></pre>","text":"<p>Sets tensorboard flag, which is used to decide the behavior of the writing scalar values into  tensorboard log files.</p> <p>Parameters:</p> Name Type Description Default <code>tensorboard</code> <code>bool</code> <p>if True, data contained in AddScalarReply message will be passed to tensorboard          if False, fata will only be logged on the console</p> required Source code in <code>fedbiomed/researcher/monitor.py</code> <pre><code>def set_tensorboard(self, tensorboard: bool):\n\"\"\" Sets tensorboard flag, which is used to decide the behavior of the writing scalar values into\n     tensorboard log files.\n    Args:\n        tensorboard: if True, data contained in AddScalarReply message will be passed to tensorboard\n                     if False, fata will only be logged on the console\n    \"\"\"\nif isinstance(tensorboard, bool):\nself._tensorboard = tensorboard\nelse:\nlogger.error(\"tensorboard should be a boolean\")\nself._tensorboard = False\n</code></pre>"},{"location":"developer/api/researcher/requests/","title":"Requests","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests","title":"fedbiomed.researcher.requests","text":"Module: <code>fedbiomed.researcher.requests</code> <p>Implements the message exchanges from researcher to nodes</p>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests-classes","title":"Classes","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests","title":"Requests","text":"CLASS  <pre><code>Requests(mess=None)\n</code></pre> <p>Represents the requests addressed from Researcher to nodes. It creates a task queue storing reply to each incoming message. Starts a message queue and reconfigures  message to be sent into a <code>Messaging</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>mess</code> <code>Any</code> <p>message to be sent by default.</p> <code>None</code> Source code in <code>fedbiomed/researcher/requests.py</code> <pre><code>def __init__(self, mess: Any = None):\n\"\"\"\n    Constructor of the class\n    Args:\n        mess: message to be sent by default.\n    \"\"\"\n# Need to ensure unique per researcher instance message queue to avoid conflicts\n# in case several instances of researcher (with same researcher_id ?) are active,\n# eg: a notebook not quitted and launching a script\nself.queue = TasksQueue(environ['MESSAGES_QUEUE_DIR'] + '_' + str(uuid.uuid4()), environ['TMP_DIR'])\nif mess is None or type(mess) is not Messaging:\nself.messaging = Messaging(self.on_message,\nComponentType.RESEARCHER,\nenviron['RESEARCHER_ID'],\nenviron['MQTT_BROKER'],\nenviron['MQTT_BROKER_PORT'])\nself.messaging.start(block=False)\nelse:\nself.messaging = mess\n# defines the sequence used for ping protocol\nself._sequence = 0\nself._monitor_message_callback = None\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.messaging","title":"messaging     <code>instance-attribute</code>","text":"<pre><code>messaging = Messaging(self.on_message, ComponentType.RESEARCHER, environ['RESEARCHER_ID'], environ['MQTT_BROKER'], environ['MQTT_BROKER_PORT'])\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.queue","title":"queue     <code>instance-attribute</code>","text":"<pre><code>queue = TasksQueue(environ['MESSAGES_QUEUE_DIR'] + '_' + str(uuid.uuid4()), environ['TMP_DIR'])\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests-functions","title":"Functions","text":""},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.add_monitor_callback","title":"<pre><code>add_monitor_callback(callback)\n</code></pre>","text":"<p>Adds callback function for monitor messages</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[Dict], None]</code> <p>Callback function for handling monitor messages that come due 'general/monitoring' channel</p> required Source code in <code>fedbiomed/researcher/requests.py</code> <pre><code>def add_monitor_callback(self, callback: Callable[[Dict], None]):\n\"\"\" Adds callback function for monitor messages\n    Args:\n        callback: Callback function for handling monitor messages that come due 'general/monitoring' channel\n    \"\"\"\nself._monitor_message_callback = callback\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.get_messages","title":"<pre><code>get_messages(commands=[], time=0.0)\n</code></pre>","text":"<p>Goes through the queue and gets messages with the specific command</p> <p>Parameters:</p> Name Type Description Default <code>commands</code> <code>list</code> <p>Checks if message is containing the expecting command (the message  is discarded if it doesn't). Defaults to None (no command message checking, meaning all incoming messages are considered).</p> <code>[]</code> <code>time</code> <code>float</code> <p>Time to sleep in seconds before considering incoming messages. Defaults to .0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Responses</code> <p>Contains the corresponding answers</p> Source code in <code>fedbiomed/researcher/requests.py</code> <pre><code>def get_messages(self, commands: list = [], time: float = .0) -&gt; Responses:\n\"\"\"Goes through the queue and gets messages with the specific command\n    Args:\n        commands: Checks if message is containing the expecting command (the message  is discarded if it doesn't).\n            Defaults to None (no command message checking, meaning all incoming messages are considered).\n        time: Time to sleep in seconds before considering incoming messages. Defaults to .0.\n    Returns:\n        Contains the corresponding answers\n    \"\"\"\nsleep(time)\nanswers = []\nfor _ in range(self.queue.qsize()):\ntry:\nitem = self.queue.get(block=False)\nself.queue.task_done()\nif not commands or \\\n                    ('command' in item.keys() and item['command'] in commands):\nanswers.append(item)\nelse:\n# currently trash all other messages\npass\nexcept FedbiomedTaskQueueError:\n# may happend on self.queue.get()\n# if queue is empty -&gt; we ignore it\npass\nreturn Responses(answers)\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.get_messaging","title":"<pre><code>get_messaging()\n</code></pre>","text":"<p>Retrieves Messaging object</p> <p>Returns:</p> Type Description <code>Messaging</code> <p>Messaging object</p> Source code in <code>fedbiomed/researcher/requests.py</code> <pre><code>def get_messaging(self) -&gt; Messaging:\n\"\"\"Retrieves Messaging object\n    Returns:\n        Messaging object\n    \"\"\"\nreturn self.messaging\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.get_responses","title":"<pre><code>get_responses(look_for_commands, timeout=None, only_successful=True, while_responses=True)\n</code></pre>","text":"<p>Waits for all nodes' answers, regarding a specific command returns the list of all nodes answers</p> <p>Parameters:</p> Name Type Description Default <code>look_for_commands</code> <code>list</code> <p>instruction that has been sent to node (see <code>Message</code> commands)</p> required <code>timeout</code> <code>float</code> <p>wait for a specific duration before collecting nodes messages. Defaults to None. If set to None; uses value in global variable TIMEOUT instead.</p> <code>None</code> <code>only_successful</code> <code>bool</code> <p>deal only with messages that have been tagged as successful (ie with field <code>success=True</code>). Defaults to True.</p> <code>True</code> <code>while_responses</code> <code>bool</code> <p>if <code>True</code>, continue while we get at least one response every <code>timeout</code> seconds. If False, always terminate after <code>timeout</code> even if we get some response.</p> <code>True</code> Source code in <code>fedbiomed/researcher/requests.py</code> <pre><code>def get_responses(self,\nlook_for_commands: list,\ntimeout: float = None,\nonly_successful: bool = True,\nwhile_responses: bool = True) -&gt; Responses:\n\"\"\"Waits for all nodes' answers, regarding a specific command returns the list of all nodes answers\n    Args:\n        look_for_commands: instruction that has been sent to node (see `Message` commands)\n        timeout: wait for a specific duration before collecting nodes messages. Defaults to None. If set to None;\n            uses value in global variable TIMEOUT instead.\n        only_successful: deal only with messages that have been tagged as successful (ie with field `success=True`).\n            Defaults to True.\n        while_responses: if `True`, continue while we get at least one response every\n            `timeout` seconds. If False, always terminate after `timeout` even if we get some\n            response.\n    \"\"\"\ntimeout = timeout or environ['TIMEOUT']\nresponses = []\nwhile True:\nsleep(timeout)\nnew_responses = []\nfor resp in self.get_messages(commands=look_for_commands, time=0):\ntry:\nif not only_successful:\nnew_responses.append(resp)\nelif resp['success']:\n# TODO: test if 'success'key exists\n# what do we do if not ?\nnew_responses.append(resp)\nexcept Exception as e:\nlogger.error(f\"Incorrect message received: {resp} - error: {e}\")\npass\nif len(new_responses) == 0:\n\"Timeout finished\"\nbreak\nresponses += new_responses\nif not while_responses:\nbreak\nreturn Responses(responses)\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.list","title":"<pre><code>list(nodes=None, verbose=False)\n</code></pre>","text":"<p>Lists available data in each node</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list</code> <p>Listings datasets by given node ids. Default is None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If it is true it prints datasets in readable format</p> <code>False</code> Source code in <code>fedbiomed/researcher/requests.py</code> <pre><code>def list(self, nodes: list = None, verbose: bool = False) -&gt; dict:\n\"\"\"Lists available data in each node\n    Args:\n        nodes: Listings datasets by given node ids. Default is None.\n        verbose: If it is true it prints datasets in readable format\n    \"\"\"\n# If nodes list is provided\nif nodes:\nfor node in nodes:\nself.messaging.send_message(\nResearcherMessages.format_outgoing_message({'researcher_id': environ['RESEARCHER_ID'],\n\"command\": \"list\"}\n).get_dict(),\nclient=node)\nlogger.info(f'Listing datasets of given list of nodes : {nodes}')\nelse:\nself.messaging.send_message(\nResearcherMessages.format_outgoing_message({'researcher_id': environ['RESEARCHER_ID'],\n\"command\": \"list\"}).get_dict())\nlogger.info('Listing available datasets in all nodes... ')\n# Get datasets from node responses\ndata_found = {}\nfor resp in self.get_responses(look_for_commands=['list']):\nif not nodes:\ndata_found[resp.get('node_id')] = resp.get('databases')\nelif resp.get('node_id') in nodes:\ndata_found[resp.get('node_id')] = resp.get('databases')\n# Print dataset tables usong data_found object\nif verbose:\nfor node in data_found:\nif len(data_found[node]) &gt; 0:\nrows = [row.values() for row in data_found[node]]\nheaders = data_found[node][0].keys()\ninfo = '\\n Node: {} | Number of Datasets: {} \\n'.format(node, len(data_found[node]))\nlogger.info(info + tabulate.tabulate(rows, headers, tablefmt=\"grid\") + '\\n')\nelse:\nlogger.info('\\n Node: {} | Number of Datasets: {}'.format(node, len(data_found[node])) +\n\" No data has been set up for this node.\")\nreturn data_found\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.on_message","title":"<pre><code>on_message(msg, topic)\n</code></pre>","text":"<p>Handler called by the <code>Messaging</code> class,  when a message is received on researcher side.</p> <p>It is run in the communication process and must ba as quick as possible: - it deals with quick messages (eg: ping/pong) - it stores the replies of the nodes to the task queue, the message will bee treated by the main (computing) thread.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Dict[str, Any]</code> <p>de-serialized msg</p> required <code>topic</code> <code>str</code> <p>topic to publish message (MQTT channel)</p> required Source code in <code>fedbiomed/researcher/requests.py</code> <pre><code>def on_message(self, msg: Dict[str, Any], topic: str):\n\"\"\" Handler called by the [`Messaging`][fedbiomed.common.messaging] class,  when a message is received on\n    researcher side.\n    It is run in the communication process and must ba as quick as possible:\n    - it deals with quick messages (eg: ping/pong)\n    - it stores the replies of the nodes to the task queue, the message will bee\n    treated by the main (computing) thread.\n    Args:\n        msg: de-serialized msg\n        topic: topic to publish message (MQTT channel)\n    \"\"\"\nif topic == \"general/logger\":\n#\n# forward the treatment to node_log_handling() (same thread)\nself.print_node_log_message(ResearcherMessages.format_incoming_message(msg).get_dict())\nelif topic == \"general/researcher\":\n#\n# *Reply messages (SearchReply, TrainReply) added to the TaskQueue\nself.queue.add(ResearcherMessages.format_incoming_message(msg).get_dict())\n# we may trap FedbiomedTaskQueueError here then queue full\n# but what can we do except of quitting ?\nelif topic == \"general/monitoring\":\nif self._monitor_message_callback is not None:\n# Pass message to Monitor's on message handler\nself._monitor_message_callback(ResearcherMessages.format_incoming_message(msg).get_dict())\nelse:\nlogger.error(\"message received on wrong topic (\" + topic + \") - IGNORING\")\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.ping_nodes","title":"<pre><code>ping_nodes()\n</code></pre>","text":"<p>Pings online nodes</p> <p>Returns:</p> Type Description <code>list</code> <p>List ids of up and running nodes</p> Source code in <code>fedbiomed/researcher/requests.py</code> <pre><code>def ping_nodes(self) -&gt; list:\n\"\"\" Pings online nodes\n    Returns:\n        List ids of up and running nodes\n    \"\"\"\nself.send_message(\n{'researcher_id': environ['RESEARCHER_ID'], 'command': 'ping'},\nadd_sequence=True)\n# TODO: check sequence number in pong\n# TODO: (below, above) handle exceptions\nnodes_online = [resp['node_id'] for resp in self.get_responses(look_for_commands=['pong'])]\nreturn nodes_online\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.print_node_log_message","title":"<pre><code>print_node_log_message(log)\n</code></pre>  <code>staticmethod</code>","text":"<p>Prints logger messages coming from the node</p> <p>It is run on the communication process and must be as quick as possible: - all logs (coming from the nodes) are forwarded to the researcher logger (immediate display on console/file/whatever)</p> Source code in <code>fedbiomed/researcher/requests.py</code> <pre><code>@staticmethod\ndef print_node_log_message(log: Dict[str, Any]):\n\"\"\"Prints logger messages coming from the node\n    It is run on the communication process and must be as quick as possible:\n    - all logs (coming from the nodes) are forwarded to the researcher logger\n    (immediate display on console/file/whatever)\n    \"\"\"\n# log contains the original message sent by the node\n# FIXME: we should use `fedbiomed.common.json.deserialize` method\n# instead of the json method when extracting json message\noriginal_msg = json.loads(log[\"msg\"])\n# Loging fancy feedback for training\nlogger.info(\"\\033[1m{}\\033[0m\\n\"\n\"\\t\\t\\t\\t\\t\\033[1m NODE\\033[0m {}\\n\"\n\"\\t\\t\\t\\t\\t\\033[1m MESSAGE:\\033[0m {}\\033[0m\\n\"\n\"{}\".format(log[\"level\"],\nlog[\"node_id\"],\noriginal_msg[\"message\"],\n5 * \"-------------\"))\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.remove_monitor_callback","title":"<pre><code>remove_monitor_callback()\n</code></pre>","text":"<p>Removes callback function for Monitor class.</p> Source code in <code>fedbiomed/researcher/requests.py</code> <pre><code>def remove_monitor_callback(self):\n\"\"\" Removes callback function for Monitor class. \"\"\"\nself._monitor_message_callback = None\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.search","title":"<pre><code>search(tags, nodes=None)\n</code></pre>","text":"<p>Searches available data by tags</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>tuple</code> <p>Tuple containing tags associated to the data researcher is looking for.</p> required <code>nodes</code> <code>list</code> <p>optionally filter nodes with this list. Default is no filtering, consider all nodes</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dict with node_id as keys, and list of dicts describing available data as values</p> Source code in <code>fedbiomed/researcher/requests.py</code> <pre><code>def search(self, tags: tuple, nodes: list = None) -&gt; dict:\n\"\"\" Searches available data by tags\n    Args:\n        tags: Tuple containing tags associated to the data researcher is looking for.\n        nodes: optionally filter nodes with this list. Default is no filtering, consider all nodes\n    Returns:\n        A dict with node_id as keys, and list of dicts describing available data as values\n    \"\"\"\n# Search datasets based on node specifications\nif nodes:\nlogger.info(f'Searching dataset with data tags: {tags} on specified nodes: {nodes}')\nfor node in nodes:\nself.messaging.send_message(\nResearcherMessages.format_outgoing_message({'tags': tags,\n'researcher_id': environ['RESEARCHER_ID'],\n\"command\": \"search\"}\n).get_dict(),\nclient=node)\nelse:\nlogger.info(f'Searching dataset with data tags: {tags} for all nodes')\nself.messaging.send_message(\nResearcherMessages.format_outgoing_message({'tags': tags,\n'researcher_id': environ['RESEARCHER_ID'],\n\"command\": \"search\"}\n).get_dict())\ndata_found = {}\nfor resp in self.get_responses(look_for_commands=['search']):\nif not nodes:\ndata_found[resp.get('node_id')] = resp.get('databases')\nelif resp.get('node_id') in nodes:\ndata_found[resp.get('node_id')] = resp.get('databases')\nlogger.info('Node selected for training -&gt; {}'.format(resp.get('node_id')))\nif not data_found:\nlogger.info(\"No available dataset has found in nodes with tags: {}\".format(tags))\nreturn data_found\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.send_message","title":"<pre><code>send_message(msg, client=None, add_sequence=False)\n</code></pre>","text":"<p>Ask the messaging class to send a new message (receivers are deduced from the message content)</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict</code> <p>the message to send to nodes</p> required <code>client</code> <code>str</code> <p>defines the channel to which the message will be sent. Defaults to None (all nodes)</p> <code>None</code> <code>add_sequence</code> <code>bool</code> <p>if <code>True</code>, add unique sequence number to the message</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[int, None]</code> <p>If <code>add_sequence</code> is True return the sequence number added to the message. If <code>add_sequence</code> is False, return None</p> Source code in <code>fedbiomed/researcher/requests.py</code> <pre><code>def send_message(self, msg: dict, client: str = None, add_sequence: bool = False) -&gt; \\\n        Union[int, None]:\n\"\"\"\n    Ask the messaging class to send a new message (receivers are\n    deduced from the message content)\n    Args:\n        msg: the message to send to nodes\n        client: defines the channel to which the message will be sent. Defaults to None (all nodes)\n        add_sequence: if `True`, add unique sequence number to the message\n    Returns:\n        If `add_sequence` is True return the sequence number added to the message.\n            If `add_sequence` is False, return None\n    \"\"\"\nlogger.debug(str(environ['RESEARCHER_ID']))\nsequence = None\nif add_sequence:\nsequence = self._sequence\nself._sequence += 1\nmsg['sequence'] = sequence\nself.messaging.send_message(\nResearcherMessages.format_outgoing_message(msg).get_dict(),\nclient=client)\nreturn sequence\n</code></pre>"},{"location":"developer/api/researcher/requests/#fedbiomed.researcher.requests.Requests.training_plan_approve","title":"<pre><code>training_plan_approve(training_plan, description='no description provided', nodes=[], timeout=5)\n</code></pre>","text":"<p>Send a training plan and a ApprovalRequest message to node(s).</p> <p>If a list of node id(s) is provided, the message will be individually sent to all nodes of the list. If the node id(s) list is None (default), the message is broadcast to all nodes.</p> <p>Parameters:</p> Name Type Description Default <code>training_plan</code> <code>BaseTrainingPlan</code> <p>the training plan to upload and send to the nodes for approval.    It can be:    - a path_name (str)    - a training plan (class)    - an instance of a training plan (TrainingPlan instance)</p> required <code>nodes</code> <code>list</code> <p>list of nodes (specified by their UUID)</p> <code>[]</code> <code>description</code> <code>str</code> <p>Description of training plan approval request</p> <code>'no description provided'</code> <code>timeout</code> <code>int</code> <p>maximum waiting time for the answers</p> <code>5</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>a dictionary of pairs (node_id: status), where status indicates to the researcher</p> <code>dict</code> <p>that the training plan has been correctly downloaded on the node side.</p> <code>Warning</code> <code>dict</code> <p>status does not mean that the training plan is approved, only that it has been added</p> <code>dict</code> <p>to the \"approval queue\" on the node side.</p> Source code in <code>fedbiomed/researcher/requests.py</code> <pre><code>def training_plan_approve(self,\ntraining_plan: 'BaseTrainingPlan',\ndescription: str = \"no description provided\",\nnodes: list = [],\ntimeout: int = 5) -&gt; dict:\n\"\"\"Send a training plan and a ApprovalRequest message to node(s).\n    If a list of node id(s) is provided, the message will be individually sent\n    to all nodes of the list.\n    If the node id(s) list is None (default), the message is broadcast to all nodes.\n    Args:\n        training_plan: the training plan to upload and send to the nodes for approval.\n               It can be:\n               - a path_name (str)\n               - a training plan (class)\n               - an instance of a training plan (TrainingPlan instance)\n        nodes: list of nodes (specified by their UUID)\n        description: Description of training plan approval request\n        timeout: maximum waiting time for the answers\n    Returns:\n        a dictionary of pairs (node_id: status), where status indicates to the researcher\n        that the training plan has been correctly downloaded on the node side.\n        Warning: status does not mean that the training plan is approved, only that it has been added\n        to the \"approval queue\" on the node side.\n    \"\"\"\n# first verify all arguments\nif not isinstance(nodes, list):\nlogger.error(\"bad nodes argument, training plan not sent\")\nreturn {}\n# verify the training plan and save it to a local file name if necessary\nif isinstance(training_plan, str):\n# training plan is provided as a file\n# TODO: verify that this file a a proper TrainingPlan\nif os.path.isfile(training_plan) and os.access(training_plan, os.R_OK):\ntraining_plan_file = training_plan\nelse:\nlogger.error(f\"cannot access to the file ({training_plan})\")\nreturn {}\nelse:\n# we need a training plan instance in other cases\nif inspect.isclass(training_plan):\n# case if `training_plan` is a class\ntry:\ntraining_plan_instance = training_plan()\ndeps = training_plan_instance.init_dependencies()\ntraining_plan_instance.add_dependency(deps)\nexcept Exception as e:  # TODO: be more specific\nlogger.error(f\"cannot instantiate the given training plan ({e})\")\nreturn {}\nelse:\n# also handle case where training plan is already an instance of a class\ntraining_plan_instance = training_plan\n# then save this instance to a file\ntraining_plan_file = os.path.join(environ['TMP_DIR'],\n\"training_plan_\" + str(uuid.uuid4()) + \".py\")\ntry:\ntraining_plan_instance.save_code(training_plan_file)\nexcept Exception as e:  # TODO: be more specific\nlogger.error(f\"Cannot save the training plan to a file ({e})\")\nlogger.error(f\"Are you sure that {training_plan} is a TrainingPlan ?\")\nreturn {}\n# verify that the file can be minified before sending\n#\n# TODO: enforce a stronger check here (user story #179)\ntry:\nwith open(training_plan_file, \"r\") as f:\ncontent = f.read()\nminify(content,\nremove_annotations=False,\ncombine_imports=False,\nremove_pass=False,\nhoist_literals=False,\nremove_object_base=True,\nrename_locals=False)\nexcept Exception as e:\n# minify does not provide any specific exception\nlogger.error(f\"This file is not a python file ({e})\")\nreturn {}\n# create a repository instance and upload the training plan file\nrepository = Repository(environ['UPLOADS_URL'],\nenviron['TMP_DIR'],\nenviron['CACHE_DIR'])\nupload_status = repository.upload_file(training_plan_file)\nlogger.debug(f\"training_plan_approve: upload_status = {upload_status}\")\n# send message to node(s)\nmessage = {\n'researcher_id': environ['RESEARCHER_ID'],\n'description': str(description),\n'training_plan_url': upload_status['file'],\n'command': 'approval'}\nif nodes:\n# send message to each node\nfor n in nodes:\nsequence = self.send_message(message, client=n, add_sequence=True)\nelse:\n# broadcast message\nsequence = self.send_message(message, add_sequence=True)\n# wait for answers for a certain timeout\nresult = {}\nfor resp in self.get_responses(look_for_commands=['approval'],\ntimeout=timeout):\nif sequence != resp['sequence']:\nlogger.error(\"received an approval_reply with wrong sequence, ignoring it\")\ncontinue\nn = resp['node_id']\ns = resp['success']\nresult[n] = s\nif s:\nlogger.info(f\"node ({n}) has correctly downloaded the training plan\")\nelse:\nlogger.info(f\"node ({n}) has not correctly downloaded the training plan\")\n# print info to the user regarding the result\nif not result or not any(result.values()):\nlogger.info(\"no nodes have acknowledged correct training plan reception before the timeout\")\n# eventually complete the result with expected results\n# (if the message was sent to specific nodes)\nfor n in nodes:\nif n not in result:\nresult[n] = False\nlogger.info(f\"node ({n}) has not acknowledge training plan reception before the timeout\")\n# return the result\nreturn result\n</code></pre>"},{"location":"developer/api/researcher/responses/","title":"Responses","text":""},{"location":"developer/api/researcher/responses/#fedbiomed.researcher.responses","title":"fedbiomed.researcher.responses","text":"Module: <code>fedbiomed.researcher.responses</code>"},{"location":"developer/api/researcher/responses/#fedbiomed.researcher.responses-classes","title":"Classes","text":""},{"location":"developer/api/researcher/responses/#fedbiomed.researcher.responses.Responses","title":"Responses","text":"CLASS  <pre><code>Responses(data)\n</code></pre> <p>Class parsing Nodes' responses Reconfigures input data into either a dictionary in a list (List[dict]), or a list with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[list, dict]</code> <p>input response</p> required Source code in <code>fedbiomed/researcher/responses.py</code> <pre><code>def __init__(self, data: Union[list, dict]):\n\"\"\"Constructor of `Responses` class.\n    Args:\n        data: input response\n    \"\"\"\nself._map_node: Dict[str, int] = {}\nif isinstance(data, dict):\nself._data = [data]\nself._update_map_node(data)\nelif isinstance(data, list):\nself._data = []\n# create a list containing unique fields\nfor d in data:\nif d not in self._data:\nself._data.append(d)\nself._update_map_node(d)\nelse:\nraise FedbiomedResponsesError(f\"data argument should be of type list or dict, not {type(data)}\")\n</code></pre>"},{"location":"developer/api/researcher/responses/#fedbiomed.researcher.responses.Responses-functions","title":"Functions","text":""},{"location":"developer/api/researcher/responses/#fedbiomed.researcher.responses.Responses.append","title":"<pre><code>append(response)\n</code></pre>","text":"<p>Appends new responses to existing responses</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Union[List[Dict], Dict, _R]</code> <p>List of response as dict or single response as dict that will be appended</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of dict as responses</p> <p>Raises:</p> Type Description <code>FedbiomedResponsesError</code> <p>When <code>response</code> argument is not in valid type</p> Source code in <code>fedbiomed/researcher/responses.py</code> <pre><code>def append(self, response: Union[List[Dict], Dict, _R]) -&gt; list:\n\"\"\" Appends new responses to existing responses\n    Args:\n        response: List of response as dict or single response as dict that will be appended\n    Returns:\n        List of dict as responses\n    Raises:\n        FedbiomedResponsesError: When `response` argument is not in valid type\n    \"\"\"\nif isinstance(response, List):\n#self._data = self._data + response\nfor resp in response:\nif isinstance(resp, (dict, self.__class__)):\nself.append(resp)\nelse:\nself._data = self._data + response\nelif isinstance(response, Dict):\nself._data = self._data + [response]\nelif isinstance(response, self.__class__):\nself._data = self._data + response.data()\nelse:\nraise FedbiomedResponsesError(f'`The argument must be instance of List, '\nf'Dict or Responses` not {type(response)}')\nself._update_map_node(response)\nreturn self._data\n</code></pre>"},{"location":"developer/api/researcher/responses/#fedbiomed.researcher.responses.Responses.data","title":"<pre><code>data()\n</code></pre>","text":"<p>Gets all responses that are received</p> <p>Returns:</p> Type Description <code>list</code> <p>Data of the class <code>Responses</code></p> Source code in <code>fedbiomed/researcher/responses.py</code> <pre><code>def data(self) -&gt; list:\n\"\"\"Gets all responses that are received\n    Returns:\n        Data of the class `Responses`\n    \"\"\"\nreturn self._data\n</code></pre>"},{"location":"developer/api/researcher/responses/#fedbiomed.researcher.responses.Responses.dataframe","title":"<pre><code>dataframe()\n</code></pre>","text":"<p>This method converts the list that includes responses to pandas dataframe</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Pandas DataFrame includes node responses. Each row of dataframe represent single response that comes from a node.</p> Source code in <code>fedbiomed/researcher/responses.py</code> <pre><code>def dataframe(self) -&gt; pd.DataFrame:\n\"\"\" This method converts the list that includes responses to pandas dataframe\n    Returns:\n        Pandas DataFrame includes node responses. Each row of dataframe represent single response that comes\n            from a node.\n    \"\"\"\nreturn pd.DataFrame(self._data)\n</code></pre>"},{"location":"developer/api/researcher/responses/#fedbiomed.researcher.responses.Responses.get_index_from_node_id","title":"<pre><code>get_index_from_node_id(node_id)\n</code></pre>","text":"<p>Helper that allows to retrieve the index of a given node_id, assuming that all content of the object Responses are nodes' answers</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>id of the node</p> required <p>Returns:</p> Type Description <code>Union[int, None]</code> <p>Union[int, None]: returns the index of the corresponding</p> <code>Union[int, None]</code> <p>node_id. If not found, returns None</p> Source code in <code>fedbiomed/researcher/responses.py</code> <pre><code>def get_index_from_node_id(self, node_id: str) -&gt; Union[int, None]:\n\"\"\"\n    Helper that allows to retrieve the index of a given node_id,\n    assuming that all content of the object Responses are nodes' answers\n    Args:\n        node_id (str): id of the node\n    Returns:\n        Union[int, None]: returns the index of the corresponding\n        node_id. If not found, returns None\n    \"\"\"\nreturn self._map_node.get(node_id)\n</code></pre>"},{"location":"developer/api/researcher/responses/#fedbiomed.researcher.responses.Responses.set_data","title":"<pre><code>set_data(data)\n</code></pre>","text":"<p>Setter for ._data attribute</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Dict]</code> <p>List of responses as python dictionary</p> required <p>Returns:</p> Type Description <code>List</code> <p>List of responses as Dict</p> <p>Raises:</p> Type Description <code>FedbiomedResponsesError</code> <p>When <code>data</code> argument is not in valid type</p> Source code in <code>fedbiomed/researcher/responses.py</code> <pre><code>def set_data(self, data: List[Dict]) -&gt; List:\n\"\"\"Setter for ._data attribute\n    Args:\n        data: List of responses as python dictionary\n    Returns:\n        List of responses as Dict\n    Raises:\n        FedbiomedResponsesError: When `data` argument is not in valid type\n    \"\"\"\n# TODO: Check elements of list are Dict\nif not isinstance(data, List):\nraise FedbiomedResponsesError(f'`data` argument should instance of list not {type(data)}')\nself._data = data\nfor datum in data:\nself._update_map_node(datum)\nreturn self._data\n</code></pre>"},{"location":"developer/api/researcher/secagg/","title":"Secagg","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg","title":"fedbiomed.researcher.secagg","text":"Module: <code>fedbiomed.researcher.secagg</code>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg-classes","title":"Classes","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggBiprimeContext","title":"SecaggBiprimeContext","text":"CLASS  <pre><code>SecaggBiprimeContext(parties, secagg_id=None)\n</code></pre> <p>           Bases: <code>SecaggContext</code></p> <p>Handles a Secure Aggregation biprime context element on the researcher side.</p> <p>Parameters:</p> Name Type Description Default <code>parties</code> <code>List[str]</code> <p>list of parties participating to the secagg context element setup, named by their unique id (<code>node_id</code>, <code>researcher_id</code>). There must be at least 3 parties, and the first party is this researcher</p> required <code>secagg_id</code> <code>Union[str, None]</code> <p>optional secagg context element ID to use for this element. Default is None, which means a unique element ID will be generated.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedSecaggError</code> <p>bad argument type or value</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>def __init__(self, parties: List[str], secagg_id: Union[str, None] = None):\n\"\"\"Constructor of the class.\n    Args:\n        parties: list of parties participating to the secagg context element setup, named\n            by their unique id (`node_id`, `researcher_id`).\n            There must be at least 3 parties, and the first party is this researcher\n        secagg_id: optional secagg context element ID to use for this element.\n            Default is None, which means a unique element ID will be generated.\n    Raises:\n        FedbiomedSecaggError: bad argument type or value\n    \"\"\"\nsuper().__init__(parties, None, secagg_id)\nself._element = SecaggElementTypes.BIPRIME\nself._secagg_manager = _BPrimeManager\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext","title":"SecaggContext","text":"CLASS  <pre><code>SecaggContext(parties, job_id, secagg_id=None)\n</code></pre> <p>           Bases: <code>ABC</code></p> <p>Handles a Secure Aggregation context element on the researcher side.</p> <p>Parameters:</p> Name Type Description Default <code>parties</code> <code>List[str]</code> <p>list of parties participating in the secagg context element setup, named by their unique id (<code>node_id</code>, <code>researcher_id</code>). There must be at least 3 parties, and the first party is this researcher</p> required <code>job_id</code> <code>Union[str, None]</code> <p>ID of the job to which this secagg context element is attached. None means the element is not attached to a specific job</p> required <code>secagg_id</code> <code>Union[str, None]</code> <p>optional secagg context element ID to use for this element. Default is None, which means a unique element ID will be generated.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedSecaggError</code> <p>bad argument type or value</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>def __init__(self, parties: List[str], job_id: Union[str, None], secagg_id: Union[str, None] = None):\n\"\"\"Constructor of the class.\n    Args:\n        parties: list of parties participating in the secagg context element setup, named\n            by their unique id (`node_id`, `researcher_id`).\n            There must be at least 3 parties, and the first party is this researcher\n        job_id: ID of the job to which this secagg context element is attached.\n            None means the element is not attached to a specific job\n        secagg_id: optional secagg context element ID to use for this element.\n            Default is None, which means a unique element ID will be generated.\n    Raises:\n        FedbiomedSecaggError: bad argument type or value\n    \"\"\"\nself._v = Validator()\nself._v.register(\"nonempty_str_or_none\", self._check_secagg_id_type, override=True)\ntry:\nself._v.validate(secagg_id, \"nonempty_str_or_none\")\nexcept ValidatorError as e:\nerrmess = f'{ErrorNumbers.FB415.value}: bad parameter `secagg_id` must be a None or non-empty string: {e}'\nlogger.error(errmess)\nraise FedbiomedSecaggError(errmess)\ntry:\nself._v.validate(parties, list)\nfor p in parties:\nself._v.validate(p, str)\nexcept ValidatorError as e:\nerrmess = f'{ErrorNumbers.FB415.value}: bad parameter `parties` must be a list of strings: {e}'\nlogger.error(errmess)\nraise FedbiomedSecaggError(errmess)\nif len(parties) &lt; 3:\nerrmess = f'{ErrorNumbers.FB415.value}: bad parameter `parties` : {parties} : need  ' \\\n                  'at least 3 parties for secure aggregation'\nlogger.error(errmess)\nraise FedbiomedSecaggError(errmess)\nif environ['ID'] != parties[0]:\nraise FedbiomedSecaggError(\nf'{ErrorNumbers.FB415.value}: researcher should be the first party.'\n)\nself._secagg_id = secagg_id if secagg_id is not None else 'secagg_' + str(uuid.uuid4())\nself._parties = parties\nself._researcher_id = environ['ID']\nself._requests = Requests()\nself._status = False\nself._context = None\nself._job_id = None\n# set job ID using setter to validate\nself.set_job_id(job_id)\n# one controller per secagg object to prevent any file conflict\nself._MPC = MPCController(\ntmp_dir=environ[\"TMP_DIR\"],\ncomponent_type=ComponentType.RESEARCHER,\ncomponent_id=environ[\"ID\"]\n)\n# to be set in subclasses\nself._secagg_manager = None\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secagg_context.SecaggContext.context","title":"context     <code>property</code>","text":"<pre><code>context: Union[dict, None]\n</code></pre> <p>Getter for secagg context element content</p> <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>secagg context element, or <code>None</code> if it doesn't exist</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secagg_context.SecaggContext.job_id","title":"job_id     <code>property</code>","text":"<pre><code>job_id: Union[str, None]\n</code></pre> <p>Getter for secagg context element job_id</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>secagg context element job_ib (or None if no job_id is attached to the element)</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secagg_context.SecaggContext.parties","title":"parties     <code>property</code>","text":"<pre><code>parties: str\n</code></pre> <p>Getter for secagg parties</p> <p>Returns:</p> Type Description <code>str</code> <p>Parties that participates secure aggregation</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secagg_context.SecaggContext.secagg_id","title":"secagg_id     <code>property</code>","text":"<pre><code>secagg_id: str\n</code></pre> <p>Getter for secagg context element ID </p> <p>Returns:</p> Type Description <code>str</code> <p>secagg context element unique ID</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secagg_context.SecaggContext.status","title":"status     <code>property</code>","text":"<pre><code>status: bool\n</code></pre> <p>Getter for secagg context element status</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if secagg context element exists, <code>False</code> otherwise</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggContext-functions","title":"Functions","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secagg_context.SecaggContext.delete","title":"<pre><code>delete(timeout=0)\n</code></pre>","text":"<p>Delete secagg context element on defined parties.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>maximum duration for the deletion phase. Defaults to <code>environ['TIMEOUT']</code> if unset or equals 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if secagg context element could be deleted for all parties, False if at least one of the parties could not delete context element.</p> <p>Raises:</p> Type Description <code>FedbiomedSecaggError</code> <p>bad argument type</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>def delete(\nself,\ntimeout: float = 0\n) -&gt; bool:\n\"\"\"Delete secagg context element on defined parties.\n    Args:\n        timeout: maximum duration for the deletion phase. Defaults to `environ['TIMEOUT']` if unset\n            or equals 0.\n    Returns:\n        True if secagg context element could be deleted for all parties, False if at least\n            one of the parties could not delete context element.\n    Raises:\n        FedbiomedSecaggError: bad argument type\n    \"\"\"\nif isinstance(timeout, int):\ntimeout = float(timeout)  # accept int (and bool...)\ntry:\nself._v.validate(timeout, float)\nexcept ValidatorError as e:\nerrmess = f'{ErrorNumbers.FB415.value}: bad parameter `timeout`: {e}'\nlogger.error(errmess)\nraise FedbiomedSecaggError(errmess)\nself._status = False\nself._context = None\nmsg = {\n'researcher_id': self._researcher_id,\n'secagg_id': self._secagg_id,\n'element': self._element.value,\n'job_id': self._job_id,\n'command': 'secagg-delete',\n}\nreturn self._secagg_round(msg, 'secagg-delete', False, self._delete_payload, timeout)\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secagg_context.SecaggContext.load_state","title":"<pre><code>load_state(state)\n</code></pre>  <code>staticmethod</code>","text":"<p>Method for loading secagg state from breakpoint state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Dict[str, Any]</code> <p>The state that will be loaded</p> required Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>@staticmethod\ndef load_state(\nstate: Dict[str, Any]\n) -&gt; 'SecaggContext':\n\"\"\"\n    Method for loading secagg state from breakpoint state\n    Args:\n        state: The state that will be loaded\n    \"\"\"\n# Get class\ncls = getattr(importlib.import_module(state[\"module\"]), state[\"class\"])\n# Validate job id\nspec = get_method_spec(cls)\nif 'job_id' in spec:\nsecagg = cls(**state[\"arguments\"])\nelse:\nstate[\"arguments\"].pop('job_id')\nsecagg = cls(**state[\"arguments\"])\nfor key, value in state[\"attributes\"].items():\nsetattr(secagg, key, value)\nreturn secagg\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secagg_context.SecaggContext.save_state","title":"<pre><code>save_state()\n</code></pre>","text":"<p>Method for saving secagg state for saving breakpoints</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The state of the secagg</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>def save_state(self) -&gt; Dict[str, Any]:\n\"\"\"Method for saving secagg state for saving breakpoints\n    Returns:\n        The state of the secagg\n    \"\"\"\n# `_v` and `_requests` dont need to be savec (properly initiated in constructor)\nstate = {\n\"class\": type(self).__name__,\n\"module\": self.__module__,\n\"arguments\": {\n\"secagg_id\": self._secagg_id,\n\"parties\": self._parties,\n\"job_id\": self._job_id,\n},\n\"attributes\": {\n\"_status\": self._status,\n\"_context\": self._context,\n\"_researcher_id\": self._researcher_id,\n}\n}\nreturn state\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secagg_context.SecaggContext.set_job_id","title":"<pre><code>set_job_id(job_id)\n</code></pre>","text":"<p>Setter for secagg context element job_id</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>Union[str, None]</code> <p>ID of the job to which this secagg context element is attached.</p> required <p>Raises:</p> Type Description <code>FedbiomedSecaggError</code> <p>bad argument type or value</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>def set_job_id(self, job_id: Union[str, None]) -&gt; None:\n\"\"\"Setter for secagg context element job_id\n    Args:\n        job_id: ID of the job to which this secagg context element is attached.\n    Raises:\n        FedbiomedSecaggError: bad argument type or value\n    \"\"\"\nif not isinstance(job_id, (str, type(None))):\nerrmess = f'{ErrorNumbers.FB415.value}: bad parameter `job_id` must be a str or None if the ' \\\n                  f'context is set for biprime.'\nlogger.error(errmess)\nraise FedbiomedSecaggError(errmess)\nself._job_id = job_id\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secagg_context.SecaggContext.setup","title":"<pre><code>setup(timeout=0)\n</code></pre>","text":"<p>Setup secagg context element on defined parties.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>maximum time waiting for answers from other nodes, after completing the setup locally. It does not include the time for the local setup payload. Defaults to <code>environ['TIMEOUT']</code> if unset or equals 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if secagg context element could be setup for all parties, False if at least one of the parties could not setup context element.</p> <p>Raises:</p> Type Description <code>FedbiomedSecaggError</code> <p>bad argument type</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>def setup(\nself,\ntimeout: float = 0\n) -&gt; bool:\n\"\"\"Setup secagg context element on defined parties.\n    Args:\n        timeout: maximum time waiting for answers from other nodes, after completing the setup locally.\n            It does not include the time for the local setup payload.\n            Defaults to `environ['TIMEOUT']` if unset or equals 0.\n    Returns:\n        True if secagg context element could be setup for all parties, False if at least\n            one of the parties could not setup context element.\n    Raises:\n        FedbiomedSecaggError: bad argument type\n    \"\"\"\nif isinstance(timeout, int):\ntimeout = float(timeout)  # accept int (and bool...)\ntry:\nself._v.validate(timeout, float)\nexcept ValidatorError as e:\nerrmess = f'{ErrorNumbers.FB415.value}: bad parameter `timeout`: {e}'\nlogger.error(errmess)\nraise FedbiomedSecaggError(errmess)\nmsg = {\n'researcher_id': self._researcher_id,\n'secagg_id': self._secagg_id,\n'element': self._element.value,\n'job_id': self._job_id,\n'parties': self._parties,\n'command': 'secagg',\n}\nreturn self._secagg_round(msg, 'secagg', True, self._payload, timeout)\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecaggServkeyContext","title":"SecaggServkeyContext","text":"CLASS  <pre><code>SecaggServkeyContext(parties, job_id, secagg_id=None)\n</code></pre> <p>           Bases: <code>SecaggContext</code></p> <p>Handles a Secure Aggregation server key context element on the researcher side.</p> <p>Parameters:</p> Name Type Description Default <code>parties</code> <code>List[str]</code> <p>list of parties participating in the secagg context element setup, named by their unique id (<code>node_id</code>, <code>researcher_id</code>). There must be at least 3 parties, and the first party is this researcher</p> required <code>job_id</code> <code>str</code> <p>ID of the job to which this secagg context element is attached.</p> required <code>secagg_id</code> <code>Union[str, None]</code> <p>optional secagg context element ID to use for this element. Default is None, which means a unique element ID will be generated.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedSecaggError</code> <p>bad argument type or value</p> Source code in <code>fedbiomed/researcher/secagg/_secagg_context.py</code> <pre><code>def __init__(self, parties: List[str], job_id: str, secagg_id: Union[str, None] = None):\n\"\"\"Constructor of the class.\n    Args:\n        parties: list of parties participating in the secagg context element setup, named\n            by their unique id (`node_id`, `researcher_id`).\n            There must be at least 3 parties, and the first party is this researcher\n        job_id: ID of the job to which this secagg context element is attached.\n        secagg_id: optional secagg context element ID to use for this element.\n            Default is None, which means a unique element ID will be generated.\n    Raises:\n        FedbiomedSecaggError: bad argument type or value\n    \"\"\"\nsuper().__init__(parties, job_id, secagg_id)\nif not self._job_id:\nerrmess = f'{ErrorNumbers.FB415.value}: bad parameter `job_id` must be non empty string'\nlogger.error(errmess)\nraise FedbiomedSecaggError(errmess)\nself._element = SecaggElementTypes.SERVER_KEY\nself._secagg_manager = _SKManager\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecureAggregation","title":"SecureAggregation","text":"CLASS  <pre><code>SecureAggregation(active=True, timeout=10, clipping_range=None)\n</code></pre> <p>Secure aggregation controller of researcher component.</p> <p>This class is responsible for;</p> <ul> <li>setting up the context for Joye-Libert secure aggregation</li> <li>Applying secure aggregation after receiving encrypted model parameters from nodes</li> </ul> <p>Attributes:</p> Name Type Description <code>timeout</code> <code>int</code> <p>Maximum time waiting for answers from other nodes for each secagg context element (server key and biprime).</p> <code>clipping_range</code> <code>Optional[int]</code> <p>Clipping range that will be used for quantization of model parameters on the node side.</p> <code>_biprime</code> <code>Optional[SecaggBiprimeContext]</code> <p>Biprime-key context setup instance.</p> <code>_parties</code> <code>Optional[List[str]]</code> <p>Nodes and researcher that participates federated training</p> <code>_job_id</code> <code>Optional[str]</code> <p>ID of the current Job launched by the experiment.</p> <code>_servkey</code> <code>Optional[SecaggServkeyContext]</code> <p>Server-key context setup instance.</p> <code>_secagg_crypter</code> <code>SecaggCrypter</code> <p>Secure aggregation encrypter and decrypter to decrypt encrypted model parameters.</p> <code>_secagg_random</code> <code>Optional[float]</code> <p>Random float generated tobe sent to node to validate secure aggregation after aggregation encrypted parameters.</p> <p>Assigns default values for attributes</p> <p>Parameters:</p> Name Type Description Default <code>active</code> <code>bool</code> <p>True if secure aggregation is activated for the experiment</p> <code>True</code> <code>timeout</code> <code>int</code> <p>Maximum time waiting for answers from other nodes for each secagg context element (server key and biprime). Thus total secagg setup is at most twice the <code>timeout</code>, plus the local setup payload execution time for server key and biprime. Defaults to <code>environ['TIMEOUT']</code> if unset or equals 0.</p> <code>10</code> <code>clipping_range</code> <code>Union[None, int]</code> <p>Clipping range that will be used for quantization of model parameters on the node side. The default will be <code>VEParameters.CLIPPING_RANGE</code>. The default value will be automatically set on the node side.</p> <code>None</code> <p>Raises:</p> Type Description <code>FedbiomedSecureAggregationError</code> <p>bad argument type</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def __init__(\nself,\nactive: bool = True,\ntimeout: int = 10,\nclipping_range: Union[None, int] = None,\n) -&gt; None:\n\"\"\"Class constructor\n    Assigns default values for attributes\n    Args:\n        active: True if secure aggregation is activated for the experiment\n        timeout: Maximum time waiting for answers from other nodes for each\n            secagg context element (server key and biprime). Thus total secagg\n            setup is at most twice the `timeout`, plus the local setup payload\n            execution time for server key and biprime. Defaults to `environ['TIMEOUT']`\n            if unset or equals 0.\n        clipping_range: Clipping range that will be used for quantization of model\n            parameters on the node side. The default will be\n            [`VEParameters.CLIPPING_RANGE`][fedbiomed.common.constants.VEParameters].\n            The default value will be automatically set on the node side.\n    Raises:\n        FedbiomedSecureAggregationError: bad argument type\n    \"\"\"\nif not isinstance(active, bool):\nraise FedbiomedSecureAggregationError(\nf\"{ErrorNumbers.FB417.value}: The argument `active` should be  bool of type, \"\nf\"but got {type(active)} \"\n)\nif not isinstance(timeout, int):\nraise FedbiomedSecureAggregationError(\nf\"{ErrorNumbers.FB417.value}: The argument `timeout` should be  an integer, \"\nf\"but got {type(active)} \"\n)\nif clipping_range is not None and \\\n            (not isinstance(clipping_range, int) or isinstance(clipping_range, bool)):\nraise FedbiomedSecureAggregationError(\nf\"{ErrorNumbers.FB417.value}: Clipping range should be None or an integer, \"\nf\"but got not {type(clipping_range)}\"\n)\nself.timeout: int = timeout\nself.clipping_range: Optional[int] = clipping_range\nself._active: bool = active\nself._parties: Optional[List[str]] = None\nself._job_id: Optional[str] = None\nself._servkey: Optional[SecaggServkeyContext] = None\nself._biprime: Optional[SecaggBiprimeContext] = None\nself._secagg_random: Optional[float] = None\nself._secagg_crypter: SecaggCrypter = SecaggCrypter()\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecureAggregation-attributes","title":"Attributes","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secure_aggregation.SecureAggregation.active","title":"active     <code>property</code>","text":"<pre><code>active: bool\n</code></pre> <p>Gets secagg activation status</p> <p>Returns:</p> Type Description <code>bool</code> <p>bool, True if secagg is activated</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secure_aggregation.SecureAggregation.biprime","title":"biprime     <code>property</code>","text":"<pre><code>biprime: Union[None, SecaggBiprimeContext]\n</code></pre> <p>Gets biprime object</p> <p>Returns:</p> Type Description <code>Union[None, SecaggBiprimeContext]</code> <p>Biprime object, None if biprime is not setup</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secure_aggregation.SecureAggregation.clipping_range","title":"clipping_range     <code>instance-attribute</code>","text":"<pre><code>clipping_range: Optional[int] = clipping_range\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secure_aggregation.SecureAggregation.job_id","title":"job_id     <code>property</code>","text":"<pre><code>job_id: Union[str, None]\n</code></pre> <p>Gets secagg associated job_id</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>str of associated job_id if it exists, or None</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secure_aggregation.SecureAggregation.parties","title":"parties     <code>property</code>","text":"<pre><code>parties: Union[List[str], None]\n</code></pre> <p>Gets secagg parties</p> <p>Returns:</p> Type Description <code>Union[List[str], None]</code> <p>List of secagg parties if it exists, or None</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secure_aggregation.SecureAggregation.servkey","title":"servkey     <code>property</code>","text":"<pre><code>servkey: Union[None, SecaggServkeyContext]\n</code></pre> <p>Gets servkey object</p> <p>Returns:</p> Type Description <code>Union[None, SecaggServkeyContext]</code> <p>Servkey object, None if servkey is not setup</p>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secure_aggregation.SecureAggregation.timeout","title":"timeout     <code>instance-attribute</code>","text":"<pre><code>timeout: int = timeout\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg.SecureAggregation-functions","title":"Functions","text":""},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secure_aggregation.SecureAggregation.activate","title":"<pre><code>activate(status)\n</code></pre>","text":"<p>Set activate status of secure aggregation</p> <p>Returns:</p> Type Description <code>bool</code> <p>Status of secure aggregation True if it is activated</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def activate(self, status) -&gt; bool:\n\"\"\"Set activate status of secure aggregation\n    Returns:\n        Status of secure aggregation True if it is activated\n    \"\"\"\nif not isinstance(status, bool):\nraise FedbiomedSecureAggregationError(\nf\"{ErrorNumbers.FB417.value}: The argument `status` for activation should be True or False, \"\nf\"but got {type(status)} \"\n)\nself._active = status\nreturn self._active\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secure_aggregation.SecureAggregation.aggregate","title":"<pre><code>aggregate(round_, total_sample_size, model_params, encryption_factors=None)\n</code></pre>","text":"<p>Aggregates given model parameters</p> <p>Parameters:</p> Name Type Description Default <code>round_</code> <code>int</code> <p>current training round number</p> required <code>total_sample_size</code> <code>int</code> <p>sum of number of samples used by all nodes</p> required <code>model_params</code> <code>Dict[str, List[int]]</code> <p>model parameters from the participating nodes</p> required <code>encryption_factors</code> <code>Union[Dict[str, List[int]], None]</code> <p>encryption factors from the participating nodes</p> <code>None</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>Aggregated parameters</p> <p>Raises:</p> Type Description <code>FedbiomedSecureAggregationError</code> <p>secure aggregation context not properly configured</p> <code>FedbiomedSecureAggregationError</code> <p>secure aggregation computation error</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def aggregate(\nself,\nround_: int,\ntotal_sample_size: int,\nmodel_params: Dict[str, List[int]],\nencryption_factors: Union[Dict[str, List[int]], None] = None,\n) -&gt; List[float]:\n\"\"\"Aggregates given model parameters\n    Args:\n        round_: current training round number\n        total_sample_size: sum of number of samples used by all nodes\n        model_params: model parameters from the participating nodes\n        encryption_factors: encryption factors from the participating nodes\n    Returns:\n        Aggregated parameters\n    Raises:\n        FedbiomedSecureAggregationError: secure aggregation context not properly configured\n        FedbiomedSecureAggregationError: secure aggregation computation error\n    \"\"\"\nif self._biprime is None or self._servkey is None:\nraise FedbiomedSecureAggregationError(\nf\"{ErrorNumbers.FB417.value}: Can not aggregate parameters, one of Biprime or Servkey context is\"\nf\"not configured. Please setup secure aggregation before the aggregation.\")\nif not self._biprime.status or not self._servkey.status:\nraise FedbiomedSecureAggregationError(\nf\"{ErrorNumbers.FB417.value}: Can not aggregate parameters, one of Biprime or Servkey context is\"\nf\"not set properly\")\nbiprime = self._biprime.context[\"context\"][\"biprime\"]\nkey = self._servkey.context[\"context\"][\"server_key\"]\nnum_nodes = len(model_params)\naggregate = functools.partial(self._secagg_crypter.aggregate,\ncurrent_round=round_,\nnum_nodes=num_nodes,\nkey=key,\ntotal_sample_size=total_sample_size,\nbiprime=biprime,\nclipping_range=self.clipping_range)\n# Validate secure aggregation\nif self._secagg_random is not None:\nif encryption_factors is None:\nraise FedbiomedSecureAggregationError(\nf\"{ErrorNumbers.FB417.value}: Secure aggregation random validation has been set but the encryption \"\nf\"factors are not provided. Please provide encrypted `secagg_random` values in different parties. \"\nf\"Or to not set/get `secagg_random()` before the aggregation.\")\nlogger.info(\"Validating secure aggregation results...\")\nencryption_factors = [f for k, f in encryption_factors.items()]\nvalidation: List[float] = aggregate(params=encryption_factors)\nif len(validation) != 1 or not math.isclose(validation[0], self._secagg_random, abs_tol=0.03):\nraise FedbiomedSecureAggregationError(\nf\"{ErrorNumbers.FB417.value}: Aggregation is failed due to incorrect decryption.\"\n)\nlogger.info(\"Validation is completed.\")\nelif encryption_factors is not None:\nlogger.warning(\"Encryption factors are provided while secagg random is None. Please make sure secure \"\n\"aggregation steps are applied correctly.\")\nlogger.info(\"Aggregating encrypted parameters. This process may take some time depending on model size.\")\n# Aggregate parameters\nparams = [p for _, p in model_params.items()]\naggregated_params = aggregate(params=params)\nreturn aggregated_params\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secure_aggregation.SecureAggregation.load_state","title":"<pre><code>load_state(state)\n</code></pre>  <code>classmethod</code>","text":"<p>Create a <code>SecureAggregation</code> object from a saved state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Dict</code> <p>saved state to restore in the created object</p> required <p>Returns:</p> Type Description <code>SecureAggregation</code> <p>The created <code>SecureAggregation</code> object</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>@classmethod\ndef load_state(\ncls,\nstate: Dict\n) -&gt; 'SecureAggregation':\n\"\"\"Create a `SecureAggregation` object from a saved state\n    Args:\n        state: saved state to restore in the created object\n    Returns:\n        The created `SecureAggregation` object\n    \"\"\"\nsecagg = cls(**state[\"arguments\"])\nif state[\"attributes\"][\"_biprime\"] is not None:\nstate[\"attributes\"][\"_biprime\"] = SecaggBiprimeContext. \\\n            load_state(state=state[\"attributes\"][\"_biprime\"])\nif state[\"attributes\"][\"_servkey\"] is not None:\nstate[\"attributes\"][\"_servkey\"] = SecaggServkeyContext. \\\n            load_state(state=state[\"attributes\"][\"_servkey\"])\n# Set attributes\nfor name, val in state[\"attributes\"].items():\nsetattr(secagg, name, val)\nreturn secagg\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secure_aggregation.SecureAggregation.save_state","title":"<pre><code>save_state()\n</code></pre>","text":"<p>Saves state of the secagg</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The secagg state to be saved</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def save_state(self) -&gt; Dict[str, Any]:\n\"\"\"Saves state of the secagg\n    Returns:\n        The secagg state to be saved\n    \"\"\"\nstate = {\n\"class\": type(self).__name__,\n\"module\": self.__module__,\n\"arguments\": {\n'active': self._active,\n'timeout': self.timeout,\n'clipping_range': self.clipping_range,\n},\n\"attributes\": {\n\"_biprime\": self._biprime.save_state() if self._biprime is not None else None,\n\"_servkey\": self._servkey.save_state() if self._servkey is not None else None,\n\"_job_id\": self._job_id,\n\"_parties\": self._parties\n}\n}\nreturn state\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secure_aggregation.SecureAggregation.setup","title":"<pre><code>setup(parties, job_id, force=False)\n</code></pre>","text":"<p>Setup secure aggregation instruments.</p> <p>Requires setting <code>parties</code> and <code>job_id</code> if they are not set in previous secagg setups. It is possible to execute without any argument if SecureAggregation has already <code>parties</code> and <code>job_id</code> defined. This feature provides researcher execute <code>secagg.setup()</code> if any connection issue</p> <p>Parameters:</p> Name Type Description Default <code>parties</code> <code>List[str]</code> <p>Parties that participates secure aggregation</p> required <code>job_id</code> <code>str</code> <p>The id of the job of experiment</p> required <code>force</code> <code>bool</code> <p>Forces secagg setup even context is already existing</p> <code>False</code> <p>Returns:</p> Type Description <p>Status of setup</p> <p>Raises     FedbiomedSecureAggregationError: Invalid argument type</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def setup(self,\nparties: List[str],\njob_id: str,\nforce: bool = False):\n\"\"\"Setup secure aggregation instruments.\n    Requires setting `parties` and `job_id` if they are not set in previous secagg\n    setups. It is possible to execute without any argument if SecureAggregation\n    has already `parties` and `job_id` defined. This feature provides researcher\n    execute `secagg.setup()` if any connection issue\n    Args:\n        parties: Parties that participates secure aggregation\n        job_id: The id of the job of experiment\n        force: Forces secagg setup even context is already existing\n    Returns:\n        Status of setup\n    Raises\n        FedbiomedSecureAggregationError: Invalid argument type\n    \"\"\"\nif not isinstance(parties, list):\nraise FedbiomedSecureAggregationError(\nf\"{ErrorNumbers.FB417.value}: Expected argument `parties` list but got {type(parties)}\"\n)\nif not isinstance(job_id, str):\nraise FedbiomedSecureAggregationError(\nf\"{ErrorNumbers.FB417.value}: Expected argument `job_id` string but got {type(parties)}\"\n)\nself._configure_round(parties, job_id)\nif self._biprime is None or self._servkey is None:\nraise FedbiomedSecureAggregationError(\nf\"{ErrorNumbers.FB417.value}: server key or biprime contexts is not fully configured.\"\n)\nif not self._biprime.status or force:\nself._biprime.setup(timeout=self.timeout)\nif not self._servkey.status or force:\nself._servkey.setup(timeout=self.timeout)\nreturn True\n</code></pre>"},{"location":"developer/api/researcher/secagg/#fedbiomed.researcher.secagg._secure_aggregation.SecureAggregation.train_arguments","title":"<pre><code>train_arguments()\n</code></pre>","text":"<p>Gets train arguments for secagg train request</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Arguments that is going tobe attached to the experiment.</p> Source code in <code>fedbiomed/researcher/secagg/_secure_aggregation.py</code> <pre><code>def train_arguments(self) -&gt; Dict:\n\"\"\"Gets train arguments for secagg train request\n    Returns:\n        Arguments that is going tobe attached to the experiment.\n    \"\"\"\nreturn {'secagg_servkey_id': self._servkey.secagg_id if self._servkey is not None else None,\n'secagg_biprime_id': self._biprime.secagg_id if self._biprime is not None else None,\n'secagg_random': self._secagg_random,\n'secagg_clipping_range': self.clipping_range}\n</code></pre>"},{"location":"developer/api/researcher/strategies/","title":"Strategies","text":""},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies","title":"fedbiomed.researcher.strategies","text":"Module: <code>fedbiomed.researcher.strategies</code>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies-classes","title":"Classes","text":""},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.DefaultStrategy","title":"DefaultStrategy","text":"CLASS  <pre><code>DefaultStrategy(data)\n</code></pre> <p>           Bases: <code>Strategy</code></p> <p>Default strategy to be used when sampling/selecting nodes and checking whether nodes have responded or not</p> <p>Strategy is: - select all node for each round - raise an error if one node does not answer - raise an error is one node returns an error</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FederatedDataSet</code> <p>Object that includes all active nodes and the meta-data of the dataset that is going to be used for federated training. Should be passed to <code>super().__init__</code> to initialize parent class</p> required Source code in <code>fedbiomed/researcher/strategies/default_strategy.py</code> <pre><code>def __init__(self, data: FederatedDataSet):\n\"\"\" Constructor of Default Strategy\n    Args:\n        data: Object that includes all active nodes and the meta-data of the dataset that is going to be\n            used for federated training. Should be passed to `super().__init__` to initialize parent class\n    \"\"\"\nsuper().__init__(data)\n</code></pre>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.DefaultStrategy-functions","title":"Functions","text":""},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.default_strategy.DefaultStrategy.refine","title":"<pre><code>refine(training_replies, round_i)\n</code></pre>","text":"<p>The method where node selection is completed by extracting parameters and length from the training replies</p> <p>Parameters:</p> Name Type Description Default <code>training_replies</code> <code>Responses</code> <p>is a list of elements of type  Response( { 'success': m['success'],              'msg': m['msg'],              'dataset_id': m['dataset_id'],              'node_id': m['node_id'],              'params_path': params_path,              'params': params,              'sample_size': sample_size              } )</p> required <code>round_i</code> <code>int</code> <p>Current round of experiment</p> required <p>Returns:</p> Name Type Description <code>weights</code> <code>Dict[str, float]</code> <p>Proportions list, each element of this list represents a dictionary with its only key as the node_id and its value the proportion of lines the node has with respect to the whole,</p> <code>model_params</code> <code>Dict[str, Dict[str, Union[torch.Tensor, numpy.ndarray]]]</code> <p>list with each element representing a dictionary. Its only key represents the node_id and the corresponding value is a dictionary containing list of weight matrices of every node : [{\"n1\":{\"layer1\":m1,\"layer2\":m2},{\"layer3\":\"m3\"}},{\"n2\": ...}] Including the node_id is useful for the proper functioning of some strategies like Scaffold : At each round, local model params are linked to a certain correction. The correction is updated every round. The computation of correction states at round i is dependant to client states and correction states of round i-1. Since training_replies can potentially order the node replies differently from round to round, the bridge between all these parameters is represented by the node_id.</p> <code>total_rows</code> <code>int</code> <p>sum of number of samples used by all nodes</p> <code>encryption_factors</code> <code>Dict[str, List[int]]</code> <p>encryption factors from the participating nodes</p> <p>Raises:</p> Type Description <code>FedbiomedStrategyError</code> <ul> <li>Miss-matched in answered nodes and existing nodes</li> <li>If not all nodes successfully completes training</li> <li>if a Node has not sent <code>sample_size</code> value in the TrainingReply, making it impossible to compute aggregation weights.</li> </ul> Source code in <code>fedbiomed/researcher/strategies/default_strategy.py</code> <pre><code>def refine(\nself,\ntraining_replies: Responses,\nround_i: int\n) -&gt; Tuple[Dict[str, float],\nDict[str, Dict[str, Union['torch.Tensor', 'numpy.ndarray']]],\nint,\nDict[str, List[int]]]:\n\"\"\"\n    The method where node selection is completed by extracting parameters and length from the training replies\n    Args:\n        training_replies: is a list of elements of type\n             Response( { 'success': m['success'],\n                         'msg': m['msg'],\n                         'dataset_id': m['dataset_id'],\n                         'node_id': m['node_id'],\n                         'params_path': params_path,\n                         'params': params,\n                         'sample_size': sample_size\n                         } )\n        round_i: Current round of experiment\n    Returns:\n        weights: Proportions list, each element of this list represents a dictionary with its only key as\n            the node_id and its value the proportion of lines the node has with respect to the whole,\n        model_params: list with each element representing a dictionary. Its only key represents the node_id\n            and the corresponding value is a dictionary containing list of weight matrices of every node : [{\"n1\":{\"layer1\":m1,\"layer2\":m2},{\"layer3\":\"m3\"}},{\"n2\": ...}]\n            Including the node_id is useful for the proper functioning of some strategies like Scaffold :\n            At each round, local model params are linked to a certain correction. The correction is updated every round.\n            The computation of correction states at round i is dependant to client states and correction states of round i-1.\n            Since training_replies can potentially order the node replies differently from round to round, the bridge between\n            all these parameters is represented by the node_id.\n        total_rows: sum of number of samples used by all nodes\n        encryption_factors: encryption factors from the participating nodes\n    Raises:\n        FedbiomedStrategyError: - Miss-matched in answered nodes and existing nodes\n            - If not all nodes successfully completes training\n            - if a Node has not sent `sample_size` value in the TrainingReply, making it\n            impossible to compute aggregation weights.\n    \"\"\"\n# check that all nodes answered\ncl_answered = [val['node_id'] for val in training_replies.data()]\nanswers_count = 0\nif self._sampling_node_history.get(round_i) is None:\nraise FedbiomedStrategyError(ErrorNumbers.FB408.value + f\": Missing Nodes Responses for round: {round_i}\")\nfor cl in self._sampling_node_history[round_i]:\nif cl in cl_answered:\nanswers_count += 1\nelse:\n# this node did not answer\nlogger.error(ErrorNumbers.FB408.value +\n\" (node = \" +\ncl +\n\")\"\n)\nif len(self._sampling_node_history[round_i]) != answers_count:\nif answers_count == 0:\n# none of the nodes answered\nmsg = ErrorNumbers.FB407.value\nelse:\nmsg = ErrorNumbers.FB408.value\nlogger.critical(msg)\nraise FedbiomedStrategyError(msg)\n# check that all nodes that answer could successfully train\nself._success_node_history[round_i] = []\nall_success = True\nmodel_params = {}\nsample_sizes = {}\nencryption_factors = {}\ntotal_rows = 0\nfor tr in training_replies:\nif tr['success'] is True:\n# TODO: Attach sample_size, weights and params in a single dict object\nmodel_params[tr[\"node_id\"]] = tr[\"params\"]\nencryption_factors[tr[\"node_id\"]] = tr.get(\"encryption_factor\", None)\nif tr[\"sample_size\"] is None:\n# if a Node `sample_size` is None, we cannot compute the weigths: in this case\n# return an error\nraise FedbiomedStrategyError(ErrorNumbers.FB402.value + f\" : Node {tr['node_id']} did not return \" +\n\"any `sample_size` value (number of samples seen during one Round),\" +\n\" can not compute weigths for the aggregation. Aborting\")\nsample_sizes[tr[\"node_id\"]] = tr[\"sample_size\"]\ntotal_rows += tr['sample_size']\nself._success_node_history[round_i].append(tr['node_id'])\nelse:\nall_success = False\nlogger.error(f\"{ErrorNumbers.FB409.value} (node = {tr['node_id']} )\")\nif not all_success:\nraise FedbiomedStrategyError(ErrorNumbers.FB402.value)\nweights = {node_id: sample_size / total_rows if total_rows != 0 else 1 / len(sample_sizes)\nfor node_id, sample_size in sample_sizes.items()}\nlogger.info(f\"Nodes that successfully reply in round {round_i} {self._success_node_history[round_i]}\")\nreturn model_params, weights, total_rows, encryption_factors\n</code></pre>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.default_strategy.DefaultStrategy.sample_nodes","title":"<pre><code>sample_nodes(round_i)\n</code></pre>","text":"<p>Samples and selects nodes on which to train local model. In this strategy we will consider all existing nodes</p> <p>Parameters:</p> Name Type Description Default <code>round_i</code> <code>int</code> <p>number of round.</p> required <p>Returns:</p> Name Type Description <code>node_ids</code> <code>List[uuid.UUID]</code> <p>list of all node ids considered for training during this round <code>round_i</code>.</p> Source code in <code>fedbiomed/researcher/strategies/default_strategy.py</code> <pre><code>def sample_nodes(self, round_i: int) -&gt; List[uuid.UUID]:\n\"\"\" Samples and selects nodes on which to train local model. In this strategy we will consider all existing\n    nodes\n    Args:\n        round_i: number of round.\n    Returns:\n        node_ids: list of all node ids considered for training during\n            this round `round_i`.\n    \"\"\"\nself._sampling_node_history[round_i] = self._fds.node_ids()\nreturn self._fds.node_ids()\n</code></pre>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.Strategy","title":"Strategy","text":"CLASS  <pre><code>Strategy(data)\n</code></pre> <p>Default Strategy as Parent class. Custom strategy classes must inherit from this parent class.</p> <pre><code>    used for federated training.\n</code></pre> Source code in <code>fedbiomed/researcher/strategies/strategy.py</code> <pre><code>def __init__(self, data: FederatedDataSet):\n\"\"\"\n    Args:\n        data: Object that includes all active nodes and the meta-data of the dataset that is going to be\n            used for federated training.\n    \"\"\"\nself._fds = data\nself._sampling_node_history = {}\nself._success_node_history = {}\nself._parameters = None\n</code></pre>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.Strategy-functions","title":"Functions","text":""},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.strategy.Strategy.load_state","title":"<pre><code>load_state(state=None, kwargs)\n</code></pre>","text":"<p>Method for loading strategy state from breakpoint state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Dict[str, Any]</code> <p>The state that will be loaded</p> <code>None</code> Source code in <code>fedbiomed/researcher/strategies/strategy.py</code> <pre><code>def load_state(self, state: Dict[str, Any] = None, **kwargs):\n\"\"\"\n    Method for loading strategy state from breakpoint state\n    Args:\n        state: The state that will be loaded\n    \"\"\"\n# fds may be modified and diverge from Experiment\nself._fds = FederatedDataSet(state.get('fds'))\nself._parameters = state['parameters']\n</code></pre>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.strategy.Strategy.refine","title":"<pre><code>refine(training_replies, round_i)\n</code></pre>","text":"<p>Abstract method that must be implemented by child class</p> <p>Parameters:</p> Name Type Description Default <code>training_replies</code> <code>Responses</code> <p>is a list of elements of type  Response( { 'success': m['success'],              'msg': m['msg'],              'dataset_id': m['dataset_id'],              'node_id': m['node_id'],              'params_path': params_path,              'params': params } )</p> required <code>round_i</code> <code>int</code> <p>Current round of experiment</p> required <p>Raises:</p> Type Description <code>FedbiomedStrategyError</code> <p>If method is not implemented by child class</p> Source code in <code>fedbiomed/researcher/strategies/strategy.py</code> <pre><code>def refine(self, training_replies: Responses, round_i: int) -&gt; tuple[list, list]:\n\"\"\"\n    Abstract method that must be implemented by child class\n    Args:\n        training_replies: is a list of elements of type\n             Response( { 'success': m['success'],\n                         'msg': m['msg'],\n                         'dataset_id': m['dataset_id'],\n                         'node_id': m['node_id'],\n                         'params_path': params_path,\n                         'params': params } )\n        round_i: Current round of experiment\n    Raises:\n        FedbiomedStrategyError: If method is not implemented by child class\n    \"\"\"\nmsg = ErrorNumbers.FB402.value + \\\n        \": refine method should be overloaded by the provided strategy\"\nlogger.critical(msg)\nraise FedbiomedStrategyError(msg)\n</code></pre>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.strategy.Strategy.sample_nodes","title":"<pre><code>sample_nodes(round_i)\n</code></pre>","text":"<p>Abstract method that must be implemented by child class</p> <p>Parameters:</p> Name Type Description Default <code>round_i</code> <code>int</code> <p>Current round of experiment</p> required Source code in <code>fedbiomed/researcher/strategies/strategy.py</code> <pre><code>def sample_nodes(self, round_i: int):\n\"\"\"\n    Abstract method that must be implemented by child class\n    Args:\n        round_i: Current round of experiment\n    \"\"\"\nmsg = ErrorNumbers.FB402.value + \\\n        \": sample nodes method should be overloaded by the provided strategy\"\nlogger.critical(msg)\nraise FedbiomedStrategyError(msg)\n</code></pre>"},{"location":"developer/api/researcher/strategies/#fedbiomed.researcher.strategies.strategy.Strategy.save_state","title":"<pre><code>save_state()\n</code></pre>","text":"<p>Method for saving strategy state for saving breakpoints</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The state of the strategy</p> Source code in <code>fedbiomed/researcher/strategies/strategy.py</code> <pre><code>def save_state(self) -&gt; Dict[str, Any]:\n\"\"\"\n    Method for saving strategy state for saving breakpoints\n    Returns:\n        The state of the strategy\n    \"\"\"\nstate = {\n\"class\": type(self).__name__,\n\"module\": self.__module__,\n\"parameters\": self._parameters,\n\"fds\": self._fds.data()\n}\nreturn state\n</code></pre>"},{"location":"getting-started/fedbiomed-architecture/","title":"Fedbiomed Main components","text":"<p>Fed-BioMed has three main components ensuring the correct execution of federated learning algorithms.These components  are <code>Node</code>, <code>Researcher</code> and <code>Network</code>, and are defined in the following sections:</p>"},{"location":"getting-started/fedbiomed-architecture/#node","title":"<code>Node</code>","text":"<p>In Fed-BioMed, <code>Node</code> provides 2 main functionalities: </p> <ul> <li><code>Node</code> stores datasets, upon which the Federated Learning Model will be trained. Datasets paths towards files and folders in a TinyDB database, as well as other metadata such as datatype, data shape, ... .</li> <li><code>Node</code> trains a model upon a <code>TrainRequest</code> (sent by <code>Researcher</code>), and send it back to the <code>Researcher</code> once training is completed.</li> </ul> <p><code>Node</code> is a client that is responsible for sending replies in response to Researcher requests. Since <code>Node</code> is not a  server, communication between node and researcher is provided through <code>Network</code> component. When a node is started, it directly connects to the <code>Network</code> component.</p> <p>More details about <code>Node</code> installation can be found here.</p>"},{"location":"getting-started/fedbiomed-architecture/#researcher","title":"<code>Researcher</code>","text":"<p>In Fed-BioMed, a <code>Researcher</code> is an entity that defines the Federated Learning model. In order to define a model, <code>Researcher</code> has to provide 3 elements:</p> <ul> <li>a <code>TrainingPlan</code>(containing a model, method to load/pre-process data and dependencies)</li> <li>a <code>Strategy</code>, which defines how nodes are selected / sampled while training a Federated Mode</li> <li>an <code>Aggregator</code>, which purpose is to aggregate the local model coming from each node into an aggregated model. Aggregation is performed on <code>Researcher</code> side. <code>Researcher</code> can be run using plain Python scripts or Jupyter Notebook (thus in an interactive fashion).</li> </ul> <p><code>Researcher</code> orchestrates the training, by sending Requests to <code>Nodes</code>. Thus, for each Request sent by the <code>Researcher</code>,  <code>Node</code> must answer back with an appropriate Reply.</p> <p>More details about <code>Researcher</code> and its installation can be found here.</p>"},{"location":"getting-started/fedbiomed-architecture/#network","title":"<code>Network</code>","text":"<p><code>Network</code> is an entity connecting researcher to the nodes. It provides a Restful HTTP server used for sending TrainingPlan  and model weights between <code>Nodes</code> and <code>Researcher</code>, and a MQTT server for short and fast message and <code>Requests</code> exchange  between <code>Node</code> and <code>Researcher</code>. Furthermore, <code>MQTT</code> server enables each <code>Node</code> to communicate with other <code>Nodes</code>  without passing by the <code>Researcher</code>, permitting much more flexibility than other communication protocols such as gRPC. </p> <p><code>Network</code> should work as a Trusted Third Party when considering advanced security options &amp; protocols. More details about its configuration and deployment can be found here.</p>"},{"location":"getting-started/fedbiomed-architecture/#fed-biomed-architecture","title":"Fed-BioMed Architecture","text":"<p>Relationship between each main component aforementioned is detailed in the figure below:</p> <p> Fed-BioMed Architecture, with three main components: Nodes; containing datasets to be   used for training models,  Researcher; running the training of the model and Network; connecting Nodes to Researcher.</p> <p>As shown in the diagram, <code>Network</code> is a central component in Fed-BioMed, that links <code>Nodes</code> to <code>Researcher</code>, and ensures  message and files delivery. <code>Nodes</code> are in charge of running the model sent by the <code>Researcher</code>, and send the resulting  trained model to the <code>Researcher</code>. Large files such as TrainingPlan and model parameters are exchanged over a Restful  HTTP server whereas messages, Requests and Replies are sent through a MQTT server.</p>"},{"location":"getting-started/fedbiomed-architecture/#network-configuration","title":"<code>Network</code> configuration","text":"<p>For information on how to configure <code>Network</code>,  please follow <code>Network</code> configuration steps</p>"},{"location":"getting-started/fedbiomed-architecture/#node-configuration","title":"<code>Node</code> configuration","text":"<p>For information on how to configure <code>Node</code>,  please follow <code>Node</code> configuration steps</p>"},{"location":"getting-started/fedbiomed-architecture/#researcher-configuration","title":"<code>Researcher</code> configuration","text":"<p>For information on how to configure <code>Researcher</code>,  please follow <code>Researcher</code> configuration steps</p>"},{"location":"getting-started/fedbiomed-workflow/","title":"Fed-BioMed Workflow","text":"<p>We present in the following page a short step-by-step illustration detailing the workflow of Fed-BioMed.</p> <p>The steps are:</p> <ol> <li>Setting up the <code>Network</code> and <code>Nodes</code></li> <li>Deploying dataset on <code>Nodes</code> </li> <li>Write a Federated Model</li> <li>Run and monitor a Federated Model</li> <li>Model retrieval and evaluation</li> </ol> <p>For installation, please visit the software installation page.</p>"},{"location":"getting-started/fedbiomed-workflow/#step-1-setting-up-network-and-nodes","title":"Step 1: Setting up <code>Network</code> and <code>Nodes</code>.","text":"<p>In order to run Fed-BioMed, you need to start first the <code>Network</code> component and then one or several <code>Nodes</code>. When setting up the <code>Nodes</code>, each of them will connect to the <code>Network</code>, as shown in the diagram below (Diagram 1). </p> <p> Diagram 1: <code>Nodes</code> and <code>Network</code> in Fed-BioMed, and their interactions with the other components.</p>"},{"location":"getting-started/fedbiomed-workflow/#step-2-deploying-a-dataset-on-the-nodes","title":"Step 2: Deploying a dataset on the <code>Nodes</code>.","text":""},{"location":"getting-started/fedbiomed-workflow/#step-21-loading-a-dataset-into-a-node","title":"Step 2.1: Loading a dataset into a <code>Node</code>.","text":"<p>Fed-BioMed supports standard data sources, such .csv files and image folders, and provides specific tools for loading medical data formats, such as  medical imaging, signals and genomics information (Diagram 2).</p> <p> *Diagram 2: loading data into a <code>Node</code>. Different data types are available, especially for medical datasets. Folder and Genes icons courtesy of Freepik, Clinical icon courtesy of Parzival' 1997, Flaticon.</p> <p>Once provided with a dataset, a <code>Node</code> is able to train the models sent by the <code>Researcher</code>.</p> <p> Diagram 3: <code>Nodes</code> with respective datasets loaded.</p>"},{"location":"getting-started/fedbiomed-workflow/#step-22-retrieving-nodes-dataset-information-on-the-researcher-side","title":"Step 2.2: Retrieving <code>Nodes</code> dataset information on the <code>Researcher</code> side.","text":"<p>It is possible for the <code>Researcher</code> to obtain information about the dataset of each <code>Node</code>, as shown in the diagram 4 below.  * Diagram 4: <code>Node</code> datasets information that <code>Researcher</code> can retrieve. The researcher can access datasets' metadata such as datasets name, dataset data_type, dataset tags, description and shape stored on each node.*</p>"},{"location":"getting-started/fedbiomed-workflow/#step-3-write-a-federated-model-trainingplan-aggregator-and-strategy","title":"Step 3: Write a federated Model (<code>TrainingPlan</code>, <code>Aggregator</code> and <code>Strategy</code>)","text":"<p>To create a Federated Model <code>Experiment</code> in Fed-BioMed, three principal ingredients must be provided:</p> <ol> <li>a <code>Training Plan</code>, which is basically a Python class, containing the model definition and related objects, such as cost function and optimizer, and eventually methods for pre-processing (e.g., data standardization and/or imputation), and post-processing (run after that the training of the model on the <code>Node</code> is completed).</li> <li>an <code>Aggregator</code> that defines how the model parameters obtained on each node after training are aggregated once received by the <code>Researcher</code>. Examples of <code>Aggregator</code> can be <code>FedProx</code> or <code>SCAFFOLD</code>.</li> <li>a <code>Strategy</code> that handles both node sampling and node management (e.g., how to deal with non responding nodes). </li> </ol> <p> Diagram 5: the ingredients needed to train a Federated Model in Fed-BioMed.</p>"},{"location":"getting-started/fedbiomed-workflow/#step-4-how-to-run-and-monitor-an-experiment","title":"Step 4:  how to run and monitor an <code>Experiment</code>","text":""},{"location":"getting-started/fedbiomed-workflow/#running-an-experiment","title":"Running an <code>Experiment</code>","text":"<p>The animation of Diagram 6 shows how a federated model is trained within Fed-BioMed: </p> <ol> <li>The global model is sent to the <code>Nodes</code> through the <code>Network</code>. The model's architecture is defined in a <code>TrainingPlan</code>, and weights are contained in a specific file exchanged over the <code>Network</code>;</li> <li>Each <code>Node</code> trains the model on the available local data;</li> <li>The resulting optimized local models are sent back to the <code>Researcher</code> through the <code>Network</code>;</li> <li>The shared local models are aggregated to form a new aggregated global model, according to the <code>Aggregator</code>.</li> </ol> <p> Diagram 6: animation showcasing an iteration of federated training in Fed-BioMed. The model defined in a <code>TrainingPlan</code> is sent to the <code>Nodes</code> and trained on their local data, to be subsequently aggregated. Grayed-out models represent an untrained model, while colored ones represent a model trained on local data.</p> <p>Diagram 7 provides a more technical description of the training process within Fed-BioMed:</p> <ol> <li>The <code>Researcher</code> initiates training round by issuing to the <code>Nodes</code> a TrainRequest through MQTT component of the <code>Network</code>, and by sending the model and the current global parameters through the Restful server component of the <code>Network</code>. </li> <li>Upon a TrainRequest, each <code>Node</code> trains the model and issues a TrainingReply to the Researcher passing through the MQTT of the <code>Network</code>, as well as the updated parameters (through the Restful server)</li> <li>Once updated, the model parameters coming from each Node are collected by the <code>Researcher</code>, and aggregated to create the new global model.</li> </ol> <p> Diagram 7: details of the Requests and Replies sent to each components when performing one round of training. </p>"},{"location":"getting-started/fedbiomed-workflow/#monitoring-an-experiment","title":"Monitoring an <code>Experiment</code>.","text":"<p>The loss evolution is sent back to the <code>Researcher</code> at each evaluation step during training. The <code>Researcher</code> can keep track of the loss using Tensorboard, as shown in DIagram 8.</p> <p> Diagram 8: model training monitoring facility available in Fed-BioMed</p>"},{"location":"getting-started/fedbiomed-workflow/#step-5-retrieving-the-model-and-performing-model-evaluation","title":"Step 5: retrieving the model and performing model evaluation","text":"<p>Once federated training is complete, the <code>Researcher</code> can retrieve the final global model, as well as other relevant information such as the timing between each connection, loss and the testing metrics value (if a validation dataset is provided). Fed-BioMed provides a number of standard metrics, such as accuracy for classification, or mean squarred error for regression, and allows the definition of custom ones. </p> <p> Diagram 9: model and results collected after training a model using Fed-BioMed framework. Icons courtesy of Ramy W. - Flaticon</p>"},{"location":"getting-started/fedbiomed-workflow/#going-further","title":"Going Further","text":"<p>Installation Guide </p> <p>Detailed steps on how to install Fed-BioMed on your computer.</p> <p>Tutorials </p> <p>More tutorials, examples and how-to.</p> <p><code>Nodes</code> configuration Guide </p> <p>Provides an exhaustive overview of Fed-BioMed <code>Nodes</code>.</p> <p><code>Researcher</code> configuration Guide </p> <p>Provides additional info on Fed-BioMed <code>Researcher</code>.</p>"},{"location":"getting-started/getting-started/","title":"Getting Started with Fed-BioMed - Fed-BioMed basic usage example","text":""},{"location":"getting-started/getting-started/#network","title":"<code>Network</code>","text":"<p><code>Network</code> is the Fed-BioMed component that allows communication between the other components (<code>Researcher</code> and <code>Node</code>s). To launch a <code>Network</code>, open in a terminal (from the <code>Fed-BioMed</code> repository):</p> <p><pre><code>./scripts/fedbiomed_run network start\n</code></pre> </p>"},{"location":"getting-started/getting-started/#node","title":"<code>Node</code>","text":"<p>In Federated Learning, a <code>Node</code> is a component on which sensitive data are stored. To run a node, nothing simpler! From <code>Fed-BioMed</code> repository, enter the following command on a terminal:</p> <pre><code>./scripts/fedbiomed_run node start\n</code></pre> <p>Your <code>Node</code> is now running. Next steps will be to add into the <code>Node</code>any datasets, each of them specified with a <code>Tag</code>, allowing any <code>Researcher</code> to search for <code>Node</code>s with specific dataset to train his/her model on. Further information about how to add data, how to configure your <code>Node</code>s and deploy it are given <code>Node</code> subsection.</p> <p></p>"},{"location":"getting-started/getting-started/#researcher","title":"<code>Researcher</code>","text":""},{"location":"getting-started/getting-started/#1-launch-your-researcher-environment","title":"1. Launch your <code>Researcher</code> environment","text":"<p>The <code>Researcher</code> is the component used for designing and training Federated models. In a terminal, enter:</p> <pre><code>source ./scripts/fedbiomed_environment researcher\n./scripts/fedbiomed_run researcher start\n</code></pre> <p>A jupyter-notebook will pop up, in which you can write your first Federated model.</p> <p></p>"},{"location":"getting-started/getting-started/#2-create-and-train-your-federated-learning-model","title":"2. Create and train your Federated Learning model","text":"<p>Models designed in Fed-BioMed using Pytorch are very similar to classic Pytorch models. </p> <p>Here we provide a code snippet in Pytorch (left) and its equivalent in Fed-BioMed (right). Basically, to define a Pytorch model in Fed-BioMed, create a class as you would do for a Pytorch model, and implement a <code>training_step</code> method for loss computation, a <code>training_data</code> method wrapping a <code>DataLoader</code> for data preprocessing. Finally select an <code>Aggregator</code> and a node selection strategy for Federated model training. <code>Researcher</code> have to specify on which dataset he/she wants to train their data using <code>tags</code>. That's all you have to know!</p> <p></p>"},{"location":"getting-started/getting-started/#template-of-a-model-definition-in-pytorch-left-and-in-fed-biomed-using-pytorch-right","title":"Template of a model definition in Pytorch (left) and in Fed-BioMed using Pytorch (right)","text":"Classical Pytorch Fed-BioMed"},{"location":"getting-started/getting-started/#template-of-a-model-training-loop-in-pytorch-left-and-in-fed-biomed-using-pytorch-right","title":"Template of a model training loop in Pytorch (left) and in Fed-BioMed using Pytorch (right)","text":"Classical Pytorch Fed-BioMed"},{"location":"getting-started/getting-started/#basic-full-example-for-jupyter-notebook","title":"Basic Full Example (for jupyter-notebook)","text":"<p>We present here a Multilayer Perceptron model in classic Pytorch and its Federated equivalent in Fed-BioMed.</p>"},{"location":"getting-started/getting-started/#multilayer-perceptron-in-pytorch-model-definition","title":"Multilayer Perceptron in Pytorch: model definition","text":"<pre><code>import os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass MyLocalTrainingPlan(nn.Module): \n    def __init__(self):\n        super(MyLocalTrainingPlan, self).__init__()\n        self.in_features = 28 * 28\n        self.out_features = 10\n\n        self.fc1 = nn.Linear(self.in_features, 50)\n        self.fc2 = nn.Linear(50, self.out_features)\n        # optimizer parameters\n        lr = 0.01\n        self.optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n\n    def forward(self, x):\n        x = x.reshape(-1, 28*28)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n    def training_step(self, data, target):\n        output = self.forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n\n    def training_data(self, batch_size = 48):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n\n        home = os.path.expanduser(\"~\")\n        path_file = os.path.join(home, 'data')\n        mnist_dataset = datasets.MNIST(path_file, train=True,\n                                       download=True, transform=transform)\n        data_loader = torch.utils.data.DataLoader(mnist_dataset,\n                                                  batch_size=batch_size,\n                                                   shuffle=True)\n        return data_loader\n</code></pre>"},{"location":"getting-started/getting-started/#multilayer-perceptron-in-fed-biomed-with-pytorch-model-definition-same-example-as-previous-one-but-transposed-to-fed-biomed-framework","title":"Multilayer Perceptron in Fed-BioMed with PyTorch: model definition (same example as previous one but transposed to Fed-BioMed Framework)","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\n\n\nclass MyRemoteTrainingPlan(TorchTrainingPlan): \n\n    def init_model(self):\n        return self.Net()\n\n    def init_optimizer(self):\n        return torch.optim.SGD(self.parameters(), lr=0.01)\n\n    def init_dependencies(self):\n        return [\"from torchvision import datasets, transforms\"]\n\n    class Net(nn.Module):\n        def __init__(self, model_args: dict = {}):\n            super().__init__()\n            self.in_features = 28*28\n            self.out_features = 10\n            self.fc1 = nn.Linear(self.in_features, 50)\n            self.fc2 = nn.Linear(50, self.out_features)\n\n\n        def forward(self, x):\n            x = x.reshape(-1, 28*28)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.fc2(x)\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_step(self, data, target):\n        output = self.forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n\n    def training_data(self, batch_size = 48):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        mnist_dataset = datasets.MNIST(self.dataset_path,\n                                       train=True,\n                                       download=False,\n                                       transform=transform)\n        return DataManager(mnist_dataset, batch_size=batch_size, shuffle=True)\n</code></pre>"},{"location":"getting-started/getting-started/#multilayer-perceptron-in-pytorch-model-training-loop-same-example-as-previous-one-but-transposed-to-fed-biomed","title":"Multilayer Perceptron in Pytorch: model training loop (same example as previous one but transposed to Fed-BioMed)","text":"<pre><code># model parameters\nn_epochs = 20\n\n\nmodel = MyLocalTrainingPlan()\n\n# model training\nfor _ in range(n_epochs):\n    for data, targets in model.training_data():\n        model.optimizer.zero_grad()\n        res = model.training_step(data, targets)\n        res.backward()\n        model.optimizer.step()\n</code></pre>"},{"location":"getting-started/getting-started/#multilayer-perceptron-in-fed-biomed-with-pytorch-model-training-loop","title":"Multilayer Perceptron in Fed-BioMed with Pytorch: model training loop","text":"<pre><code>model_args = {}\n\ntraining_args = {\n    'batch_size': 48, \n    'epochs': 20, \n    'dry_run': False,  \n}\n\nfrom fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MNIST', '#dataset']\nrounds = 2\n\n# model training\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyRemoteTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\nexp.run()\n</code></pre>"},{"location":"getting-started/getting-started/#going-further","title":"Going Further","text":""},{"location":"getting-started/getting-started/#installation-guide","title":"Installation Guide","text":"<p>Detailed steps on how to install Fed-BioMed on your computer.</p>"},{"location":"getting-started/getting-started/#tutorials","title":"Tutorials","text":"<p>More tutorials, examples and how-to.</p>"},{"location":"getting-started/getting-started/#nodes-configuration-guide","title":"<code>Nodes</code> configuration Guide","text":"<p>Provides an exhaustive overview of Fed-BioMed <code>Nodes</code>.</p>"},{"location":"getting-started/getting-started/#researcher-configuration-guide","title":"<code>Researcher</code> configuration Guide","text":"<p>Provides additional info on Fed-BioMed <code>Researcher</code>.</p>"},{"location":"getting-started/what-is-fedbiomed/","title":"What is Fed-BioMed","text":"<p>Fed-BioMed, an open-source federated learning framework</p>"},{"location":"getting-started/what-is-fedbiomed/#what-is-fed-biomed","title":"What is Fed-BioMed?","text":"<p>Fed-BioMed is an open source project focused on empowering biomedical research using non-centralized approaches for statistical analysis.  The project is currently based on Python and PyTorch, and enables developing and deploying federated learning analysis in real-world machine learning application. </p> <p>The goal of Fed-BioMed is to provide a simplified and secured environment to:</p> <ul> <li>Easily deploy state-of-the art federated learning analysis frameworks, </li> <li>Provide a friendly user interface to share data for federated learning experiments,</li> <li>Allow researchers to easily deploy their models and analysis methods,</li> <li>Foster research and collaborations in federated learning.</li> </ul> <p>Fed-BioMed is an ongoing initiative, and the code is available on GitHub.</p>"},{"location":"getting-started/what-is-fedbiomed/#what-is-federated-learning","title":"What is Federated Learning ?","text":""},{"location":"getting-started/what-is-fedbiomed/#introduction","title":"Introduction","text":"<p>Standard machine learning approaches require to have a centralized dataset in order to train a model. In certain scenarios like in the biomedical field, this is not straightforward due to several reasons like:</p> <ul> <li>Privacy concerns:<ul> <li>General Data Protection Regulation (GDPR): General Data Protection Regulation (GDPR) \u2013 Official Legal Text</li> <li>Californian Consumer Privacy Act (CCPA): California Consumer Privacy Act (CCPA) | State of California - Department of Justice - Office of the Attorney General</li> <li>Health Insurance Portability and Accountability (HIPAA): Health Information Privacy regulation | U.S. Department of Health and Human Service | HHS official website</li> <li>Family Educational Rights and Privacy Act (FERPA): Family Educational Rights and Privacy Act (FERPA) | U.S. Department of Education official website</li> </ul> </li> <li>Ethical committee approval</li> <li>Transferring data to a centralized location</li> </ul> <p>This slows down research in healthcare and limits the generalization of certain models.</p>"},{"location":"getting-started/what-is-fedbiomed/#federated-learning","title":"Federated Learning","text":"<p>Federated learning (FL) is a machine learning procedure whose goal is to train a model without having data centralized. The goal of FL is to train higher quality models by having access to more data than centralized approaches, as well as to keep data securely decentralized. </p>"},{"location":"getting-started/what-is-fedbiomed/#infrastructure-of-a-federated-learning-setting-in-healthcare","title":"Infrastructure of a federated learning setting in healthcare","text":"<p>A common scenario of federated learning in healthcare is shown as follows:</p> <p></p> <p>Hospitals (a.k.a. clients or nodes) across several geographical locations hold data of interest for a researcher. These data can be \"made available\" for local training but, only the model is authorized to be shared with a third thrusted party (e.g. research center). Once all the models are gathered, different techniques are proposed for aggregating them as a single global model. Then, the Aggregated model can be used as purposed (e.g. training a neural network for segmentation).</p>"},{"location":"getting-started/what-is-fedbiomed/#theoretical-background","title":"Theoretical background","text":"<p>One of the critical points in FL is to know how to aggregate the models submitted by the clients. The main problem relies on finding the best set of parameters that define your model in function of the submissions made by the clients.</p> <p>In a canonical form:</p> \\[ \\min_w F(w) ,\\quad \\textrm{where} F(w):=\\sum_{k=1}^{m} p_k F_k(w) \\] <p>Where \\(m\\) is the total number of nodes, \\(p_k&gt;=0\\), and \\(\\sum_k p_k=1\\) , and \\(F_k\\) is the local objective function for the \\(k\\)-th node. The impact (contribution) of each node to the aggregation of the global model is given by \\(p_k\\).</p> <p>One of the first proposed methodologies in FL for model aggregation was Federated Averaging <code>FedAVG</code> by (MacMahan et al, 2016), the idea behind it was to define the contribution of each node as \\(p_k=\\frac{n_k}{n}\\) where \\(n_k\\) is the number of datapoints in the node \\(k\\) and \\(n\\) is the total number of observations studied.</p>"},{"location":"getting-started/what-is-fedbiomed/#challenges-in-federated-learning","title":"Challenges in federated learning","text":"<p>The main challenges in FL are associated to:</p> <ul> <li> <p>Communication efficiency: number of iterations between nodes and central location to train an optimal model.</p> </li> <li> <p>Data heterogeneity: how to build generalized models with heterogeneous data?</p> </li> <li> <p>Security: adversarial attacks and data leakage.</p> </li> </ul>"},{"location":"getting-started/what-is-fedbiomed/#references","title":"References","text":"<ol> <li> <p>Kone\u010dn\u00fd, J., McMahan, et al. (2016). Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492.</p> </li> <li> <p>Li, T., Sahu, et al. (2018). Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127.</p> </li> <li> <p>Li, T., Sahu, A. K., Talwalkar, A., &amp; Smith, V. (2020). Federated learning: Challenges, methods, and future directions. IEEE Signal Processing Magazine, 37(3), 50-60.</p> </li> </ol>"},{"location":"news/","title":"News","text":"News"},{"location":"news/CAL_oncology/","title":"CAL oncology","text":""},{"location":"news/CAL_oncology/#fed-biomed-serving-oncology-research","title":"Fed-BioMed serving oncology research","text":"<p>Professor Olivier Humbert, from Centre Antoine Lacassagne, presents his research and teaching projects using artificial intelligence for medicine in this interview.</p> <p>Fed-BioMed is the technology empowering the federated learning project !</p>"},{"location":"news/CHB/","title":"Federated Learning Experiment with CHB","text":""},{"location":"news/CHB/#fed-biomed-at-hospital-centre-henri-becquerel-of-rouen","title":"Fed-BioMed at Hospital Centre Henri Becquerel of Rouen","text":"<p>A big thanks to Romain Modzelewski, Nathan Lapel, and Bastien Houis for the successful deployment of Fed-BioMed in Centre Henri-Becquerel!</p> <p>Fed-BioMed is currently providing a secure federated learning environment between Inria, and the hospitals Centre Antoine Lacassagne (Nice) and Centre Henri-Becquerel (Rouen).</p> <p>Looking forward to the next steps of this collaborative AI project in healthcare!</p>"},{"location":"news/OSE-2022/","title":"OSE 2022","text":""},{"location":"news/OSE-2022/#fed-biomed-open-source-experience-2022","title":"Fed-BioMed @ Open Source Experience 2022","text":"<p>Fed-BioMed participated to Open Source Experience 2022 meeting of the European open source community, Paris, Nov 8-9.</p> <p>View Francesco Cremonesi's introduction to Fed-BioMed:</p>"},{"location":"news/Release-01-2022/","title":"Release 01 2022","text":""},{"location":"news/Release-01-2022/#fed-biomed-v33-new-release","title":"Fed-BioMed v3.3 new release","text":"<p>Fed-BioMed v3.3 is now public. Here are some key new features:</p> <ul> <li>researcher: MONAI support and example notebooks</li> <li>researcher: real time monitoring with Tensorboard</li> <li>client: simplified inclusion of dataset in nodes</li> <li>security: model manager to register and check authorised training models on the node</li> <li>improved support for macOSX </li> <li>new tutorials and notebooks</li> </ul> <p>All the new specifics are in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-01-2023/","title":"Release 01 2023","text":""},{"location":"news/Release-01-2023/#fed-biomed-v41-new-release","title":"Fed-BioMed v4.1 new release","text":"<p>Fed-BioMed v4.1 is now available. Here are some key new features:</p> <ul> <li>Introducing Scaffold <code>Aggregation</code> method for PyTorch, focused to cope with the client drift issue, useful when dealing with heterogenous datasets</li> <li>Adding <code>num_updates</code> as a new <code>training_args</code> Argument: <code>num_updates</code> allows you to iterate your model over a specific number of updates, regardless of the size of data accross each <code>Node</code>. It is an alternative to number of epochs <code>epochs</code></li> <li>Adding more integration tests / introducing nightly tests in order to improve code quality</li> <li>improving <code>Researcher</code> log message, by introducing <code>Round</code> number</li> <li>Bug fixes (FedProx <code>Aggregation</code> method, percentage completion logged when using Opacus, and other minor fixes)</li> </ul> <p>More details about the new features can be found in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-02-2022/","title":"Release 02 2022","text":""},{"location":"news/Release-02-2022/#fed-biomed-v34-new-release","title":"Fed-BioMed v3.4 new release","text":"<p>Fed-BioMed v3.4 is now available. Here are some key new features:</p> <ul> <li>node-side differential privacy using Opacus notebook</li> <li>node GUI for managing shared data</li> <li>single Nvidia GPU training acceleration for PyTorch </li> <li>new tutorials and notebooks</li> <li>refactoring and testing for robustness and code maturity</li> </ul> <p>All the new specifics are in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-04-2023/","title":"Release 04 2023","text":""},{"location":"news/Release-04-2023/#fed-biomed-v43-new-release","title":"Fed-BioMed v4.3 new release","text":"<p>Fed-BioMed v4.3 is now available.</p> <p>It introduces Secure Aggregation functionality which further protects the federated learning process. Secure Aggregation encrypts model parameters sent by the nodes to the researcher. The researcher then computes aggregated model parameters but cannot access individual node's model parameters in cleartext.</p> <p>Fed-BioMed Secure Aggregation uses Joye-Libert additively homomorphic encryption scheme (based on fault-tolerant-secure-agg implementation) and Shamir multi-party computation protocol (from MP-SPDZ software).</p> <p>Bug fixes and misc updates are also included in the release.</p> <p>More details about the new features can be found in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-05-2022/","title":"Release 05 2022","text":""},{"location":"news/Release-05-2022/#fed-biomed-v35-new-release","title":"Fed-BioMed v3.5 new release","text":"<p>Fed-BioMed v3.5 is now available. Here are some key new features:</p> <ul> <li>FedProx optimization scheme support for PyTorch</li> <li>model evaluation/validation</li> <li>NIFTI and MedNIST datasets</li> <li>API documentation</li> <li>better VPN/containers support</li> <li>etc.</li> </ul> <p>More details about the new features can be found in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/Release-11-2022/","title":"Release 11 2022","text":""},{"location":"news/Release-11-2022/#fed-biomed-v40-new-major-release","title":"Fed-BioMed v4.0 new major release","text":"<p>Fed-BioMed v4.0 is now available. Here are some key new features:</p> <ul> <li>Improved ML security: Differential Privacy (local and central) for Pytorch</li> <li>Improved GUI security: user accounts management and authentication</li> <li>Improved Node-side security: training plan approval mechanism</li> <li>Support for biomedical data: MedicalFolderDataset supporting imaging + csv data</li> <li>Integration with open biomedical datasets: IXI brain imaging, and FLamby repository</li> <li>New tutorials: medical image segmentation, filtering datasets based on number of samples</li> <li>Several backend improvements: mini-batch SGD for scikit-learn, validation of training arguments, redesign of training plan, etc..</li> </ul> <p>More details about the new features can be found in the Fed-BioMed CHANGELOG.</p>"},{"location":"news/federated-pet/","title":"Federated pet","text":""},{"location":"news/federated-pet/#fed-biomed-for-federated-pet-project","title":"Fed-BioMed for Federated-PET project","text":"<p>Federated-PET is an oncology research project lead by Pr Olivier Humbert (Centre Antoine Lacassagne/Universit\u00e9 C\u00f4te d\u2019Azur/3IA C\u00f4te d\u2019Azur). Federated-PET focuses on predicting the response to immunotherapy of patients followed for lung cancer and personalize the therapeutic strategy to improve the quality and life expectancy of patients. Federated-PET groups 8 hospitals and 4 research centers.</p> <p>We are thrilled to announce that Federated-PET uses Fed-BioMed as its federated learning platform.</p> <p>Read the project presentations on Centre Antoine Lacassagne website and Universit\u00e9 C\u00f4te d'Azur website</p>"},{"location":"news/vivatech-2023/","title":"Vivatech 2023","text":""},{"location":"news/vivatech-2023/#fed-biomed-viva-technology-2023","title":"Fed-BioMed @ Viva Technology 2023","text":"<p>Fed-BioMed will participate to Viva Technology 2023 Europe Startup and Tech event.</p> <p>Come and meet us on the Inria booth on Friday June 16 !</p>"},{"location":"news/welcome_08-2021/","title":"Welcome to Sergen and Yannick!","text":""},{"location":"news/welcome_08-2021/#a-warm-welcome-to-sergen-cansiz-and-yannick-bouillard-in-the-fed-biomed-team","title":"A warm welcome to Sergen Cansiz and Yannick Bouillard in the Fed-BioMed team!","text":"<p>09/2021.</p> <p>Sergen Cansiz and Yannick Bouillard are the new research engineers of Fed-BioMed. They will take care of the development of the platform and of the deployment of Fed-BioMed in the hospitals of our collaborating partners.</p>"},{"location":"pages/CAL/","title":"CAL Fed-BioMed Experiment","text":"<p>We are proud to announce the successful deployment and testing of Fed-BioMed into the Hospital Antoine Lacassagne (CAL) of Nice! CAL is the first partner among the network of oncology centers of the UNICANCER consortium, in which Fed-BioMed is providing a secured federated learning infrastructure for IA in healthcare. A big thanks to Olivier Humbert and Hamid Laceb of CAL, and to the Fed-BioMed team!</p>"},{"location":"pages/about-us/","title":"About","text":""},{"location":"pages/about-us/#about-us","title":"About us","text":"<p>Federated learning for Healthcare</p>"},{"location":"pages/about-us/#what-is-fed-biomed","title":"What is Fed-BioMed?","text":"<p>Fed-BioMed is an open-source research and development initiative aiming at translating federated learning (FL) into real-world medical research applications.</p> <p>Fed-BioMed provides:</p> <ul> <li>A demonstrated framework for deploying federated learning in hospital networks,</li> <li>Easy deployment of state-of-the art federated learning methods,</li> <li>User-friendly tools for data managment and client participation to federated learning,</li> <li>A framework-agnostic environment for easily deploying machine learning methods,</li> <li>Clear solutions compliant with data providers' privacy, and nodes governance requirements.</li> </ul> <p>Fed-BioMed is an ongoing initiative, and the code is available on GitHub.</p>"},{"location":"pages/about-us/#contributors","title":"Contributors:","text":"<p>Fed-BioMed software was originally developed by Inria (Institut National de Recherche en Informatique et Automatique), and of Universit\u00e9 C\u00f4te d\u2019Azur (UCA).</p> <p>Authors come from academic research, private companies and open source community.</p>"},{"location":"pages/funding/","title":"Acknowledgements","text":"<p>Fed-BioMed is grateful for the support from the Agence nationale de la recherche (ANR), from the Inria National Artificial Intelligence Research Programme, from the Marie Sklodowska-Curie EU program and from the Universit\u200c\u00e9 Cote d'Azur.  </p>"},{"location":"pages/publications/","title":"Publications","text":"<ul> <li> <p>Irene Balelli, Santiago Silva and Marco Lorenzi. A Probabilistic Framework for Modeling the Variability Across Federated Datasets of Heterogeneous Multi-View Observations. In Proceedings of The 27th international conference on Information Processing in Medical Imaging (IPMI), 2021. link</p> </li> <li> <p>Yann Fraboni, Richard Vidal and Marco Lorenzi. Free-rider Attacks on Model Aggregation in Federated Learning. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, PMLR 130:1846-1854, 2021. link</p> </li> <li> <p>Santiago Silva, Andre Altmann, Boris  Gutman, and Marco Lorenzi. Fed-BioMed: A General Open-Source Frontend Framework for Federated Learning in Healthcare. Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning. Springer, Cham, 2020. 201-210. link</p> </li> <li> <p>Santiago Silva, Boris Gutman, Barbara Bardoni, Paul M Thompson, Andre Altmann and Marco Lorenzi. Multivariate Learning in Distributed Biomedical Databases: Meta-analysis of Large-scale Brain Imaging Data. IEEE International Symposium on Biomedical Imaging (ISBI), Venice, 2019. link</p> </li> <li> <p>Marco Lorenzi, Boris Gutman, Paul M. Thompson, Daniel C. Alexander, Sebastien Ourselin, Andre Altmann. Secure multivariate large-scale multi-centric analysis through on-line learning: an imaging genetics case study. Proceedings of the 12th International Symposium on Medical Information Processing and Analysis. link</p> </li> </ul>"},{"location":"pages/roadmap/","title":"Fed-BioMed Road Map","text":""},{"location":"tutorials/advanced/breakpoints/","title":"Breakpoints (experiment saving facility)","text":"<p>An experiment can crash or stop during training due to unexpected events : software component crash (researcher/node/network), host server failure, network outage, etc. </p> <p>If an experiment crashes during training, one can launch an experiment with the same parameters and re-do a similar training. But if the experiment already ran for a long time, we don't want to lose the previous training effort. This is where breakpoints help.</p> <p>Breakpoints can also be used for a wide variety of reasons such as: the model start to oscillate at some point, the model get over-trained. In such cases, you may want to revert to a previous round's results using a breakpoint, and continue from this breakpoint.</p> <p>A Fed-BioMed breakpoint is a researcher side function that saves an intermediate status and training results of an experiment to disk files. It can later be used to create an experiment from these saved status and results, to continue and complete the partially run experiment.</p>"},{"location":"tutorials/advanced/breakpoints/#basic-use-continue-from-last-breakpoint-of-last-experiment","title":"Basic use : continue from last breakpoint of last experiment","text":"<p>Note</p> <p>Before running the following cells, please make sure you have already a running node. Please follow the following tutorials explaining how to launch Pytorch training plans \" and Scikit-Learn training plans. Don't forget to specify the data under <code>#dummy_tag</code> tag.</p> <p>By default, a Fed-BioMed experiment does not save breakpoints. To create an experiment that save breakpoints after each training round completes, use the <code>save_breakpoints=True</code> tag :</p> <pre><code>exp = Experiment(tags=[ '#dummy_tag' ],\n                training_plan_class=MyTrainingPlan,\n                round_limit=2,\n                save_breakpoints=True)\n</code></pre> <p>If the experiment crashes or is stopped during training, and at least one round was completed, one can later continue from the last experiment breakpoint.</p> <p>First step is to stop and restart all Fed-BioMed components still running (researcher, nodes, and sometimes network), as they often have lost their state due to the crash beyond the software automatic recovery capability.</p> <p>Second step is then to create an experiment on the researcher with status loaded from the last breakpoint of the previously running experiment :</p> <pre><code>new_exp = Experiment.load_breakpoint()\n</code></pre> <p>Optionally check the experiment folder and current training round for the loaded breakpoint :</p> <pre><code>print(f'Experimentation folder {new_exp.experimentation_folder()}')\nprint(f'Number of experimentation rounds completed {new_exp.round_current()}')\n</code></pre> <p>Then continue and complete experiment loaded from the breakpoint :</p> <pre><code>new_exp.run()\n</code></pre>"},{"location":"tutorials/advanced/breakpoints/#continue-from-a-specific-breakpoint-and-experiment","title":"Continue from a specific breakpoint and experiment","text":"<p>In some cases, it is needed to indicate which breakpoint should be used to create the experiment, because the last breakpoint of the last experiment cannot be automatically guessed, or because it is desirable to select and continue from an older breakpoint or experiment.</p> <p>Fed-BioMed saves experiment results and breakpoints with the following file tree structure : <pre><code>${FEDBIOMED_DIR}/var/experiments\n|\n\\-- Experiment_0\n|\n\\-- Experiment_1\n|    |\n|    \\-- breakpoint_0\n|    \\-- breakpoint_1\n|\n\\-- Experiment_2\n|    |\n|    \\-- breakpoint_0\n|\n\\-- Experiment_3\n</code></pre></p> <p>Result files and breakpoints for each experiment are saved in a distinct folder under <code>${FEDBIOMED_DIR}/var/experiments</code>. By default, each experiment is assigned a folder named <code>Experiment_{num}</code> with a unique increasing num. When an experiment saves breakpoint, breakpoint for round round is saved under <code>Experiment_{num}/breakpoint_{round}</code>.</p> <p>When loading last breakpoint of last experiment, it selects the highest existing num experiment and then the highest round for this experiment. In this examples, automatic selection of last breakpoint fails because last experiment (<code>Experiment_3</code>) has not saved breakpoints or could not complete its first round.</p> <p>To load a specific breakpoint, indicate the path for this breakpoint (absolute or relative to the current directory). For example to load breakpoint after round 1 for <code>Experiment_1</code>, with current running directory <code>${FEDBIOMED_DIR}/notebooks</code> :</p> <pre><code>new_exp = Experiment.load_breakpoint(\n    \"${FEDBIOMED_DIR}/var/experiments/Experiment_1/breakpoint_1\")\n</code></pre> <p>or</p> <pre><code># works if current directory is ${FEDBIOMED_DIR}/notebooks\nnew_exp = Experiment.load_breakpoint(\n    \"../var/experiments/Experiment_1/breakpoint_1\")\n</code></pre> <p>Optionally check experimentation loaded from the breakpoint :</p> <pre><code>print(f'Experimentation path {new_exp.experimentation_path()}')\n</code></pre> <p>If running a python script from another directory than <code>${FEDBIOMED_DIR}/notebooks</code> and using a relative path, then adapt the relative path accordingly. For example if the experiment is launched from the <code>${FEDBIOMED_DIR}</code> directory : <pre><code>cd ${FEDBIOMED_DIR}\npython ./notebooks/my_example.py`\n</code></pre></p> <p>then the current directory needs to be relative to <code>${FEDBIOMED_DIR}</code> :</p> <pre><code>new_exp = Experiment.load_breakpoint(\n    \"./var/experiments/Experiment_1/breakpoint_1\")\n</code></pre>"},{"location":"tutorials/advanced/breakpoints/#limitations","title":"Limitations","text":"<p>Info</p> <p>Breakpoints currently do not support copying to another path, as they use absolute path. </p> <p>For example this doesn't work in the current version :</p> <p><pre><code>!mv ./var/experiments/Experiment_1 ./var/experiments/mydir\n</code></pre> <pre><code>new_exp = Experiment.load_breakpoint(\"./var/experiments/mydir\")\n</code></pre></p> <p>Info</p> <p>Breakpoints currently do not save tensorboard monitoring status and tensorboard logs. If you continue from a breakpoint, tensorboard monitoring is not restarted and logs from pre-breakpoint run are not restored.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/","title":"In Depth Experiment Configuration","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\n\n# Here we define the training plan to be used.\n# You can use any class name (here 'MyTrainingPlan')\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    # Defines and return model\n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n\n    # Defines and return optimizer\n    def init_optimizer(self, optimizer_args):\n        return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n\n    # Declares and return dependencies\n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\"]\n        return deps\n\n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            self.fc1 = nn.Linear(9216, 128)\n            self.fc2 = nn.Linear(128, 10)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.dropout2(x)\n            x = self.fc2(x)\n\n\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_data(self, batch_size = 48):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        loader_arguments = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset=dataset1, **loader_arguments)\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn import torch.nn.functional as F from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms   # Here we define the training plan to be used. # You can use any class name (here 'MyTrainingPlan') class MyTrainingPlan(TorchTrainingPlan):      # Defines and return model     def init_model(self, model_args):         return self.Net(model_args = model_args)      # Defines and return optimizer     def init_optimizer(self, optimizer_args):         return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])      # Declares and return dependencies     def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\"]         return deps      class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.conv1 = nn.Conv2d(1, 32, 3, 1)             self.conv2 = nn.Conv2d(32, 64, 3, 1)             self.dropout1 = nn.Dropout(0.25)             self.dropout2 = nn.Dropout(0.5)             self.fc1 = nn.Linear(9216, 128)             self.fc2 = nn.Linear(128, 10)          def forward(self, x):             x = self.conv1(x)             x = F.relu(x)             x = self.conv2(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = self.dropout1(x)             x = torch.flatten(x, 1)             x = self.fc1(x)             x = F.relu(x)             x = self.dropout2(x)             x = self.fc2(x)               output = F.log_softmax(x, dim=1)             return output      def training_data(self, batch_size = 48):         # Custom torch Dataloader for MNIST data         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         loader_arguments = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset=dataset1, **loader_arguments)      def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss <p>After running the cells above, your training plan class will be ready, and it will be declared in the experiment as training plan which going to be sent to the nodes to perform federated training.</p> <p>After building an empty experiment you won't be able to perform federated training, since it is not fully configured. That's why the output of the initialization of <code>Experiment</code> will always remind you that the experiment is not fully configured.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nexp = Experiment()\n</pre> from fedbiomed.researcher.experiment import Experiment exp = Experiment() In\u00a0[\u00a0]: Copied! <pre>exp.info()\n</pre> exp.info() <p>Based on the output, some arguments are defined with default values, while others are not. Model arguments, training arguments, tags, round limit, training data etc. have no default value, and therefore are required to be set in order to run an experiment. However, these arguments are related to each other. For example, to be able to define your federated training data you need to define the <code>tags</code> first, and then while setting your training data argument, experiment will be able to send search request to the nodes to receive information about the datasets. These relations between the arguments will be explained in the following steps.</p> In\u00a0[\u00a0]: Copied! <pre>exp.set_training_plan_class(training_plan_class=MyTrainingPlan)\n</pre> exp.set_training_plan_class(training_plan_class=MyTrainingPlan) <p>If you set your training plan path first, setter will log a debug message which will inform you about the training plan is not defined yet. This is because the training plan class has not been set yet</p> In\u00a0[\u00a0]: Copied! <pre># Model arguments should be an empty Dict, since our model does not require \n# any argument for initialization\nmodel_args = {}\n\n# Training Arguments\ntraining_args = {\n    'batch_size': 48,\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 1, \n    'dry_run': False,  \n    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n\nexp.set_model_args(model_args=model_args)\nexp.set_training_args(training_args=training_args)\n</pre> # Model arguments should be an empty Dict, since our model does not require  # any argument for initialization model_args = {}  # Training Arguments training_args = {     'batch_size': 48,     'optimizer_args': {         'lr': 1e-3     },     'epochs': 1,      'dry_run': False,       'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples }  exp.set_model_args(model_args=model_args) exp.set_training_args(training_args=training_args) In\u00a0[\u00a0]: Copied! <pre>tags = ['#MNIST', '#dataset']\nexp.set_tags(tags = tags)\n</pre> tags = ['#MNIST', '#dataset'] exp.set_tags(tags = tags) <p>To see the tags that are set, you can run <code>tags()</code> method of experiment object.</p> In\u00a0[\u00a0]: Copied! <pre>exp.tags()\n</pre> exp.tags() In\u00a0[\u00a0]: Copied! <pre>training_data = exp.set_training_data(training_data=None, from_tags=True)\n</pre> training_data = exp.set_training_data(training_data=None, from_tags=True) <p>Since the training data setter will send search request to the nodes, the output will inform you about selected nodes for training. It means that those nodes have the dataset, and they will be able to train your model defined in the training plan class.</p> <p><code>set_training_data</code> will return a <code>FederatedDataSet</code> object. You can either use the return value of the setter or the getter for training data which is <code>training_data()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>training_data = exp.training_data()\n</pre> training_data = exp.training_data() <p>To inspect the result in detail you can call the method <code>data()</code> of the <code>FederatedDataSet</code> object. This will return a python dictionary that includes information about the datasets that has been found in the nodes.</p> In\u00a0[\u00a0]: Copied! <pre>training_data.data()\n</pre> training_data.data() <p>As it is mentioned before, setting training data once doesn't mean that you can't change it, for you can create a new <code>FederatedDataSet</code> with a <code>dict</code> that includes the information about the datasets. This will allow you to select the datasets that will be used for federated training.</p> <p>Since the dataset information will be provided, there will be no need to send request to the nodes</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.datasets import FederatedDataSet \n\ntr_data = training_data.data()\ntr_data = training_data.data()\nfederated_dataset = FederatedDataSet(tr_data)\nexp.set_training_data(training_data = federated_dataset)\n</pre> from fedbiomed.researcher.datasets import FederatedDataSet   tr_data = training_data.data() tr_data = training_data.data() federated_dataset = FederatedDataSet(tr_data) exp.set_training_data(training_data = federated_dataset) <p>Or, you can directly use <code>tr_data</code> in <code>set_training_data()</code></p> In\u00a0[\u00a0]: Copied! <pre>exp.set_training_data(training_data = tr_data)\n</pre> exp.set_training_data(training_data = tr_data) <p>         If you change the tags for the dataset by using <code>set_tags</code> and if there is already a defined training data in your experiment object, you have to update your training data by running <code>exp.set_training_data(training_data=None)</code>.       </p> In\u00a0[\u00a0]: Copied! <pre>exp.aggregator()\n</pre> exp.aggregator()  <p>Let's supposed that you have created your own aggregator: then, you can set it as follows:</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.aggregators.fedavg import FedAverage\nexp.set_aggregator(aggregator=FedAverage)\n</pre> from fedbiomed.researcher.aggregators.fedavg import FedAverage exp.set_aggregator(aggregator=FedAverage) <p>If your aggregator class needs initialization parameters, you can build your class and pass as an instance of an object.</p> In\u00a0[\u00a0]: Copied! <pre>fed_average = FedAverage()\nexp.set_aggregator(aggregator=fed_average)\n</pre> fed_average = FedAverage() exp.set_aggregator(aggregator=fed_average) In\u00a0[\u00a0]: Copied! <pre>exp.set_strategy(node_selection_strategy=None)\n</pre> exp.set_strategy(node_selection_strategy=None) <p>Or, you can directly pass <code>DefaultStrategy</code> (or any Strategy class) as an argument</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\nexp.set_strategy(node_selection_strategy=DefaultStrategy)\n\n# To make sure the strategy has been set\nexp.strategy()\n</pre> from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy exp.set_strategy(node_selection_strategy=DefaultStrategy)  # To make sure the strategy has been set exp.strategy() In\u00a0[\u00a0]: Copied! <pre>exp.set_round_limit(round_limit=2)\nexp.round_limit()\n</pre> exp.set_round_limit(round_limit=2) exp.round_limit() In\u00a0[\u00a0]: Copied! <pre>exp.set_job()\nexp.job()\n</pre> exp.set_job() exp.job() In\u00a0[\u00a0]: Copied! <pre>exp.info()\n</pre> exp.info() <p>If the experiment is ready, you will see the message that says <code>Experiment can be run now (fully defined)</code> at the bottom of the output. So now, we can run the experiment</p> In\u00a0[\u00a0]: Copied! <pre>exp.run_once()\n</pre> exp.run_once() <p>After running the experiment for once, you can check the current round. It returns <code>1</code> which means only one round has been run.</p> In\u00a0[\u00a0]: Copied! <pre>exp.round_current()\n</pre> exp.round_current() <p>Now, let's run the experiment with <code>run_once()</code> again.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run_once()\n</pre> exp.run_once() <p>Since the round limit has been set to <code>2</code> the round limit had been reached. If you try to run <code>run()</code> or <code>run_once()</code> the experiment will indicate that the round limit has been reached.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run_once()\n</pre> exp.run_once() In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>After this point, if you would like to run the experiment you can increase round limit with <code>set_round_limit(round)</code></p> In\u00a0[\u00a0]: Copied! <pre>exp.set_round_limit(4)\nprint('Round Limit    : ' , exp.round_limit())\nprint('Current Round  : ' , exp.round_current())\n</pre> exp.set_round_limit(4) print('Round Limit    : ' , exp.round_limit()) print('Current Round  : ' , exp.round_current()) <p>The round limit of the experiment has been set to <code>4</code> and the completed number of rounds is <code>2</code>. It means if you run the experiment with method <code>run()</code> without passing any argument, it will run the experiment for <code>2</code> rounds.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>Let's check the current round status of the experiment.</p> In\u00a0[\u00a0]: Copied! <pre>print('Round Limit    : ' , exp.round_limit())\nprint('Current Round  : ' , exp.round_current())\n</pre> print('Round Limit    : ' , exp.round_limit()) print('Current Round  : ' , exp.round_current()) <p>Another way to run your experiment if the round limit is reached is by passing <code>rounds</code> argument to the method <code>run()</code>. For example, following cell will run the experiment for <code>2</code> more rounds.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run(rounds=2, increase=True) # increase is True by default\n</pre> exp.run(rounds=2, increase=True) # increase is True by default <p>If the argument <code>increase</code> is <code>False</code>, it will not increase the round limit automatically.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run(rounds=2, increase=False)\n</pre> exp.run(rounds=2, increase=False) In\u00a0[\u00a0]: Copied! <pre>print('Round Limit    : ' , exp.round_limit())\nprint('Current Round  : ' , exp.round_current())\n</pre> print('Round Limit    : ' , exp.round_limit()) print('Current Round  : ' , exp.round_current()) <p>It is also possible to increase number of rounds while running the experiment with <code>run_once()</code> by passing <code>increase</code> argument as <code>True</code></p> In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) In\u00a0[\u00a0]: Copied! <pre>print('Round Limit    : ' , exp.round_limit())\nprint('Current Round  : ' , exp.round_current())\n</pre> print('Round Limit    : ' , exp.round_limit()) print('Current Round  : ' , exp.round_current()) In\u00a0[\u00a0]: Copied! <pre># Training Arguments\ntraining_args = {\n    'batch_size': 64,\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 1, \n    'dry_run': False,  \n    'batch_maxnum': 50\n}\n\nexp.set_training_args(training_args=training_args)\n</pre> # Training Arguments training_args = {     'batch_size': 64,     'optimizer_args': {         'lr': 1e-3     },     'epochs': 1,      'dry_run': False,       'batch_maxnum': 50 }  exp.set_training_args(training_args=training_args) In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True)"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#in-depth-experiment-configuration","title":"In Depth Experiment Configuration\u00b6","text":""},{"location":"tutorials/advanced/in-depth-experiment-configuration/#introduction","title":"Introduction\u00b6","text":"<p>The Experiment class provides an interface that you can manage your experiment with backward compatibility. It means that even if your Experiment has been built/defined you will be able to configure its parameters, and allow you to run your notebooks created using previous Fed-BioMed versions (&lt;3.4). This feature will provide more control over your experiment even after you have been running your experiment for several rounds. In this tutorial, the experiment interface will be explained using MNIST basic example.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#1-configuring-fed-biomed-environment","title":"1. Configuring Fed-BioMed Environment\u00b6","text":"<p>Before running this notebook, you need to configure your environment by completing the following steps:</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#11-starting-the-network-component","title":"1.1. Starting the Network Component\u00b6","text":"<p>Please run following command to start Network component that provided communication between your notebook and the node;</p> <pre>${FEDBIOMED_DIR}/scripts/fedbiomed_run network\n</pre> <p>This command will launch docker containers. Therefore, please make sure that your Docker engine is up and running.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#12-deploying-mnist-dataset-in-the-node","title":"1.2. Deploying MNIST Dataset in the Node\u00b6","text":"<p>Please run following command to add MNIST dataset into your Node. This command will deploy MNIST dataset in your default node whose config file is located in <code>${FEDBIOMED_DIR}/etc</code> directory as <code>config_node.ini</code></p> <p>After running following command, please select data type <code>2) default</code>, use default <code>tags</code> and select the folder where MNIST dataset will be saved.</p> <pre>${FEDBIOMED_DIR}/scripts/fedbiomed_run node add\n</pre>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#13-starting-the-node","title":"1.3. Starting the Node\u00b6","text":"<p>After you have successfully completed previous step, please run following command to start your node.</p> <pre>${FEDBIOMED_DIR}/scripts/fedbiomed_run node start\n</pre>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#2-creating-a-training-plan","title":"2. Creating a Training Plan\u00b6","text":"<p>Before declaring an experiment, the training plan that will be used for federated training should be defined. The training plan below is the same training plan that is created in the Basic MNIST tutorial. We recommend you to follow Basic MNIST tutorial on PyTorch Framework to understand following steps.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#3-creating-an-experiment-step-by-step","title":"3. Creating an Experiment Step by Step\u00b6","text":"<p>The experiment class can be created without passing any argument. This will just build an empty experiment object. Afterwards, you will be able to define your arguments using setters provided by <code>Experiment</code> class.</p> <p>It is always possible to create a fully configured experiment by passing all arguments during the initialization. You can also create your experiment with some arguments and set the other arguments afterwards.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#31-building-an-empty-experiment","title":"3.1. Building an Empty Experiment\u00b6","text":""},{"location":"tutorials/advanced/in-depth-experiment-configuration/#32-displaying-current-status-of-experiment","title":"3.2. Displaying Current Status of Experiment\u00b6","text":"<p>As an addition to output of the initialization, to find out more about the current status of the experiment, you can call the <code>info()</code> method of your experiment object. This method will print the information about your experiment and what you should complete to be able to start your federated training.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#33-setting-training-plan-for-the-experiment","title":"3.3. Setting Training Plan for The Experiment\u00b6","text":"<p>The training plan that is going to be used for the experiment can be set using the method <code>set_training_plan_class</code>.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#34-setting-the-argument-training-plan-path-special-case","title":"3.4. Setting The Argument Training Plan Path (Special Case)\u00b6","text":"<p>The <code>training_plan_path</code> is the path your training plan is saved as a python script. This argument should be used if your training plan class is defined in different directory as python script. However, the experiment also need to now your class name. You can set your class name as a <code>string</code> with <code>set_training_plan_class</code>. Since it is a python script (module), class name will be used for importing operation at the back-end.</p> <pre>exp.set_training_plan_path(training_plan_path='path/to/your/script.py')\nexp.set_training_plan_class(training_plan_class='TrainingPlanClassAsString')\n</pre>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#35-setting-model-and-training-arguments","title":"3.5. Setting Model and Training Arguments\u00b6","text":"<p>In the previous step, the training plan has been defined for the experiment. Now, you can define your model arguments and training arguments that will be used respectively for building your model class and training your model on the node side. The methods <code>set_model_args</code> and <code>set_training_args</code> of the experiment class will allow you to set these arguments.</p> <p>There isn't any requirement on the order of defining training plan class and mode/training arguments. It is also possible to         define model/training arguments first and training plan class after.     </p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#36-setting-tags","title":"3.6. Setting Tags\u00b6","text":"<p>The tags for the dataset search request can be set using <code>set_tags</code> method of experiment object.</p> <p>Setting tags does not mean sending dataset search request. Search request is sent while setting training data. <code>tags</code> is the argument that is required for the search request.</p> <p>The arguments <code>tags</code> of <code>set_tags</code> method should be an array of tags which are in <code>string</code> type or just a tag in <code>string</code> type.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#37-setting-nodes","title":"3.7. Setting Nodes\u00b6","text":"<p>The <code>nodes</code> arguments indicates the nodes that are going to be used for the experiment. By default, it is equal to <code>None</code> which means every node up and running will be part of the experiment as long as they have the dataset that is going to be used for training (and that has been registered under the tags). If the <code>nodes</code> argument has been set in advance when configuring <code>Experiment</code>, the search request for the dataset search will be sent only to nodes that have been indicated. You can set nodes using the method <code>exp.set_nodes(noes=nodes)</code>. This method takes <code>nodes</code> argument which should be an array of node ids which are of type <code>string</code> or just a single node id passed as a <code>string</code>.</p> <p>Since each node id is created randomly to the node when they are configured, we won't be setting <code>nodes</code> for this experiment, so it is possible to run this notebook regardless of the environment.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#37-setting-training-data","title":"3.7. Setting Training Data\u00b6","text":"<p>Training data is a <code>FederatedDataset</code> instance which comes from the module <code>fedbiomed.researcher.datasets</code>. There are several ways to define your training data.</p> <ol> <li>You can run <code>set_training_data(training_data=None, from_tags=True)</code>. This will send search request to the nodes to get dataset information by using the <code>tags</code> which are defined before.</li> <li>You can provide <code>training_data</code> argument which is an instance of <code>FederatedDataSet</code>.</li> <li>You can provide <code>training_data</code> argument as python dictionary <code>dict</code> and setter will create a <code>FederatedDataSet</code> object by itself.</li> </ol> <p>While using the last option please make sure that your <code>dict</code> object is configured accordingly to <code>FederatedDataSet</code> schema. Otherwise, you might get error while running your experiment.</p> <p>A <code>FederatedDataSet</code> object must have one unique dataset per node to ensure training uses only one dataset for each node. This is checked and enforced when creating a <code>FederatedDataSet</code></p> <p>If you run <code>set_training_data(training_data=None)</code>, this means that no training data is defined yet for the experiment (<code>training_data</code> is set to <code>None</code>).</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#38-setting-an-aggregator","title":"3.8. Setting an Aggregator\u00b6","text":"<p>An aggregator is one of the required arguments for the experiment. It is used for aggregating model parameters that are received from the nodes after every round (ie once training is done on each node). By default, when the experiment is initialized without passing any aggregator, it will automatically use the default <code>FedAverage</code> aggregator class. However, it is also possible to set a different aggregation algorithm with the method <code>set_aggregator</code>. Currently, Fed-BioMed has only <code>FedAverage</code> but it is possible to create custom aggregator classes.</p> <p>You can get the current aggregator by running <code>exp.aggregator()</code>. It will return the aggregator object that will be used for aggregation.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#39-setting-node-selection-strategy","title":"3.9. Setting Node Selection Strategy\u00b6","text":"<p>Node selection Strategy is also one of the required arguments for the experiment. It is used for selecting nodes before each round of training. Since the strategy will be used for selecting nodes, thus, training data should be already set before setting any strategies. Then, strategy will be able to select for training nodes that are currently available regarding their dataset.</p> <p>By default, <code>set_strategy(node_selection_strategy=None)</code> will use the default <code>DefaultStrategy</code> strategy. It is the default strategy in Fed-BioMed that selects for the training all the nodes available regardless their datasets. However, it is also possible to set different strategies. Currently, Fed-BioMed only provides <code>DefaultStrategy</code> but you can create your custom strategy classes.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#310-setting-round-limit","title":"3.10. Setting Round Limit\u00b6","text":"<p><code>round_limit</code> argument is the limit that indicates max number of rounds of the training. By default, it is <code>None</code> and it needs to be set before running your experiment. You can set the round limit with the method <code>set_round_limit</code>. <code>round_limit</code> can  be changed after running one or several rounds of training. You can always execute <code>exp.round_limit()</code> to see current round limit.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#311-setting-job-to-manage-federated-training-rounds","title":"3.11. Setting Job to Manage Federated Training Rounds\u00b6","text":"<p>Job is a class that manages federated training rounds. Before setting job, strategy for selecting nodes, training plan and training data should be set. Therefore, please make sure that they are all defined before setting job.  The method <code>set_job</code> creates the Job instance, and it does not take any argument.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#312-controlling-experiment-status-before-starting-training-rounds","title":"3.12. Controlling Experiment Status Before Starting Training Rounds\u00b6","text":"<p>Now, let's see if our experiment is ready for the training.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#4-running-the-experiment","title":"4. Running The Experiment\u00b6","text":"<p>As long as <code>info()</code> says that the experiment is fully defined you will be able to run your experiment. Experiment has two methods: <code>run()</code> and <code>run_once()</code> for running training rounds.</p> <ul> <li><p><code>run()</code> runs the experiment rounds from current round to round limit. If the round limit is reached it will indicate that the round limit has been reached. However, the method <code>run</code> takes 2 arguments as <code>round</code> and <code>increase</code>.</p> <ul> <li><code>round</code> is an integer that indicates number of rounds that are going to be run. If the experiment is at round <code>0</code>, the round limit is <code>4</code>, and if you pass <code>round</code> as 3, it will run the experiment only for <code>3</code> rounds.</li> <li><code>increase</code> is a boolean that indicates whether round limit should be increased if the given <code>round</code> passes over the round limit. For example, if the current round is <code>3</code>, the round limit is <code>4</code>, and the <code>round</code> argument is <code>2</code>, the experiment will increase round limit to <code>5</code></li> </ul> </li> <li><p><code>run_once()</code> runs the experiment for single round of training. If the round limit is reached it will indicate that the round limit has been reached. However, if it is executed as <code>run_once(increase=True)</code> when the round limit is reached, it increases the round limit for one round.</p> </li> </ul>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#41-running-the-experiment-once","title":"4.1. Running the Experiment once\u00b6","text":""},{"location":"tutorials/advanced/in-depth-experiment-configuration/#42-changing-training-arguments-for-the-next-round","title":"4.2. Changing Training Arguments for the Next Round\u00b6","text":"<p>The method <code>set_training_args()</code> allows you to change the training arguments even if you've already run your experiment several times. Thanks to the method <code>set_training_args()</code> you will be able to configure your training from one round to another. For example, we can change our <code>batch_size</code> to <code>64</code> and <code>batch_maxnum</code> to <code>50</code> for the next round.</p>"},{"location":"tutorials/advanced/in-depth-experiment-configuration/#conclusions","title":"Conclusions\u00b6","text":"<p>The <code>Experiment</code> class is the interface and the orchestrator of the whole processes behind federated training on the researcher side. It allows you to manage your federated training experiment easily. It has been extended with setter and getter methods to ease its declaration. This also provides more control before, during or after the training rounds. The purpose of the experiment class is to provide a robust interface for end-user to make them able to easily perform their federated training on Fed-BioMed nodes.</p>"},{"location":"tutorials/advanced/training-with-gpu/","title":"PyTorch model training using a GPU","text":"<p>This example demonstrates using a Nvidia GPU for training a model.</p> <p>The nodes for this example need to run on a machine providing a Nvidia GPU with enough GPU memory (and from a not-too-old model, so that it is supported by PyTorch).</p> <p>If GPU doesn't have enough memory you will get a out of memory error at run time.</p> <p>You can check Fed-BioMed GPU documentation for some background about using GPUs with Fed-BioMed.</p> <p>All this part is the same as when running a model using CPU : model in unchanged</p> <p>Declare a training plan class to send for training on the node</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\n\n# Here we define the training plan to be used.\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    # Defines and return model\n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n\n    # Defines and return optimizer\n    def init_optimizer(self, optimizer_args):\n        return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n\n    # Declares and returns dependencies\n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\"]\n        return deps\n\n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            self.fc1 = nn.Linear(9216, 128)\n            self.fc2 = nn.Linear(128, 10)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.dropout2(x)\n            x = self.fc2(x)\n\n\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_data(self, batch_size = 48):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        loader_arguments = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset=dataset1, **loader_arguments)\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms   # Here we define the training plan to be used. class MyTrainingPlan(TorchTrainingPlan):      # Defines and return model     def init_model(self, model_args):         return self.Net(model_args = model_args)      # Defines and return optimizer     def init_optimizer(self, optimizer_args):         return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])      # Declares and returns dependencies     def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\"]         return deps      class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.conv1 = nn.Conv2d(1, 32, 3, 1)             self.conv2 = nn.Conv2d(32, 64, 3, 1)             self.dropout1 = nn.Dropout(0.25)             self.dropout2 = nn.Dropout(0.5)             self.fc1 = nn.Linear(9216, 128)             self.fc2 = nn.Linear(128, 10)          def forward(self, x):             x = self.conv1(x)             x = F.relu(x)             x = self.conv2(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = self.dropout1(x)             x = torch.flatten(x, 1)             x = self.fc1(x)             x = F.relu(x)             x = self.dropout2(x)             x = self.fc2(x)               output = F.log_softmax(x, dim=1)             return output      def training_data(self, batch_size = 48):         # Custom torch Dataloader for MNIST data         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         loader_arguments = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset=dataset1, **loader_arguments)      def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'batch_size': 48,\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'use_gpu': True, # Activates GPU\n    'epochs': 1,\n    'dry_run': False,  \n    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n</pre> model_args = {}  training_args = {     'batch_size': 48,     'optimizer_args': {         'lr': 1e-3     },     'use_gpu': True, # Activates GPU     'epochs': 1,     'dry_run': False,       'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MNIST', '#dataset']\nrounds = 2\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['#MNIST', '#dataset'] rounds = 2  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) <p>Let's start the experiment.</p> <p>By default, this function doesn't stop until all the <code>round_limit</code> rounds are done for all the nodes</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>You have completed training a <code>TorchTrainingPlan</code> using a GPU for acceleration.</p>"},{"location":"tutorials/advanced/training-with-gpu/#pytorch-model-training-using-a-gpu","title":"PyTorch model training using a GPU\u00b6","text":""},{"location":"tutorials/advanced/training-with-gpu/#introduction","title":"Introduction\u00b6","text":""},{"location":"tutorials/advanced/training-with-gpu/#start-the-network","title":"Start the network\u00b6","text":"<p>Before running this notebook, start the network with <code>{FEDBIOMED_DIR}/scripts/fedbiomed_run network</code></p>"},{"location":"tutorials/advanced/training-with-gpu/#set-up-the-nodes-up","title":"Set up the nodes up\u00b6","text":"<p>We need at least 1 node, let's test using 3 nodes.</p> <ol> <li>For each node, add the MNIST dataset :</li> </ol> <pre>{FEDBIOMED_DIR}/scripts/fedbiomed_run node config config1.ini add\n{FEDBIOMED_DIR}/scripts/fedbiomed_run node config config2.ini add\n{FEDBIOMED_DIR}/scripts/fedbiomed_run node config config3.ini add\n</pre> <ul> <li>Select option 2 (default) to add MNIST to the node</li> <li>Confirm default tags by hitting \"y\" and ENTER</li> <li>Pick the folder where MNIST is already downloaded (or where to download MNIST)</li> </ul> <ol> <li>Check that your data has been added by executing</li> </ol> <pre>{FEDBIOMED_DIR}/scripts/fedbiomed_run node config config1.ini list\n{FEDBIOMED_DIR}/scripts/fedbiomed_run node config config2.ini list\n{FEDBIOMED_DIR}/scripts/fedbiomed_run node config config3.ini list\n</pre> <ol> <li>Run the first node using</li> </ol> <pre>{FEDBIOMED_DIR}/scripts/fedbiomed_run node config config1.ini start --gpu\n</pre> <p>so that the node offers to use GPU for training, with the default GPU device.</p> <ol> <li>Run the second node using</li> </ol> <pre>{FEDBIOMED_DIR}/scripts/fedbiomed_run node config config2.ini start --gpu-only --gpunum 1\n</pre> <p>so that the node enforces use of GPU for training even if the researcher doesn't request it, and requests using the 2nd GPU (device 1) but will fallback to default device if you don't have 2 GPUs on this machine.</p> <ol> <li>Run the third node using</li> </ol> <pre>{FEDBIOMED_DIR}/scripts/fedbiomed_run node config config3.ini start\n</pre> <p>so that the node doesn't offer to use GPU for training (default behaviour).</p> <ol> <li>Wait until you get <code>Starting task manager</code> for each node, it means you are online.</li> </ol>"},{"location":"tutorials/advanced/training-with-gpu/#define-the-training-plan","title":"Define the training plan\u00b6","text":""},{"location":"tutorials/advanced/training-with-gpu/#define-the-experiment-parameters","title":"Define the experiment parameters\u00b6","text":"<p><code>training_args</code> are used by the researcher to request the nodes to use GPU for training, if the node has a GPU and offers to use it.</p>"},{"location":"tutorials/advanced/training-with-gpu/#declare-and-run-the-experiment","title":"Declare and run the experiment\u00b6","text":"<p>All this part is the same as when running a model using CPU : experiment declaration and running is unchanged</p>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/","title":"FLamby integration in Fed-BioMed","text":"In\u00a0[\u00a0]: Copied! <pre>! pip install wget nibabel  # monai comes already packaged within fed-biomed\n</pre> ! pip install wget nibabel  # monai comes already packaged within fed-biomed In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\nfrom fedbiomed.common.data import FlambyDataset, DataManager\n\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args):\n        return Baseline()\n\n    def init_optimizer(self, optimizer_args):\n        return Optimizer(self.model().parameters(), lr=optimizer_args[\"lr\"])\n\n    def init_dependencies(self):\n        return [\"from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\",\n                \"from fedbiomed.common.data import FlambyDataset, DataManager\"]\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        return BaselineLoss().forward(output, target)\n\n    def training_data(self, batch_size=2):\n        dataset = FlambyDataset()\n        loader_arguments = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset, **loader_arguments)\n</pre> from fedbiomed.common.training_plans import TorchTrainingPlan from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer from fedbiomed.common.data import FlambyDataset, DataManager   class MyTrainingPlan(TorchTrainingPlan):     def init_model(self, model_args):         return Baseline()      def init_optimizer(self, optimizer_args):         return Optimizer(self.model().parameters(), lr=optimizer_args[\"lr\"])      def init_dependencies(self):         return [\"from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\",                 \"from fedbiomed.common.data import FlambyDataset, DataManager\"]      def training_step(self, data, target):         output = self.model().forward(data)         return BaselineLoss().forward(output, target)      def training_data(self, batch_size=2):         dataset = FlambyDataset()         loader_arguments = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset, **loader_arguments) In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'batch_size': 8,\n    'optimizer_args': {\n        \"lr\" : 1e-3\n    },\n    'epochs': 1,\n    'dry_run': False,\n    'batch_maxnum': 2 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n</pre> model_args = {}  training_args = {     'batch_size': 8,     'optimizer_args': {         \"lr\" : 1e-3     },     'epochs': 1,     'dry_run': False,     'batch_maxnum': 2 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['flixi']\nrounds = 1\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['flixi'] rounds = 1  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom flamby.datasets.fed_heart_disease import Baseline, BaselineLoss, Optimizer\nfrom fedbiomed.common.data import FlambyDataset, DataManager\n\nclass FedHeartTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args):\n        return Baseline()\n\n    def init_optimizer(self, optimizer_args):\n        return Optimizer(self.model().parameters(), lr=optimizer_args[\"lr\"])\n\n    def init_dependencies(self):\n        return [\"from flamby.datasets.fed_heart_disease import Baseline, BaselineLoss, Optimizer\",\n                \"from fedbiomed.common.data import FlambyDataset, DataManager\"]\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        return BaselineLoss().forward(output, target)\n\n    def training_data(self, batch_size=2):\n        dataset = FlambyDataset()\n        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset, **train_kwargs)\n</pre> from fedbiomed.common.training_plans import TorchTrainingPlan from flamby.datasets.fed_heart_disease import Baseline, BaselineLoss, Optimizer from fedbiomed.common.data import FlambyDataset, DataManager  class FedHeartTrainingPlan(TorchTrainingPlan):     def init_model(self, model_args):         return Baseline()      def init_optimizer(self, optimizer_args):         return Optimizer(self.model().parameters(), lr=optimizer_args[\"lr\"])      def init_dependencies(self):         return [\"from flamby.datasets.fed_heart_disease import Baseline, BaselineLoss, Optimizer\",                 \"from fedbiomed.common.data import FlambyDataset, DataManager\"]      def training_step(self, data, target):         output = self.model().forward(data)         return BaselineLoss().forward(output, target)      def training_data(self, batch_size=2):         dataset = FlambyDataset()         train_kwargs = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset, **train_kwargs) In\u00a0[\u00a0]: Copied! <pre>training_args = {\n    'batch_size': 4,\n    'optimizer_args': {\n        'lr': 0.001,\n    },\n    'epochs': 1,\n    'dry_run': False,\n    'log_interval': 2,\n    'batch_maxnum': 8,\n    'test_ratio' : 0.0,\n    'test_on_global_updates': False,\n    'test_on_local_updates': False,\n}\n\nmodel_args = {}\n</pre> training_args = {     'batch_size': 4,     'optimizer_args': {         'lr': 0.001,     },     'epochs': 1,     'dry_run': False,     'log_interval': 2,     'batch_maxnum': 8,     'test_ratio' : 0.0,     'test_on_global_updates': False,     'test_on_local_updates': False, }  model_args = {} In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['flheart']\nnum_rounds = 1\n\nexp = Experiment(tags=tags,\n                 training_plan_class=FedHeartTrainingPlan,\n                 training_args=training_args,\n                 model_args=model_args,\n                 round_limit=num_rounds,\n                 aggregator=FedAverage(),\n                )\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['flheart'] num_rounds = 1  exp = Experiment(tags=tags,                  training_plan_class=FedHeartTrainingPlan,                  training_args=training_args,                  model_args=model_args,                  round_limit=num_rounds,                  aggregator=FedAverage(),                 ) In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom torch.optim import AdamW\nfrom torch import nn\nimport torch.nn.functional as F\nfrom unet import UNet\nfrom monai.transforms import Compose, NormalizeIntensity, Resize\n\nclass UNetTrainingPlan(TorchTrainingPlan):\n\n    class MyUNet(nn.Module):\n        CHANNELS_DIMENSION = 1\n\n        def __init__(self, model_args):\n            super().__init__()\n            self.unet = UNet(\n            in_channels = model_args.get('in_channels',1),\n            out_classes = model_args.get('out_classes',2),\n            dimensions = model_args.get('dimensions',2),\n            num_encoding_blocks = model_args.get('num_encoding_blocks',5),\n            out_channels_first_layer = model_args.get('out_channels_first_layer',64),\n            normalization = model_args.get('normalization', None),\n            pooling_type = model_args.get('pooling_type', 'max'),\n            upsampling_type = model_args.get('upsampling_type','conv'),\n            preactivation = model_args.get('preactivation',False),\n            residual = model_args.get('residual',False),\n            padding = model_args.get('padding',0),\n            padding_mode = model_args.get('padding_mode','zeros'),\n            activation = model_args.get('activation','ReLU'),\n            initial_dilation = model_args.get('initial_dilation',None),\n            dropout = model_args.get('dropout',0),\n            monte_carlo_dropout = model_args.get('monte_carlo_dropout',0)\n        )\n\n        def forward(self, x):\n            x = self.unet.forward(x)\n            x = F.softmax(x, dim=UNetTrainingPlan.MyUNet.CHANNELS_DIMENSION)\n            return x\n\n    def init_model(self, model_args):\n        return UNetTrainingPlan.MyUNet(model_args)\n\n    def init_dependencies(self):\n        return [\"from torch import nn\",\n               'import torch.nn.functional as F',\n               'from torch.optim import AdamW',\n               'from unet import UNet',\n               'from monai.transforms import Compose, NormalizeIntensity, Resize',\n                'from fedbiomed.common.data import FlambyDataset']\n\n    def init_optimizer(self, optimizer_args):\n        return AdamW(self.model().parameters(),\n                     lr=optimizer_args[\"lr\"],\n                     betas=optimizer_args[\"betas\"],\n                     eps=optimizer_args[\"eps\"])\n\n    @staticmethod\n    def get_dice_loss(output, target, epsilon=1e-9):\n        SPATIAL_DIMENSIONS = 2, 3, 4\n        p0 = output\n        g0 = target\n        p1 = 1 - p0\n        g1 = 1 - g0\n        tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n        fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n        fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n        num = 2 * tp\n        denom = 2 * tp + fp + fn + epsilon\n        dice_score = num / denom\n        return 1. - dice_score\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss = UNetTrainingPlan.get_dice_loss(output, target)\n        avg_loss = loss.mean()\n        return avg_loss\n\n    def testing_step(self, data, target):\n        prediction = self.model().forward(data)\n        loss = UNetTrainingPlan.get_dice_loss(prediction, target)\n        avg_loss = loss.mean()  # average per batch\n        return avg_loss\n\n    def training_data(self, batch_size=2):\n        dataset = FlambyDataset()\n        transform = Compose([Resize((48,60,48)), NormalizeIntensity()])\n        dataset.init_transform(transform)\n        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset, **train_kwargs)\n</pre> from fedbiomed.common.training_plans import TorchTrainingPlan from torch.optim import AdamW from torch import nn import torch.nn.functional as F from unet import UNet from monai.transforms import Compose, NormalizeIntensity, Resize  class UNetTrainingPlan(TorchTrainingPlan):      class MyUNet(nn.Module):         CHANNELS_DIMENSION = 1          def __init__(self, model_args):             super().__init__()             self.unet = UNet(             in_channels = model_args.get('in_channels',1),             out_classes = model_args.get('out_classes',2),             dimensions = model_args.get('dimensions',2),             num_encoding_blocks = model_args.get('num_encoding_blocks',5),             out_channels_first_layer = model_args.get('out_channels_first_layer',64),             normalization = model_args.get('normalization', None),             pooling_type = model_args.get('pooling_type', 'max'),             upsampling_type = model_args.get('upsampling_type','conv'),             preactivation = model_args.get('preactivation',False),             residual = model_args.get('residual',False),             padding = model_args.get('padding',0),             padding_mode = model_args.get('padding_mode','zeros'),             activation = model_args.get('activation','ReLU'),             initial_dilation = model_args.get('initial_dilation',None),             dropout = model_args.get('dropout',0),             monte_carlo_dropout = model_args.get('monte_carlo_dropout',0)         )          def forward(self, x):             x = self.unet.forward(x)             x = F.softmax(x, dim=UNetTrainingPlan.MyUNet.CHANNELS_DIMENSION)             return x      def init_model(self, model_args):         return UNetTrainingPlan.MyUNet(model_args)      def init_dependencies(self):         return [\"from torch import nn\",                'import torch.nn.functional as F',                'from torch.optim import AdamW',                'from unet import UNet',                'from monai.transforms import Compose, NormalizeIntensity, Resize',                 'from fedbiomed.common.data import FlambyDataset']      def init_optimizer(self, optimizer_args):         return AdamW(self.model().parameters(),                      lr=optimizer_args[\"lr\"],                      betas=optimizer_args[\"betas\"],                      eps=optimizer_args[\"eps\"])      @staticmethod     def get_dice_loss(output, target, epsilon=1e-9):         SPATIAL_DIMENSIONS = 2, 3, 4         p0 = output         g0 = target         p1 = 1 - p0         g1 = 1 - g0         tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)         fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)         fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)         num = 2 * tp         denom = 2 * tp + fp + fn + epsilon         dice_score = num / denom         return 1. - dice_score      def training_step(self, data, target):         output = self.model().forward(data)         loss = UNetTrainingPlan.get_dice_loss(output, target)         avg_loss = loss.mean()         return avg_loss      def testing_step(self, data, target):         prediction = self.model().forward(data)         loss = UNetTrainingPlan.get_dice_loss(prediction, target)         avg_loss = loss.mean()  # average per batch         return avg_loss      def training_data(self, batch_size=2):         dataset = FlambyDataset()         transform = Compose([Resize((48,60,48)), NormalizeIntensity()])         dataset.init_transform(transform)         train_kwargs = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset, **train_kwargs) In\u00a0[\u00a0]: Copied! <pre>model_args = {\n    'in_channels': 1,\n    'out_classes': 2,\n    'dimensions': 3,\n    'num_encoding_blocks': 3,\n    'out_channels_first_layer': 8,\n    'normalization': 'batch',\n    'upsampling_type': 'linear',\n    'padding': True,\n    'activation': 'PReLU',\n}\n\ntraining_args = {\n    'batch_size': 16,\n    'optimizer_args': {\n        'lr': 0.001,\n        'betas': (0.9, 0.999),\n        'eps': 1e-08\n    },\n    'epochs': 1,\n    'dry_run': False,\n    'log_interval': 2,\n    'test_ratio' : 0.0,\n    'test_on_global_updates': False,\n    'test_on_local_updates': False,\n    'batch_maxnum': 2 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n</pre> model_args = {     'in_channels': 1,     'out_classes': 2,     'dimensions': 3,     'num_encoding_blocks': 3,     'out_channels_first_layer': 8,     'normalization': 'batch',     'upsampling_type': 'linear',     'padding': True,     'activation': 'PReLU', }  training_args = {     'batch_size': 16,     'optimizer_args': {         'lr': 0.001,         'betas': (0.9, 0.999),         'eps': 1e-08     },     'epochs': 1,     'dry_run': False,     'log_interval': 2,     'test_ratio' : 0.0,     'test_on_global_updates': False,     'test_on_local_updates': False,     'batch_maxnum': 2 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples }   In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['flixi']\nnum_rounds = 1\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=UNetTrainingPlan,\n                 training_args=training_args,\n                 round_limit=num_rounds,\n                 aggregator=FedAverage(),\n                )\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['flixi'] num_rounds = 1  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=UNetTrainingPlan,                  training_args=training_args,                  round_limit=num_rounds,                  aggregator=FedAverage(),                 ) In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True)"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#flamby-integration-in-fed-biomed","title":"FLamby integration in Fed-BioMed\u00b6","text":"<p>This notebook showcases some examples of the integration between FLamby and Fed-BioMed.</p> <p>For a thorough understanding, please visit the Tutorials section of our documentation.</p> <p>This tutorial assumes that you know and understand the basics of Fed-BioMed, that you have already set up the network component, and are familiar with flow of adding data through the node CLI interface. For an introduction to Fed-BioMed, please follow our PyTorch MNIST tutorial.</p>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#downloading-flamby-datasets","title":"Downloading FLamby datasets\u00b6","text":"<p>Before using FLamby, you need to download the FLamby datasets that you plan to use. For licensing reasons, these are not including directly in the FLamby installation.</p> <p>To download the <code>fed_ixi</code> dataset in <code>${FEDBIOMED_DIR}/data</code>, follow FLamby download instructions. In a nutshell:</p> <ul> <li>execute on the researcher</li> </ul> <pre>source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment researcher\npip install nibabel\n</pre> <ul> <li>then execute on each node (where <code>${FEDBIOMED_DIR}</code> is the base directory of Fed-BioMed):</li> </ul> <pre>source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment node\npip install nibabel\npython $(find $CONDA_PREFIX -path */fed_ixi/dataset_creation_scripts/download.py) -o ${FEDBIOMED_DIR}/data\n</pre> <p>To download the <code>fed_heart_disease</code> dataset in <code>${FEDBIOMED_DIR}/data</code>, follow FLamby download instructions. In a nutshell:</p> <ul> <li>execute on the researcher</li> </ul> <pre>source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment researcher\npip install wget\n</pre> <ul> <li>then execute on each node (where <code>${FEDBIOMED_DIR}</code> is the base directory of Fed-BioMed):</li> </ul> <pre>source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment node\npip install wget\npython $(find $CONDA_PREFIX -path */fed_heart_disease/dataset_creation_scripts/download.py) --output-folder ${FEDBIOMED_DIR}/data\n</pre>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>If you haven't done so already, install the additional dependencies required by the flamby datasets/features that you intend on using.</p> <p>You may check out which dependencies are needed by each dataset directly from Flamby's <code>setup.py</code> file. In our case we'll be using the federated IXI and federated heart disease datasets, hence we'll need wget, monai and nibabel.</p>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#running-a-flamby-experiment-in-a-federated-setting-with-fed-biomed","title":"Running a FLamby experiment in a federated setting with Fed-BioMed\u00b6","text":"<p>Before running a federated experiment, we need to add a FLamby dataset to a node. From a terminal, <code>cd</code> to the Fed-BioMed root installation directory and run</p> <pre>./scripts/fedbiomed_run node add\n</pre> <p>Then follow these instructions:</p> <ul> <li>Select option 6 (<code>flamby</code>) when prompted about the data type</li> <li>type any name for the database (suggested <code>flamby-ixi</code>), press Enter to continue</li> <li>type <code>flixi</code> when prompted for tags, press Enter to continue</li> <li>type any description (suggested <code>flamby-ixi</code>), press Enter to continue</li> <li>select option 3 (<code>fed_ixi</code>) when prompted for the FLamby dataset to be configured</li> <li>type a number in the given range, press Enter to continue</li> <li>type any description for the data loading plan (suggested <code>flamby-ixi-dlp</code>), press Enter to continue</li> </ul> <p>Optionally, repeat the instructions above for the <code>fed_heart_disease</code> dataset, using <code>flheart</code> for tags.</p> <p>Finally, start the node with</p> <pre>./scripts/fedbiomed_run node start\n</pre>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#basic-example-fed-ixi","title":"Basic example: Fed-IXI\u00b6","text":"<p>The first example will use the model, optimizer and loss function provided by FLamby for the IXI dataset.</p> <p>The instructions for using FLamby are:</p> <ul> <li>define a <code>TorchTrainingPlan</code></li> <li>in the <code>training_data</code> function, instantiate a <code>FlambyDataset</code></li> <li>make sure to include the necessary dependencies in the <code>init_dependencies</code> function</li> </ul>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#basic-example-fed-heart-disease","title":"Basic example: Fed-Heart-Disease\u00b6","text":"<p>We showcase similar functionalities as the above fed-ixi case, but with FLamby's Heart Disease dataset.</p>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#complex-example-fed-ixi-with-data-preprocessing-and-custom-training-elements","title":"Complex example: Fed-IXI with data preprocessing and custom training elements\u00b6","text":"<p>This example demonstrates how to define transformations for data preprocessing and provide a customized model, optimizer, and loss function. Incidentally, it also shows how to use <code>model_args</code> and <code>training_args</code> to parametrize the model, optimizer, and training loop.</p>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#definition-of-preprocessing-transforms","title":"Definition of preprocessing transforms\u00b6","text":"<p>This is achieved in the <code>training_data</code> function. After instantiating the <code>FlambyDataset</code>, you may use the <code>init_transform</code> function to attach a preprocessing transformation for your data. Note that the transform that you define must be of type <code>torchvision.transforms.Compose</code> or <code>monai.transforms.Compose</code>.</p>"},{"location":"tutorials/flamby/flamby-integration-into-fedbiomed/#definition-of-custom-model-optimizer-and-loss","title":"Definition of custom model, optimizer and loss\u00b6","text":"<p>This is achieved just like any <code>TorchTrainingPlan</code>, through the functions <code>init_model</code>, <code>init_optimizer</code>, and <code>training_step</code>.</p>"},{"location":"tutorials/flamby/flamby/","title":"FLamby integration in Fed-BioMed general concepts","text":"<p>Fed-BioMed supports easy integration with Owkin's FLamby. FLamby is a benchmark and dataset suite for cross-silo federated learning with natural partitioning, focused on healthcare applications. FLamby may be used as either a dataset suite or as a fully-fledged benchmark to compare the performance of ML algorithms against a set of standardized approaches and data.</p> <p>Fed-BioMed integration with FLamby is only supported with the PyTorch framework. Hence, to use FLamby you must declare a <code>TorchTrainingPlan</code> for your experiment. Fed-BioMed provides a <code>FlambyDataset</code> class that, together with a correctly configured <code>DataLoadingPlan</code>, takes care of all the boilerplate necessary for loading a FLamby dataset in a federated experiment.</p> <p>Summary</p> <p>To use FLamby in your Fed-BioMed experiment, follow these simple rules:</p> <ul> <li>use a <code>TorchTrainingPlan</code></li> <li>create a <code>FlambyDataset</code> in your <code>training_data</code> function</li> <li>Make sure to properly configure a <code>DataLoadingPlan</code> when loading the data to the node</li> </ul>"},{"location":"tutorials/flamby/flamby/#installing-flamby-and-downloading-the-datasets","title":"Installing FLamby and downloading the datasets","text":"<p>Fed-BioMed comes with the FLamby library pre-installed.</p> <p>However, your manual intervention is still required to:</p> <ul> <li>install any dependencies required by the FLamby datasets that you wish to use.</li> <li>download those datasets.</li> </ul> <p>To install the dependencies:</p> <ul> <li>check in FLamby's setup.py the dependencies <code>&lt;PACKAGES&gt;</code> for the dataset you wish to use. For example, dataset <code>tcga</code> needs <code>lifelines</code>.</li> <li>install the dependencies by executing on the researcher (where <code>${FEDBIOMED_DIR}</code> is Fed-BioMed's base directory) <pre><code>source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment researcher\npip install &lt;PACKAGES&gt;\n</code></pre></li> <li>install dependencies by executing on each node <pre><code>source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment node\npip install &lt;PACKAGES&gt;\n</code></pre></li> </ul> <p>To download the dataset named <code>&lt;DATASET&gt;</code> (eg <code>fed_ixi</code> for IXI):</p> <ul> <li>download the dataset by executing on each node <pre><code>source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment node\npython $(find $CONDA_PREFIX -path */&lt;DATASET&gt;/dataset_creation_scripts/download.py) -o ${FEDBIOMED_DIR}/data\n</code></pre></li> </ul>"},{"location":"tutorials/flamby/flamby/#defining-the-training-plan","title":"Defining the Training Plan","text":"<p>In Fed-BioMed, researchers create a training plan  to define various aspects of their federated ML experiment, such as the model, the data, the optimizer, and others. To leverage FLamby functionalities within your Fed-BioMed experiment, you will be required to create a  custom training plan inheriting from <code>fedbiomed.common.data.TorchTrainingPlan</code>. </p> <p>For details on the meaning of the different functions in a training plan, and how to implement them correctly,  please follow the TrainingPlan user guide.  Since FLamby is highly compatible with PyTorch, you may use the models and optimizers provided by FLamby in your  Fed-BioMed experiment seamlessly. See the code below for an example:</p> <pre><code>from fedbiomed.common.data import TorchTrainingPlan\nfrom flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args):\n        return Baseline()\n\n    def init_optimizer(self, optimizer_args):\n        return Optimizer()\n\n    def init_dependencies(self):\n        return [\"from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\"]\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        return BaselineLoss(output, target)\n\n    def training_data(self, batch_size=2):\n        # See explanation below\n        pass\n</code></pre> <p>Obviously, you may also plug different definitions for the model, optimizer, and loss function, provided that you respect the conditions and guidelines for <code>TorchTrainingPlan</code>.</p>"},{"location":"tutorials/flamby/flamby/#implementing-the-training_data-function","title":"Implementing the <code>training_data</code> function","text":"<p>Fed-BioMed provides a <code>FlambyDataset</code> class that enables simple integration with FLamby datasets. This class requires  an associated <code>DataLoadingPlan</code> to be properly configured in order to work correctly on the node side. If you follow the data adding process through either the CLI or the GUI, the configuration of the <code>DataLoadingPlan</code> will be done automatically for you. </p> <p>To use Flamby, you need to create a FLamby dataset in your <code>training_data</code> function following the example below: <pre><code>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import FlambyDataset, DataManager\n\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_dependencies(self):\n        return [\"from fedbiomed.common.data import FlambyDataset, DataManager\"]\n\n    def training_data(self, batch_size=2):\n        dataset = FlambyDataset()\n        loader_arguments = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset, **loader_arguments)\n\n    # ... Implement the other functions as needed ...\n</code></pre></p>"},{"location":"tutorials/flamby/flamby/#data-transformations","title":"Data transformations","text":"<p>Functional data transformations can be specified in the <code>training_data</code> function, similarly to the common  <code>TorchTrainingPlan</code> pattern. However, for FLamby you are required to use the special function <code>init_transform</code> of <code>FlambyDataset</code>, as per the example below.</p> <pre><code>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom monai.transforms import Compose, Resize, NormalizeIntensity\nfrom fedbiomed.common.data import FlambyDataset, DataManager\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_dependencies(self):\n        return [\"from fedbiomed.common.data import FlambyDataset, DataManager\",\n                \"from monai.transforms import Compose, Resize, NormalizeIntensity\",\n                ]\n\n    def training_data(self, batch_size=2):\n        dataset = FlambyDataset()\n\n        myComposedTransform = Compose([Resize((48,60,48)), NormalizeIntensity()])\n        dataset.init_transform(myComposedTransform)\n\n        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset, **train_kwargs)\n\n    # ... Implement the other functions as needed ...\n</code></pre> <p>Tranforms must always be of <code>Compose</code> type</p> <p>Transforms added to a <code>FlambyDataset</code> must always be either of type <code>torch.transforms.Compose</code> or <code>monai.transforms.Compose</code></p> <p>Do not forget to always add your transform as dependencies in the <code>init_dependencies</code> function!</p>"},{"location":"tutorials/installation/","title":"Installation step-by-step tutorial","text":"<ol> <li>Software installation </li> <li>Setting up environments</li> </ol>"},{"location":"tutorials/installation/0-basic-software-installation/","title":"Fed-BioMed software installation","text":"<p>This tutorial gives steps for installing Fed-BioMed components (network, node, researcher) on a single machine. Deployment documentation explains other available setups.</p>"},{"location":"tutorials/installation/0-basic-software-installation/#hardware-requirements","title":"Hardware requirements","text":"<ul> <li>8GB RAM minimum for handling PyTorch &amp; ML packages</li> </ul>"},{"location":"tutorials/installation/0-basic-software-installation/#system-requirements","title":"System requirements","text":"<p>Fed-BioMed is developed and tested under up to date version of :</p> <ul> <li>Linux Fedora, should also work or be easily ported under most Linux distributions (Ubuntu, etc.)</li> <li>MacOS</li> </ul> <p>Check specific guidelines for installation on Windows 10.</p>"},{"location":"tutorials/installation/0-basic-software-installation/#software-packages","title":"Software packages","text":"<p>The following packages are required for Fed-BioMed :</p> <ul> <li><code>docker</code></li> <li><code>docker-compose</code></li> <li><code>conda</code></li> <li><code>git</code></li> </ul>"},{"location":"tutorials/installation/0-basic-software-installation/#install-docker-and-docker-compose","title":"Install docker and docker-compose","text":""},{"location":"tutorials/installation/0-basic-software-installation/#linux-fedora","title":"Linux Fedora","text":"<p>Install and start docker engine packages. In simple cases it is enough to run :</p> <pre><code>$ sudo dnf install -y dnf-plugins-core\n$ sudo dnf config-manager \\\n    --add-repo \\\n    https://download.docker.com/linux/fedora/docker-ce.repo\n$ sudo dnf install -y docker-ce docker-ce-cli containerd.io\n$ sudo systemctl start docker\n</code></pre> <p>Allow current account to use docker :</p> <pre><code>$ sudo usermod -aG docker $USER\n</code></pre> <p>Check with the account used to run Fed-BioMed that docker is up and can be used by the current account without error :</p> <pre><code>$ docker run hello-world\n</code></pre> <p>Install docker-compose and git : <pre><code>$ sudo dnf install -y docker-compose git\n</code></pre></p>"},{"location":"tutorials/installation/0-basic-software-installation/#macos","title":"MacOS","text":"<p>Install docker and docker-compose choosing one of the available options for example :</p> <ul> <li>official full Docker Desktop installation process, please check product license</li> <li>your favorite third party package manager for example :<ul> <li>macports provides docker docker-compose and git ports</li> <li>homebrew provides docker docker-compose and git formulae</li> </ul> </li> </ul> <p>Check with the account used to run Fed-BioMed docker is up and can be used by the current account without error :</p> <pre><code>$ docker run hello-world\n</code></pre>"},{"location":"tutorials/installation/0-basic-software-installation/#other","title":"Other","text":"<p>Connect under an account with administrator privileges, install <code>docker</code>, ensure it is started and give docker privilege for the account used for running Fed-BioMed. Also install <code>docker-compose</code> and <code>git</code></p> <p>Check with the account used to run Fed-BioMed docker is up and can be used by the current account without error :</p> <pre><code>$ docker run hello-world\n</code></pre>"},{"location":"tutorials/installation/0-basic-software-installation/#install-conda","title":"Install conda","text":""},{"location":"tutorials/installation/0-basic-software-installation/#linux-fedora_1","title":"Linux Fedora","text":"<p>Simply install the package :</p> <pre><code>$ sudo dnf install conda\n</code></pre> <p>Check conda is properly initialized with the following command that should answer the default <code>(base)</code> environment:</p> <pre><code>$ conda env list\n</code></pre>"},{"location":"tutorials/installation/0-basic-software-installation/#other_1","title":"Other","text":"<p>Install conda package manager.</p> <p>During the installation process, let the conda installer initialize conda (answer \"yes\" to \u201cDo you wish the installer to initialize Anaconda3 by running conda init ?\u201d)</p> <p>Check conda is properly initialized with the following command that should answer the default <code>(base)</code> environment: <pre><code>$ conda env list\n</code></pre></p>"},{"location":"tutorials/installation/0-basic-software-installation/#fed-biomed-software","title":"Fed-BioMed software","text":"<p>Download Fed-BioMed software by cloning the git repository :</p> <pre><code>$ git clone -b master https://github.com/fedbiomed/fedbiomed.git\n$ cd fedbiomed\n</code></pre> <p>In the following tutorials, Fed-BioMed commands use a path relative to the base directory of the clone, noted as <code>${FEDBIOMED_DIR}</code>. This is not required for Fed-BioMed to work but enables you to run the tutorials more easily.</p> <p>The way to setup this directory depends on your operating system (Linux, macOSX, Windows), and on your SHELL.</p> <p>As an example, on Linux/macOSX with bash/zsh it could be done as:</p> <pre><code>export FEDBIOMED_DIR=$(pwd)\n</code></pre> <p>or</p> <pre><code>export FEDBIOMED_DIR=${HOME}/where/is/fedbiomed\n</code></pre> <p>Remember, that this environment variable must be initialized (to the same value) for all running shells (you may want to declare it in your shell initialization file).</p> <p>Fed-BioMed is provided under Apache 2.0 License.</p> <p>We don't provide yet a packaged version of Fed-BioMed (conda, pip).</p>"},{"location":"tutorials/installation/0-basic-software-installation/#conda-environments","title":"Conda environments","text":"<p>Fed-BioMed uses conda environments for managing package dependencies.</p> <p>Create or update the conda environments with :</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/configure_conda\n</code></pre> <p>List the existing conda environments and check the 3 environments <code>fedbiomed-network</code> <code>fedbiomed-node</code> <code>fedbiomed-researcher</code> were created :</p> <pre><code>$ conda env list\n[...]\nfedbiomed-network        /home/mylogin/.conda/envs/fedbiomed-network\nfedbiomed-node           /home/mylogin/.conda/envs/fedbiomed-node\nfedbiomed-researcher     /home/mylogin/.conda/envs/fedbiomed-researcher\n[...]\n</code></pre> <p>Conda environment for Fed-BioMed Node GUI</p> <p>Fed-BioMed comes with a user interface that allows data owners (node users) to deploy datasets and manage requested  training plans easily. To be able to use Node GUI you need to install Fed-BioMed GUI conda environment as well.  You can use following command to install GUI conda environment.</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/configure_conda gui\n</code></pre> <p>Please follow Node GUI user guide to get more information about launching GUI on your local.</p>"},{"location":"tutorials/installation/0-basic-software-installation/#the-next-step","title":"The Next Step","text":"<p>After the steps above are completed you will be ready to start Fed-BioMed components. In the following tutorial you will learn how to launch components and add data in Fed-BioMed to prepare an experiment.</p>"},{"location":"tutorials/installation/1-setting-up-environment/","title":"Set up your Fed-BioMed environment","text":""},{"location":"tutorials/installation/1-setting-up-environment/#a-word-on-fed-biomed-components","title":"A word on Fed-BioMed components","text":"<p>A Fed-BioMed instance includes 3 types of components :</p> <ul> <li>the <code>network</code> which handles the communication between the nodes and the researcher. It is composed of a MQTT messaging server and a HTTP/REST file exchange server for the models and parameters</li> <li>one or more <code>nodes</code>, each one provides datasets for experiments and locally trains the models on these datasets</li> <li>the <code>researcher</code> which defines and orchestrates a federated learning experiment. The experiment looks for nodes providing expected datasets, selects nodes, sends nodes a model and initial parameters, requests nodes to locally train the model on datasets, collects local training output, federates the output to update aggregated parameters.</li> </ul> <p><code>nodes</code> and <code>researcher</code> compose the Fed-BioMed software, <code>network</code> are supporting services based on docker images.</p> <p>In this tutorial you learn how to launch Fed-BioMed components using the <code>fedbiomed_run</code> script.</p>"},{"location":"tutorials/installation/1-setting-up-environment/#launching-fed-biomed-components","title":"Launching Fed-BioMed components","text":""},{"location":"tutorials/installation/1-setting-up-environment/#network","title":"Network","text":"<p>Network is the first Fed-BioMed component to launch, as it enables other components to communicate.</p> <p>Launch the network with : <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run network\n</code></pre></p> <p>Check the mosquitto and fedbiomed-network containers are <code>Up</code> with :</p> <pre><code>$ docker container ps\nCONTAINER ID  IMAGE                 COMMAND                CREATED       STATUS       PORTS                                                                                NAMES\n954fda27350a  fedbiomed/dev-restful \"/entrypoint.sh\"       4 seconds ago Up 3 seconds 0.0.0.0:8844-&gt;8000/tcp, :::8844-&gt;8000/tcp                                            fedbiomed-dev-restful\nff56256260b1  eclipse-mosquitto     \"/usr/sbin/mosquitto\u2026\" 4 seconds ago Up 3 seconds 0.0.0.0:1883-&gt;1883/tcp, :::1883-&gt;1883/tcp, 0.0.0.0:9001-&gt;9001/tcp, :::9001-&gt;9001/tcp fedbiomed-dev-mqtt\n</code></pre>"},{"location":"tutorials/installation/1-setting-up-environment/#node","title":"Node","text":"<p>Once the network is ready, one or more nodes shall be started and configured to provide datasets for the experiments.</p> <p>By default, a Fed-BioMed node does not provide any dataset to experiments. By adding one or more datasets to a node, you indicate which data a Fed-BioMed node provides to experiments for training future model.</p> <p>When you stop and restart a node, data sharing configuration is retained : previously added datasets remain available for next experiments.</p>"},{"location":"tutorials/installation/1-setting-up-environment/#starting-a-first-node","title":"Starting a first node","text":"<p>Launch a node with : <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node start\n</code></pre></p> <p>You should get the following output;</p> <pre><code>** Activating fedbiomed-node environment\nConda   env: fedbiomed-node\nPython  env: /path/to/python/env/\nMQTT   host: localhost\nMQTT   port: 1883\nUPLOADS url: http://localhost:8844/upload/\n2022-01-06 11:40:43,303 fedbiomed INFO - Component environment:\n2022-01-06 11:40:43,303 fedbiomed INFO - - type                             = ComponentType.NODE\n2022-01-06 11:40:43,303 fedbiomed INFO - - training_plan_approval           = False\n2022-01-06 11:40:43,303 fedbiomed INFO - - allow_default_training_plans     = True\n\n\n   __         _ _     _                          _                   _\n  / _|       | | |   (_)                        | |                 | |\n | |_ ___  __| | |__  _  ___  _ __ ___   ___  __| |  _ __   ___   __| | ___\n |  _/ _ \\/ _` | '_ \\| |/ _ \\| '_ ` _ \\ / _ \\/ _` | | '_ \\ / _ \\ / _` |/ _ \\\n | ||  __/ (_| | |_) | | (_) | | | | | |  __/ (_| | | | | | (_) | (_| |  __/\n |_| \\___|\\__,_|_.__/|_|\\___/|_| |_| |_|\\___|\\__,_| |_| |_|\\___/ \\__,_|\\___|\n\n\n\n    - \ud83c\udd94 Your node ID: &lt;generated_node_id&gt;\n\n2022-01-06 11:40:43,852 fedbiomed INFO - Node started as process with pid = 822849\nTo stop press Ctrl + C.\n2022-01-06 11:40:43,853 fedbiomed INFO - Launching node...\n2022-01-06 11:40:43,853 fedbiomed WARNING - Training plan approval for train request is not activated. This might cause security problems. Please, consider to enable training plan approval.\n2022-01-06 11:40:43,853 fedbiomed INFO - Starting communication channel with network\n2022-01-06 11:40:43,855 fedbiomed INFO - Messaging &lt;generated_node_id&gt; connected to the message broker, object = &lt;fedbiomed.common.messaging.Messaging object at 0x7ff6b39f3070&gt;\n2022-01-06 11:40:43,866 fedbiomed DEBUG -  adding handler: MQTT\n2022-01-06 11:40:43,866 fedbiomed INFO - Starting task manager\n</code></pre>"},{"location":"tutorials/installation/1-setting-up-environment/#adding-data-to-node","title":"Adding data to node","text":"<p>Starting a new node doesn't provide any data to Fed-BioMed experiments through the node. Adding a dataset is the process of indicating to a Fed-BioMed node which dataset you wish to make available for Fed-BioMed experiments through the node. Same datasets will be automatically provided at subsequent starts of the node.</p> <p>Begin adding a dataset to the node with this command : <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node add\n</code></pre></p> <p>The command then asks you the type of dataset to add to the node :</p> <pre><code>Welcome to the Fed-BioMed CLI data manager\nPlease select the data type that you're configuring:\n       1) csv\n       2) default\n       3) images\nselect:\n</code></pre> <p>Choose <code>2) default</code> to download the MNIST dataset and add it to the node. Other options allow you to add custom datasets formatted as CSV files or image banks.</p> <p>The command then asks for the tags that the node uses to advertise datasets to experiments :</p> <pre><code>MNIST will be added with tags ['#MNIST', '#dataset'] [y/N]\n</code></pre> <p>Choose <code>y</code> to confirm proposed default tags.</p> <p>A file browser opens : select the folder where you want to save the downloaded MNIST dataset. If you re-use the same folder for next tests, the cached version of MNIST will be used.</p> <p>When downloading completes you should have the following output : <pre><code>Great! Take a look at your data:\nname    data_type   tags                     description        shape                path                dataset_id\n------  ---------   ----------------------   ----------------   ------------------   -------------       ---------------------\nMNIST   default     ['#MNIST', '#dataset']   MNIST database     [60000, 1, 28, 28]   &lt;data_set_path&gt;     &lt;dataset_id&gt;\n</code></pre></p> <p>You can check at any time which datasets are provided by a node with : <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node list\n</code></pre></p> <p>When no dataset is provided by the node, the command <code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node list</code> will answer <code>No data has been set up.</code> as its final output line.</p>"},{"location":"tutorials/installation/1-setting-up-environment/#starting-more-nodes","title":"Starting more nodes","text":"<p>To launch and configure more than one node, specify a different (non-default) configuration file for all commands related a the subsequent node.</p> <p>For example to launch and add a dataset to a second node using the <code>config2.ini</code> configuration file : <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config2.ini start\n$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config2.ini list\n$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config2.ini add\n</code></pre></p> <p>Warning : if you launch more than one node with the same configuration file, no error is detected, but the nodes are not functional</p>"},{"location":"tutorials/installation/1-setting-up-environment/#researcher","title":"Researcher","text":"<p>Once the network and nodes are ready, you can start working with the researcher.</p> <p>Launch the researcher jupyter notebook console with : <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run researcher start\n</code></pre></p> <p>For next tutorials, create a new notebook (<code>New</code> and <code>Python3</code> in the top right corner of the jupyter notebook), and cut/paste the tutorial code snippets in the notebook.</p> <p>Several example notebooks are also provided with Fed-BioMed.</p>"},{"location":"tutorials/installation/1-setting-up-environment/#clean-and-restart-fed-biomed-components","title":"Clean and restart Fed-BioMed components","text":""},{"location":"tutorials/installation/1-setting-up-environment/#a-word-on-working-with-environments","title":"A word on working with environments","text":"<p>This tutorial explained how to launch Fed-BioMed components using the <code>fedbiomed_run</code> script. Behind the hood, each Fed-BioMed component runs in its own environment (conda, variables).</p> <p>If at some point you want to work interactively in the same environment as a Fed-BioMed component (eg. for debugging), you can activate this environment from a console.</p> <p>Warning : this feature only works with bash, ksh and zsh shells (other shells like csh/tcsh are not yet suppported)</p> <p>To activate the network environment:</p> <pre><code>$ source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment network\n</code></pre> <p>To activate the node environment:</p> <pre><code>$ source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment node\n</code></pre> <p>To activate the researcher environment:</p> <pre><code>$ source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment researcher\n</code></pre> <p>You can also, reset the environment with:</p> <pre><code>$ source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment reset\n</code></pre>"},{"location":"tutorials/installation/1-setting-up-environment/#clean","title":"Clean","text":"<p>A Fed-BioMed instance can handle successive operations like adding and then removing nodes or datasets, conducting sequential experiments. But after testing and tweeking, thing may get wrong. At this point, we provide you a script to clean all things. Afterwards, you will need to restart from scratch (start network, add datasets to nodes, start nodes, etc...)</p> <p>To clean your Fed-BioMed instance :</p> <ul> <li>stop the researcher : shutdown the notebook kernel (<code>Quit</code> in on the notebook interface or <code>ctrl-C</code> on the console)</li> <li>stop the nodes : interrupt (<code>ctrl-C</code>) on the nodes console</li> <li>stop the network and remove all configuration files, dataset sharing configuration, temporary files, caches for all Fed-BioMed components with :</li> </ul> <pre><code>$ source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment clean\n</code></pre> <p>When you restart a node after cleaning the Fed-BioMed instance, the node doesn't provides any dataset, as the dataset sharing configuration was reset in the cleaning process. Of course, Fed-BioMed did not delete any data, it just stopped sharing them.</p>"},{"location":"tutorials/installation/1-setting-up-environment/#restart","title":"Restart","text":"<p>After cleaning your Fed-BioMed environment, restart the network, a node and the researcher to be ready for the next tutorial ... do you remember the commands ?</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run network\n$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node add\n$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node start\n$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run researcher start\n</code></pre>"},{"location":"tutorials/installation/1-setting-up-environment/#whats-next","title":"What's Next?","text":"<p>You now have a network and a node ready for an experiment. You also know how to stop an experiment, clean and restart your Fed-BioMed environment. In the following tutorial you will launch your first Fed-BioMed experiment.</p>"},{"location":"tutorials/medical/download_and_split_ixi/","title":"Download and split ixi","text":"In\u00a0[\u00a0]: Copied! <pre>import hashlib\nimport os\nimport requests\nfrom tqdm import tqdm\nimport argparse\nfrom zipfile import ZipFile\nimport shutil\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n</pre> import hashlib import os import requests from tqdm import tqdm import argparse from zipfile import ZipFile import shutil import pandas as pd from sklearn.model_selection import train_test_split In\u00a0[\u00a0]: Copied! <pre>config_file = \"\"\"\n[default]\nnode_id = CENTER_ID\nuploads_url = http://localhost:8844/upload/\n\n[mqtt]\nbroker_ip = localhost\nport = 1883\nkeep_alive = 60\n\n[security]\nhashing_algorithm = SHA256\nallow_default_training_plans = True\ntraining_plan_approval = False\n\"\"\"\n</pre> config_file = \"\"\" [default] node_id = CENTER_ID uploads_url = http://localhost:8844/upload/  [mqtt] broker_ip = localhost port = 1883 keep_alive = 60  [security] hashing_algorithm = SHA256 allow_default_training_plans = True training_plan_approval = False \"\"\" In\u00a0[\u00a0]: Copied! <pre>def parse_args():\n    parser = argparse.ArgumentParser(description='IXI Sample downloader and splitter')\n    parser.add_argument('-f', '--root_folder', required=True, type=str)\n\n    return parser.parse_args()\n</pre> def parse_args():     parser = argparse.ArgumentParser(description='IXI Sample downloader and splitter')     parser.add_argument('-f', '--root_folder', required=True, type=str)      return parser.parse_args() In\u00a0[\u00a0]: Copied! <pre>def has_correct_checksum_md5(filename, hash):\n    with open(filename, \"rb\") as f:\n        file_hash = hashlib.md5()\n        while chunk := f.read(8192):\n            file_hash.update(chunk)\n    return str(file_hash.hexdigest()) == hash\n</pre> def has_correct_checksum_md5(filename, hash):     with open(filename, \"rb\") as f:         file_hash = hashlib.md5()         while chunk := f.read(8192):             file_hash.update(chunk)     return str(file_hash.hexdigest()) == hash In\u00a0[\u00a0]: Copied! <pre>def download_file(url, filename):\n\"\"\"\n    Helper method handling downloading large files from `url` to `filename`. Returns a pointer to `filename`.\n    \"\"\"\n    print('Downloading file from:', url)\n    print('File will be saved as:', filename)\n    chunkSize = 1024\n    r = requests.get(url, stream=True)\n    with open(filename, 'wb') as f:\n        pbar = tqdm(unit=\"B\", total=int(r.headers['Content-Length']))\n        for chunk in r.iter_content(chunk_size=chunkSize):\n            if chunk:  # filter out keep-alive new chunks\n                pbar.update(len(chunk))\n                f.write(chunk)\n    return filename\n</pre> def download_file(url, filename):     \"\"\"     Helper method handling downloading large files from `url` to `filename`. Returns a pointer to `filename`.     \"\"\"     print('Downloading file from:', url)     print('File will be saved as:', filename)     chunkSize = 1024     r = requests.get(url, stream=True)     with open(filename, 'wb') as f:         pbar = tqdm(unit=\"B\", total=int(r.headers['Content-Length']))         for chunk in r.iter_content(chunk_size=chunkSize):             if chunk:  # filter out keep-alive new chunks                 pbar.update(len(chunk))                 f.write(chunk)     return filename In\u00a0[\u00a0]: Copied! <pre>def download_and_extract_ixi_sample(root_folder):\n    url = 'https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/7kd5wj7v7p-3.zip'\n    zip_filename = os.path.join(root_folder, 'notebooks', 'data', '7kd5wj7v7p-3.zip')\n    data_folder = os.path.join(root_folder, 'notebooks', 'data')\n    extracted_folder = os.path.join(data_folder, '7kd5wj7v7p-3', 'IXI_sample')\n\n    # Extract if ZIP exists but not folder\n    if not os.path.exists(zip_filename):\n        # Download if it does not exist\n        download_file(url, zip_filename)\n        \n    # Check if extracted folder exists\n    if os.path.isdir(extracted_folder):\n        print(f'Dataset folder already exists in {extracted_folder}')\n        return extracted_folder\n\n    assert has_correct_checksum_md5(zip_filename, 'eecb83422a2685937a955251fa45cb03')\n    with ZipFile(zip_filename, 'r') as zip_obj:\n        zip_obj.extractall(data_folder)\n\n    assert os.path.isdir(extracted_folder)\n    return extracted_folder\n</pre> def download_and_extract_ixi_sample(root_folder):     url = 'https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/7kd5wj7v7p-3.zip'     zip_filename = os.path.join(root_folder, 'notebooks', 'data', '7kd5wj7v7p-3.zip')     data_folder = os.path.join(root_folder, 'notebooks', 'data')     extracted_folder = os.path.join(data_folder, '7kd5wj7v7p-3', 'IXI_sample')      # Extract if ZIP exists but not folder     if not os.path.exists(zip_filename):         # Download if it does not exist         download_file(url, zip_filename)              # Check if extracted folder exists     if os.path.isdir(extracted_folder):         print(f'Dataset folder already exists in {extracted_folder}')         return extracted_folder      assert has_correct_checksum_md5(zip_filename, 'eecb83422a2685937a955251fa45cb03')     with ZipFile(zip_filename, 'r') as zip_obj:         zip_obj.extractall(data_folder)      assert os.path.isdir(extracted_folder)     return extracted_folder In\u00a0[\u00a0]: Copied! <pre>if __name__ == '__main__':\n    args = parse_args()\n    root_folder = os.path.abspath(os.path.expanduser(args.root_folder))\n    assert os.path.isdir(root_folder), f'Folder does not exist: {root_folder}'\n\n    # Centralized dataset\n    centralized_data_folder = download_and_extract_ixi_sample(root_folder)\n\n    # Federated Dataset\n    federated_data_folder = os.path.join(root_folder, 'notebooks', 'data', 'Hospital-Centers')\n    shutil.rmtree(federated_data_folder, ignore_errors=True)\n\n    csv_global = os.path.join(centralized_data_folder, 'participants.csv')\n    allcenters = pd.read_csv(csv_global)\n\n    # Split centers\n    center_names = ['Guys', 'HH', 'IOP']\n    center_dfs = list()\n\n    for center_name in center_names:\n        cfg_folder = os.path.join(args.root_folder, 'etc')\n        os.makedirs(cfg_folder, exist_ok=True)\n        cfg_file = os.path.join(cfg_folder, f'{center_name.lower()}.ini')\n\n        print(f'Creating node at: {cfg_file}')\n        with open(cfg_file, 'w') as f:\n            f.write(config_file.replace('CENTER_ID', center_name))\n\n        df = allcenters[allcenters.SITE_NAME == center_name]\n        center_dfs.append(df)\n\n        train, test = train_test_split(df, test_size=0.1, random_state=21)\n\n        train_folder = os.path.join(federated_data_folder, center_name, 'train')\n        holdout_folder = os.path.join(federated_data_folder, center_name, 'holdout')\n        if not os.path.exists(train_folder):\n            os.makedirs(train_folder)\n        if not os.path.exists(holdout_folder):\n            os.makedirs(holdout_folder)\n\n        for subject_folder in train.FOLDER_NAME.values:\n            shutil.copytree(\n                src=os.path.join(centralized_data_folder, subject_folder),\n                dst=os.path.join(train_folder, subject_folder),\n                dirs_exist_ok=True\n            )\n\n        train_participants_csv = os.path.join(train_folder, 'participants.csv')\n        train.to_csv(train_participants_csv)\n\n        for subject_folder in test.FOLDER_NAME.values:\n            shutil.copytree(\n                src=os.path.join(centralized_data_folder, subject_folder),\n                dst=os.path.join(holdout_folder, subject_folder),\n                dirs_exist_ok=True\n            )\n        test.to_csv(os.path.join(holdout_folder, 'participants.csv'))\n\n    print(f'Centralized dataset located at: {centralized_data_folder}')\n    print(f'Federated dataset located at: {federated_data_folder}')\n\n    print()\n    print('Please add the data to your nodes executing and using the `bids-train` tag:')\n    for center_name in center_names:\n        print(f'\\t./scripts/fedbiomed_run node config {center_name.lower()}.ini add')\n\n    print()\n    print('Then start your nodes by executing:')\n    for center_name in center_names:\n        print(f'\\t./scripts/fedbiomed_run node config {center_name.lower()}.ini start')\n</pre> if __name__ == '__main__':     args = parse_args()     root_folder = os.path.abspath(os.path.expanduser(args.root_folder))     assert os.path.isdir(root_folder), f'Folder does not exist: {root_folder}'      # Centralized dataset     centralized_data_folder = download_and_extract_ixi_sample(root_folder)      # Federated Dataset     federated_data_folder = os.path.join(root_folder, 'notebooks', 'data', 'Hospital-Centers')     shutil.rmtree(federated_data_folder, ignore_errors=True)      csv_global = os.path.join(centralized_data_folder, 'participants.csv')     allcenters = pd.read_csv(csv_global)      # Split centers     center_names = ['Guys', 'HH', 'IOP']     center_dfs = list()      for center_name in center_names:         cfg_folder = os.path.join(args.root_folder, 'etc')         os.makedirs(cfg_folder, exist_ok=True)         cfg_file = os.path.join(cfg_folder, f'{center_name.lower()}.ini')          print(f'Creating node at: {cfg_file}')         with open(cfg_file, 'w') as f:             f.write(config_file.replace('CENTER_ID', center_name))          df = allcenters[allcenters.SITE_NAME == center_name]         center_dfs.append(df)          train, test = train_test_split(df, test_size=0.1, random_state=21)          train_folder = os.path.join(federated_data_folder, center_name, 'train')         holdout_folder = os.path.join(federated_data_folder, center_name, 'holdout')         if not os.path.exists(train_folder):             os.makedirs(train_folder)         if not os.path.exists(holdout_folder):             os.makedirs(holdout_folder)          for subject_folder in train.FOLDER_NAME.values:             shutil.copytree(                 src=os.path.join(centralized_data_folder, subject_folder),                 dst=os.path.join(train_folder, subject_folder),                 dirs_exist_ok=True             )          train_participants_csv = os.path.join(train_folder, 'participants.csv')         train.to_csv(train_participants_csv)          for subject_folder in test.FOLDER_NAME.values:             shutil.copytree(                 src=os.path.join(centralized_data_folder, subject_folder),                 dst=os.path.join(holdout_folder, subject_folder),                 dirs_exist_ok=True             )         test.to_csv(os.path.join(holdout_folder, 'participants.csv'))      print(f'Centralized dataset located at: {centralized_data_folder}')     print(f'Federated dataset located at: {federated_data_folder}')      print()     print('Please add the data to your nodes executing and using the `bids-train` tag:')     for center_name in center_names:         print(f'\\t./scripts/fedbiomed_run node config {center_name.lower()}.ini add')      print()     print('Then start your nodes by executing:')     for center_name in center_names:         print(f'\\t./scripts/fedbiomed_run node config {center_name.lower()}.ini start')"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/","title":"Brain Segmentation","text":"In\u00a0[1]: Copied! <pre>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.logger import logger\nfrom fedbiomed.common.data import DataManager, MedicalFolderDataset\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom unet import UNet\n\nclass UNetTrainingPlan(TorchTrainingPlan):\n\n    def init_model(self, model_args):\n        model = self.Net(model_args)\n        return model\n\n\n    def init_optimizer(self):\n        optimizer = AdamW(self.model().parameters())\n        return optimizer\n\n    def init_dependencies(self):\n        # Here we define the custom dependencies that will be needed by our custom Dataloader\n        deps = [\"from monai.transforms import (Compose, NormalizeIntensity, AddChannel, Resize, AsDiscrete)\",\n               \"import torch.nn as nn\",\n               'import torch.nn.functional as F',\n               \"from fedbiomed.common.data import MedicalFolderDataset\",\n               'import numpy as np',\n               'from torch.optim import AdamW',\n               'from unet import UNet']\n        return deps\n\n\n    class Net(nn.Module):\n        # Init of UNetTrainingPlan\n        def __init__(self, model_args: dict = {}):\n            super().__init__()\n            self.CHANNELS_DIMENSION = 1\n\n            self.unet = UNet(\n                in_channels = model_args.get('in_channels',1),\n                out_classes = model_args.get('out_classes',2),\n                dimensions = model_args.get('dimensions',2),\n                num_encoding_blocks = model_args.get('num_encoding_blocks',5),\n                out_channels_first_layer = model_args.get('out_channels_first_layer',64),\n                normalization = model_args.get('normalization', None),\n                pooling_type = model_args.get('pooling_type', 'max'),\n                upsampling_type = model_args.get('upsampling_type','conv'),\n                preactivation = model_args.get('preactivation',False),\n                residual = model_args.get('residual',False),\n                padding = model_args.get('padding',0),\n                padding_mode = model_args.get('padding_mode','zeros'),\n                activation = model_args.get('activation','ReLU'),\n                initial_dilation = model_args.get('initial_dilation',None),\n                dropout = model_args.get('dropout',0),\n                monte_carlo_dropout = model_args.get('monte_carlo_dropout',0)\n            )\n\n        def forward(self, x):\n            x = self.unet.forward(x)\n            x = F.softmax(x, dim=self.CHANNELS_DIMENSION)\n            return x\n\n    @staticmethod\n    def get_dice_loss(output, target, epsilon=1e-9):\n        SPATIAL_DIMENSIONS = 2, 3, 4\n        p0 = output\n        g0 = target\n        p1 = 1 - p0\n        g1 = 1 - g0\n        tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n        fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n        fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n        num = 2 * tp\n        denom = 2 * tp + fp + fn + epsilon\n        dice_score = num / denom\n        return 1. - dice_score\n\n    @staticmethod\n    def demographics_transform(demographics: dict):\n\"\"\"Transforms dict of demographics into data type for ML.\n\n        This function is provided for demonstration purposes, but\n        note that if you intend to use demographics data as part\n        of your model's input, you **must** provide a\n        `demographics_transform` function which at the very least\n        converts the demographics dict into a torch.Tensor.\n\n        Must return either a torch Tensor or something Tensor-like\n        that can be easily converted through the torch.as_tensor()\n        function.\"\"\"\n\n        if isinstance(demographics, dict) and len(demographics) == 0:\n            # when input is empty dict, we don't want to transform anything\n            return demographics\n\n        # simple example: keep only some keys\n        keys_to_keep = ['HEIGHT', 'WEIGHT']\n        out = np.array([float(val) for key, val in demographics.items() if key in keys_to_keep])\n\n        # more complex: generate dummy variables for site name\n        # not ideal as it requires knowing the site names in advance\n        # could be better implemented with some preprocess\n        site_names = ['Guys', 'IOP', 'HH']\n        len_dummy_vars = len(site_names) + 1\n        dummy_vars = np.zeros(shape=(len_dummy_vars,))\n        site_name = demographics['SITE_NAME']\n        if site_name in site_names:\n            site_idx = site_names.index(site_name)\n        else:\n            site_idx = len_dummy_vars - 1\n        dummy_vars[site_idx] = 1.\n\n        return np.concatenate((out, dummy_vars))\n\n\n    def training_data(self,  batch_size = 4):\n    # The training_data creates the Dataloader to be used for training in the general class Torchnn of fedbiomed\n        common_shape = (48, 60, 48)\n        training_transform = Compose([AddChannel(), Resize(common_shape), NormalizeIntensity(),])\n        target_transform = Compose([AddChannel(), Resize(common_shape), AsDiscrete(to_onehot=2)])\n\n        dataset = MedicalFolderDataset(\n            root=self.dataset_path,\n            data_modalities='T1',\n            target_modalities='label',\n            transform=training_transform,\n            target_transform=target_transform,\n            demographics_transform=UNetTrainingPlan.demographics_transform)\n        loader_arguments = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset, **loader_arguments)\n\n\n    def training_step(self, data, target):\n        #this function must return the loss to backward it\n        img = data[0]['T1']\n        demographics = data[1]\n        output = self.model().forward(img)\n        loss = UNetTrainingPlan.get_dice_loss(output, target['label'])\n        avg_loss = loss.mean()\n        return avg_loss\n\n    def testing_step(self, data, target):\n        img = data[0]['T1']\n        demographics = data[1]\n        target = target['label']\n        prediction = self.model().forward(img)\n        loss = UNetTrainingPlan.get_dice_loss(prediction, target)\n        avg_loss = loss.mean()  # average per batch\n        return avg_loss\n</pre> from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.logger import logger from fedbiomed.common.data import DataManager, MedicalFolderDataset import torch.nn as nn from torch.optim import AdamW from unet import UNet  class UNetTrainingPlan(TorchTrainingPlan):      def init_model(self, model_args):         model = self.Net(model_args)         return model       def init_optimizer(self):         optimizer = AdamW(self.model().parameters())         return optimizer      def init_dependencies(self):         # Here we define the custom dependencies that will be needed by our custom Dataloader         deps = [\"from monai.transforms import (Compose, NormalizeIntensity, AddChannel, Resize, AsDiscrete)\",                \"import torch.nn as nn\",                'import torch.nn.functional as F',                \"from fedbiomed.common.data import MedicalFolderDataset\",                'import numpy as np',                'from torch.optim import AdamW',                'from unet import UNet']         return deps       class Net(nn.Module):         # Init of UNetTrainingPlan         def __init__(self, model_args: dict = {}):             super().__init__()             self.CHANNELS_DIMENSION = 1              self.unet = UNet(                 in_channels = model_args.get('in_channels',1),                 out_classes = model_args.get('out_classes',2),                 dimensions = model_args.get('dimensions',2),                 num_encoding_blocks = model_args.get('num_encoding_blocks',5),                 out_channels_first_layer = model_args.get('out_channels_first_layer',64),                 normalization = model_args.get('normalization', None),                 pooling_type = model_args.get('pooling_type', 'max'),                 upsampling_type = model_args.get('upsampling_type','conv'),                 preactivation = model_args.get('preactivation',False),                 residual = model_args.get('residual',False),                 padding = model_args.get('padding',0),                 padding_mode = model_args.get('padding_mode','zeros'),                 activation = model_args.get('activation','ReLU'),                 initial_dilation = model_args.get('initial_dilation',None),                 dropout = model_args.get('dropout',0),                 monte_carlo_dropout = model_args.get('monte_carlo_dropout',0)             )          def forward(self, x):             x = self.unet.forward(x)             x = F.softmax(x, dim=self.CHANNELS_DIMENSION)             return x      @staticmethod     def get_dice_loss(output, target, epsilon=1e-9):         SPATIAL_DIMENSIONS = 2, 3, 4         p0 = output         g0 = target         p1 = 1 - p0         g1 = 1 - g0         tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)         fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)         fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)         num = 2 * tp         denom = 2 * tp + fp + fn + epsilon         dice_score = num / denom         return 1. - dice_score      @staticmethod     def demographics_transform(demographics: dict):         \"\"\"Transforms dict of demographics into data type for ML.          This function is provided for demonstration purposes, but         note that if you intend to use demographics data as part         of your model's input, you **must** provide a         `demographics_transform` function which at the very least         converts the demographics dict into a torch.Tensor.          Must return either a torch Tensor or something Tensor-like         that can be easily converted through the torch.as_tensor()         function.\"\"\"          if isinstance(demographics, dict) and len(demographics) == 0:             # when input is empty dict, we don't want to transform anything             return demographics          # simple example: keep only some keys         keys_to_keep = ['HEIGHT', 'WEIGHT']         out = np.array([float(val) for key, val in demographics.items() if key in keys_to_keep])          # more complex: generate dummy variables for site name         # not ideal as it requires knowing the site names in advance         # could be better implemented with some preprocess         site_names = ['Guys', 'IOP', 'HH']         len_dummy_vars = len(site_names) + 1         dummy_vars = np.zeros(shape=(len_dummy_vars,))         site_name = demographics['SITE_NAME']         if site_name in site_names:             site_idx = site_names.index(site_name)         else:             site_idx = len_dummy_vars - 1         dummy_vars[site_idx] = 1.          return np.concatenate((out, dummy_vars))       def training_data(self,  batch_size = 4):     # The training_data creates the Dataloader to be used for training in the general class Torchnn of fedbiomed         common_shape = (48, 60, 48)         training_transform = Compose([AddChannel(), Resize(common_shape), NormalizeIntensity(),])         target_transform = Compose([AddChannel(), Resize(common_shape), AsDiscrete(to_onehot=2)])          dataset = MedicalFolderDataset(             root=self.dataset_path,             data_modalities='T1',             target_modalities='label',             transform=training_transform,             target_transform=target_transform,             demographics_transform=UNetTrainingPlan.demographics_transform)         loader_arguments = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset, **loader_arguments)       def training_step(self, data, target):         #this function must return the loss to backward it         img = data[0]['T1']         demographics = data[1]         output = self.model().forward(img)         loss = UNetTrainingPlan.get_dice_loss(output, target['label'])         avg_loss = loss.mean()         return avg_loss      def testing_step(self, data, target):         img = data[0]['T1']         demographics = data[1]         target = target['label']         prediction = self.model().forward(img)         loss = UNetTrainingPlan.get_dice_loss(prediction, target)         avg_loss = loss.mean()  # average per batch         return avg_loss  In\u00a0[2]: Copied! <pre>model_args = {\n    'in_channels': 1,\n    'out_classes': 2,\n    'dimensions': 3,\n    'num_encoding_blocks': 3,\n    'out_channels_first_layer': 8,\n    'normalization': 'batch',\n    'upsampling_type': 'linear',\n    'padding': True,\n    'activation': 'PReLU',\n}\n\ntraining_args = {\n    'batch_size': 16, \n    'epochs': 2, \n    'dry_run': False,\n    'log_interval': 2,\n    'test_ratio' : 0.1,\n    'test_on_global_updates': True,\n    'test_on_local_updates': True,\n}\n</pre> model_args = {     'in_channels': 1,     'out_classes': 2,     'dimensions': 3,     'num_encoding_blocks': 3,     'out_channels_first_layer': 8,     'normalization': 'batch',     'upsampling_type': 'linear',     'padding': True,     'activation': 'PReLU', }  training_args = {     'batch_size': 16,      'epochs': 2,      'dry_run': False,     'log_interval': 2,     'test_ratio' : 0.1,     'test_on_global_updates': True,     'test_on_local_updates': True, } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['ixi-train']\nnum_rounds = 3\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=UNetTrainingPlan,\n                 training_args=training_args,\n                 round_limit=num_rounds,\n                 aggregator=FedAverage(),\n                 tensorboard=True\n                )\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['ixi-train'] num_rounds = 3  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=UNetTrainingPlan,                  training_args=training_args,                  round_limit=num_rounds,                  aggregator=FedAverage(),                  tensorboard=True                 ) In\u00a0[\u00a0]: Copied! <pre>%load_ext tensorboard\nfrom fedbiomed.researcher.environ import environ\ntensorboard_dir = environ['TENSORBOARD_RESULTS_DIR']\n%tensorboard --logdir \"$tensorboard_dir\"\n</pre> %load_ext tensorboard from fedbiomed.researcher.environ import environ tensorboard_dir = environ['TENSORBOARD_RESULTS_DIR'] %tensorboard --logdir \"$tensorboard_dir\" <p>On a Macbook Pro from 2015 with a 2,5 GHz Quad-Core Intel Core i7 processor and 16GB of DRAM, training for 3 rounds of 2 epochs each took about 30 minutes. The final training curves look like this:</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() In\u00a0[6]: Copied! <pre>local_training_plan = UNetTrainingPlan()\nlocal_model = local_training_plan.init_model(model_args)\n</pre> local_training_plan = UNetTrainingPlan() local_model = local_training_plan.init_model(model_args) In\u00a0[7]: Copied! <pre>for dependency_statement in local_training_plan.init_dependencies():\n    exec(dependency_statement)\n</pre> for dependency_statement in local_training_plan.init_dependencies():     exec(dependency_statement) In\u00a0[8]: Copied! <pre>local_model.load_state_dict(exp.aggregated_params()[exp.round_current()-1]['params'])\n</pre> local_model.load_state_dict(exp.aggregated_params()[exp.round_current()-1]['params']) Out[8]: <pre>&lt;All keys matched successfully&gt;</pre> In\u00a0[274]: Copied! <pre>from torch.utils.data import DataLoader\n\ndata_loaders = []\n\ndatasets = [{\n    'dataset_path' : '&lt;febiomed-dir&gt;/notebooks/data/ixi-data/Hospital-Centers/Guys/holdout/',\n    'dataset_parameters': {\n        'tabular_file': '&lt;febiomed-dir&gt;/notebooks/data/ixi-data/Hospital-Centers/Guys/holdout/participants.csv',\n        'index_col': 14\n        }\n    },\n    {\n    'dataset_path' : '&lt;febiomed-dir&gt;/notebooks/data/ixi-data/Hospital-Centers/HH/holdout/',\n    'dataset_parameters': {\n        'tabular_file': '&lt;febiomed-dir&gt;/notebooks/data/ixi-data/Hospital-Centers/HH/holdout/participants.csv',\n        'index_col': 14\n        }\n    },\n    {\n    'dataset_path' : '&lt;febiomed-dir&gt;/notebooks/data/ixi-data/Hospital-Centers/IOP/holdout/',\n    'dataset_parameters': {\n        'tabular_file': '&lt;febiomed-dir&gt;/notebooks/data/ixi-data/Hospital-Centers/IOP/holdout/participants.csv',\n        'index_col': 14\n        }\n    },\n    \n\n]\n\n\nfor dataset in datasets:\n    local_training_plan.dataset_path = dataset['dataset_path']\n    val_data_manager = local_training_plan.training_data(batch_size=4)\n    val_data_manager._dataset.set_dataset_parameters(dataset['dataset_parameters'])\n    data_loaders.append(DataLoader(val_data_manager._dataset))\n</pre> from torch.utils.data import DataLoader  data_loaders = []  datasets = [{     'dataset_path' : '/notebooks/data/ixi-data/Hospital-Centers/Guys/holdout/',     'dataset_parameters': {         'tabular_file': '/notebooks/data/ixi-data/Hospital-Centers/Guys/holdout/participants.csv',         'index_col': 14         }     },     {     'dataset_path' : '/notebooks/data/ixi-data/Hospital-Centers/HH/holdout/',     'dataset_parameters': {         'tabular_file': '/notebooks/data/ixi-data/Hospital-Centers/HH/holdout/participants.csv',         'index_col': 14         }     },     {     'dataset_path' : '/notebooks/data/ixi-data/Hospital-Centers/IOP/holdout/',     'dataset_parameters': {         'tabular_file': '/notebooks/data/ixi-data/Hospital-Centers/IOP/holdout/participants.csv',         'index_col': 14         }     },       ]   for dataset in datasets:     local_training_plan.dataset_path = dataset['dataset_path']     val_data_manager = local_training_plan.training_data(batch_size=4)     val_data_manager._dataset.set_dataset_parameters(dataset['dataset_parameters'])     data_loaders.append(DataLoader(val_data_manager._dataset)) In\u00a0[327]: Copied! <pre>import torch\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter1d\n\n\nlocal_model.eval()\nlosses = []\nlabels = []\n\nfor i, dl in enumerate(data_loaders):\n    \n    losses_ = []\n    labels.append(f\"Center {i+1}\")\n    with torch.no_grad():\n        for i, ((images, demographics), targets) in enumerate(dl):\n            \n            image = images['T1']\n            target = targets['label']\n            prediction = local_model.forward(image)\n            loss = UNetTrainingPlan.get_dice_loss(prediction, target)\n            losses_.append(loss.mean())\n    losses.append(losses_)\n\n    \nplt.subplot(111)    \nbxplt = plt.boxplot(losses,\n                    vert=True,\n                    patch_artist=True)   \nplt.title(\"Mean `dice loss` values on validation images\") \nplt.gca().xaxis.set_ticklabels(labels)\n\ncolors = ['pink', 'lightblue', 'lightgreen']\nfor patch, color in zip(bxplt['boxes'], colors):\n        patch.set_facecolor(color) \nplt.show()\n</pre> import torch import matplotlib.pyplot as plt from scipy.ndimage import gaussian_filter1d   local_model.eval() losses = [] labels = []  for i, dl in enumerate(data_loaders):          losses_ = []     labels.append(f\"Center {i+1}\")     with torch.no_grad():         for i, ((images, demographics), targets) in enumerate(dl):                          image = images['T1']             target = targets['label']             prediction = local_model.forward(image)             loss = UNetTrainingPlan.get_dice_loss(prediction, target)             losses_.append(loss.mean())     losses.append(losses_)       plt.subplot(111)     bxplt = plt.boxplot(losses,                     vert=True,                     patch_artist=True)    plt.title(\"Mean `dice loss` values on validation images\")  plt.gca().xaxis.set_ticklabels(labels)  colors = ['pink', 'lightblue', 'lightgreen'] for patch, color in zip(bxplt['boxes'], colors):         patch.set_facecolor(color)  plt.show() In\u00a0[328]: Copied! <pre># Visualize training loss\n\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter1d\n\n\nmonitor = exp.monitor()\nmetrics = monitor._metric_store\n\n\ntraining_metrics = [('training', k) for k in list(metrics[list(metrics.keys())[0]]['training'].keys())]\ntesting_global_metrics = [('testing_global_updates', k) for k in list(metrics[list(metrics.keys())[0]]['testing_global_updates'].keys())]\ntesting_local_metrics = [('testing_local_updates', k) for k in list(metrics[list(metrics.keys())[0]]['testing_local_updates'].keys())]\n\n\nmetrics_ = [*training_metrics, *testing_local_metrics, *testing_global_metrics]\n\ncols = len(metrics_)\n\n\nfig, axes = plt.subplots(1, cols, figsize=( cols * 4, len(metrics) * 1.5))\n\nfor i, (node, store) in enumerate(metrics.items()):\n    \n    title = \"\"\n    for k, (for_, m_) in enumerate(metrics_):\n        \n        title = f\"Metrics {for_}\" if title != f\"Metrics {for_}\" else title\n        data = [i  for k, l in store[for_][m_].items() for i in l[\"values\"]]\n        smoothed = gaussian_filter1d(data, sigma=1.5)\n        axes[k].plot(smoothed, label=f\"Node {i+1}\")\n        axes[k].set_title(title)\n        axes[k].set_ylabel(m_)\n        axes[k].set_xlabel(\"Iterations\")\n        axes[k].legend()\n        \nfig.tight_layout()\nplt.show()\n</pre> # Visualize training loss  import matplotlib.pyplot as plt from scipy.ndimage import gaussian_filter1d   monitor = exp.monitor() metrics = monitor._metric_store   training_metrics = [('training', k) for k in list(metrics[list(metrics.keys())[0]]['training'].keys())] testing_global_metrics = [('testing_global_updates', k) for k in list(metrics[list(metrics.keys())[0]]['testing_global_updates'].keys())] testing_local_metrics = [('testing_local_updates', k) for k in list(metrics[list(metrics.keys())[0]]['testing_local_updates'].keys())]   metrics_ = [*training_metrics, *testing_local_metrics, *testing_global_metrics]  cols = len(metrics_)   fig, axes = plt.subplots(1, cols, figsize=( cols * 4, len(metrics) * 1.5))  for i, (node, store) in enumerate(metrics.items()):          title = \"\"     for k, (for_, m_) in enumerate(metrics_):                  title = f\"Metrics {for_}\" if title != f\"Metrics {for_}\" else title         data = [i  for k, l in store[for_][m_].items() for i in l[\"values\"]]         smoothed = gaussian_filter1d(data, sigma=1.5)         axes[k].plot(smoothed, label=f\"Node {i+1}\")         axes[k].set_title(title)         axes[k].set_ylabel(m_)         axes[k].set_xlabel(\"Iterations\")         axes[k].legend()          fig.tight_layout() plt.show()  In\u00a0[17]: Copied! <pre>val_data_loader = data_loaders[0]\none_batch = next(iter(val_data_loader))\n</pre> val_data_loader = data_loaders[0] one_batch = next(iter(val_data_loader)) <p><code>one_batch</code> contains both input features and labels. Both are 3D images, which can be accessed in the following way (<code>k</code> represents the height in the stack of images):</p> In\u00a0[\u00a0]: Copied! <pre>k = 24\none_batch[1]['label'][..., k].shape\n</pre> k = 24 one_batch[1]['label'][..., k].shape In\u00a0[\u00a0]: Copied! <pre>k = 24\none_batch[0][0]['T1'][..., k].shape\n</pre> k = 24 one_batch[0][0]['T1'][..., k].shape In\u00a0[308]: Copied! <pre>import matplotlib.pyplot as plt\n\n\n# Display only layer 24 of 3D image\nk = 24\nmax_samples = 4\nfig, axes = plt.subplots(max_samples, 3, figsize=(6, max_samples * 1.7 ))\n\n\n# Set labels\naxes[0][0].set_title(\"MRI\")\naxes[0][1].set_title(\"Target\")\naxes[0][2].set_title(\"Predicted\")\n    \nfor b, val in enumerate(val_data_loader):\n    \n    if b &gt;= max_samples:\n        break\n    \n    (image, demographic), target = val\n\n    mri = image['T1']\n    target = target['label']\n\n    with torch.no_grad():\n        prediction = local_model.forward(mri)\n \n    image_mri = mri[0, 0, ..., k]\n    axes[b][0].imshow(image_mri)\n    \n    image_target = target[0, 0, ..., k] \n    axes[b][1].imshow(image_target)\n\n    pred = prediction[0, 0, ..., k]\n    axes[b][2].imshow(pred)\n    \n    \nfig.tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt   # Display only layer 24 of 3D image k = 24 max_samples = 4 fig, axes = plt.subplots(max_samples, 3, figsize=(6, max_samples * 1.7 ))   # Set labels axes[0][0].set_title(\"MRI\") axes[0][1].set_title(\"Target\") axes[0][2].set_title(\"Predicted\")      for b, val in enumerate(val_data_loader):          if b &gt;= max_samples:         break          (image, demographic), target = val      mri = image['T1']     target = target['label']      with torch.no_grad():         prediction = local_model.forward(mri)       image_mri = mri[0, 0, ..., k]     axes[b][0].imshow(image_mri)          image_target = target[0, 0, ..., k]      axes[b][1].imshow(image_target)      pred = prediction[0, 0, ..., k]     axes[b][2].imshow(pred)           fig.tight_layout() plt.show()"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#brain-segmentation","title":"Brain Segmentation\u00b6","text":"<p>This tutorial will show how to use Fed-BioMed to perform image segmentation on 3D medical MRI images of brains, using the publicly available IXI dataset. It uses a 3D U-Net model for the segmentation, trained on data from 3 separate centers.</p> <p>Here we display a very complex case, using advanced Fed-BioMed functionalities such as:</p> <ul> <li>loading a <code>MedicalFolderDataset</code></li> <li>implementing a custom Node Selection Strategy</li> <li>setting a non-default Optimizer</li> <li>monitoring training loss with Tensorboard</li> </ul> <p>This tutorial is based on TorchIO's tutorial.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#automatic-download-and-wrangling-for-the-impatient","title":"Automatic download and wrangling for the impatient\u00b6","text":"<p>If you're not interested in the details, you may simply execute the <code>download_and_split_ixi.py</code> script provided by us, as explained below</p> <pre>mkdir -p ${FEDBIOMED_DIR}/notebooks/data\ndownload_and_split_ixi.py -f ${FEDBIOMED_DIR}\n</pre> <p>After successfully running the command, follow the instructions printed to add the datasets and run the nodes. The tag used for this experiment is <code>ixi-train</code>.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#details-about-data-preparation","title":"Details about data preparation\u00b6","text":"<p>If you just want to run the notebook, you may skip this section and skip to <code>Define a new strategy</code>.</p> <p>First, download the IXI dataset from the Mendeley archive.</p> <p>In this tutorial we are going to use the <code>MedicalFolderDataset</code> class provided by the Fed-BioMed library to load medical images in NIFTI format. Using this dataset class for image segmentation problems guarantees maximum compatibility with the rest of the Fed-BioMed functionalities and features.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#folder-structure-for-medicalfolderdataset","title":"Folder structure for MedicalFolderDataset\u00b6","text":"<p>The <code>MedicalFolderDataset</code> is heavily inspired by PyTorch's <code>ImageFolder</code> Dataset, and requires you to manually prepare the image folders in order to respect a precise structure. The format assumes that you are dealing with imaging data, possibly acquired through multiple modalities, for different study subjects. Hence, you should provide one folder per subject, containing multiple subfolders for each image acquisition modality. Optionally, you may provide a <code>csv</code> file containing additional tabular data associated with each subject. This file is typically used for demographics data, and by default is called <code>participants.csv</code>.</p> <pre>_ root-folder\n |_ participants.csv\n |_ subject-1\n | |_ modality-1\n | |_ modality-2\n |_ subject-2\n | |_ modality-1\n | |_ modality-2\n |_ subject-3\n | |_ modality-1\n . .\n . .\n . .\n</pre>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#folder-structure-for-this-tutorial","title":"Folder structure for this tutorial\u00b6","text":"<p>In the specific case of this tutorial, we encourage you to further divide your images into additional subfolders, according to two criteria: the hospital that generated the data (there are three: Guys, HH and IOP) and a random train/holdout split.</p> <p>!!! info Note that each subject's folder will have a name with the following structure: <code>IXI&lt;SUBJECT_ID&gt;-&lt;HOSPITAL&gt;-&lt;RANDOM_ID&gt;</code>, for example <code>IXI002-Guys-0828</code>. In conclusion, combining the splits above with the structure required by the <code>MedicalFolderDataset</code>, your folder tree should look like this:</p> <pre>_root-folder\n |_ Guys\n | |_ train\n | | |_ participants.csv\n | | |_ IXI002-Guys-0828\n | | | |_ T1                &lt;-- T1 is the first imaging modality\n | | | |_ T2\n | | | |_ label\n | | |_ IXI022-Guys-0701\n | | | |_ T1\n | | | |_ T2\n . . .\n . . .\n . . .\n | |_ holdout\n | | |_ participants.csv\n | | |_ IXI004-Guys-0321\n | | | |_ T1\n | | | |_ T2\n | | | |_ label\n | | | |_ T2\n . . .\n . . .\n . . .\n |_ HH\n | |_ train\n . . .\n . . .\n . . .\n | |_ holdout\n . . .\n . . .\n . . .\n |_ IOP\n . . .\n . . .\n</pre>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#add-the-ixi-dataset-to-the-federated-nodes","title":"Add the IXI dataset to the federated nodes\u00b6","text":"<p>For each of the three hospitals, create a federated node and add the corresponding train dataset by selecting the <code>medical-folder</code> data type, and inputting <code>ixi-train</code> as the tag. Then start the nodes.</p> <p>         Dataset for demograhics of the subjects     </p> <p>         After selecting the folder that contains the patients for training the CLI will ask for CSV file where demographics of the patient are stored. These CSV files are named as `participants.csv`, and you can find these CSV files in the folder where the subject folders are located e.g `Guys/train/participant.csv`.     </p> <p>If you don't know how to add datasets to a node, or start a node, please read our user guide or follow the basic tutorial.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#create-a-training-plan","title":"Create a Training Plan\u00b6","text":"<p>We create a training plan that incorporates the UNet model. We rely on the unet package for simplicity. Please refer to the original package for more details about UNet: P\u00e9rez-Garc\u00eda, Fernando. (2020). fepegar/unet: PyTorch implementation of 2D and 3D U-Net (v0.7.5). Zenodo. https://doi.org/10.5281/zenodo.3697931</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#define-the-model-via-the-init_model-function","title":"Define the model via the <code>init_model</code> function\u00b6","text":"<p>The <code>init_model</code> function must return a UNet instance. Please refer to the TrainingPlan documentation for more details.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#define-the-loss-function-via-the-training_step-function","title":"Define the loss function via the <code>training_step</code> function\u00b6","text":"<p>Loss function is computed based on the Dice Loss.</p> <p>Carole H Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and M Jorge Cardoso. Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations. In Deep learning in medical image analysis and multimodal learning for clinical decision support, pages 240\u2013248. Springer, 2017.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#define-data-loading-and-transformations-via-the-training_data-function","title":"Define data loading and transformations via the <code>training_data</code> function\u00b6","text":"<p>Within the <code>training_data</code> function, we create an instance of <code>MedicalFolderDataset</code> and pass it to Fed-BioMed's <code>DataManager</code> class.</p> <p>To preprocess images, we define the image transformations for the input images and the labels leveraging MONAI's transforms. Note that we also include the correct dependencies in the <code>init_dependencies</code> function.</p> <p>Additionally, we define a transformation for the demographics data contained in the associated <code>csv</code> file. In order to be able to use information extracted from the demographics data as inputs to UNet, we must convert it to a <code>torch.Tensor</code> object. To achieve this, we exploit the <code>demographics_transform</code> argument of the <code>MedicalFolderDataset</code>. The transformation defined in this tutorial is just for illustration purposes, it does little more than just extracting some variables from the tabular data and converting them to the appropriate format.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#define-training-step","title":"Define training step\u00b6","text":"<p>Here we take as input one batch of (data, target), train the model and compute the loss function.</p> <p>Note that the <code>MedicalFolderDataset</code> class returns <code>data</code> as a tuple of <code>(images, demographics)</code>, where:</p> <ul> <li><code>images</code> is a <code>dict</code> of <code>{modality: image</code>} (after image transformations)</li> <li><code>demographics</code> is a <code>dict</code> of <code>{column_name: values}</code> where the column names are taken from the demographics csv file while the <code>target</code> is a <code>dict</code> of <code>{modality: image</code>} (after target transformations).</li> </ul> <p>In our case, the modality used is <code>T1</code> for the input images, while the modality used for the target is <code>label</code>. In this tutorial, we ignore the values of the demographics data during training because the UNet model only takes images as input. However, the code is provided for illustration purposes as it shows the recommended way to handle the associated tabular data.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#prepare-the-experiment","title":"Prepare the experiment\u00b6","text":""},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#tensorboard-setup","title":"Tensorboard setup\u00b6","text":""},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#run-the-experiment","title":"Run the experiment\u00b6","text":""},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#validate-on-a-local-holdout-set","title":"Validate on a local holdout set\u00b6","text":"<p>To ensure consistency and simplify our life, we try to reuse the already-available code as much as possible. Note that this process assumes that the held-out data is stored locally on the machine.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#create-an-instance-of-the-global-model","title":"Create an instance of the global model\u00b6","text":"<p>First, we create an instance of the model using the parameters from the latest aggregation round.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#define-a-validation-data-loader","title":"Define a validation data loader\u00b6","text":"<p>We extract the validation data loader from the training plan as well. This requires some knowledge about the internals of the <code>MedicalFolderDataset</code> class. At the end of the process, calling the <code>split</code> function with a ratio of 0 will return a data loader that loads all of the data.</p>"},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#compute-the-loss-on-validation-images","title":"Compute the loss on validation images\u00b6","text":""},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#visualize-training-loss-and-testing-metrics","title":"Visualize Training Loss and Testing Metrics\u00b6","text":""},{"location":"tutorials/medical/medical-image-segmentation-unet-library/#visualize-predictions","title":"Visualize Predictions\u00b6","text":"<p>As a bonus, we visualize the outputs of our model on the holdout dataset.</p>"},{"location":"tutorials/monai/","title":"Fed-BioMed using MONAI: a step-by-step tutorial","text":"<p>MONAI is a PyTorch-based framework for deep learning in healthcare imaging.  </p> <ol> <li> <p>2D Image Classification with MedNIST</p> </li> <li> <p>2D Image Regression with MedNIST</p> </li> </ol>"},{"location":"tutorials/monai/01_monai-2d-image-classification/","title":"Federated 2d image classification with MONAI","text":"<p>We are now ready to start the researcher environment with the command <code>source {FEDBIOMED_DIR}/scripts/fedbiomed_environment researcher</code>, and open the Jupyter notebook.</p> <p>We can first quesry the network for the mednist dataset. In this case, the nodes are sharing the respective partitions unsing the same tag <code>mednist</code>:</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.requests import Requests\nreq = Requests()\nreq.list(verbose=True)\n</pre> from fedbiomed.researcher.requests import Requests req = Requests() req.list(verbose=True)  <p>The code for network and data loader of the MONAI tutorial can now be deployed in Fed-BioMed. We first import the necessary modules from <code>fedbiomed</code> and <code>monai</code> libraries:</p> <p>We can now define the training plan. Note that we can simply use the standard <code>TorchTrainingPlan</code> natively provided in Fed-BioMed. We reuse the <code>MedNISTDataset</code> data loader defined in the original MONAI tutorial, which is returned by the method <code>training_data</code>, which also implements the data parsing from the nodes <code>dataset_path</code>. Following the MONAI tutorial, the model is the <code>DenseNet121</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\nfrom monai.apps import download_and_extract\nfrom monai.config import print_config\nfrom monai.data import decollate_batch\nfrom monai.metrics import ROCAUCMetric\nfrom monai.networks.nets import DenseNet121\nfrom monai.transforms import (\n    Activations,\n    AddChannel,\n    AsDiscrete,\n    Compose,\n    LoadImage,\n    RandFlip,\n    RandRotate,\n    RandZoom,\n    ScaleIntensity,\n    EnsureType,\n)\nfrom monai.utils import set_determinism\n\n\n\n# Here we define the training plan to be used. \n# You can use any class name (here 'MyTrainingPlan')\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    # Declare dependencies\n    def init_dependencies(self):\n        deps = [\"import numpy as np\",\n                \"import os\",\n                \"from monai.apps import download_and_extract\",\n                \"from monai.config import print_config\",\n                \"from monai.data import decollate_batch\",\n                \"from monai.metrics import ROCAUCMetric\",\n                \"from monai.networks.nets import DenseNet121\",\n                \"from monai.transforms import ( Activations, AddChannel, AsDiscrete, Compose, LoadImage, RandFlip, RandRotate, RandZoom, ScaleIntensity, EnsureType, )\",\n                \"from monai.utils import set_determinism\"]\n        \n        return deps\n    \n    # Define and return model\n    def init_model(self):\n\n        model = DenseNet121(spatial_dims=2, in_channels=1,\n                    out_channels = self.model_args()[\"num_class\"])\n        \n        return model \n        \n    class MedNISTDataset(torch.utils.data.Dataset):\n            def __init__(self, image_files, labels, transforms):\n                self.image_files = image_files\n                self.labels = labels\n                self.transforms = transforms\n\n            def __len__(self):\n                return len(self.image_files)\n\n            def __getitem__(self, index):\n                return self.transforms(self.image_files[index]), self.labels[index]\n    \n    def parse_data(self, path):\n        \n        class_names = sorted(x for x in os.listdir(path)\n                     if os.path.isdir(os.path.join(path, x)))\n        num_class = len(class_names)\n        image_files = [\n                        [\n                            os.path.join(path, class_names[i], x)\n                            for x in os.listdir(os.path.join(path, class_names[i]))\n                        ]\n                        for i in range(num_class)\n                      ]\n        \n        return image_files, num_class\n    \n    def training_data(self, batch_size = 48):\n        self.image_files, num_class = self.parse_data(self.dataset_path)\n        \n        if self.model_args()[\"num_class\"] != num_class:\n                raise Exception('number of available classes does not match declared classes')\n        \n        num_each = [len(self.image_files[i]) for i in range(self.model_args()[\"num_class\"])]\n        image_files_list = []\n        image_class = []\n        \n        for i in range(self.model_args()[\"num_class\"]):\n            image_files_list.extend(self.image_files[i])\n            image_class.extend([i] * num_each[i])\n        num_total = len(image_class)\n        \n        \n        length = len(image_files_list)\n        indices = np.arange(length)\n        np.random.shuffle(indices)\n\n        val_split = int(1. * length) \n        train_indices = indices[:val_split]\n\n        train_x = [image_files_list[i] for i in train_indices]\n        train_y = [image_class[i] for i in train_indices]\n\n\n        train_transforms = Compose(\n            [\n                LoadImage(image_only=True),\n                AddChannel(),\n                ScaleIntensity(),\n                RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),\n                RandFlip(spatial_axis=0, prob=0.5),\n                RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n                EnsureType(),\n            ]\n        )\n\n        val_transforms = Compose(\n            [LoadImage(image_only=True), AddChannel(), ScaleIntensity(), EnsureType()])\n\n        y_pred_trans = Compose([EnsureType(), Activations(softmax=True)])\n        y_trans = Compose([EnsureType(), AsDiscrete(to_onehot=num_class)])\n                \n        train_ds = self.MedNISTDataset(train_x, train_y, train_transforms)\n        \n        return DataManager(dataset=train_ds, batch_size=batch_size, shuffle=True)\n    \n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.cross_entropy(output, target)\n        return loss\n</pre> import os import numpy as np import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms  from monai.apps import download_and_extract from monai.config import print_config from monai.data import decollate_batch from monai.metrics import ROCAUCMetric from monai.networks.nets import DenseNet121 from monai.transforms import (     Activations,     AddChannel,     AsDiscrete,     Compose,     LoadImage,     RandFlip,     RandRotate,     RandZoom,     ScaleIntensity,     EnsureType, ) from monai.utils import set_determinism    # Here we define the training plan to be used.  # You can use any class name (here 'MyTrainingPlan') class MyTrainingPlan(TorchTrainingPlan):      # Declare dependencies     def init_dependencies(self):         deps = [\"import numpy as np\",                 \"import os\",                 \"from monai.apps import download_and_extract\",                 \"from monai.config import print_config\",                 \"from monai.data import decollate_batch\",                 \"from monai.metrics import ROCAUCMetric\",                 \"from monai.networks.nets import DenseNet121\",                 \"from monai.transforms import ( Activations, AddChannel, AsDiscrete, Compose, LoadImage, RandFlip, RandRotate, RandZoom, ScaleIntensity, EnsureType, )\",                 \"from monai.utils import set_determinism\"]                  return deps          # Define and return model     def init_model(self):          model = DenseNet121(spatial_dims=2, in_channels=1,                     out_channels = self.model_args()[\"num_class\"])                  return model               class MedNISTDataset(torch.utils.data.Dataset):             def __init__(self, image_files, labels, transforms):                 self.image_files = image_files                 self.labels = labels                 self.transforms = transforms              def __len__(self):                 return len(self.image_files)              def __getitem__(self, index):                 return self.transforms(self.image_files[index]), self.labels[index]          def parse_data(self, path):                  class_names = sorted(x for x in os.listdir(path)                      if os.path.isdir(os.path.join(path, x)))         num_class = len(class_names)         image_files = [                         [                             os.path.join(path, class_names[i], x)                             for x in os.listdir(os.path.join(path, class_names[i]))                         ]                         for i in range(num_class)                       ]                  return image_files, num_class          def training_data(self, batch_size = 48):         self.image_files, num_class = self.parse_data(self.dataset_path)                  if self.model_args()[\"num_class\"] != num_class:                 raise Exception('number of available classes does not match declared classes')                  num_each = [len(self.image_files[i]) for i in range(self.model_args()[\"num_class\"])]         image_files_list = []         image_class = []                  for i in range(self.model_args()[\"num_class\"]):             image_files_list.extend(self.image_files[i])             image_class.extend([i] * num_each[i])         num_total = len(image_class)                           length = len(image_files_list)         indices = np.arange(length)         np.random.shuffle(indices)          val_split = int(1. * length)          train_indices = indices[:val_split]          train_x = [image_files_list[i] for i in train_indices]         train_y = [image_class[i] for i in train_indices]           train_transforms = Compose(             [                 LoadImage(image_only=True),                 AddChannel(),                 ScaleIntensity(),                 RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),                 RandFlip(spatial_axis=0, prob=0.5),                 RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),                 EnsureType(),             ]         )          val_transforms = Compose(             [LoadImage(image_only=True), AddChannel(), ScaleIntensity(), EnsureType()])          y_pred_trans = Compose([EnsureType(), Activations(softmax=True)])         y_trans = Compose([EnsureType(), AsDiscrete(to_onehot=num_class)])                          train_ds = self.MedNISTDataset(train_x, train_y, train_transforms)                  return DataManager(dataset=train_ds, batch_size=batch_size, shuffle=True)          def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.cross_entropy(output, target)         return loss  <p>We now set the model and training parameters. Note that we use only 1 epoch for this experiment, and perform the training on ~26% of the locally available training data.</p> In\u00a0[\u00a0]: Copied! <pre>model_args = {\n    'num_class': 6,  \n}\n\ntraining_args = {\n    'batch_size': 20, \n    'optimizer_args': {\n        'lr': 1e-5\n    }, \n    'epochs': 1, \n    'dry_run': False,  \n    'batch_maxnum':250 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n</pre> model_args = {     'num_class': 6,   }  training_args = {     'batch_size': 20,      'optimizer_args': {         'lr': 1e-5     },      'epochs': 1,      'dry_run': False,       'batch_maxnum':250 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } <p>The experiment can be now defined, by providing the <code>mednist</code> tag, and running the local training on nodes with training plan defined in <code>training_plan_path</code>, standard <code>aggregator</code> (FedAvg) and <code>client_selection_strategy</code> (all nodes used). Federated learning is going to be perfomed through 3 optimization rounds.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MEDNIST', '#dataset']\nrounds = 3\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None\n                )\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['#MEDNIST', '#dataset'] rounds = 3  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None                 ) <p>Let's start the experiment.</p> <p>By default, this function doesn't stop until all the <code>round_limit</code> rounds are done for all the clients</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>Once the federated model is obtained, it is possible to test it locally on an independent testing partition. The test dataset is available at this link:</p> <p>https://drive.google.com/file/d/1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD/</p> In\u00a0[\u00a0]: Copied! <pre>!pip install gdown\n</pre> !pip install gdown In\u00a0[\u00a0]: Copied! <pre>import os\nimport shutil\nimport tempfile\nimport PIL\nimport torch\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\nfrom monai.config import print_config\nfrom monai.data import decollate_batch\nfrom monai.metrics import ROCAUCMetric\nfrom monai.networks.nets import DenseNet121\nimport zipfile\nfrom monai.transforms import (\n    Activations,\n    AddChannel,\n    AsDiscrete,\n    Compose,\n    LoadImage,\n    RandFlip,\n    RandRotate,\n    RandZoom,\n    ScaleIntensity,\n    EnsureType,\n)\nfrom monai.utils import set_determinism\n\nprint_config()\n</pre> import os import shutil import tempfile import PIL import torch import numpy as np from sklearn.metrics import classification_report  from monai.config import print_config from monai.data import decollate_batch from monai.metrics import ROCAUCMetric from monai.networks.nets import DenseNet121 import zipfile from monai.transforms import (     Activations,     AddChannel,     AsDiscrete,     Compose,     LoadImage,     RandFlip,     RandRotate,     RandZoom,     ScaleIntensity,     EnsureType, ) from monai.utils import set_determinism  print_config() <p>Download the testing dataset on the local temporary folder.</p> In\u00a0[\u00a0]: Copied! <pre>import gdown\nimport zipfile\nimport tempfile\nimport os\nfrom fedbiomed.researcher.environ import environ\n\ntmp_dir = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+os.sep)\n\nresource = \"https://drive.google.com/uc?id=1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD\"\nbase_dir = tmp_dir.name\ntest_file = os.path.join(base_dir, \"MedNIST_testing.zip\")\n\ngdown.download(resource, test_file, quiet=False)\n\nzf = zipfile.ZipFile(test_file)\n\nfor file in zf.infolist():\n    zf.extract(file, base_dir)\n    \ndata_dir = os.path.join(base_dir, \"MedNIST_testing\")\n</pre> import gdown import zipfile import tempfile import os from fedbiomed.researcher.environ import environ  tmp_dir = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+os.sep)  resource = \"https://drive.google.com/uc?id=1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD\" base_dir = tmp_dir.name test_file = os.path.join(base_dir, \"MedNIST_testing.zip\")  gdown.download(resource, test_file, quiet=False)  zf = zipfile.ZipFile(test_file)  for file in zf.infolist():     zf.extract(file, base_dir)      data_dir = os.path.join(base_dir, \"MedNIST_testing\") <p>Parse the data and create the testing data loader:</p> In\u00a0[\u00a0]: Copied! <pre>class_names = sorted(x for x in os.listdir(data_dir)\n                     if os.path.isdir(os.path.join(data_dir, x)))\nnum_class = len(class_names)\nimage_files = [\n    [\n        os.path.join(data_dir, class_names[i], x)\n        for x in os.listdir(os.path.join(data_dir, class_names[i]))\n    ]\n    for i in range(num_class)\n]\n\nnum_each = [len(image_files[i]) for i in range(num_class)]\nimage_files_list = []\n\nimage_class = []\nfor i in range(num_class):\n    image_files_list.extend(image_files[i])\n    image_class.extend([i] * num_each[i])\nnum_total = len(image_class)\nimage_width, image_height = PIL.Image.open(image_files_list[0]).size\n\nprint(f\"Total image count: {num_total}\")\nprint(f\"Image dimensions: {image_width} x {image_height}\")\nprint(f\"Label names: {class_names}\")\nprint(f\"Label counts: {num_each}\")\n</pre> class_names = sorted(x for x in os.listdir(data_dir)                      if os.path.isdir(os.path.join(data_dir, x))) num_class = len(class_names) image_files = [     [         os.path.join(data_dir, class_names[i], x)         for x in os.listdir(os.path.join(data_dir, class_names[i]))     ]     for i in range(num_class) ]  num_each = [len(image_files[i]) for i in range(num_class)] image_files_list = []  image_class = [] for i in range(num_class):     image_files_list.extend(image_files[i])     image_class.extend([i] * num_each[i]) num_total = len(image_class) image_width, image_height = PIL.Image.open(image_files_list[0]).size  print(f\"Total image count: {num_total}\") print(f\"Image dimensions: {image_width} x {image_height}\") print(f\"Label names: {class_names}\") print(f\"Label counts: {num_each}\") In\u00a0[\u00a0]: Copied! <pre>length = len(image_files_list)\nindices = np.arange(length)\nnp.random.shuffle(indices)\n\n\ntest_split = int(0.1 * length)\ntest_indices = indices[:test_split]\n\ntest_x = [image_files_list[i] for i in test_indices]\ntest_y = [image_class[i] for i in test_indices]\n\nval_transforms = Compose(\n    [LoadImage(image_only=True), AddChannel(), ScaleIntensity(), EnsureType()])\n\ny_pred_trans = Compose([EnsureType(), Activations(softmax=True)])\ny_trans = Compose([EnsureType(), AsDiscrete(to_onehot=num_class)])\n</pre> length = len(image_files_list) indices = np.arange(length) np.random.shuffle(indices)   test_split = int(0.1 * length) test_indices = indices[:test_split]  test_x = [image_files_list[i] for i in test_indices] test_y = [image_class[i] for i in test_indices]  val_transforms = Compose(     [LoadImage(image_only=True), AddChannel(), ScaleIntensity(), EnsureType()])  y_pred_trans = Compose([EnsureType(), Activations(softmax=True)]) y_trans = Compose([EnsureType(), AsDiscrete(to_onehot=num_class)]) In\u00a0[\u00a0]: Copied! <pre>class MedNISTDataset(torch.utils.data.Dataset):\n    def __init__(self, image_files, labels, transforms):\n        self.image_files = image_files\n        self.labels = labels\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, index):\n        return self.transforms(self.image_files[index]), self.labels[index]\n\n\ntest_ds = MedNISTDataset(test_x, test_y, val_transforms)\ntest_loader = torch.utils.data.DataLoader(\n    test_ds, batch_size=300)\n</pre> class MedNISTDataset(torch.utils.data.Dataset):     def __init__(self, image_files, labels, transforms):         self.image_files = image_files         self.labels = labels         self.transforms = transforms      def __len__(self):         return len(self.image_files)      def __getitem__(self, index):         return self.transforms(self.image_files[index]), self.labels[index]   test_ds = MedNISTDataset(test_x, test_y, val_transforms) test_loader = torch.utils.data.DataLoader(     test_ds, batch_size=300) <p>Define testing metric:</p> In\u00a0[\u00a0]: Copied! <pre>auc_metric = ROCAUCMetric()\n</pre> auc_metric = ROCAUCMetric() <p>To test the federated model we need to create a model instance and assign to it the model parameters estimated at the last federated optimization round.</p> In\u00a0[\u00a0]: Copied! <pre>model = exp.training_plan().model()\nmodel.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n</pre> model = exp.training_plan().model() model.load_state_dict(exp.aggregated_params()[rounds - 1]['params']) <p>Compute the testing performance:</p> In\u00a0[\u00a0]: Copied! <pre>y_true = []\ny_pred = []\nwith torch.no_grad():\n    for test_data in test_loader:\n        test_images, test_labels = (\n            test_data[0],\n            test_data[1],\n        )\n        pred = model(test_images).argmax(dim=1)\n        for i in range(len(pred)):\n            y_true.append(test_labels[i].item())\n            y_pred.append(pred[i].item())\n</pre> y_true = [] y_pred = [] with torch.no_grad():     for test_data in test_loader:         test_images, test_labels = (             test_data[0],             test_data[1],         )         pred = model(test_images).argmax(dim=1)         for i in range(len(pred)):             y_true.append(test_labels[i].item())             y_pred.append(pred[i].item())  In\u00a0[\u00a0]: Copied! <pre>print(classification_report(\n    y_true, y_pred, target_names=class_names, digits=4))\n</pre> print(classification_report(     y_true, y_pred, target_names=class_names, digits=4)) <p>In spite of the relatively small training performed on the data shared in the 3 nodes, the performance of the federated model seems pretty good. Well done!</p>"},{"location":"tutorials/monai/01_monai-2d-image-classification/#federated-2d-image-classification-with-monai","title":"Federated 2d image classification with MONAI\u00b6","text":""},{"location":"tutorials/monai/01_monai-2d-image-classification/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial shows how to deploy in Fed-BioMed the 2d image classification example provided in the project MONAI (https://monai.io/):</p> <p>https://github.com/Project-MONAI/tutorials/blob/master/2d_classification/mednist_tutorial.ipynb</p> <p>Being MONAI based on PyTorch, the deployment within Fed-BioMed follows seamlessly the same general structure of general PyTorch training plans.</p> <p>Following the MONAI example, this tutorial is based on the MedNIST dataset.</p>"},{"location":"tutorials/monai/01_monai-2d-image-classification/#creating-mednist-nodes","title":"Creating MedNIST nodes\u00b6","text":"<p>MedNIST provides an artificial 2d classification dataset created by gathering different medical imaging datasets from TCIA, the RSNA Bone Age Challenge, and the NIH Chest X-ray dataset. The dataset is kindly made available by Dr. Bradley J. Erickson M.D., Ph.D. (Department of Radiology, Mayo Clinic) under the Creative Commons CC BY-SA 4.0 license.</p> <p>To proceed with the tutorial, we created an iid partitioning of the MedNIST dataset between 3 clients. Each client has 3000 image samples for each class. The training partitions are availables at the following link:</p> <p>https://drive.google.com/file/d/1vLIcBdtdAhh6K-vrgCFy_0Y55dxOWZwf/view</p> <p>The dataset owned by each client has structure:</p> <p>\u2514\u2500\u2500 client_*/</p> <pre><code>\u251c\u2500\u2500 AbdomenCT/\n\n\u2514\u2500\u2500 BreastMRI/\n\n\u2514\u2500\u2500 CXR/\n\n\u2514\u2500\u2500 ChestCT/\n\n\u2514\u2500\u2500 Hand/\n\n\u2514\u2500\u2500 HeadCT/   </code></pre> <p>To create the federated dataset, we follow the standard procedure for node creation/population of Fed-BioMed. After activating the fedbiomed network with the commands</p> <p><code>source ./scripts/fedbiomed_environment network</code></p> <p>and</p> <p><code>{FEDBIOMED_DIR}/scripts/fedbiomed_run network</code></p> <p>we create a first node by using the commands</p> <p><code>source {FEDBIOMED_DIR}/scripts/fedbiomed_environment node</code></p> <p><code>{FEDBIOMED_DIR}/scripts/fedbiomed_run node start</code></p> <p>We then poulate the node with the data of first client:</p> <p><code>{FEDBIOMED_DIR}/scripts/fedbiomed_run node add</code></p> <p>We select option 3 (images) to add MedNIST partition of client 1, by just picking the folder of client 1. Assign tag <code>#MEDNIST, #dataset</code> to the data when asked.</p> <p>We can further check that the data has been added by executing <code>{FEDBIOMED_DIR}/scripts/fedbiomed_run node list</code></p> <p>Following the same procedure, we create the other two nodes with the datasets of client 2 and client 3 respectively.</p>"},{"location":"tutorials/monai/01_monai-2d-image-classification/#running-fed-biomed-researcher","title":"Running Fed-BioMed Researcher\u00b6","text":""},{"location":"tutorials/monai/01_monai-2d-image-classification/#create-an-experiment-to-train-a-model-on-the-data-found","title":"Create an experiment to train a model on the data found\u00b6","text":""},{"location":"tutorials/monai/01_monai-2d-image-classification/#warning","title":"WARNING:\u00b6","text":"<p>For running this experiment, you need a computer with the following specifications:</p> <ul> <li>more than 16 GB of RAM</li> <li>2.5 GHz processor or higher, with at least 4 cores</li> </ul> <p>If your computer specification are lower, you can reduce the number of data passed when training model (set <code>batchnum</code> from 250 to 25) and the number of <code>rounds</code> (from 3 to 1) but model performances may decrease dramatically</p>"},{"location":"tutorials/monai/01_monai-2d-image-classification/#testing","title":"Testing\u00b6","text":""},{"location":"tutorials/monai/02_monai-2d-image-registration/","title":"Federated 2d XRay registration with MONAI","text":"<p>We are now ready to start the researcher environment with the command <code>source {FEDBIOMED_DIR}/scripts/fedbiomed_environment researcher</code>, and open the Jupyter notebook with <code>{FEDBIOMED_DIR}/scripts/fedbiomed_run researcher start</code>.</p> <p>We can first query the network for the <code>Mednist</code> dataset. In this case, the nodes are sharing the respective partitions using the same tag <code>mednist</code>:</p> In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.requests import Requests\nreq = Requests()\nreq.list(verbose=True)\n</pre> from fedbiomed.researcher.requests import Requests req = Requests() req.list(verbose=True) <p>The code for network and data loader of the MONAI tutorial can now be deployed in Fed-BioMed. We first import the necessary modules from <code>fedbiomed</code> and <code>monai</code> libraries:</p> <p>We can now define the training plan. Note that we use the standard <code>TorchTrainingPlan</code> natively provided in Fed-BioMed. We reuse the <code>MedNISTDataset</code> data loader defined in the original MONAI tutorial, which is returned by the method <code>training_data</code>, which also implements the data parsing from the nodes <code>dataset_path</code>. We should also properly define the <code>training_routine</code>, following the MONAI tutorial. According to the MONAI tutorial, the model is the <code>GlobalNet</code> and the loss is <code>MSELoss</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import mse_loss\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.logger import logger\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\nfrom typing import Union, List\n#from torch.utils.data import Dataset, DataLoader\nimport monai\nfrom monai.utils import set_determinism, first\nfrom monai.transforms import (\n    EnsureChannelFirstD,\n    Compose,\n    LoadImageD,\n    RandRotateD,\n    RandZoomD,\n    ScaleIntensityRanged,\n    EnsureTypeD,\n)\nfrom monai.data import DataLoader, Dataset, CacheDataset\nfrom monai.config import print_config, USE_COMPILED\nfrom monai.networks.nets import GlobalNet\nfrom monai.networks.blocks import Warp\nfrom monai.apps import MedNISTDataset\n\n\n# Here we define the training plan to be used. \nclass MyMonaiTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args = None):\n        model_= GlobalNet(\n            image_size=(64, 64),\n            spatial_dims=2,\n            in_channels=2,  # moving and fixed\n            num_channel_initial=16,\n            depth=3)\n\n        if USE_COMPILED:\n            model_.warp_layer = Warp(3, \"border\")\n        else:\n            model_.warp_layer = Warp(\"bilinear\", \"border\")\n\n        return model_\n\n    def init_dependencies(self):\n        return [\"import numpy as np\",\n                \"import monai\",\n                \"from torch.nn.functional import mse_loss\",\n                \"from monai.utils import set_determinism, first\",\n                \"from monai.transforms import (EnsureChannelFirstD,Compose,LoadImageD,RandRotateD,RandZoomD,ScaleIntensityRanged,EnsureTypeD,)\",\n                \"from monai.data import DataLoader, Dataset, CacheDataset\",\n                \"from monai.networks.nets import GlobalNet\",\n                \"from monai.config import USE_COMPILED\",\n                \"from monai.networks.blocks import Warp\",\n                \"from monai.apps import MedNISTDataset\",]\n\n    def init_optimizer(self, optimizer_args):\n        lr = optimizer_args.get('lr', 1e-5)\n        return torch.optim.Adam(self.model().parameters(), lr)\n        \n    def training_data(self, batch_size = 20):\n        # Custom torch Dataloader for MedNIST data\n        data_path = self.dataset_path\n        # The following line is needed if client structure does not contain the \"/MedNIST\" folder\n        MedNISTDataset.dataset_folder_name = \"\"\n        train_data = MedNISTDataset(root_dir=data_path, section=\"training\", download=False, transform=None)\n        training_datadict = [\n            {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}\n            for item in train_data.data if item[\"label\"] == 4  # label 4 is for xray hands\n        ]\n        train_transforms = Compose(\n            [\n                LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],\n                                     a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),\n                RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),\n                RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1,\n                          monaiprob=1.0, mode=\"bicubic\", align_corners=False),\n                EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),\n            ]\n        )\n        train_ds = CacheDataset(data=training_datadict[:1000], transform=train_transforms,\n                                cache_rate=1.0, num_workers=0)\n        dl = self.MednistDataLoader(train_ds)\n        \n        return DataManager(dl, batch_size=batch_size, shuffle=True, num_workers=0)\n\n    def training_step(self, moving, fixed):\n        ddf = self.model().forward(torch.cat((moving, fixed), dim=1))\n        pred_image = self.model().warp_layer(moving, ddf)\n        loss = mse_loss(pred_image, fixed)\n        return loss\n    \n    class MednistDataLoader(monai.data.Dataset):\n        # Custom DataLoader that inherits from monai's Dataset object\n        def __init__(self, dataset):\n            self.dataset = dataset\n\n        def __len__(self):\n            return len(self.dataset)\n\n        def __getitem__(self, idx):\n            return (self.dataset[idx][\"moving_hand\"],\n                    self.dataset[idx][\"fixed_hand\"])\n</pre> import os import numpy as np import torch import torch.nn as nn from torch.nn.functional import mse_loss from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.logger import logger from fedbiomed.common.data import DataManager from torchvision import datasets, transforms from typing import Union, List #from torch.utils.data import Dataset, DataLoader import monai from monai.utils import set_determinism, first from monai.transforms import (     EnsureChannelFirstD,     Compose,     LoadImageD,     RandRotateD,     RandZoomD,     ScaleIntensityRanged,     EnsureTypeD, ) from monai.data import DataLoader, Dataset, CacheDataset from monai.config import print_config, USE_COMPILED from monai.networks.nets import GlobalNet from monai.networks.blocks import Warp from monai.apps import MedNISTDataset   # Here we define the training plan to be used.  class MyMonaiTrainingPlan(TorchTrainingPlan):     def init_model(self, model_args = None):         model_= GlobalNet(             image_size=(64, 64),             spatial_dims=2,             in_channels=2,  # moving and fixed             num_channel_initial=16,             depth=3)          if USE_COMPILED:             model_.warp_layer = Warp(3, \"border\")         else:             model_.warp_layer = Warp(\"bilinear\", \"border\")          return model_      def init_dependencies(self):         return [\"import numpy as np\",                 \"import monai\",                 \"from torch.nn.functional import mse_loss\",                 \"from monai.utils import set_determinism, first\",                 \"from monai.transforms import (EnsureChannelFirstD,Compose,LoadImageD,RandRotateD,RandZoomD,ScaleIntensityRanged,EnsureTypeD,)\",                 \"from monai.data import DataLoader, Dataset, CacheDataset\",                 \"from monai.networks.nets import GlobalNet\",                 \"from monai.config import USE_COMPILED\",                 \"from monai.networks.blocks import Warp\",                 \"from monai.apps import MedNISTDataset\",]      def init_optimizer(self, optimizer_args):         lr = optimizer_args.get('lr', 1e-5)         return torch.optim.Adam(self.model().parameters(), lr)              def training_data(self, batch_size = 20):         # Custom torch Dataloader for MedNIST data         data_path = self.dataset_path         # The following line is needed if client structure does not contain the \"/MedNIST\" folder         MedNISTDataset.dataset_folder_name = \"\"         train_data = MedNISTDataset(root_dir=data_path, section=\"training\", download=False, transform=None)         training_datadict = [             {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}             for item in train_data.data if item[\"label\"] == 4  # label 4 is for xray hands         ]         train_transforms = Compose(             [                 LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),                 EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),                 ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],                                      a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),                 RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),                 RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1,                           monaiprob=1.0, mode=\"bicubic\", align_corners=False),                 EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),             ]         )         train_ds = CacheDataset(data=training_datadict[:1000], transform=train_transforms,                                 cache_rate=1.0, num_workers=0)         dl = self.MednistDataLoader(train_ds)                  return DataManager(dl, batch_size=batch_size, shuffle=True, num_workers=0)      def training_step(self, moving, fixed):         ddf = self.model().forward(torch.cat((moving, fixed), dim=1))         pred_image = self.model().warp_layer(moving, ddf)         loss = mse_loss(pred_image, fixed)         return loss          class MednistDataLoader(monai.data.Dataset):         # Custom DataLoader that inherits from monai's Dataset object         def __init__(self, dataset):             self.dataset = dataset          def __len__(self):             return len(self.dataset)          def __getitem__(self, idx):             return (self.dataset[idx][\"moving_hand\"],                     self.dataset[idx][\"fixed_hand\"])  <p>We now set the model and training parameters. Note that in this case, no model argument is required.</p> In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'batch_size': 16,\n    'epochs': 3,\n    'dry_run': False,  \n    'batch_maxnum':250, # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n    'optimizer_args': {\n        'lr': 1e-5,\n    },\n    'use_gpu': True # Training on GPU\n}\n</pre> model_args = {}  training_args = {     'batch_size': 16,     'epochs': 3,     'dry_run': False,       'batch_maxnum':250, # Fast pass for development : only use ( batch_maxnum * batch_size ) samples     'optimizer_args': {         'lr': 1e-5,     },     'use_gpu': True # Training on GPU } <p>The experiment can be now defined, by providing the <code>mednist</code> tag, and running the local training on nodes with training plan defined in <code>training_plan_path</code>, standard <code>aggregator</code> (FedAvg) and <code>client_selection_strategy</code> (all nodes used). Federated learning is going to be perfomed through 5 optimization rounds.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MEDNIST']\nrounds = 5\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyMonaiTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None\n                )\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['#MEDNIST'] rounds = 5  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyMonaiTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None                 ) <p>Let's start the experiment.</p> <p>By default, this function doesn't stop until all the <code>round_limit</code> rounds are done for all the clients</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>Once the federated model is obtained, it is possible to test it locally on an independent testing partition. The test dataset is available at this link:</p> <p>https://drive.google.com/file/d/1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD/</p> <p>Following the Monai tutorial, in this section we will create a set of previously unseen pairs of moving vs fixed hands, and use the final federated model to predict the transformation between each pair.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install matplotlib\n!pip install gdown\n</pre> !pip install matplotlib !pip install gdown In\u00a0[\u00a0]: Copied! <pre>import os\nimport tempfile\nimport PIL\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gdown\nimport zipfile\nimport matplotlib.pyplot as plt\n\nprint_config()\nset_determinism(42)\n</pre> import os import tempfile import PIL import torch import numpy as np import matplotlib.pyplot as plt import gdown import zipfile import matplotlib.pyplot as plt  print_config() set_determinism(42) <p>Download the testing dataset on the local temporary folder.</p> In\u00a0[\u00a0]: Copied! <pre>import gdown\nimport zipfile\nimport tempfile\nimport os\n\nfrom fedbiomed.researcher.environ import environ\n\ntmp_dir = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+os.sep)\n\nresource = \"https://drive.google.com/uc?id=1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD\"\nbase_dir = tmp_dir.name\ntest_file = os.path.join(base_dir, \"MedNIST_testing.zip\")\n\ngdown.download(resource, test_file, quiet=False)\n\nzf = zipfile.ZipFile(test_file)\n\nfor file in zf.infolist():\n    zf.extract(file, base_dir)\n    \ndata_dir = os.path.join(base_dir, \"MedNIST_testing\")\n</pre> import gdown import zipfile import tempfile import os  from fedbiomed.researcher.environ import environ  tmp_dir = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+os.sep)  resource = \"https://drive.google.com/uc?id=1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD\" base_dir = tmp_dir.name test_file = os.path.join(base_dir, \"MedNIST_testing.zip\")  gdown.download(resource, test_file, quiet=False)  zf = zipfile.ZipFile(test_file)  for file in zf.infolist():     zf.extract(file, base_dir)      data_dir = os.path.join(base_dir, \"MedNIST_testing\") <p>We redefine our custom dataloader (defined previously in  the <code>TrainingPlan</code>):</p> In\u00a0[\u00a0]: Copied! <pre>from monai.data import DataLoader, Dataset, CacheDataset\nimport monai\n\nclass MednistDataLoader(monai.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        return (self.dataset[idx][\"moving_hand\"],\n                self.dataset[idx][\"fixed_hand\"])\n</pre> from monai.data import DataLoader, Dataset, CacheDataset import monai  class MednistDataLoader(monai.data.Dataset):     def __init__(self, dataset):         self.dataset = dataset      def __len__(self):         return len(self.dataset)      def __getitem__(self, idx):         return (self.dataset[idx][\"moving_hand\"],                 self.dataset[idx][\"fixed_hand\"]) <p>Create the testing data loader and pairs of moving vs fixed hands:</p> In\u00a0[\u00a0]: Copied! <pre># Use a GPU if you have one + enough memory available\n#\n#use_cuda = torch.cuda.is_available()\n#device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\ndevice = 'cpu'\n\n\n# recreate model\nmodel = GlobalNet(\n    image_size=(64, 64),\n    spatial_dims=2,\n    in_channels=2,  # moving and fixed\n    num_channel_initial=16,\n    depth=3).to(device)\n\nif USE_COMPILED:\n    warp_layer = Warp(3, \"border\").to(device)\nelse:\n    warp_layer = Warp(\"bilinear\", \"border\").to(device)\n\nMedNISTDataset.dataset_folder_name = \"\"\ntest_data = MedNISTDataset(root_dir=data_dir, section=\"test\", download=False, transform=None)\ntesting_datadict = [\n    {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}\n    for item in test_data.data if item[\"label\"] == 4  # label 4 is for xray hands\n]\ntest_transforms = Compose(\n            [\n                LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],\n                                     a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),\n                RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),\n                RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1, prob=1.0, mode=\"bicubic\", align_corners=False),\n                EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),\n            ]\n        )\nval_ds = CacheDataset(data=testing_datadict[:1000], transform=test_transforms,\n                      cache_rate=1.0, num_workers=0)\nval_dl = MednistDataLoader(val_ds)\nval_loader = DataLoader(val_dl, batch_size=16, num_workers=0)\n</pre> # Use a GPU if you have one + enough memory available # #use_cuda = torch.cuda.is_available() #device = torch.device(\"cuda:0\" if use_cuda else \"cpu\") device = 'cpu'   # recreate model model = GlobalNet(     image_size=(64, 64),     spatial_dims=2,     in_channels=2,  # moving and fixed     num_channel_initial=16,     depth=3).to(device)  if USE_COMPILED:     warp_layer = Warp(3, \"border\").to(device) else:     warp_layer = Warp(\"bilinear\", \"border\").to(device)  MedNISTDataset.dataset_folder_name = \"\" test_data = MedNISTDataset(root_dir=data_dir, section=\"test\", download=False, transform=None) testing_datadict = [     {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}     for item in test_data.data if item[\"label\"] == 4  # label 4 is for xray hands ] test_transforms = Compose(             [                 LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),                 EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),                 ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],                                      a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),                 RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),                 RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1, prob=1.0, mode=\"bicubic\", align_corners=False),                 EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),             ]         ) val_ds = CacheDataset(data=testing_datadict[:1000], transform=test_transforms,                       cache_rate=1.0, num_workers=0) val_dl = MednistDataLoader(val_ds) val_loader = DataLoader(val_dl, batch_size=16, num_workers=0) <p>Create a model instance and assign to it the model parameters estimated at the last federated optimization round. Generate predictions of the transformation between pairs.</p> In\u00a0[\u00a0]: Copied! <pre># extract federated model into PyTorch framework\nmodel = exp.training_plan().model()\nmodel.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n\nfor moving, fixed in val_loader:\n    ddf = model(torch.cat((moving, fixed), dim=1))\n    pred_image = warp_layer(moving, ddf)\n    break\n\nfixed_image = fixed.detach().cpu().numpy()[:, 0]\nmoving_image = moving.detach().cpu().numpy()[:, 0]\npred_image = pred_image.detach().cpu().numpy()[:, 0]\n</pre> # extract federated model into PyTorch framework model = exp.training_plan().model() model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])  for moving, fixed in val_loader:     ddf = model(torch.cat((moving, fixed), dim=1))     pred_image = warp_layer(moving, ddf)     break  fixed_image = fixed.detach().cpu().numpy()[:, 0] moving_image = moving.detach().cpu().numpy()[:, 0] pred_image = pred_image.detach().cpu().numpy()[:, 0] <p>We can finally print some example of predictions from the testing dataset.</p> In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\nbatch_size = 10\nplt.subplots(batch_size, 4, figsize=(12, 20))\nfor b in range(batch_size):\n    # moving image\n    plt.subplot(batch_size, 4, b * 4 + 1)\n    plt.axis('off')\n    plt.title(\"moving image\")\n    plt.imshow(moving_image[b], cmap=\"gray\")\n    # fixed image\n    plt.subplot(batch_size, 4, b * 4 + 2)\n    plt.axis('off')\n    plt.title(\"fixed image\")\n    plt.imshow(fixed_image[b], cmap=\"gray\")\n    # warped moving\n    plt.subplot(batch_size, 4, b * 4 + 3)\n    plt.axis('off')\n    plt.title(\"predicted image\")\n    plt.imshow(pred_image[b], cmap=\"gray\")\n    \n    #error\n    plt.subplot(batch_size, 4, b * 4 + 4)\n    plt.axis('off')\n    plt.title(\"error between predicted \\nand fixed image\")\n    plt.imshow(pred_image[b] - fixed_image[b], cmap=\"gray\")\nplt.axis('off')\nplt.show()\n</pre> %matplotlib inline batch_size = 10 plt.subplots(batch_size, 4, figsize=(12, 20)) for b in range(batch_size):     # moving image     plt.subplot(batch_size, 4, b * 4 + 1)     plt.axis('off')     plt.title(\"moving image\")     plt.imshow(moving_image[b], cmap=\"gray\")     # fixed image     plt.subplot(batch_size, 4, b * 4 + 2)     plt.axis('off')     plt.title(\"fixed image\")     plt.imshow(fixed_image[b], cmap=\"gray\")     # warped moving     plt.subplot(batch_size, 4, b * 4 + 3)     plt.axis('off')     plt.title(\"predicted image\")     plt.imshow(pred_image[b], cmap=\"gray\")          #error     plt.subplot(batch_size, 4, b * 4 + 4)     plt.axis('off')     plt.title(\"error between predicted \\nand fixed image\")     plt.imshow(pred_image[b] - fixed_image[b], cmap=\"gray\") plt.axis('off') plt.show()"},{"location":"tutorials/monai/02_monai-2d-image-registration/#federated-2d-xray-registration-with-monai","title":"Federated 2d XRay registration with MONAI\u00b6","text":""},{"location":"tutorials/monai/02_monai-2d-image-registration/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial shows how to deploy in Fed-BioMed the 2d image registration example provided in the project MONAI (https://monai.io/):</p> <p>https://github.com/Project-MONAI/tutorials/blob/master/2d_registration/registration_mednist.ipynb</p> <p>Being MONAI based on PyTorch, the deployment within Fed-BioMed follows seamlessly the same general structure of general PyTorch training plans.</p> <p>Following the MONAI example, this tutorial is based on the MedNIST dataset&gt;</p>"},{"location":"tutorials/monai/02_monai-2d-image-registration/#image-registration","title":"Image Registration\u00b6","text":"<p>Image registration is the process of transforming and recalibrating different images into one coordinate system. It makes possible to compare several images captured with the same modality.</p> <p>In this tutorial, we are using a UNet-like registration network ( https://arxiv.org/abs/1711.01666 ). Goal of the notebook is to train a model given moving images and fixed images (recalibrated images).</p>"},{"location":"tutorials/monai/02_monai-2d-image-registration/#creating-mednist-nodes","title":"Creating MedNIST nodes\u00b6","text":"<p>MedNIST provides an artificial 2d classification dataset created by gathering different medical imaging datasets from TCIA, the RSNA Bone Age Challenge, and the NIH Chest X-ray dataset. The dataset is kindly made available by Dr. Bradley J. Erickson M.D., Ph.D. (Department of Radiology, Mayo Clinic) under the Creative Commons CC BY-SA 4.0 license.</p> <p>To proceed with the tutorial, we created an iid partitioning of the MedNIST dataset between 3 clients. Each client has 3000 image samples for each class. The training partitions are availables at the following link:</p> <p>https://drive.google.com/file/d/1vLIcBdtdAhh6K-vrgCFy_0Y55dxOWZwf/view</p> <p>The dataset owned by each client has structure:</p> <p>\u2514\u2500\u2500 client_*/</p> <pre><code>\u251c\u2500\u2500 AbdomenCT/\n\n\u2514\u2500\u2500 BreastMRI/\n\n\u2514\u2500\u2500 CXR/\n\n\u2514\u2500\u2500 ChestCT/\n\n\u2514\u2500\u2500 Hand/\n\n\u2514\u2500\u2500 HeadCT/      </code></pre> <p>To create the federated dataset, we follow the standard procedure for node creation/population of Fed-BioMed. After activating the fedbiomed network with the commands</p> <p><code>source {FEDBIOMED_DIR}/scripts/fedbiomed_environment network</code></p> <p>and</p> <p><code>{FEDBIOMED_DIR}/scripts/fedbiomed_run network</code></p> <p>we create a first node by using the commands</p> <p><code>source {FEDBIOMED_DIR}/scripts/fedbiomed_environment node</code></p> <p><code>{FEDBIOMED_DIR}/scripts/fedbiomed_run node start</code></p> <p>We then poulate the node with the data of first client:</p> <p><code>{FEDBIOMED_DIR}/scripts/fedbiomed_run node add</code></p> <p>We select option 3 (images) to add MedNIST partition of client 1, by just picking the folder of client 1. We use <code>mednist</code> as tag to save the selected dataset. We can further check that the data has been added by executing <code>{FEDBIOMED_DIR}/scripts/fedbiomed_run node list</code></p> <p>Following the same procedure, we create the other two nodes with the datasets of client 2 and client 3 respectively.</p>"},{"location":"tutorials/monai/02_monai-2d-image-registration/#running-fed-biomed-researcher","title":"Running Fed-BioMed Researcher\u00b6","text":""},{"location":"tutorials/monai/02_monai-2d-image-registration/#create-an-experiment-to-train-a-model-on-the-data-found","title":"Create an experiment to train a model on the data found\u00b6","text":""},{"location":"tutorials/monai/02_monai-2d-image-registration/#testing","title":"Testing\u00b6","text":""},{"location":"tutorials/pytorch/","title":"Fed-BioMed using Pytorch Deep Learning framework: a step-by-step tutorial","text":"<p>Pytorch is one of the one of the primary open source machine learning libraries.  </p> <ol> <li> <p>Basic Pytorch example with MNIST</p> </li> <li> <p>Write your own Pytorch training plan</p> </li> <li> <p>Comparing PyTorch federated model vs model trained locally</p> </li> </ol>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/","title":"PyTorch MNIST Basic Example","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install matplotlib\n</pre> !pip install matplotlib In\u00a0[\u00a0]: Copied! <pre>import os\nfrom fedbiomed.researcher.environ import environ\n</pre> import os from fedbiomed.researcher.environ import environ In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torchvision import datasets, transforms\n\n\ntransform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\n# Get the test dataset\ntest_set = datasets.MNIST(root = os.path.join(environ['TMP_DIR'], 'mnist_testing.tmp'),\n                          download = True, train = False, transform = transform)\n</pre> import torch from torchvision import datasets, transforms   transform = transforms.Compose([             transforms.ToTensor(),             transforms.Normalize((0.1307,), (0.3081,))         ])  # Get the test dataset test_set = datasets.MNIST(root = os.path.join(environ['TMP_DIR'], 'mnist_testing.tmp'),                           download = True, train = False, transform = transform)   In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\n# display a few digits from MNIST testing dataset\n\nnb_digits_to_display = 10\n\nplt.figure(figsize=(10,2)) \nplt.title(\"Few images of MNIST dataset\")\nfor i in range(nb_digits_to_display):\n    plt.subplot(1,nb_digits_to_display, i+1)\n    plt.imshow(test_set.data[i].numpy())\n    plt.title(f\"label: {test_set.targets[i].numpy()}\")\n\nplt.suptitle(\"Few images of MNIST testing dataset\", fontsize=16)\n</pre> import matplotlib.pyplot as plt  # display a few digits from MNIST testing dataset  nb_digits_to_display = 10  plt.figure(figsize=(10,2))  plt.title(\"Few images of MNIST dataset\") for i in range(nb_digits_to_display):     plt.subplot(1,nb_digits_to_display, i+1)     plt.imshow(test_set.data[i].numpy())     plt.title(f\"label: {test_set.targets[i].numpy()}\")  plt.suptitle(\"Few images of MNIST testing dataset\", fontsize=16) In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\n# Here we define the training plan to be used.\n# You can use any class name (here 'MyTrainingPlan')\nclass MyTrainingPlan(TorchTrainingPlan):\n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            self.fc1 = nn.Linear(9216, 128)\n            self.fc2 = nn.Linear(128, 10)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.dropout2(x)\n            x = self.fc2(x)\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n\n    def init_optimizer(self, optimizer_args):\n        return Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n\n    def init_dependencies(self):\n        return [\"from torchvision import datasets, transforms\",\n                \"from torch.optim import Adam\"]\n\n    def training_data(self, batch_size = 48):\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        loader_arguments = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset1, **loader_arguments)\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn import torch.nn.functional as F from torch.optim import Adam from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms  # Here we define the training plan to be used. # You can use any class name (here 'MyTrainingPlan') class MyTrainingPlan(TorchTrainingPlan):     class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.conv1 = nn.Conv2d(1, 32, 3, 1)             self.conv2 = nn.Conv2d(32, 64, 3, 1)             self.dropout1 = nn.Dropout(0.25)             self.dropout2 = nn.Dropout(0.5)             self.fc1 = nn.Linear(9216, 128)             self.fc2 = nn.Linear(128, 10)          def forward(self, x):             x = self.conv1(x)             x = F.relu(x)             x = self.conv2(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = self.dropout1(x)             x = torch.flatten(x, 1)             x = self.fc1(x)             x = F.relu(x)             x = self.dropout2(x)             x = self.fc2(x)             output = F.log_softmax(x, dim=1)             return output      def init_model(self, model_args):         return self.Net(model_args = model_args)      def init_optimizer(self, optimizer_args):         return Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])      def init_dependencies(self):         return [\"from torchvision import datasets, transforms\",                 \"from torch.optim import Adam\"]      def training_data(self, batch_size = 48):         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         loader_arguments = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset1, **loader_arguments)      def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss  <p>         Fed-BioMed nodes can be configured to accept only approved training plans. Under this configuration, the training plan files that are sent by a researcher must be approved by the node in advance. For more details, you can visit the tutorial for working with approved training plan file and user guide for managing nodes. </p> In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'batch_size': 48,\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 1, \n    'dry_run': False,  \n    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n\ntags =  ['#MNIST', '#dataset']\nrounds = 4\n</pre> model_args = {}  training_args = {     'batch_size': 48,     'optimizer_args': {         'lr': 1e-3     },     'epochs': 1,      'dry_run': False,       'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples }  tags =  ['#MNIST', '#dataset'] rounds = 4  <p>Training plan class should be passed to the experiment. The experiment will be responsible for uploading the training plan file to the file repository. Afterwards, the nodes will receive training request that includes the URL where the training plan class is stored.</p> <p>Finally, you should indicate which method should be chosen to aggregate model parameters after every round. The basic federation scheme is federated averaging, implemented in Fed-BioMed in the class  <code>FedAverage</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage   exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) <p>As an output, you should see the log message from the node which dispose of the MNIST dataset. It means that the search request that contains <code>#MNIST, #dataset</code> tags has been successfully received by the node. In the example displayed here, we received messages only from one node, which we created before.</p> <p>The experiment also receives loss values during training on each node. In Fed-BioMed, it is possible to use a tensorboard to display loss values during training. Please refer to Fed-BioMed's tensorboard documentation for how to enable the tensorboard.</p> <p>Now, let's run the experiment.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>After running the experiment, according to the provided arguments 4 training rounds should be completed on the node that you created. You can check the node id from the output and compare it with the node id which is defined in the config.ini file. After the process is finished, you are ready to inspect the model parameters.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the training rounds : \", exp.training_replies().keys())\n</pre> print(\"\\nList the training rounds : \", exp.training_replies().keys()) <p>Now, let's see how training details can be accessed from <code>training_replies()</code>. The following parameters will be inspected;</p> <ul> <li><code>rtime_training</code> : Real-time (clock time) spent in the training function on the node</li> <li><code>ptime_training</code>: Process time (user and system CPU) spent in the training function on the node</li> <li><code>rtime_total</code>   : Real-time (clock time) spent in the researcher between sending training requests and handling the responses</li> </ul> <p>Note: The following code accesses the training replies of the last round of the experiment.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the nodes for the last training round and their timings : \")\nround_data = exp.training_replies()[rounds - 1].data()\nfor c in range(len(round_data)):\n    print(\"\\t- {id} :\\\n\\n\\t\\trtime_training={rtraining:.2f} seconds\\\n\\n\\t\\tptime_training={ptraining:.2f} seconds\\\n\\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n        rtraining = round_data[c]['timing']['rtime_training'],\n        ptraining = round_data[c]['timing']['ptime_training'],\n        rtotal = round_data[c]['timing']['rtime_total']))\n</pre> print(\"\\nList the nodes for the last training round and their timings : \") round_data = exp.training_replies()[rounds - 1].data() for c in range(len(round_data)):     print(\"\\t- {id} :\\     \\n\\t\\trtime_training={rtraining:.2f} seconds\\     \\n\\t\\tptime_training={ptraining:.2f} seconds\\     \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],         rtraining = round_data[c]['timing']['rtime_training'],         ptraining = round_data[c]['timing']['ptime_training'],         rtotal = round_data[c]['timing']['rtime_total'])) In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n\nprint(\"\\nAccess the federated params for the last training round : \")\nprint(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n</pre> print(\"\\nList the training rounds : \", exp.aggregated_params().keys())  print(\"\\nAccess the federated params for the last training round : \") print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())  <p>You can also access the path where the model parameters are saved.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path'])\n</pre> print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path']) <p>Finally, to access specific parameters of last round:</p> In\u00a0[\u00a0]: Copied! <pre>print(\"\\t- Parameters of CONV1 layer's biases of last round: \\n\", exp.aggregated_params()[rounds - 1]['params']['conv1.bias'])\n</pre> print(\"\\t- Parameters of CONV1 layer's biases of last round: \\n\", exp.aggregated_params()[rounds - 1]['params']['conv1.bias']) In\u00a0[\u00a0]: Copied! <pre>exp.training_plan()\n</pre> exp.training_plan() In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\nmodel_args = {}\n\ntraining_args = {\n    'batch_size': 48,\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 1,\n    'dry_run': False,  \n    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n\ntags =  ['#MNIST', '#dataset']\nrounds = 4\n\nexpN2 = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  model_args = {}  training_args = {     'batch_size': 48,     'optimizer_args': {         'lr': 1e-3     },     'epochs': 1,     'dry_run': False,       'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples }  tags =  ['#MNIST', '#dataset'] rounds = 4  expN2 = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) <p>You can see from the output that the search request has been sent to 2 nodes by the experiment.</p> <p>Now, let's run the experiment.</p> In\u00a0[\u00a0]: Copied! <pre>expN2.run()\n</pre> expN2.run() In\u00a0[\u00a0]: Copied! <pre>fed_model = expN2.training_plan().model()\nfed_model.load_state_dict(expN2.aggregated_params()[rounds - 1]['params'])\n</pre> fed_model = expN2.training_plan().model() fed_model.load_state_dict(expN2.aggregated_params()[rounds - 1]['params']) In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport torch.nn.functional as F\n# Test function \ndef testing_accuracy(model, data_loader):\n    model.eval()\n    test_loss = 0\n    device = 'cpu'\n\n    y_pred = []\n    y_actu = []\n    with torch.no_grad():\n        for data, target in data_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            y_pred.extend(torch.flatten(pred).tolist()) \n            y_actu.extend(target.tolist())\n           \n    y_pred = pd.Series(y_pred, name='Actual')\n    y_actu = pd.Series(y_actu, name='Predicted')\n    cm = pd.crosstab(y_actu, y_pred)\n    correct = sum([cm.iloc[i,i] for i in range(len(cm))])\n    \n    test_loss /= len(data_loader.dataset)\n    accuracy = 100*correct/len(test_loader.dataset)\n\n    return(test_loss, accuracy, cm)\n</pre> import pandas as pd import torch.nn.functional as F # Test function  def testing_accuracy(model, data_loader):     model.eval()     test_loss = 0     device = 'cpu'      y_pred = []     y_actu = []     with torch.no_grad():         for data, target in data_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             y_pred.extend(torch.flatten(pred).tolist())              y_actu.extend(target.tolist())                 y_pred = pd.Series(y_pred, name='Actual')     y_actu = pd.Series(y_actu, name='Predicted')     cm = pd.crosstab(y_actu, y_pred)     correct = sum([cm.iloc[i,i] for i in range(len(cm))])          test_loss /= len(data_loader.dataset)     accuracy = 100*correct/len(test_loader.dataset)      return(test_loss, accuracy, cm) <p>We will use the MNIST test dataset for testing our federated model. You can download this dataset set from <code>torchvision.dataset</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torchvision import datasets, transforms\n\n\ntransform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\n# Get the test dataset\ntest_set = datasets.MNIST(root = os.path.join(environ['TMP_DIR'], 'mnist_testing.tmp'), download = True, train = False, transform = transform)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)\n</pre> import torch from torchvision import datasets, transforms   transform = transforms.Compose([             transforms.ToTensor(),             transforms.Normalize((0.1307,), (0.3081,))         ])  # Get the test dataset test_set = datasets.MNIST(root = os.path.join(environ['TMP_DIR'], 'mnist_testing.tmp'), download = True, train = False, transform = transform) test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True) <p>Now, it is time to get performance results</p> In\u00a0[\u00a0]: Copied! <pre>test_results = testing_accuracy(fed_model, test_loader)\n\nprint(\"- Test Loss: \", test_results[0], \"\\n\")\nprint(\"- Accuracy: \", test_results[1], \"\\n\")\nprint(\"- Confusion Matrix: \\n \\n\",  test_results[2] )\n</pre> test_results = testing_accuracy(fed_model, test_loader)  print(\"- Test Loss: \", test_results[0], \"\\n\") print(\"- Accuracy: \", test_results[1], \"\\n\") print(\"- Confusion Matrix: \\n \\n\",  test_results[2] ) <p>We will use <code>matplotlib</code> for plotting results. If you have followed all the steps in this tutorial, you must have installed this library in the section 2. Otherwise, you can run <code>!pip install matplotlib</code> command in a notebook cell.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nconf_matrix = test_results[2].to_numpy()\n\nfig, ax = plt.subplots(figsize=(10,5))\nim = ax.imshow(conf_matrix)\n\nax.set_xticks(np.arange(10))\nax.set_yticks(np.arange(10))\n\nfor i in range(conf_matrix.shape[0]):\n    for j in range(conf_matrix.shape[1]):\n        text = ax.text(j, i, conf_matrix[i, j],\n                       ha=\"center\", va=\"center\", color=\"w\")\n        \nax.set_xlabel('Actual targets')\nax.set_ylabel('Predicted targets')\nax.set_title('Confusion Matrix')\n</pre> import numpy as np import matplotlib.pyplot as plt  conf_matrix = test_results[2].to_numpy()  fig, ax = plt.subplots(figsize=(10,5)) im = ax.imshow(conf_matrix)  ax.set_xticks(np.arange(10)) ax.set_yticks(np.arange(10))  for i in range(conf_matrix.shape[0]):     for j in range(conf_matrix.shape[1]):         text = ax.text(j, i, conf_matrix[i, j],                        ha=\"center\", va=\"center\", color=\"w\")          ax.set_xlabel('Actual targets') ax.set_ylabel('Predicted targets') ax.set_title('Confusion Matrix') In\u00a0[\u00a0]: Copied! <pre>errors = []\n\nfor i in range(rounds):\n    fed_model = expN2.training_plan().model()\n    fed_model.load_state_dict(expN2.aggregated_params()[i]['params'])\n    loss = testing_accuracy(fed_model, test_loader)[0]\n    errors.append(loss)\n</pre> errors = []  for i in range(rounds):     fed_model = expN2.training_plan().model()     fed_model.load_state_dict(expN2.aggregated_params()[i]['params'])     loss = testing_accuracy(fed_model, test_loader)[0]     errors.append(loss) In\u00a0[\u00a0]: Copied! <pre>### Plotting \nplt.plot(errors, label = 'Federated Test Loss')\nplt.xlabel('Round')\nplt.ylabel('Loss')\nplt.title(\"Log Likelihood Loss evolution over number of rounds\")\nplt.legend()\n</pre> ### Plotting  plt.plot(errors, label = 'Federated Test Loss') plt.xlabel('Round') plt.ylabel('Loss') plt.title(\"Log Likelihood Loss evolution over number of rounds\") plt.legend()"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#pytorch-mnist-basic-example","title":"PyTorch MNIST Basic Example\u00b6","text":""},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial focuses on how to train a CNN model with Fed-BioMed nodes using the PyTorch framework on the MNIST dataset. You will learn;</p> <ul> <li>How to prepare your environment to be able to train your model</li> <li>How to create a training plan class to run it in a single node which works as a different process in your local machine.</li> <li>How to create a federated learning experiment.</li> <li>How to load and inspect your model parameters.</li> <li>How to test your model using test dataset.</li> </ul> <p>Note: In the following steps, we will be running this example using two nodes.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#before-you-start","title":"Before you start\u00b6","text":"<p>Before starting this tutorial please make sure that you have a clean environment. Additionally, please terminate running nodes if there is any. Please run the following command in the main Fed-BioMed directory to clean your environment. If you need more help to manage your environment please visit the tutorial for setting up an enviroment.</p> <pre><code>$ source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment clean\n</code></pre> <p>Note: <code>${FEDBIOMED_DIR}</code> is a path relative to based directory of the cloned Fed-BioMed repository. You can set it by running command <code>export FEDBIOMED_DIR=/path/to/fedbiomed</code>. This is not required for Fed-BioMed to work but enables you to run the tutorials more easily.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#1-configuring-nodes","title":"1. Configuring Nodes\u00b6","text":"<p>In this tutorial, you will learn how to train your model with a single Fed-BioMed node. Thus, you need to configure a node and add MNIST dataset into it. Node configuration steps require <code>fedbiomed-node</code> conda environment. Please make sure that you have the necessary conda environment which is explained in the installation tutorial. You can check your environment by running the following command.</p> <pre><code>$ conda env list\n</code></pre> <p>If you have all Fed-BioMed environments you are good to go for the node configuration steps.</p> <p>Please open a terminal and follow the steps below.</p> <ul> <li>Configuration Steps:<ul> <li>Run <code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node add</code> in the terminal</li> <li>It will ask you to select the data type that you want to add. The second option (which is the default) has been configured to add the MNIST dataset. Please type <code>2</code> and continue.</li> <li>Please use default tags which are <code>#MNIST</code> and <code>#dataset</code>.</li> <li>For the next step, please select the directory that you want to download the MNIST dataset.</li> <li>After the download is completed you will see the details of the MNIST dataset on the screen.</li> </ul> </li> </ul> <p>Please run the command below in the same terminal to make sure the MNIST dataset is successfully added to the node.</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node list\n</code></pre> <p>Before starting the node, please make sure that you have already launched the network using command <code>scripts/fedbiomed_run network</code>. Afterward, all you need to do is to start the node.</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node start\n</code></pre>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#start-your-notebook","title":"Start your notebook\u00b6","text":"<p>You need to start a jupyter notebook and create a new notebook to be able to follow the tutorial. Please open a new terminal window and run the following command to start your notebook.</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run researcher start\n</code></pre> <p>Note: If you are having a problem understanding the steps above, we recommend you to follow the installation tutorial.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#2-what-is-mnist-dataset","title":"2. What is MNIST dataset?\u00b6","text":"<p>MNIST dataset contains 60000 grayscale images (of size 28 * 28 pixels) of handwritten digits between 0 and 9. MNIST is commonly used for image classification task: the goal is to classify each image by assigning it to the correct digit.</p> <p>For a better visual understanding, we display a few samples from MNIST testing dataset.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#3-training-a-model","title":"3. Training a model\u00b6","text":"<p>In this section, you will learn how to train a model in Fed-BioMed, by creating a training plan class that includes special methods to retrieve mode, optimizer, training data as well as training step to execute at each iteration of training. Then the federated training will be launched by wrapping the training plan in an experiment.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#31-creating-a-pytorch-training-plan","title":"3.1 Creating A PyTorch Training Plan\u00b6","text":"<p>A PyTorch training plan is a Python class that inherits from <code>fedbiomed.common.training_plans.TorchTrainingPlan</code> which is an abstract class. The abstract methods <code>init_model</code>, <code>training_data</code> and <code>training_step</code> should be provided in the training plan to retrieve/execute model (<code>nn.Module</code>), data loader and training actions at each iteration.</p> <p>         Please visit training plan user guide for more information and other special methods of training plan.     </p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#32-define-an-experiment","title":"3.2 Define an Experiment\u00b6","text":"<p>An experiment is a class that orchestrates the training processes that run on different nodes.</p> <ul> <li><code>model_arg</code> includes arguments that will pass into <code>init_model</code> method of training plan class. In our case, we don't need to add any arguments.</li> <li><code>training_args</code> includes arguments related to optimizer, data loader and training step/routine such as learning rate, number of epochs.</li> <li><code>tags</code> is a list that includes tags that are going to be used for searching related datasets in nodes. In our case, we saved the MNIST dataset with #MNIST and #dataset tags.</li> <li><code>rounds</code> represents the number of training rounds that will be applied in nodes. In each round every node complete epochs and send model parameters to the experiment.</li> </ul>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#33-what-happens-during-the-initialization-of-an-experiment","title":"3.3 What happens during the initialization of an experiment?\u00b6","text":"<ol> <li>The experiment searches for nodes whose available datasets have been saved with tags indicated in the <code>tags</code> argument. Nodes are selected based on a node selection strategy.</li> <li>The experiment prepares the <code>job</code> for the nodes according to the provided arguments</li> <li>The training plan file is uploaded to the provided repository to make it available for the nodes.</li> </ol> <p>For more details, you can visit <code>Experiment</code> webpage.</p> <p>Now, let's create our experiment. Since only one node has been created, the experiment will only find a single node for training.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#34-extracting-training-results","title":"3.4 Extracting Training Results\u00b6","text":""},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#timing","title":"Timing\u00b6","text":"<p>Training replies for each round are available via <code>exp.training_replies()</code> (index 0 to (<code>rounds</code> - 1) ). You can display the keys of each round by running the following script.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#federated-parameters","title":"Federated Parameters\u00b6","text":"<p>Federated model parameters for each round are available via <code>exp.aggregated_params()</code> (index 0 to (<code>rounds</code> - 1) ). For example, you can easily view the federated parameters for the last round of the experiment:</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#4-pytorch-mnist-example-with-two-nodes","title":"4. PyTorch MNIST Example with Two Nodes\u00b6","text":"<p>In this section, we will be working on two nodes. Following the previous example, the experiment and training routine will remain unchanged.  Therefore, you just need to configure another node, and add the MNIST dataset with the default tags.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#41-configuring-second-node","title":"4.1 Configuring Second Node\u00b6","text":"<p>While creating a second node you need to be careful with the node that has already been created. To configure the second node, a different config file has to be defined. Please follow the steps below to configure your second node.</p> <ol> <li>Please open a new terminal and cd into the base directory of fedbiomed.</li> <li>In this step, you need to name a new config file using the <code>config</code> parameter. Instead of downloading a new MNIST dataset, you can use the one that you already downloaded in the previous example. To do that please run <code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n2.ini --add-mnist /path/to/your/mnist</code>. It will create a config file named <code>config-n2.ini</code></li> <li>You need to start the new node by indicating the newely created config file. Please run the following command to start the second node <code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n2.ini start</code></li> </ol>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#42-defining-an-experiment","title":"4.2 Defining an Experiment\u00b6","text":"<p>Since a training plan has already been created and saved in the previous example, you don't need to repeat this step here again: the same training plan with same model will be used for training. However, you can define a new experiment for testing purposes. The experiment will search the MNIST dataset in available nodes. Training arguments are kept the same as in the previous example.</p> <p>You can also list datasets and select specific nodes to perform traning. You can visit listing datasets and selecting nodes documentation to get more information.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#5-testing-federated-model","title":"5. Testing Federated Model\u00b6","text":"<p>In this section, we will create a test function to obtain accuracy, loss, and confusion matrix using the test MNIST dataset.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#51-aggregated-parameters","title":"5.1 Aggregated Parameters\u00b6","text":"<p><code>training_plan()</code> returns the training plan and <code>training_plan().model()</code> returns the model that is created in the training plan.  It is possible to load specific aggregated parameters which are obtained in every round. Thereafter, it will be ready to make predictions using those parameters. The last round gives the last aggregated model parameters which represents the final model.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#52-creating-a-test-function","title":"5.2 Creating A Test Function\u00b6","text":"<p>Let's create a test function that returns loss, accuracy, and confusion matrix.</p>"},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#53-creating-heatmap-for-confusion-matrix","title":"5.3 Creating Heatmap for Confusion Matrix\u00b6","text":""},{"location":"tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/#54-plotting-loss-for-each-round","title":"5.4 Plotting Loss for Each round\u00b6","text":"<p>In this section, we will plot loss values that are obtained over the test dataset using model parameters of every round.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/","title":"How to Create Your Custom PyTorch Training Plan","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport pandas as pd\nimport shutil\n\n# Celeba folder\nparent_dir = os.path.join(\".\", \"data\", \"Celeba\") \nceleba_raw_folder = os.path.join(\"Celeba_raw\", \"raw\")\nimg_dir = os.path.join(parent_dir, celeba_raw_folder, 'img_align_celeba') + os.sep\nout_dir = os.path.join(\".\", \"data\", \"Celeba\", \"celeba_preprocessed\")\n\n# Read attribute CSV and only load Smilling column\ndf = pd.read_csv(os.path.join(parent_dir, celeba_raw_folder, 'list_attr_celeba.txt'),\n                 sep=\"\\s+\", skiprows=1, usecols=['Smiling'])\n\n# data is on the form : 1 if the person is smiling, -1 otherwise. we set all -1 to 0 for the model to train faster\ndf.loc[df['Smiling'] == -1, 'Smiling'] = 0\n\n# Split csv in 3 part\nlength = len(df)\ndata_node_1 = df.iloc[:int(length/3)]\ndata_node_2 = df.iloc[int(length/3):int(length/3) * 2]\ndata_node_3 = df.iloc[int(length/3) * 2:]\n\n# Create folder for each node\nif not os.path.exists(os.path.join(out_dir, \"data_node_1\")):\n    os.makedirs(os.path.join(out_dir, \"data_node_1\", \"data\"))\nif not os.path.exists(os.path.join(out_dir, \"data_node_2\")):\n    os.makedirs(os.path.join(out_dir, \"data_node_2\", \"data\"))\nif not os.path.exists(os.path.join(out_dir, \"data_node_3\")):\n    os.makedirs(os.path.join(out_dir, \"data_node_3\", \"data\"))\n\n# Save each node's target CSV to the corect folder\ndata_node_1.to_csv(os.path.join(out_dir, 'data_node_1', 'target.csv'), sep='\\t')\ndata_node_2.to_csv(os.path.join(out_dir, 'data_node_2', 'target.csv'), sep='\\t')\ndata_node_3.to_csv(os.path.join(out_dir, 'data_node_3', 'target.csv'), sep='\\t')\n\n# Copy all images of each node in the correct folder\nfor im in data_node_1.index:\n    shutil.copy(img_dir+im, os.path.join(out_dir,\"data_node_1\", \"data\", im))\nprint(\"data for node 1 succesfully created\")\n\nfor im in data_node_2.index:\n    shutil.copy(img_dir+im, os.path.join(out_dir, \"data_node_2\", \"data\", im))\nprint(\"data for node 2 succesfully created\")\n\nfor im in data_node_3.index:\n    shutil.copy(img_dir+im, os.path.join(out_dir, \"data_node_3\", \"data\", im))\nprint(\"data for node 3 succesfully created\")\n</pre> import os import numpy as np import pandas as pd import shutil  # Celeba folder parent_dir = os.path.join(\".\", \"data\", \"Celeba\")  celeba_raw_folder = os.path.join(\"Celeba_raw\", \"raw\") img_dir = os.path.join(parent_dir, celeba_raw_folder, 'img_align_celeba') + os.sep out_dir = os.path.join(\".\", \"data\", \"Celeba\", \"celeba_preprocessed\")  # Read attribute CSV and only load Smilling column df = pd.read_csv(os.path.join(parent_dir, celeba_raw_folder, 'list_attr_celeba.txt'),                  sep=\"\\s+\", skiprows=1, usecols=['Smiling'])  # data is on the form : 1 if the person is smiling, -1 otherwise. we set all -1 to 0 for the model to train faster df.loc[df['Smiling'] == -1, 'Smiling'] = 0  # Split csv in 3 part length = len(df) data_node_1 = df.iloc[:int(length/3)] data_node_2 = df.iloc[int(length/3):int(length/3) * 2] data_node_3 = df.iloc[int(length/3) * 2:]  # Create folder for each node if not os.path.exists(os.path.join(out_dir, \"data_node_1\")):     os.makedirs(os.path.join(out_dir, \"data_node_1\", \"data\")) if not os.path.exists(os.path.join(out_dir, \"data_node_2\")):     os.makedirs(os.path.join(out_dir, \"data_node_2\", \"data\")) if not os.path.exists(os.path.join(out_dir, \"data_node_3\")):     os.makedirs(os.path.join(out_dir, \"data_node_3\", \"data\"))  # Save each node's target CSV to the corect folder data_node_1.to_csv(os.path.join(out_dir, 'data_node_1', 'target.csv'), sep='\\t') data_node_2.to_csv(os.path.join(out_dir, 'data_node_2', 'target.csv'), sep='\\t') data_node_3.to_csv(os.path.join(out_dir, 'data_node_3', 'target.csv'), sep='\\t')  # Copy all images of each node in the correct folder for im in data_node_1.index:     shutil.copy(img_dir+im, os.path.join(out_dir,\"data_node_1\", \"data\", im)) print(\"data for node 1 succesfully created\")  for im in data_node_2.index:     shutil.copy(img_dir+im, os.path.join(out_dir, \"data_node_2\", \"data\", im)) print(\"data for node 2 succesfully created\")  for im in data_node_3.index:     shutil.copy(img_dir+im, os.path.join(out_dir, \"data_node_3\", \"data\", im)) print(\"data for node 3 succesfully created\") <p>Now if you go to the <code>${FEDBIOMED_DIR}/notebooks/data/Celaba</code> directory you can see the folder called <code>celeba_preprocessed</code>. There will be three different folders that contain an image dataset for 3 nodes. The next step will be configuring the nodes and deplying the datasets. In the next steps, we will be configuring only two nodes. The dataset for the third node is going to be used for the testing.</p> <p>Create 2 nodes for training :</p> <ul> <li><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config node1.ini start</code></li> <li><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config node2.ini start</code></li> </ul> <p>Add data to each node :</p> <ul> <li><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config node1.ini add</code></li> <li><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config node2.ini add</code></li> </ul> <p>Note: <code>${FEDBIOMED_DIR}</code> is a path relative to based directory of the cloned Fed-BioMed repository. You can set it by running command <code>export FEDBIOMED_DIR=/path/to/fedbiomed</code>. This is not required for Fed-BioMed to work but enables you to run the tutorials more easily.</p> <p>Next step is to create our <code>Net</code> class based on the methods that have been explained in the previous sections. This class is part of the training plan that will be passed to the Experiment.  Afterwards, the nodes will receive the training plan and perform the training by retrieving training data and passing it to the <code>training_step</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nfrom fedbiomed.common.data import DataManager\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport os\n\n\nclass CelebaTrainingPlan(TorchTrainingPlan):\n\n    # Defines model\n    def init_model(self):\n        model = self.Net()\n        return model\n\n    # Here we define the custom dependencies that will be needed by our custom Dataloader\n    def init_dependencies(self):\n        deps = [\"from torch.utils.data import Dataset\",\n                \"from torchvision import transforms\",\n                \"import pandas as pd\",\n                \"from PIL import Image\",\n                \"import os\",\n                \"import numpy as np\"]\n        return deps\n\n    # Torch modules class\n    class Net(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            #convolution layers\n            self.conv1 = nn.Conv2d(3, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 32, 3, 1)\n            self.conv3 = nn.Conv2d(32, 32, 3, 1)\n            self.conv4 = nn.Conv2d(32, 32, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            # classifier\n            self.fc1 = nn.Linear(3168, 128)\n            self.fc2 = nn.Linear(128, 2)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.max_pool2d(x, 2)\n            x = F.relu(x)\n\n            x = self.conv2(x)\n            x = F.max_pool2d(x, 2)\n            x = F.relu(x)\n\n            x = self.conv3(x)\n            x = F.max_pool2d(x, 2)\n            x = F.relu(x)\n\n            x = self.conv4(x)\n            x = F.max_pool2d(x, 2)\n            x = F.relu(x)\n\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n\n            x = self.dropout2(x)\n            x = self.fc2(x)\n            output = F.log_softmax(x, dim=1)\n            return output\n\n\n    class CelebaDataset(Dataset):\n\"\"\"Custom Dataset for loading CelebA face images\"\"\"\n\n        # we dont load the full data of the images, we retrieve the image with the get item.\n        # in our case, each image is 218*178 * 3colors. there is 67533 images. this take at leas 7G of ram\n        # loading images when needed takes more time during training but it wont impact the ram usage as much as loading everything\n        def __init__(self, txt_path, img_dir, transform=None):\n            df = pd.read_csv(txt_path, sep=\"\\t\", index_col=0)\n            self.img_dir = img_dir\n            self.txt_path = txt_path\n            self.img_names = df.index.values\n            self.y = df['Smiling'].values\n            self.transform = transform\n            print(\"celeba dataset finished\")\n\n        def __getitem__(self, index):\n            img = np.asarray(Image.open(os.path.join(self.img_dir,\n                                        self.img_names[index])))\n            img = transforms.ToTensor()(img)\n            label = self.y[index]\n            return img, label\n\n        def __len__(self):\n            return self.y.shape[0]\n\n    # The training_data creates the Dataloader to be used for training in the\n    # general class Torchnn of fedbiomed\n    def training_data(self,  batch_size = 48):\n        dataset = self.CelebaDataset(self.dataset_path + \"/target.csv\", self.dataset_path + \"/data/\")\n        loader_arguments = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset, **loader_arguments)\n\n    # This function must return the loss to backward it\n    def training_step(self, data, target):\n\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan import torch.nn.functional as F from torchvision import transforms from torch.utils.data import Dataset from fedbiomed.common.data import DataManager import pandas as pd import numpy as np from PIL import Image import os   class CelebaTrainingPlan(TorchTrainingPlan):      # Defines model     def init_model(self):         model = self.Net()         return model      # Here we define the custom dependencies that will be needed by our custom Dataloader     def init_dependencies(self):         deps = [\"from torch.utils.data import Dataset\",                 \"from torchvision import transforms\",                 \"import pandas as pd\",                 \"from PIL import Image\",                 \"import os\",                 \"import numpy as np\"]         return deps      # Torch modules class     class Net(nn.Module):          def __init__(self):             super().__init__()             #convolution layers             self.conv1 = nn.Conv2d(3, 32, 3, 1)             self.conv2 = nn.Conv2d(32, 32, 3, 1)             self.conv3 = nn.Conv2d(32, 32, 3, 1)             self.conv4 = nn.Conv2d(32, 32, 3, 1)             self.dropout1 = nn.Dropout(0.25)             self.dropout2 = nn.Dropout(0.5)             # classifier             self.fc1 = nn.Linear(3168, 128)             self.fc2 = nn.Linear(128, 2)          def forward(self, x):             x = self.conv1(x)             x = F.max_pool2d(x, 2)             x = F.relu(x)              x = self.conv2(x)             x = F.max_pool2d(x, 2)             x = F.relu(x)              x = self.conv3(x)             x = F.max_pool2d(x, 2)             x = F.relu(x)              x = self.conv4(x)             x = F.max_pool2d(x, 2)             x = F.relu(x)              x = self.dropout1(x)             x = torch.flatten(x, 1)             x = self.fc1(x)             x = F.relu(x)              x = self.dropout2(x)             x = self.fc2(x)             output = F.log_softmax(x, dim=1)             return output       class CelebaDataset(Dataset):         \"\"\"Custom Dataset for loading CelebA face images\"\"\"          # we dont load the full data of the images, we retrieve the image with the get item.         # in our case, each image is 218*178 * 3colors. there is 67533 images. this take at leas 7G of ram         # loading images when needed takes more time during training but it wont impact the ram usage as much as loading everything         def __init__(self, txt_path, img_dir, transform=None):             df = pd.read_csv(txt_path, sep=\"\\t\", index_col=0)             self.img_dir = img_dir             self.txt_path = txt_path             self.img_names = df.index.values             self.y = df['Smiling'].values             self.transform = transform             print(\"celeba dataset finished\")          def __getitem__(self, index):             img = np.asarray(Image.open(os.path.join(self.img_dir,                                         self.img_names[index])))             img = transforms.ToTensor()(img)             label = self.y[index]             return img, label          def __len__(self):             return self.y.shape[0]      # The training_data creates the Dataloader to be used for training in the     # general class Torchnn of fedbiomed     def training_data(self,  batch_size = 48):         dataset = self.CelebaDataset(self.dataset_path + \"/target.csv\", self.dataset_path + \"/data/\")         loader_arguments = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset, **loader_arguments)      # This function must return the loss to backward it     def training_step(self, data, target):          output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss  <p>This group of arguments corresponds respectively to:</p> <ul> <li><code>model_args</code>: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node-side.</li> <li><code>training_args</code>: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node-side.</li> </ul> <p>Note: Typos and/or lack of positional (required) arguments might raise an error.</p> In\u00a0[\u00a0]: Copied! <pre>training_args = {\n    'batch_size': 32, \n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 1, \n    'dry_run': False,  \n    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n</pre> training_args = {     'batch_size': 32,      'optimizer_args': {         'lr': 1e-3     },     'epochs': 1,      'dry_run': False,       'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#celeba']\nrounds = 3\n\nexp = Experiment(tags=tags,\n                 training_plan_class=CelebaTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['#celeba'] rounds = 3  exp = Experiment(tags=tags,                  training_plan_class=CelebaTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) <p>Let's start the experiment.</p> <p>By default, this function doesn't stop until all the <code>round_limit</code> rounds are done for all the nodes. While the experiment runs you can open the terminals where you have started the nodes and see the training progress. However, the loss values obtained from each node during the training will be printed as output in real time. Since we are working on an image dataset, training might take some time.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() In\u00a0[\u00a0]: Copied! <pre>fed_model = exp.training_plan().model()\nfed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n</pre> fed_model = exp.training_plan().model() fed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params']) In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\n\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport os\n\ndef testing_Accuracy(model, data_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n\n    device = \"cpu\"\n\n    correct = 0\n\n    loader_size = len(data_loader)\n    with torch.no_grad():\n        for idx, (data, target) in enumerate(data_loader):\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n            #only uses 10% of the dataset, results are similar but faster\n            if idx &gt;= loader_size / 10:\n                pass\n                break\n\n    \n        pred = output.argmax(dim=1, keepdim=True)\n\n    test_loss /= len(data_loader.dataset)\n    accuracy = 100* correct/(data_loader.batch_size * idx)\n\n    return(test_loss, accuracy)\n</pre>  import torch import torch.nn as nn  import torch.nn.functional as F from torchvision import transforms from torch.utils.data import Dataset import pandas as pd import numpy as np from PIL import Image import os  def testing_Accuracy(model, data_loader):     model.eval()     test_loss = 0     correct = 0      device = \"cpu\"      correct = 0      loader_size = len(data_loader)     with torch.no_grad():         for idx, (data, target) in enumerate(data_loader):             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()              #only uses 10% of the dataset, results are similar but faster             if idx &gt;= loader_size / 10:                 pass                 break               pred = output.argmax(dim=1, keepdim=True)      test_loss /= len(data_loader.dataset)     accuracy = 100* correct/(data_loader.batch_size * idx)      return(test_loss, accuracy) <p>We also need to define a custom Dataset class for the test dataset in order to load it using PyTorch's <code>DataLoader</code>. This will be the same class that has been already defined in the training plan.</p> In\u00a0[\u00a0]: Copied! <pre>from torch.utils.data import DataLoader\ntest_dataset_path = \"../data/celeba/celeba_preprocessed/data_node_3\"\n\nclass CelebaDataset(Dataset):\n\"\"\"Custom Dataset for loading CelebA face images\"\"\"\n\n    def __init__(self, txt_path, img_dir, transform=None):\n        df = pd.read_csv(txt_path, sep=\"\\t\", index_col=0)\n        self.img_dir = img_dir\n        self.txt_path = txt_path\n        self.img_names = df.index.values\n        self.y = df['Smiling'].values\n        self.transform = transform\n        print(\"celeba dataset finished\")\n\n    def __getitem__(self, index):\n        img = np.asarray(Image.open(os.path.join(self.img_dir,\n                                        self.img_names[index])))\n        img = transforms.ToTensor()(img)\n        label = self.y[index]\n        return img, label\n\n    def __len__(self):\n        return self.y.shape[0]\n\n\ndataset = CelebaDataset(test_dataset_path + \"/target.csv\", test_dataset_path + \"/data/\")\ntrain_kwargs = {'batch_size': 128, 'shuffle': True}\ndata_loader = DataLoader(dataset, **train_kwargs)\n</pre> from torch.utils.data import DataLoader test_dataset_path = \"../data/celeba/celeba_preprocessed/data_node_3\"  class CelebaDataset(Dataset):     \"\"\"Custom Dataset for loading CelebA face images\"\"\"      def __init__(self, txt_path, img_dir, transform=None):         df = pd.read_csv(txt_path, sep=\"\\t\", index_col=0)         self.img_dir = img_dir         self.txt_path = txt_path         self.img_names = df.index.values         self.y = df['Smiling'].values         self.transform = transform         print(\"celeba dataset finished\")      def __getitem__(self, index):         img = np.asarray(Image.open(os.path.join(self.img_dir,                                         self.img_names[index])))         img = transforms.ToTensor()(img)         label = self.y[index]         return img, label      def __len__(self):         return self.y.shape[0]   dataset = CelebaDataset(test_dataset_path + \"/target.csv\", test_dataset_path + \"/data/\") train_kwargs = {'batch_size': 128, 'shuffle': True} data_loader = DataLoader(dataset, **train_kwargs) In\u00a0[\u00a0]: Copied! <pre>acc_federated = testing_Accuracy(fed_model, data_loader)\nacc_federated[1]\n</pre> acc_federated = testing_Accuracy(fed_model, data_loader) acc_federated[1]"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#how-to-create-your-custom-pytorch-training-plan","title":"How to Create Your Custom PyTorch Training Plan\u00b6","text":"<p>Fed-BioMed allows you to perform model training without changing your PyTorch training plan class completely. Integrating your PyToch model to Fed-BioMed only requires to add extra attributes and methods to train your model based on a federated approach.  In this tutorial, you will learn how to write/define your <code>TrainingPlan</code> (wrapping your model) in Fed-BioMed for PyTorch framework.</p> <p>Note: Before starting this tutorial we highly recommend you to follow the previous tutorials to understand the basics of Fed-BioMed.</p> <p>In this tutorial, we will be using Celaba (CelebaFaces) dataset to train the model. You can see details of the dataset here. In the following sections, you will have the instructions for downloading and configuring Celeba dataset for Fed-BioMed framework.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#1-fed-biomed-training-plan","title":"1. Fed-BioMed Training Plan\u00b6","text":"<p>In this section, you will learn how to write your custom training plan.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#what-is-training-plan","title":"What is Training Plan?\u00b6","text":"<p>The training plan is the class where all the methods and attributes are defined to train your model on the nodes. Each training plan should inherit the base training plan class of the belonging ML framework that is provided by Fed-BioMed. For more details, you can visit documentation for training plan.  The following code snippet shows a basic training plan that can be defined in Fed-BioMed for PyTorch framework.</p> <pre>from fedbiomed.common.training_plans import TorchTrainingPlan\n\n\nclass CustomTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args):\n        # ....\n        pass\n\n    def init_dependencies(self):\n        #...\n        pass\n    \n    def init_optimizer(self, optimizer_args):\n        #...\n        pass\n\n    def training_data(self,  batch_size = 48):\n        # ...\n        return\n    \n    def training_step(self, data, target):\n        # ...\n        return\n</pre>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#init_model-method-of-training-plan","title":"<code>init_model</code> Method of Training Plan\u00b6","text":"<p><code>init_model</code> method of the training plan is where you initialize your neural network module as in classical PyTorch model class. The network should be defined inside the training plan class and <code>init_model</code> should instantiate this network (<code>Module</code>), and return it.</p> <p>In this tutorial, we will be training a classification model for CelebA image dataset that will be able to predict whether the given face is smiling.</p> <pre>def init_model(self, model_args: dict = {}):\n    return self.Net(model_args)\n\nclass Net(nn.Module):\n\n    def __init__(model_args):\n        super().__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n        self.conv3 = nn.Conv2d(32, 32, 3, 1)\n        self.conv4 = nn.Conv2d(32, 32, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        # Classifier\n        self.fc1 = nn.Linear(3168, 128)\n        self.fc2 = nn.Linear(128, 2)\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n\n        x = self.conv3(x)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n\n        x = self.conv4(x)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n</pre>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#init_dependencies-method","title":"<code>init_dependencies</code> Method\u00b6","text":"<p>Next, you should define the <code>init_dependencies</code> to declare the modules that are used in the training plan. The modules should be supported by the Fed-BioMed.</p> <pre>def init_depedencies(self)\n    # Here we define the custom dependencies that will be needed by our custom Dataloader\n    deps = [\"from torch.utils.data import Dataset, DataLoader\",\n            \"from torchvision import transforms\",\n            \"import pandas as pd\",\n            \"from PIL import Image\",\n            \"import os\",\n            \"import numpy as np\"]\n    return deps\n</pre>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#init_optimizer-method","title":"<code>init_optimizer</code> Method\u00b6","text":"<p>To optimize your model, you will need an optimizer. This is where <code>init_optimizer</code> method comes into play. In this method, you may change the optimizer you want to use, add a PyTorch learning rate <code>Scheduler</code> or provide your custom optimizer.</p> <p><code>init_optimizer</code> takes <code>optimizer_args</code> as argument, an entry from <code>training_args</code> (more details later) which is a dictionary containing parameters that may be needed for optimizer initalization (such as learning rate, <code>Adagrad</code> weights decay, <code>Adam</code> beta parameters, ...). <code>init_optimizer</code> method should return the initialized <code>optimizer</code>, that will be used to optimize model.</p> <p>Defining an <code>optimizer</code> in Fed-BioMed is pretty similar to PyTorch, as shown in the example below (using PyTorch's <code>SGD</code> optimizer):</p> <pre>def init_optimizer(self, optimizer_args):\n    return torch.optim.SGD(self.model().parameters(), lr=optimizer_args['lr'])\n</pre> <p>By default (if this method is not specified in the <code>TrainingPlan</code>), model will be optimized using default <code>Adam</code> optimizer.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#training_data-and-custom-dataset","title":"<code>training_data() and Custom Dataset</code>\u00b6","text":"<p><code>training_data</code> is a method where the data is loaded for training on the node side. During each round of training, each node that particapates federated training builds the model, load the dataset using the method <code>training_data</code>, and performs the <code>training_step</code> by passing loaded dataset.</p> <p>The dataset that we will be using in this tutorial is a image dataset. Therefore, your custom PyTorch <code>Dataset</code> should be be able to load images by given index . Please see the details of custom PyTorch datasets.</p> <pre>class CelebaDataset(Dataset):\n\"\"\"Custom Dataset for loading CelebA face images\"\"\"\n\n\n        def __init__(self, txt_path, img_dir, transform=None):\n\n            # Read the csv file that includes classes for each image\n            df = pd.read_csv(txt_path, sep=\"\\t\", index_col=0)\n            self.img_dir = img_dir\n            self.txt_path = txt_path\n            self.img_names = df.index.values\n            self.y = df['Smiling'].values\n            self.transform = transform\n\n        def __getitem__(self, index):\n            img = np.asarray(Image.open(os.path.join(self.img_dir, self.img_names[index])))\n            img = transforms.ToTensor()(img)\n            label = self.y[index]\n            return img, label\n\n        def __len__(self):\n            return self.y.shape[0]\n</pre> <p>Now, you need to define a <code>training_data</code> method that will create a Fed-BioMed DataManager using custom <code>CelebaDataset</code> class.</p> <pre>def training_data(self,  batch_size = 48):\n        # The training_data creates the dataset and returns DataManager to be used for training in the general class Torchnn of Fed-BioMed\n        dataset = self.CelebaDataset(self.dataset_path + \"/target.csv\", self.dataset_path + \"/data/\")\n        loader_arguments = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset, **loader_arguments)\n</pre>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#training_step","title":"<code>training_step()</code>\u00b6","text":"<p>The last method that needs to be defined is the <code>training_step</code>. This method is responsible for executing the forward method and calculating the loss value for the backward process of the network. To access the <code>forward</code> method of the <code>torch.nn.Module</code> that is defined in the <code>init_model</code>, the getter method <code>model()</code> of training plan class should be used.</p> <pre>def training_step(self, data, target): \n    output = self.model().forward(data)\n    loss   = torch.nn.functional.nll_loss(output, target)\n    return loss\n</pre> <p>You are now ready to create your training plan class. All you need to do is to locate every method that has been explained in the previous sections in your traning plan class. In the next steps we will;</p> <ol> <li>download the CelebA dataset and deploy it on the nodes</li> <li>define our complete training</li> <li>create an experiment and run it</li> <li>evaluate our model using a testing dataset</li> </ol>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#2configuring-nodes","title":"2.Configuring Nodes\u00b6","text":"<p>We will be working with CelebA (CelebFaces) dataset. Therefore, please visit here and download the files <code>img/img_align_celeba.zip</code> and <code>Anno/list_attr_celeba.txt</code>. After the download operation is completed;</p> <ul> <li>Please go to <code>./notebooks/data/Celeba</code> in Fed-BioMed project.</li> <li>Create <code>Celeba_raw/raw</code> directory and copy the <code>list_attr_celeba.txt</code> file.</li> <li>Extract the zip file <code>img_align_celeba.zip</code></li> </ul> <p>Your folder should be same as the tree below;</p> <pre><code>Celeba\n    README.md\n    create_node_data.py    \n    .gitignore\n \n    Celeba_raw\n        raw\n            list_attr_celeba.txt\n            img_align_celeba.zip\n            img_align_celeba\n              lots of images \n</code></pre> <p>The dataset has to be processed and split to create three distinct datasets for Node 1, Node 2, and Node 3. You can do it easily by running the following script in your notebook. If you are working in a different directory than the <code>fedbiomed/notebooks</code>, please make sure that you define the correct paths in the following example.</p> <p>Running the following scripts might take some time, please be patient.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#21-configuration-steps","title":"2.1. Configuration Steps\u00b6","text":"<p>It is necessary to previously configure at least a node: 2. Check that your data has been added by executing <code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config (ini file) list</code></p> <ol> <li><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config (ini file) add</code></li> </ol> <ul> <li>Select option <code>4</code> (images) to add an image dataset to the node</li> <li>Add a name and the tag for the dataset (tag should contain '#celeba' as it is the tag used for this training) and finally add the description</li> <li>Pick a data folder from the 3 generated datasets inside <code>data/Celeba/celeba_preprocessed</code> (eg: <code>data_node_1</code>)</li> <li>Data must have been added (if you get a warning saying that data must be unique is because it's been already added)</li> </ul> <ol> <li>Run the node using <code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config &lt;ini file&gt; start</code>. Wait until you get <code>Starting task manager</code>. it means you are online.</li> </ol> <p>After the steps above are completed, you will be ready to train your classification model on two different nodes.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#3-defining-custom-pytorch-model-and-training-plan","title":"3. Defining Custom PyTorch Model and Training Plan\u00b6","text":""},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#4-training-federated-model","title":"4. Training Federated Model\u00b6","text":"<p>To provide training orchestration over two nodes we need to define an experiment which:</p> <ul> <li>searches nodes serving data for the <code>tags</code>,</li> <li>defines the local training on nodes with the training plan saved in <code>training_plan_path</code>, and federates all local updates at each round with <code>aggregator</code></li> <li>runs training for <code>round_limit</code>.</li> </ul> <p>You can visit user guide to know much more about experiment.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#loading-training-parameters","title":"Loading Training Parameters\u00b6","text":"<p>After all the rounds have been completed, you retrieve the aggregated parameters from the last round and load them.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#5-testing-federated-model","title":"5. Testing Federated Model\u00b6","text":"<p>We will define a testing routine to extract the accuracy metrics on the testing dataset. We will use the dataset that has been extracted into <code>data_node_3</code>.</p>"},{"location":"tutorials/pytorch/02_Create_Your_Custom_Training_Plan/#conclusions","title":"Conclusions\u00b6","text":"<p>In this tutorial, running a custom model on Fed-BioMed (by wrapping it in a custom training plan) for the PyTorch framework has been explained. Because the examples are designed for the development environment, we have been running nodes in the same host machine. In production, the nodes that you need to use to train your model will serve in remote servers. Since Fed-BioMed is still in the development phase, in future there might be updates in the function and the methods of these tutorials.</p>"},{"location":"tutorials/pytorch/03_PyTorch_MNIST_local_vs_Federated/","title":"MNIST classification with PyTorch, comparing federated model vs model trained locally","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\n\n# Here we define the training plan to be used.\n# You can use any class name (here 'MyTrainingPlan')\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    # Defines and return model\n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n\n    # Defines and return optimizer\n    def init_optimizer(self, optimizer_args):\n        return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n\n    # Declares and return dependencies\n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\"]\n        return deps\n\n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            self.fc1 = nn.Linear(9216, 128)\n            self.fc2 = nn.Linear(128, 10)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.dropout2(x)\n            x = self.fc2(x)\n\n\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_data(self, batch_size = 48):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        loader_arguments = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset=dataset1, **loader_arguments)\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn import torch.nn.functional as F from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms   # Here we define the training plan to be used. # You can use any class name (here 'MyTrainingPlan') class MyTrainingPlan(TorchTrainingPlan):      # Defines and return model     def init_model(self, model_args):         return self.Net(model_args = model_args)      # Defines and return optimizer     def init_optimizer(self, optimizer_args):         return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])      # Declares and return dependencies     def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\"]         return deps      class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.conv1 = nn.Conv2d(1, 32, 3, 1)             self.conv2 = nn.Conv2d(32, 64, 3, 1)             self.dropout1 = nn.Dropout(0.25)             self.dropout2 = nn.Dropout(0.5)             self.fc1 = nn.Linear(9216, 128)             self.fc2 = nn.Linear(128, 10)          def forward(self, x):             x = self.conv1(x)             x = F.relu(x)             x = self.conv2(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = self.dropout1(x)             x = torch.flatten(x, 1)             x = self.fc1(x)             x = F.relu(x)             x = self.dropout2(x)             x = self.fc2(x)               output = F.log_softmax(x, dim=1)             return output      def training_data(self, batch_size = 48):         # Custom torch Dataloader for MNIST data         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         loader_arguments = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset=dataset1, **loader_arguments)      def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss  In\u00a0[\u00a0]: Copied! <pre>training_args = {\n    'batch_size': 48,\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 1,\n    'dry_run': False,\n    'batch_maxnum': 200 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n\ntags =  ['#MNIST', '#dataset']\nrounds = 1\n</pre> training_args = {     'batch_size': 48,     'optimizer_args': {         'lr': 1e-3     },     'epochs': 1,     'dry_run': False,     'batch_maxnum': 200 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples }  tags =  ['#MNIST', '#dataset'] rounds = 1 In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\nremote_experiment = Experiment(tags=tags,\n                               training_plan_class=MyTrainingPlan,\n                               training_args=training_args,\n                               round_limit=rounds,\n                               aggregator=FedAverage(),\n                               node_selection_strategy=None)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  remote_experiment = Experiment(tags=tags,                                training_plan_class=MyTrainingPlan,                                training_args=training_args,                                round_limit=rounds,                                aggregator=FedAverage(),                                node_selection_strategy=None) In\u00a0[\u00a0]: Copied! <pre>remote_experiment.run()\n</pre> remote_experiment.run() In\u00a0[\u00a0]: Copied! <pre>import os\nfrom torchvision import datasets, transforms\nfrom fedbiomed.researcher.environ import environ\n\nlocal_mnist = os.path.join(environ['TMP_DIR'], 'local_mnist')\nprint(f'Using directory {local_mnist} for MNIST local copy')\n\ntransform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\ndatasets.MNIST(root = local_mnist, download = True, train = True, transform = transform)\n</pre> import os from torchvision import datasets, transforms from fedbiomed.researcher.environ import environ  local_mnist = os.path.join(environ['TMP_DIR'], 'local_mnist') print(f'Using directory {local_mnist} for MNIST local copy')  transform = transforms.Compose([             transforms.ToTensor(),             transforms.Normalize((0.1307,), (0.3081,))         ])  datasets.MNIST(root = local_mnist, download = True, train = True, transform = transform) <p>An object <code>localJob</code> has to be created: it mimics the functionalities of the class <code>Job</code> to run the model on the input local dataset</p> In\u00a0[\u00a0]: Copied! <pre># The class local job mimics the class job used in the experiment\nfrom fedbiomed.researcher.job import localJob\nfrom fedbiomed.researcher.environ import environ\n\n# local train on same amount of data as federated with 1 node\ntraining_args['epochs'] *= rounds\n\nlocal_job = localJob(dataset_path = local_mnist,\n                     training_plan_class=MyTrainingPlan,\n                     training_args=training_args)\n</pre> # The class local job mimics the class job used in the experiment from fedbiomed.researcher.job import localJob from fedbiomed.researcher.environ import environ  # local train on same amount of data as federated with 1 node training_args['epochs'] *= rounds  local_job = localJob(dataset_path = local_mnist,                      training_plan_class=MyTrainingPlan,                      training_args=training_args)  <p>Run the localJob</p> In\u00a0[\u00a0]: Copied! <pre>local_job.start_training()\n</pre> local_job.start_training() <p>Retrieve the local models parameters</p> In\u00a0[\u00a0]: Copied! <pre>local_model = local_job.model\n</pre> local_model = local_job.model In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport pandas as pd\n\ndef testing_accuracy(model, data_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    device = 'cpu'\n\n    correct = 0\n    y_pred = []\n    y_actu = []\n    \n    with torch.no_grad():\n        for data, target in data_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            y_pred.extend(torch.flatten(pred).tolist()) \n            y_actu.extend(target.tolist())\n\n        y_pred = pd.Series(y_pred, name='Actual')\n        y_actu = pd.Series(y_actu, name='Predicted')\n        cm = pd.crosstab(y_actu, y_pred)\n        #correct = sum([cm.iloc[i,i] for i in range(len(cm))])\n\n    test_loss /= len(data_loader.dataset)\n    accuracy = 100* correct/len(data_loader.dataset)\n\n    return(test_loss, accuracy, cm)\n</pre> import torch import torch.nn.functional as F import pandas as pd  def testing_accuracy(model, data_loader):     model.eval()     test_loss = 0     correct = 0     device = 'cpu'      correct = 0     y_pred = []     y_actu = []          with torch.no_grad():         for data, target in data_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()             y_pred.extend(torch.flatten(pred).tolist())              y_actu.extend(target.tolist())          y_pred = pd.Series(y_pred, name='Actual')         y_actu = pd.Series(y_actu, name='Predicted')         cm = pd.crosstab(y_actu, y_pred)         #correct = sum([cm.iloc[i,i] for i in range(len(cm))])      test_loss /= len(data_loader.dataset)     accuracy = 100* correct/len(data_loader.dataset)      return(test_loss, accuracy, cm) In\u00a0[\u00a0]: Copied! <pre>from torchvision import datasets, transforms\n\ntest_set = datasets.MNIST(root = os.path.join(environ['TMP_DIR'], 'local_mnist.tmp'),\n                          download = True,\n                          train = False,\n                          transform = transform)\n\n\ntest_loader = torch.utils.data.DataLoader(test_set,\n                                          batch_size=64,\n                                          shuffle=False)\n</pre> from torchvision import datasets, transforms  test_set = datasets.MNIST(root = os.path.join(environ['TMP_DIR'], 'local_mnist.tmp'),                           download = True,                           train = False,                           transform = transform)   test_loader = torch.utils.data.DataLoader(test_set,                                           batch_size=64,                                           shuffle=False)   <p>Load remote model</p> In\u00a0[\u00a0]: Copied! <pre>remote_model = remote_experiment.training_plan().model()\nremote_model.load_state_dict(remote_experiment.aggregated_params()[rounds - 1]['params'])\n</pre> remote_model = remote_experiment.training_plan().model() remote_model.load_state_dict(remote_experiment.aggregated_params()[rounds - 1]['params']) <p>Compute errors for both remote (federated) and local model</p> In\u00a0[\u00a0]: Copied! <pre># remote accuracy and error computation\nremote_loss, remote_acc, remote_conf_matrix = testing_accuracy(remote_model, test_loader)\n\n\n# local accuracy and error computation\nlocal_loss, local_acc, local_conf_matrix = testing_accuracy(local_model, test_loader)\n</pre> # remote accuracy and error computation remote_loss, remote_acc, remote_conf_matrix = testing_accuracy(remote_model, test_loader)   # local accuracy and error computation local_loss, local_acc, local_conf_matrix = testing_accuracy(local_model, test_loader) In\u00a0[\u00a0]: Copied! <pre>print('\\nAccuracy local training: {:.4f}, \\nAccuracy federated training:  {:.4f}\\nDifference: {:.4f}'.format(\n             local_acc, remote_acc, abs(local_acc - remote_acc)))\n\nprint('\\nError local training: {:.4f}, \\nError federated training:  {:.4f}\\nDifference: {:.4f}'.format(\n             local_loss, remote_loss, abs(local_loss - remote_loss)))\n</pre> print('\\nAccuracy local training: {:.4f}, \\nAccuracy federated training:  {:.4f}\\nDifference: {:.4f}'.format(              local_acc, remote_acc, abs(local_acc - remote_acc)))  print('\\nError local training: {:.4f}, \\nError federated training:  {:.4f}\\nDifference: {:.4f}'.format(              local_loss, remote_loss, abs(local_loss - remote_loss))) In\u00a0[\u00a0]: Copied! <pre>!pip install matplotlib\n</pre> !pip install matplotlib In\u00a0[\u00a0]: Copied! <pre>def plot_confusion_matrix(fig, ax, conf_matrix, title, xlabel, ylabel, n_image=0):\n    \n    im = ax[n_image].imshow(conf_matrix)\n\n    ax[n_image].set_xticks(np.arange(10))\n    ax[n_image].set_yticks(np.arange(10))\n\n    for i in range(conf_matrix.shape[0]):\n        for j in range(conf_matrix.shape[1]):\n            text = ax[n_image].text(j, i, conf_matrix[i, j],\n                           ha=\"center\", va=\"center\", color=\"w\")\n\n    ax[n_image].set_xlabel(xlabel)\n    ax[n_image].set_ylabel(ylabel)\n    ax[n_image].set_title(title)\n</pre> def plot_confusion_matrix(fig, ax, conf_matrix, title, xlabel, ylabel, n_image=0):          im = ax[n_image].imshow(conf_matrix)      ax[n_image].set_xticks(np.arange(10))     ax[n_image].set_yticks(np.arange(10))      for i in range(conf_matrix.shape[0]):         for j in range(conf_matrix.shape[1]):             text = ax[n_image].text(j, i, conf_matrix[i, j],                            ha=\"center\", va=\"center\", color=\"w\")      ax[n_image].set_xlabel(xlabel)     ax[n_image].set_ylabel(ylabel)     ax[n_image].set_title(title) In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2,figsize=(10,5)) \nplot_confusion_matrix(fig, axs, remote_conf_matrix.to_numpy(),\n                     'Confusion Matrix for remote model',\n                     'Actual targets',\n                      'Predicted targets', n_image=0)\n\n\nplot_confusion_matrix(fig, axs, local_conf_matrix.to_numpy(),\n                     'Confusion Matrix for local model',\n                     'Actual targets',\n                      'Predicted targets', n_image=1)\n</pre> import numpy as np import matplotlib.pyplot as plt    fig, axs = plt.subplots(nrows=1, ncols=2,figsize=(10,5))  plot_confusion_matrix(fig, axs, remote_conf_matrix.to_numpy(),                      'Confusion Matrix for remote model',                      'Actual targets',                       'Predicted targets', n_image=0)   plot_confusion_matrix(fig, axs, local_conf_matrix.to_numpy(),                      'Confusion Matrix for local model',                      'Actual targets',                       'Predicted targets', n_image=1)"},{"location":"tutorials/pytorch/03_PyTorch_MNIST_local_vs_Federated/#mnist-classification-with-pytorch-comparing-federated-model-vs-model-trained-locally","title":"MNIST classification with PyTorch, comparing federated model vs model trained locally\u00b6","text":"<p>Overview of the tutorial:</p> <p>In this tutorial, we are going to compare Federated models created through Fed-BioMed framework and a model trained locally (through <code>LocalJob</code> function provided by Fed-BioMed). To this end, we will re-use the model trained in the first PyTorch tutorial: MNIST basic Example and compare it to a model trained locally. Thus, it is recommended to run this first tutorial before this one.</p> <p>At the end of this tutorial, you will learn:</p> <ul> <li>how to train a model in Pytorch designed for Fed-BioMed locally</li> <li>how to evaluate both models</li> </ul> <p>HINT : to reload the notebook,  please click on the following button:</p> <p><code>Kernel</code> -&gt; <code>Restart and clear Output</code></p> <p></p>"},{"location":"tutorials/pytorch/03_PyTorch_MNIST_local_vs_Federated/#0-clean-your-environments","title":"0. Clean your environments\u00b6","text":"<p>Before executing notebook and starting nodes, it is safer to remove all configuration scripts automatically generated by Fed-BioMed. To do so, enter the following in a terminal:</p> <pre>source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment clean\n</pre> <p>Note: <code>${FEDBIOMED_DIR}</code> is a path relative to based directory of the cloned Fed-BioMed repository. You can set it by running command <code>export FEDBIOMED_DIR=/path/to/fedbiomed</code>. This is not required for Fed-BioMed to work but enables you to run the tutorials more easily.</p>"},{"location":"tutorials/pytorch/03_PyTorch_MNIST_local_vs_Federated/#1-configuring-nodes","title":"1. Configuring Nodes\u00b6","text":"<p>In this tutorial, you will learn how to train your model with a single Fed-BioMed node. Thus, we need to configure a node and add MNIST dataset to it. Node configuration steps require <code>fedbiomed-node</code> conda environment. Please make sure that you have the necessary conda environment: this is explained in the installation tutorial. You can check your environment by running the following command.</p> <pre><code>$ conda env list\n</code></pre> <p>If you have all Fed-BioMed environments you are ready to go for the node configuration steps.</p> <p>Please open a terminal, <code>cd</code> to the base directory of the cloned fedbiomed project and follow the steps below.</p> <ul> <li>Configuration Steps:<ul> <li>Run <code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node add</code> in the terminal</li> <li>It will ask you to select the data type that you want to add. The second option (which is the default) has been configured to add the MNIST dataset. Please type <code>2</code> and continue.</li> <li>Please use default tags which are <code>#MNIST</code> and <code>#dataset</code>.</li> <li>For the next step, please select the directory that you want to download the MNIST dataset.</li> <li>After the download is completed you will see the details of the MNIST dataset on the screen.</li> </ul> </li> </ul> <p>Please run the command below in the same terminal to make sure the MNIST dataset is successfully added to the node.</p>"},{"location":"tutorials/pytorch/03_PyTorch_MNIST_local_vs_Federated/#2-running-tutorial-basic-pytorch-on-mnist-dataset","title":"2. Running Tutorial: Basic PyTorch on MNIST dataset\u00b6","text":"<p>In this notebook tutorial, we are going to re-use the Convolution neural network model defined in this first tutorial. Hence, this notebook will be considered to be the continuation of the first tutorial. For more details; please refer to the forementioned tutorial material.</p>"},{"location":"tutorials/pytorch/03_PyTorch_MNIST_local_vs_Federated/#3-defining-a-fed-biomed-training-plan-and-model-on-mnist-dataset","title":"3. Defining a Fed-BioMed Training Plan and Model on MNIST dataset\u00b6","text":""},{"location":"tutorials/pytorch/03_PyTorch_MNIST_local_vs_Federated/#4-training-the-model-in-a-federated-setting","title":"4. Training the Model in a Federated setting\u00b6","text":"<p>We will reproduce the same steps as in Tutorial: Basic PyTorch on MNIST dataset. Remote model will be trained on a single Node.</p>"},{"location":"tutorials/pytorch/03_PyTorch_MNIST_local_vs_Federated/#5-training-fed-biomed-model-locally","title":"5. Training Fed-BioMed model locally\u00b6","text":"<p>In this section, we are going to re-use the defined model and train it locally using <code>localJob</code> function provide by Fed-BioMed. This function is only used for comparing model locally; on researcher side.</p> <p>To use <code>localJob</code> function could prove useful and wise for testing a federated model on your own system, and checking if it is working correctly before deploying it on nodes.</p> <p>First you need to create a folder containing your dataset on your system (ie on <code>environ['TMP_DIR']/local_mnist.tmp</code> folder).</p>"},{"location":"tutorials/pytorch/03_PyTorch_MNIST_local_vs_Federated/#6-comparison-between-federated-model-and-model-trained-locally","title":"6. Comparison between Federated model and model trained locally\u00b6","text":"<p>Let's try to compare our local model against the Federated model, on the MNIST testing dataset</p>"},{"location":"tutorials/pytorch/03_PyTorch_MNIST_local_vs_Federated/#plotting-confusion-matrix-of-both-remote-and-local-models","title":"Plotting Confusion Matrix of both remote and local Models\u00b6","text":""},{"location":"tutorials/pytorch/03_PyTorch_MNIST_local_vs_Federated/#congrats","title":"Congrats!\u00b6","text":"<p>Now you know how to train a Fed-BioMed model designed with Pytorch framework locally.</p> <p>Check out other tutorials and documentation to learn more about Fed-BioMed Federated Learning Framework</p>"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/","title":"PyTorch Used Cars Dataset Example","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport os\n\naudi = pd.read_csv(\"data/UsedCars/raw/audi.csv\")\nbmw = pd.read_csv(\"data/UsedCars/raw/bmw.csv\")\nford = pd.read_csv(\"data/UsedCars/raw/ford.csv\")\n</pre> import pandas as pd import os  audi = pd.read_csv(\"data/UsedCars/raw/audi.csv\") bmw = pd.read_csv(\"data/UsedCars/raw/bmw.csv\") ford = pd.read_csv(\"data/UsedCars/raw/ford.csv\") <p>Drop columns for car <code>model</code> &amp; <code>fuelType</code> as labels are not consistent across files.</p> In\u00a0[\u00a0]: Copied! <pre>audi.drop(columns = ['model','fuelType'], inplace = True)\nbmw.drop(columns = ['model','fuelType'], inplace = True)\nford.drop(columns = ['model','fuelType'], inplace = True)\n</pre> audi.drop(columns = ['model','fuelType'], inplace = True) bmw.drop(columns = ['model','fuelType'], inplace = True) ford.drop(columns = ['model','fuelType'], inplace = True) <p>Label encoding for <code>transmission</code> column</p> In\u00a0[\u00a0]: Copied! <pre>audi['transmission'] = audi['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\nbmw['transmission'] = bmw['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\nford['transmission'] = ford['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n</pre> audi['transmission'] = audi['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3}) bmw['transmission'] = bmw['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3}) ford['transmission'] = ford['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3}) <p>Now, we can save our new CSV files into <code>data/UsedCars</code> directory</p> In\u00a0[\u00a0]: Copied! <pre>audi.to_csv('data/UsedCars/audi_transformed.csv',header = True,index= False)\nbmw.to_csv('data/UsedCars/bmw_transformed.csv',header = True,index= False)\nford.to_csv('data/UsedCars/ford_transformed.csv',header = True,index= False)\n</pre> audi.to_csv('data/UsedCars/audi_transformed.csv',header = True,index= False) bmw.to_csv('data/UsedCars/bmw_transformed.csv',header = True,index= False) ford.to_csv('data/UsedCars/ford_transformed.csv',header = True,index= False) <p>Before creating an experiment, we need to define training plan class.</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\n\n\n# Here we define the training plan to be used for the experiment.\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    # Model\n    def init_model(self):\n        model_args = self.model_args()\n        model = self.Net(model_args)\n        return model\n\n    # Dependencies\n    def init_dependencies(self):\n        deps = [\"from torch.utils.data import Dataset\",\n                \"import pandas as pd\"]\n        return deps\n\n    # network\n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.in_features = model_args['in_features']\n            self.out_features = model_args['out_features']\n            self.fc1 = nn.Linear(self.in_features, 5)\n            self.fc2 = nn.Linear(5, self.out_features)\n\n        def forward(self, x):\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.fc2(x)\n            return x\n\n    def training_step(self, data, target):\n        output = self.model().forward(data).float()\n        criterion = torch.nn.MSELoss()\n        loss   = torch.sqrt(criterion(output, target.unsqueeze(1)))\n        return loss\n\n    class csv_Dataset(Dataset):\n    # Here we define a custom Dataset class inherited from the general torch Dataset class\n    # This class takes as argument a .csv file path and creates a torch Dataset\n        def __init__(self, dataset_path, x_dim):\n            self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)\n            x_train = self.input_file.loc[:,('year','transmission','mileage','tax','mpg','engineSize')].values\n            y_train = self.input_file.loc[:,'price'].values\n            self.X_train = torch.from_numpy(x_train).float()\n            self.Y_train = torch.from_numpy(y_train).float()\n\n        def __len__(self):\n            return len(self.Y_train)\n\n        def __getitem__(self, idx):\n\n            return (self.X_train[idx], self.Y_train[idx])\n\n    def training_data(self,  batch_size = 48):\n    # The training_data creates the Dataloader to be used for training in the general class TorchTrainingPlan of fedbiomed\n        dataset = self.csv_Dataset(self.dataset_path, self.model_args()[\"in_features\"])\n        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset=dataset , **train_kwargs)\n</pre> import pandas as pd import torch import torch.nn as nn import torch.nn.functional as F  from torch.utils.data import Dataset from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager   # Here we define the training plan to be used for the experiment. class MyTrainingPlan(TorchTrainingPlan):      # Model     def init_model(self):         model_args = self.model_args()         model = self.Net(model_args)         return model      # Dependencies     def init_dependencies(self):         deps = [\"from torch.utils.data import Dataset\",                 \"import pandas as pd\"]         return deps      # network     class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.in_features = model_args['in_features']             self.out_features = model_args['out_features']             self.fc1 = nn.Linear(self.in_features, 5)             self.fc2 = nn.Linear(5, self.out_features)          def forward(self, x):             x = self.fc1(x)             x = F.relu(x)             x = self.fc2(x)             return x      def training_step(self, data, target):         output = self.model().forward(data).float()         criterion = torch.nn.MSELoss()         loss   = torch.sqrt(criterion(output, target.unsqueeze(1)))         return loss      class csv_Dataset(Dataset):     # Here we define a custom Dataset class inherited from the general torch Dataset class     # This class takes as argument a .csv file path and creates a torch Dataset         def __init__(self, dataset_path, x_dim):             self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)             x_train = self.input_file.loc[:,('year','transmission','mileage','tax','mpg','engineSize')].values             y_train = self.input_file.loc[:,'price'].values             self.X_train = torch.from_numpy(x_train).float()             self.Y_train = torch.from_numpy(y_train).float()          def __len__(self):             return len(self.Y_train)          def __getitem__(self, idx):              return (self.X_train[idx], self.Y_train[idx])      def training_data(self,  batch_size = 48):     # The training_data creates the Dataloader to be used for training in the general class TorchTrainingPlan of fedbiomed         dataset = self.csv_Dataset(self.dataset_path, self.model_args()[\"in_features\"])         train_kwargs = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset=dataset , **train_kwargs) <p>After running the notebook cell above, it will save the cell content into <code>/tmp/class_export_csv.py</code>. This will be the model that will be running in the nodes.</p> In\u00a0[\u00a0]: Copied! <pre># model parameters\nmodel_args = {\n    'in_features': 6,\n    'out_features': 1\n}\n\n# training parameters\ntraining_args = {\n    'batch_size': 40,\n    'optimizer_args': {\n          'lr': 1e-3\n    },\n    'epochs': 2,\n#    'batch_maxnum': 2,  # can be used to debugging to limit the number of batches per epoch\n#    'log_interval': 1,  # output a logging message every log_interval batches\n}\n\ntags =  ['#UsedCars']\nrounds = 5\n</pre> # model parameters model_args = {     'in_features': 6,     'out_features': 1 }  # training parameters training_args = {     'batch_size': 40,     'optimizer_args': {           'lr': 1e-3     },     'epochs': 2, #    'batch_maxnum': 2,  # can be used to debugging to limit the number of batches per epoch #    'log_interval': 1,  # output a logging message every log_interval batches }  tags =  ['#UsedCars'] rounds = 5 <p>The other arguments that should be passed to the experiment are the training plan file path and the name of the training plan class which is <code>MyTrainingPlan</code>. The experiment will be responsible for uploading the training plan to the file repository and informing nodes about how they can access and execute it.</p> <p>You should also indicate which method should be chosen to aggregate model parameters after every round. The basic federation scheme is federated averaging, implemented in Fed-BioMed in the class  <code>FedAverage</code>. You can also visit aggregation documentation to have more information about aggregation process.</p> <p>Since we are going to use every node that has <code>UsedCars</code> datasets, the <code>node_selection_strategy</code> should be <code>None</code> which means that every node will be part of the federated training.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\n\nexp = Experiment(tags=tags,\n                 training_plan_class=MyTrainingPlan,\n                 model_args=model_args,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage   exp = Experiment(tags=tags,                  training_plan_class=MyTrainingPlan,                  model_args=model_args,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) <p>The experiment also receives loss values during training on each node. In Fed-BioMed, it is possible to use a tensorboard to display loss values during training. Please refer to Fed-BioMed's tensorboard documentation to enable tensorboard.</p> <p>Let's start the experiment. By default, this function doesn't stop until all the <code>round_limit</code> rounds are done for all the nodes.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the training rounds : \", exp.training_replies().keys())\n</pre> print(\"\\nList the training rounds : \", exp.training_replies().keys()) <p>Now, let's see how training details can be accessed via <code>training_replies()</code>. The following parameters will be inspected;</p> <ul> <li><code>rtime_training</code> : Real-time (clock time) spent in the training function on the node</li> <li><code>ptime_training</code>: Process time (user and system CPU) spent in the training function on the node</li> <li><code>rtime_total</code>   : Real-time (clock time) spent in the researcher between sending training requests and handling the responses</li> </ul> <p>Note: The following code accesses the training replies of the last round of the experiment.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the nodes for the last training round and their timings : \")\nround_data = exp.training_replies()[rounds - 1].data()\nfor c in range(len(round_data)):\n    print(\"\\t- {id} :\\\n\\n\\t\\trtime_training={rtraining:.2f} seconds\\\n\\n\\t\\tptime_training={ptraining:.2f} seconds\\\n\\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n        rtraining = round_data[c]['timing']['rtime_training'],\n        ptraining = round_data[c]['timing']['ptime_training'],\n        rtotal = round_data[c]['timing']['rtime_total']))\nprint('\\n')\n    \nexp.training_replies()[rounds - 1].dataframe()\n</pre> print(\"\\nList the nodes for the last training round and their timings : \") round_data = exp.training_replies()[rounds - 1].data() for c in range(len(round_data)):     print(\"\\t- {id} :\\     \\n\\t\\trtime_training={rtraining:.2f} seconds\\     \\n\\t\\tptime_training={ptraining:.2f} seconds\\     \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],         rtraining = round_data[c]['timing']['rtime_training'],         ptraining = round_data[c]['timing']['ptime_training'],         rtotal = round_data[c]['timing']['rtime_total'])) print('\\n')      exp.training_replies()[rounds - 1].dataframe() In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\nprint(\"\\nAccess the federated params for the last training round :\")\nprint(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path'])\nprint(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n</pre> print(\"\\nList the training rounds : \", exp.aggregated_params().keys()) print(\"\\nAccess the federated params for the last training round :\") print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path']) print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())  In\u00a0[\u00a0]: Copied! <pre>exp.training_plan().model()\n</pre> exp.training_plan().model() In\u00a0[\u00a0]: Copied! <pre>fed_model = exp.training_plan().model()\nfed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n</pre> fed_model = exp.training_plan().model() fed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params']) In\u00a0[\u00a0]: Copied! <pre>fed_model\n</pre>  fed_model In\u00a0[\u00a0]: Copied! <pre>test_dataset_path = \"data/UsedCars/ford_transformed.csv\"\n</pre> test_dataset_path = \"data/UsedCars/ford_transformed.csv\" In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\n\ndef cal_rmse(actual, prediction):\n    return ((actual- prediction)**2).mean()**0.5\n\ndef testing_rmse(model, data_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    device = 'cpu'\n    preds = []\n    with torch.no_grad():\n        for data, target in data_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            preds.append(output.numpy().flatten())\n    rmse = cal_rmse(data_loader.dataset.Y_train.numpy(),np.hstack(preds))\n    return rmse\n</pre> import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset, DataLoader import pandas as pd  def cal_rmse(actual, prediction):     return ((actual- prediction)**2).mean()**0.5  def testing_rmse(model, data_loader):     model.eval()     test_loss = 0     correct = 0     device = 'cpu'     preds = []     with torch.no_grad():         for data, target in data_loader:             data, target = data.to(device), target.to(device)             output = model(data)             preds.append(output.numpy().flatten())     rmse = cal_rmse(data_loader.dataset.Y_train.numpy(),np.hstack(preds))     return rmse <p>We also need to create a Dataset class for PyTorch data loader.</p> In\u00a0[\u00a0]: Copied! <pre>class csv_Dataset(Dataset):\n        def __init__(self, dataset_path):\n            self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)\n            x_train = self.input_file.loc[:,('year','transmission','mileage','tax','mpg','engineSize')].values\n            y_train = self.input_file.loc[:,'price'].values\n            self.X_train = torch.from_numpy(x_train).float()\n            self.Y_train = torch.from_numpy(y_train).float()\n\n        def __len__(self):            \n            return len(self.Y_train)\n\n        def __getitem__(self, idx):\n\n            return (self.X_train[idx], self.Y_train[idx])\n</pre> class csv_Dataset(Dataset):         def __init__(self, dataset_path):             self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)             x_train = self.input_file.loc[:,('year','transmission','mileage','tax','mpg','engineSize')].values             y_train = self.input_file.loc[:,'price'].values             self.X_train = torch.from_numpy(x_train).float()             self.Y_train = torch.from_numpy(y_train).float()          def __len__(self):                         return len(self.Y_train)          def __getitem__(self, idx):              return (self.X_train[idx], self.Y_train[idx]) In\u00a0[\u00a0]: Copied! <pre>dataset = csv_Dataset(test_dataset_path)\ntrain_kwargs = {'batch_size': 64, 'shuffle': True}\ndata_loader = DataLoader(dataset, **train_kwargs)\n</pre> dataset = csv_Dataset(test_dataset_path) train_kwargs = {'batch_size': 64, 'shuffle': True} data_loader = DataLoader(dataset, **train_kwargs) In\u00a0[\u00a0]: Copied! <pre>rmse = testing_rmse(fed_model, data_loader)\nprint(rmse)\n</pre> rmse = testing_rmse(fed_model, data_loader) print(rmse) In\u00a0[\u00a0]: Copied! <pre>!pip install matplotlib\n</pre> !pip install matplotlib In\u00a0[\u00a0]: Copied! <pre>errors = []\n\nfor i in range(rounds):\n    fed_model = exp.training_plan().model()\n    fed_model.load_state_dict(exp.aggregated_params()[i]['params'])\n    loss = testing_rmse(fed_model, data_loader)\n    errors.append(loss)\n</pre> errors = []  for i in range(rounds):     fed_model = exp.training_plan().model()     fed_model.load_state_dict(exp.aggregated_params()[i]['params'])     loss = testing_rmse(fed_model, data_loader)     errors.append(loss) In\u00a0[\u00a0]: Copied! <pre>### Plotting \nimport matplotlib.pyplot as plt\nplt.plot(errors, label = 'Federated Test Loss')\nplt.xlabel('Round')\nplt.ylabel('Loss')\nplt.legend()\n</pre> ### Plotting  import matplotlib.pyplot as plt plt.plot(errors, label = 'Federated Test Loss') plt.xlabel('Round') plt.ylabel('Loss') plt.legend()"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#pytorch-used-cars-dataset-example","title":"PyTorch Used Cars Dataset Example\u00b6","text":""},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#1-introduction","title":"1. Introduction\u00b6","text":"<p>This tutorial focuses on how to train a federated regression model on Non-IID dataset using PyTorch framework. We will be working on the Used Cars dataset to perform federated learning. The sections of this tutorial are presented as follows;</p> <ul> <li>Dataset Preparation</li> <li>Node Configurations</li> <li>Create an Experiment to Train a Model</li> <li>Testing Federated Model</li> </ul>"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#before-you-start","title":"Before you start\u00b6","text":"<p>Before starting this tutorial please make sure that you have a clean Fed-BioMed environment. Additionally, please terminate running nodes if there are any. Please run the following command in the main Fed-BioMed directory to clean your environment. If you need more help to manage your environment please visit the tutorial for setting up an enviroment.</p> <pre>$ source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment clean\n</pre> <p>Note: <code>${FEDBIOMED_DIR}</code> is a path relative to the base directory of the cloned Fed-BioMed repository. You can set it by running the command <code>export FEDBIOMED_DIR=/path/to/fedbiomed</code>. This is not required for Fed-BioMed to work but enables you to run the tutorials more easily.</p>"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#2-dataset-preparation","title":"2. Dataset Preparation\u00b6","text":"<p>In this tutorial, we will be using the Used Cars dataset. The goal of the model will be to predict the price of the car based on given features.</p> <p>You can download the dataset from here. To be able to download this dataset you need to have a Kaggle account. After downloading, you can create folders for the dataset in the Fed-BioMed <code>notebooks/data</code> directory.</p> <pre>cd /path/to/fedbiomed/directory\nmkdir notebooks/data/UsedCars &amp;&amp; mkdir notebooks/data/UsedCars/raw\n</pre> <p>You can extract CSV files in the zip file into <code>notebooks/data/UsedCars/raw</code>. Your file tree should be like the tree below;</p> <pre>\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 UsedCars\n\u2502       \u2514\u2500\u2500 raw\n\u2502           \u251c\u2500\u2500 audi.csv\n\u2502           \u251c\u2500\u2500 bmw.csv\n\u2502           \u251c\u2500\u2500 cclass.csv\n\u2502           \u251c\u2500\u2500 focus.csv\n\u2502           \u251c\u2500\u2500 ford.csv\n\u2502           \u251c\u2500\u2500 hyundi.csv\n\u2502           \u251c\u2500\u2500 merc.csv\n\u2502           \u251c\u2500\u2500 skoda.csv\n\u2502           \u251c\u2500\u2500 toyota.csv\n\u2502           \u251c\u2500\u2500 unclean cclass.csv\n\u2502           \u251c\u2500\u2500 unclean focus.csv\n\u2502           \u251c\u2500\u2500 vauxhall.csv\n\u2502           \u2514\u2500\u2500 vw.csv\n</pre>"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#21-selecting-csv-dataset-for-each-node","title":"2.1 Selecting CSV Dataset for Each Node\u00b6","text":"<p>Each CSV dataset contains features for different car brands. It is a good example for applying federated learning through each dataset if we assume that these datasets will be stored in different locations. We will be working on 3 datasets that are <code>audi.csv</code>, <code>bmw.csv</code>, and <code>ford.csv</code>. We will deploy <code>audi.csv</code> and <code>bmw.csv</code> on different nodes and use <code>ford.csv</code> for final testing at the central researcher using the model trained on two nodes.</p>"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#22-preprocessing","title":"2.2 Preprocessing\u00b6","text":"<p>Before deploying datasets we need to apply some preprocessing to make them ready for the federated training. Since car <code>model</code> and <code>fuelType</code> features are not consistent across the dataset, we can drop them. We also need to apply label encoding for the <code>transmission</code> feature.</p> <p>Note: Dropping and encoding columns can be also done in the <code>training_data</code> method of <code>TrainingPlan</code> but it is always better to deploy clean/prepared datasets in the nodes.</p> <p>Let's starting with loading CSV datasets using <code>pd.read_csv</code> API. Please make sure that you have launched your Jupyter notebook using the command <code>${FEDBIOMED_DIR}/scripts/fedbiomed_run researcher start</code> so you can follow code examples without changing file paths.</p>"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#3-node-configurations","title":"3. Node Configurations\u00b6","text":"<p>We will deploy the <code>audi_transformed.csv</code> and <code>bmw_transformed.csv</code> datasets on different nodes.</p> <ol> <li><p>Configuring First Node</p> <ul> <li>Run <code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-audi.ini add</code></li> <li>Select option 1 to add a csv file (audi_transformed.csv)</li> <li>Choose a name for dataset, For example <code>Used-Cars-Audi</code></li> <li>Choose tag for the dataset. This part is important because we will be sending search request to nodes with this specified tag. Therefore please type <code>#UsedCars</code> and enter.</li> <li>Enter a description for the dataset</li> <li>Select the audi_transformed.csv file in the file selection window</li> </ul> </li> <li><p>Configuring Second Node</p> <ul> <li>Run <code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-bmw.ini add</code></li> <li>Select option 1 to add a csv file (bmw_transformed.csv)</li> <li>Choose a name for dataset, For example <code>Used-Cars-BMW</code></li> <li>Since we entered the tag as <code>#UsedCars</code>, we need to use the same tag for this one too.</li> <li>Enter a description for the dataset</li> <li>Select the bmw_trasnformed.csv file in the file selection window</li> </ul> </li> <li><p>Starting Nodes</p> <p>Before starting the node please make sure that the Fed-BioMed network is up and running. If not, you can start network service by running <code>${FEDBIOMED_DIR}/scripts/fedbiomed_run network</code>. Afterward, please run the following command to start the node that has the <code>audi</code> dataset.</p> <pre>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-audi.ini start\n</pre> <p>Please open a new terminal window to start the node that has the <code>bmw</code> dataset</p> <pre>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-bmw.ini start\n</pre> </li> </ol>"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#4-create-an-experiment-to-train-a-model","title":"4. Create an Experiment to Train a Model\u00b6","text":""},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#41-defining-arguments-for-the-experiment","title":"4.1 Defining Arguments for The Experiment\u00b6","text":"<p>An experiment is a class that orchestrates the training processes that run on different nodes. The experiment has to be initialized with necessary arguments to inform nodes about the training plan. In this case, first, you need to define <code>model_arg</code>, <code>training_args</code>, <code>tags</code>, and <code>round</code>.</p> <p>Please visit experiment documentation to get detailed information about the experiment class.</p>"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#42-what-happens-during-the-initialization-of-an-experiment","title":"4.2 What happens during the initialization of an experiment?\u00b6","text":"<ol> <li>The experiment searches for nodes that have datasets that have been saved with the <code>UsedCars</code> tag.</li> <li>The experiment prepares the <code>job</code> object to manage the training process across the nodes with the given arguments</li> <li>It uploads the training plan file to the provided repository to make it available for the nodes.</li> </ol> <p>Note: It is possible to send search requests to only specified nodes with the <code>nodes</code> argument of the experiment. Please visit listing datasets and selecting nodes documentation for more information.</p>"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#43-extracting-training-results","title":"4.3 Extracting Training Results\u00b6","text":""},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#timing","title":"Timing\u00b6","text":"<p>Training replies for each round are available in <code>exp.training_replies()</code> (index 0 to (<code>rounds</code> - 1) ). You can display the keys of each round by running the following script.</p>"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#federated-parameters","title":"Federated Parameters\u00b6","text":"<p>Federated model parameters for each round are available in <code>exp.aggregated_params()</code> (index 0 to (<code>rounds</code> - 1) ). For example, you can easily view the federated parameters for the last round of the experiment:</p>"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#5-testing-federated-model","title":"5. Testing Federated Model\u00b6","text":"<p>In this section, we will create a test function to obtain RMSE on <code>ford_transformed.csv</code> dataset by using federated model.</p>"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#51-aggregated-parameters","title":"5.1 Aggregated Parameters\u00b6","text":"<p><code>model_instance</code> returns the model that we have created in the previous section. You can load specific aggregated parameters which are obtained in the round. Thereafter, it will make the predictions using those parameters. The last round gives the last aggregated model parameters which represents the final model.</p>"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#52-creating-a-test-function","title":"5.2 Creating A Test Function\u00b6","text":"<p>Let's create a test function that returns <code>rmse</code>.</p>"},{"location":"tutorials/pytorch/04_PyTorch_Used_Cars_Dataset_Example/#53-plotting-rmse-values-of-each-round","title":"5.3 Plotting RMSE Values of Each Round\u00b6","text":""},{"location":"tutorials/pytorch/05-Aggregation_in_Fed-BioMed/","title":"PyTorch aggregation methods in Fed-BioMed","text":"In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.training_plans import TorchTrainingPlan\nfrom flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\nfrom fedbiomed.common.data import FlambyDataset, DataManager\n       \n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args):\n        return Baseline()\n\n    def init_optimizer(self, optimizer_args):\n        return Optimizer(self.model().parameters(), lr=optimizer_args[\"lr\"])\n\n    def init_dependencies(self):\n        return [\"from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\",\n                \"from fedbiomed.common.data import FlambyDataset, DataManager\"]\n\n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        return BaselineLoss().forward(output, target)\n\n    def training_data(self, batch_size=2):\n        dataset = FlambyDataset()\n        loader_arguments = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset, **loader_arguments)\n</pre>  from fedbiomed.common.training_plans import TorchTrainingPlan from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer from fedbiomed.common.data import FlambyDataset, DataManager          class MyTrainingPlan(TorchTrainingPlan):     def init_model(self, model_args):         return Baseline()      def init_optimizer(self, optimizer_args):         return Optimizer(self.model().parameters(), lr=optimizer_args[\"lr\"])      def init_dependencies(self):         return [\"from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\",                 \"from fedbiomed.common.data import FlambyDataset, DataManager\"]      def training_step(self, data, target):         output = self.model().forward(data)         return BaselineLoss().forward(output, target)      def training_data(self, batch_size=2):         dataset = FlambyDataset()         loader_arguments = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset, **loader_arguments)  <p>We define hereafter parameters for <code>Experiment</code> to be used with vanilla <code>FedAverage</code></p> In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'batch_size': 8,\n    'optimizer_args': {\n        \"lr\" : 1e-3\n    },\n    'dry_run': False,\n    'num_updates': 50\n}\n</pre> model_args = {}  training_args = {     'batch_size': 8,     'optimizer_args': {         \"lr\" : 1e-3     },     'dry_run': False,     'num_updates': 50 }  <p>Activate Tensorboard</p> In\u00a0[\u00a0]: Copied! <pre>%load_ext tensorboard\n</pre> %load_ext tensorboard In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.environ import environ\nimport os\nfedavg_tensorboard_dir = os.path.join(environ['ROOT_DIR'], 'fedavg_runs')\nos.makedirs(fedavg_tensorboard_dir, exist_ok=True)\nenviron['TENSORBOARD_RESULTS_DIR'] = fedavg_tensorboard_dir\n</pre> from fedbiomed.researcher.environ import environ import os fedavg_tensorboard_dir = os.path.join(environ['ROOT_DIR'], 'fedavg_runs') os.makedirs(fedavg_tensorboard_dir, exist_ok=True) environ['TENSORBOARD_RESULTS_DIR'] = fedavg_tensorboard_dir In\u00a0[\u00a0]: Copied! <pre>tensorboard --logdir \"$fedavg_tensorboard_dir\"\n</pre> tensorboard --logdir \"$fedavg_tensorboard_dir\" <p>We then import <code>FedAverage</code> <code>Aggregator</code> from Fed-BioMed's <code>Aggregators</code></p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators import FedAverage\nfrom fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n\ntags =  ['flixi']\n\n\nexp_fed_avg = Experiment()\nexp_fed_avg.set_training_plan_class(training_plan_class=MyTrainingPlan)\nexp_fed_avg.set_model_args(model_args=model_args)\nexp_fed_avg.set_training_args(training_args=training_args)\nexp_fed_avg.set_tags(tags = tags)\nexp_fed_avg.set_aggregator(aggregator=FedAverage())\nexp_fed_avg.set_round_limit(rounds)\nexp_fed_avg.set_training_data(training_data=None, from_tags=True)\nexp_fed_avg.set_job()\nexp_fed_avg.set_strategy(node_selection_strategy=DefaultStrategy)\nexp_fed_avg.set_tensorboard(True)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators import FedAverage from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy  tags =  ['flixi']   exp_fed_avg = Experiment() exp_fed_avg.set_training_plan_class(training_plan_class=MyTrainingPlan) exp_fed_avg.set_model_args(model_args=model_args) exp_fed_avg.set_training_args(training_args=training_args) exp_fed_avg.set_tags(tags = tags) exp_fed_avg.set_aggregator(aggregator=FedAverage()) exp_fed_avg.set_round_limit(rounds) exp_fed_avg.set_training_data(training_data=None, from_tags=True) exp_fed_avg.set_job() exp_fed_avg.set_strategy(node_selection_strategy=DefaultStrategy) exp_fed_avg.set_tensorboard(True) In\u00a0[\u00a0]: Copied! <pre>exp_fed_avg.run(increase=True)\n</pre> exp_fed_avg.run(increase=True) In\u00a0[\u00a0]: Copied! <pre># let's create a new folder for storing tensorbaord results for FedProx aggregator\nimport os\nfrom fedbiomed.researcher.environ import environ\nfedprox_tensorboard_dir = os.path.join(environ['ROOT_DIR'], 'fedprox_runs')\nos.makedirs(fedprox_tensorboard_dir, exist_ok=True)\n\nenviron['TENSORBOARD_RESULTS_DIR'] = fedprox_tensorboard_dir\n</pre> # let's create a new folder for storing tensorbaord results for FedProx aggregator import os from fedbiomed.researcher.environ import environ fedprox_tensorboard_dir = os.path.join(environ['ROOT_DIR'], 'fedprox_runs') os.makedirs(fedprox_tensorboard_dir, exist_ok=True)  environ['TENSORBOARD_RESULTS_DIR'] = fedprox_tensorboard_dir In\u00a0[\u00a0]: Copied! <pre>%reload_ext tensorboard\n</pre> %reload_ext tensorboard In\u00a0[\u00a0]: Copied! <pre>tensorboard --logdir \"$fedprox_tensorboard_dir\"\n</pre> tensorboard --logdir \"$fedprox_tensorboard_dir\" In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args_fedprox = {\n    'batch_size': 8,\n    'optimizer_args': {\n        \"lr\" : 1e-3\n    },\n    'dry_run': False,\n    'num_updates': 50, \n    'fedprox_mu': .1  # This parameter indicates that we are going to use FedProx\n}\n</pre> model_args = {}  training_args_fedprox = {     'batch_size': 8,     'optimizer_args': {         \"lr\" : 1e-3     },     'dry_run': False,     'num_updates': 50,      'fedprox_mu': .1  # This parameter indicates that we are going to use FedProx } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators import FedAverage\nfrom fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n\ntags =  ['flixi']\nrounds = 3\n\nexp_fedprox = Experiment()\n\n\nexp_fedprox.set_training_plan_class(training_plan_class=MyTrainingPlan)\nexp_fedprox.set_model_args(model_args=model_args)\nexp_fedprox.set_training_args(training_args=training_args_fedprox)\nexp_fedprox.set_tags(tags = tags)\nexp_fedprox.set_aggregator(aggregator=FedAverage())\nexp_fedprox.set_round_limit(rounds)\nexp_fedprox.set_training_data(training_data=None, from_tags=True)\nexp_fedprox.set_job()\nexp_fedprox.set_strategy(node_selection_strategy=DefaultStrategy)\nexp_fedprox.set_tensorboard(True)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators import FedAverage from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy  tags =  ['flixi'] rounds = 3  exp_fedprox = Experiment()   exp_fedprox.set_training_plan_class(training_plan_class=MyTrainingPlan) exp_fedprox.set_model_args(model_args=model_args) exp_fedprox.set_training_args(training_args=training_args_fedprox) exp_fedprox.set_tags(tags = tags) exp_fedprox.set_aggregator(aggregator=FedAverage()) exp_fedprox.set_round_limit(rounds) exp_fedprox.set_training_data(training_data=None, from_tags=True) exp_fedprox.set_job() exp_fedprox.set_strategy(node_selection_strategy=DefaultStrategy) exp_fedprox.set_tensorboard(True) In\u00a0[\u00a0]: Copied! <pre>exp_fedprox.run(increase=True)\n</pre> exp_fedprox.run(increase=True) In\u00a0[\u00a0]: Copied! <pre># let's create a new folder for storing tensorbaord results for SCAFFOLD aggregator\nscaffold_tensorboard_dir = os.path.join(environ['ROOT_DIR'], 'scaffold_runs')\nos.makedirs(scaffold_tensorboard_dir, exist_ok=True)\n\nenviron['TENSORBOARD_RESULTS_DIR'] = scaffold_tensorboard_dir\n</pre> # let's create a new folder for storing tensorbaord results for SCAFFOLD aggregator scaffold_tensorboard_dir = os.path.join(environ['ROOT_DIR'], 'scaffold_runs') os.makedirs(scaffold_tensorboard_dir, exist_ok=True)  environ['TENSORBOARD_RESULTS_DIR'] = scaffold_tensorboard_dir In\u00a0[\u00a0]: Copied! <pre>%reload_ext tensorboard\n</pre> %reload_ext tensorboard In\u00a0[\u00a0]: Copied! <pre>tensorboard --logdir \"$scaffold_tensorboard_dir\"\n</pre> tensorboard --logdir \"$scaffold_tensorboard_dir\" In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.aggregators import Scaffold\nfrom fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n\nserver_lr = .8\nexp_scaffold = Experiment()\n\nexp_scaffold.set_training_plan_class(training_plan_class=MyTrainingPlan)\nexp_scaffold.set_model_args(model_args=model_args)\nexp_scaffold.set_training_args(training_args=training_args)\nexp_scaffold.set_tags(tags = tags)\nexp_scaffold.set_aggregator(aggregator=FedAverage())\nexp_scaffold.set_round_limit(rounds)\nexp_scaffold.set_training_data(training_data=None, from_tags=True)\nexp_scaffold.set_job()\nexp_scaffold.set_strategy(node_selection_strategy=DefaultStrategy)\nexp_scaffold.set_tensorboard(True)\nexp_scaffold.set_aggregator(Scaffold(server_lr=server_lr))\n</pre> from fedbiomed.researcher.aggregators import Scaffold from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy  server_lr = .8 exp_scaffold = Experiment()  exp_scaffold.set_training_plan_class(training_plan_class=MyTrainingPlan) exp_scaffold.set_model_args(model_args=model_args) exp_scaffold.set_training_args(training_args=training_args) exp_scaffold.set_tags(tags = tags) exp_scaffold.set_aggregator(aggregator=FedAverage()) exp_scaffold.set_round_limit(rounds) exp_scaffold.set_training_data(training_data=None, from_tags=True) exp_scaffold.set_job() exp_scaffold.set_strategy(node_selection_strategy=DefaultStrategy) exp_scaffold.set_tensorboard(True) exp_scaffold.set_aggregator(Scaffold(server_lr=server_lr)) In\u00a0[\u00a0]: Copied! <pre>exp_scaffold.run(increase=True)\n</pre> exp_scaffold.run(increase=True) In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.aggregators import Scaffold, FedAverage\nfrom fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n\nserver_lr = .8\nexp_multi_agg = Experiment()\n\n# selecting how many rounds of each aggregator we will perform\nrounds_scaffold = 3\nrounds_fedavg = 1\n\n\nexp_multi_agg.set_training_plan_class(training_plan_class=MyTrainingPlan)\nexp_multi_agg.set_model_args(model_args=model_args)\nexp_multi_agg.set_training_args(training_args=training_args)\nexp_multi_agg.set_tags(tags = tags)\nexp_multi_agg.set_aggregator(aggregator=FedAverage())\nexp_multi_agg.set_round_limit(rounds_scaffold + rounds_fedavg)\nexp_multi_agg.set_training_data(training_data=None, from_tags=True)\nexp_multi_agg.set_job()\nexp_multi_agg.set_strategy(node_selection_strategy=DefaultStrategy)\n\nexp_multi_agg.set_aggregator(Scaffold(server_lr=server_lr))\nexp_multi_agg.run(rounds=rounds_scaffold)\n</pre> from fedbiomed.researcher.aggregators import Scaffold, FedAverage from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy  server_lr = .8 exp_multi_agg = Experiment()  # selecting how many rounds of each aggregator we will perform rounds_scaffold = 3 rounds_fedavg = 1   exp_multi_agg.set_training_plan_class(training_plan_class=MyTrainingPlan) exp_multi_agg.set_model_args(model_args=model_args) exp_multi_agg.set_training_args(training_args=training_args) exp_multi_agg.set_tags(tags = tags) exp_multi_agg.set_aggregator(aggregator=FedAverage()) exp_multi_agg.set_round_limit(rounds_scaffold + rounds_fedavg) exp_multi_agg.set_training_data(training_data=None, from_tags=True) exp_multi_agg.set_job() exp_multi_agg.set_strategy(node_selection_strategy=DefaultStrategy)  exp_multi_agg.set_aggregator(Scaffold(server_lr=server_lr)) exp_multi_agg.run(rounds=rounds_scaffold)  In\u00a0[\u00a0]: Copied! <pre>exp_multi_agg.set_aggregator(FedAverage())\nexp_multi_agg.run(rounds=rounds_fedavg)\n</pre> exp_multi_agg.set_aggregator(FedAverage()) exp_multi_agg.run(rounds=rounds_fedavg)"},{"location":"tutorials/pytorch/05-Aggregation_in_Fed-BioMed/#pytorch-aggregation-methods-in-fed-biomed","title":"PyTorch aggregation methods in Fed-BioMed\u00b6","text":"<p>Difficulty level: advanced</p>"},{"location":"tutorials/pytorch/05-Aggregation_in_Fed-BioMed/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial focuses on how to deal with heterogeneous dataset by changing its <code>Aggegator</code>. Fed-BioMed provides different methods for Aggregation. Selecting an appropriate Aggregation method can be critical when being confronted to unbalanced /heterogeneous datasets.</p> <p><code>Aggregators</code> provides a way to merge local models sent by <code>Nodes</code> into a global, more generalized model. Please note that designing <code>Nodes</code> sampling <code>Strategies</code> could also help when working on heterogeneous datasets.</p> <p>For more information about <code>Aggregators</code> object in Fed-BioMed, and on how to create your own <code>Aggregator</code>; please see <code>Aggregators</code> in the User Guide</p>"},{"location":"tutorials/pytorch/05-Aggregation_in_Fed-BioMed/#before-you-start","title":"Before you start\u00b6","text":"<p>For this tutorial, we will be using heterogenous Fed-IXI dataset, provided by FLamby. FLamby comes with a few medical datasets that have heterogenous data properties. Please have a look at the notebooks on how to use FLamby in Fed-BioMed tutorials before starting - you will indeed need to set up FLamby before running this tutorial.</p>"},{"location":"tutorials/pytorch/05-Aggregation_in_Fed-BioMed/#1-defining-an-experiment-using-fedaverage-aggregator","title":"1. Defining an <code>Experiment</code> using <code>FedAverage</code> <code>Aggregator</code>\u00b6","text":"<p>First, let's re-use the <code>TorchTrainingPlan</code> that is defined in the FLamby tutorials. FedAveraging has been introduced by McMahan et al. as the first aggregation method in the Federated Learning literature. It does the weighted sum of all <code>Nodes</code> local models parameters in order to obtain a global model:</p> <p>In this tutorial, we will keep the same <code>TrainingPlan</code> (and thus the same model) for all the <code>Experimentations</code>, we will be changing only <code>Aggregators</code></p>"},{"location":"tutorials/pytorch/05-Aggregation_in_Fed-BioMed/#2-defining-an-experiment-using-fedprox-aggregator","title":"2. Defining an <code>Experiment</code> using <code>FedProx</code> <code>Aggregator</code>\u00b6","text":"<p>In order to improve our results, we can change our <code>Aggregator</code>, by changing <code>FedAverage</code> into <code>FedProx</code>. Since <code>FedProx</code> is a <code>FedAverge</code> aggregator with a regularization term, we are re-using <code>FedAverage</code> <code>Aggregator</code> but we will be adding to the <code>training_args</code> <code>fedprox_mu</code>, that is the regularization parameter.</p>"},{"location":"tutorials/pytorch/05-Aggregation_in_Fed-BioMed/#3-defining-an-experiment-using-scaffold-aggregator","title":"3. Defining an <code>Experiment</code> using <code>SCAFFOLD</code> <code>Aggregator</code>\u00b6","text":"<p><code>Scaffold</code> purpose is to limit the so called client drift that may happen when dealing with heterogenous datasset accross <code>Nodes</code>.</p> <p>In order to use <code>Scaffold</code>, we will have to import another <code>Aggregator</code> from <code>fedbiomed.researcher.aggregators</code> module, as you can see below.</p> <p><code>Scaffold</code> takes <code>server_lr</code> and <code>fds</code> the as arguments.</p> <ul> <li><code>server_lr</code> is the server learning rate (in <code>Scaffold</code>, used to perform a gradient descent on global model's updates</li> <li><code>fds</code> is the <code>Federated Dataset</code> containing information about <code>Nodes</code> connected to the network after issuing a <code>TrainRequest</code></li> </ul> <p>Please note that it is possible to use <code>Scaffold</code> with a regularization parameter as suggested in <code>FedProx</code>. For that, you just have to specify <code>fedprox_mu</code> into the <code>training_args</code> dictionary, as shown in the <code>FedProx</code> example</p>"},{"location":"tutorials/pytorch/05-Aggregation_in_Fed-BioMed/#4-going-further","title":"4. Going further\u00b6","text":"<p>In this tutorial we presented 3 important <code>Aggregators</code> that can be found in the Federated Learning Literature. If you want to create your custom <code>Aggregator</code>, please check our Aggregation User guide</p> <p>You may have noticed that thanks to Fed-BioMed's modular structure, it is possible to alternate from one aggregator to another while conducting an <code>Experiment</code>. For instance, you may start with <code>SCAFFOLD</code> <code>Aggregator</code> for the 3 first rounds, and then switch to <code>FedAverage</code> <code>Aggregator</code> for the remaining rounds, as shown in the example below:</p>"},{"location":"tutorials/scikit-learn/","title":"Scikit-Learn with Fed-BioMed: a step-by-step tutorial","text":"<p>Scikit-Learn is one of the most famous and renowned open source machine learning library in Python. It implements several classical Machine Learning algorithms such as linear regression, SVM, random forests, ... </p> <ol> <li> <p>Classifying MNIST dataset using Scikit-Learn Perceptron</p> </li> <li> <p>Dealing with regression tasks using Scikit-Learn SGDRegressor</p> </li> <li> <p>Others Scikit-Learn models supported in Fed-BioMed</p> </li> </ol>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/","title":"MNIST classification with Scikit-Learn Classifier (Perceptron)","text":"In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.training_plans import FedPerceptron\nfrom fedbiomed.common.data import DataManager\nimport numpy as np\n\n\nclass SkLearnClassifierTrainingPlan(FedPerceptron):\n    def init_dependencies(self):\n\"\"\"Define additional dependencies.\n        return [\"from torchvision import datasets, transforms\",\n                \"from torch.utils.data import DataLoader\"]\n\n    def training_data(self, batch_size):\n        In this case, we rely on torchvision functions for preprocessing the images.\n        \"\"\"\n        return [\"from torchvision import datasets, transforms\",]\n\n    def training_data(self, batch_size):\n\"\"\"Prepare data for training.\n        This function loads a MNIST dataset from the node's filesystem, applies some\n        preprocessing and converts the full dataset to a numpy array. \n        Finally, it returns a DataManager created with these numpy arrays.\n        \"\"\"\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        \n        X_train = dataset.data.numpy()\n        X_train = X_train.reshape(-1, 28*28)\n        Y_train = dataset.targets.numpy()\n        return DataManager(dataset=X_train, target=Y_train, batch_size=batch_size, shuffle=False)\n</pre> from fedbiomed.common.training_plans import FedPerceptron from fedbiomed.common.data import DataManager import numpy as np   class SkLearnClassifierTrainingPlan(FedPerceptron):     def init_dependencies(self):         \"\"\"Define additional dependencies.         return [\"from torchvision import datasets, transforms\",                 \"from torch.utils.data import DataLoader\"]      def training_data(self, batch_size):                  In this case, we rely on torchvision functions for preprocessing the images.         \"\"\"         return [\"from torchvision import datasets, transforms\",]      def training_data(self, batch_size):         \"\"\"Prepare data for training.                  This function loads a MNIST dataset from the node's filesystem, applies some         preprocessing and converts the full dataset to a numpy array.          Finally, it returns a DataManager created with these numpy arrays.         \"\"\"         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)                  X_train = dataset.data.numpy()         X_train = X_train.reshape(-1, 28*28)         Y_train = dataset.targets.numpy()         return DataManager(dataset=X_train, target=Y_train, batch_size=batch_size, shuffle=False)  <p>Provide dynamic arguments for the model and training. These may potentially be changed at every round.</p> In\u00a0[\u00a0]: Copied! <pre>model_args = {'n_features': 28*28,\n              'n_classes' : 10,\n              'eta0':1e-6,\n              'random_state':1234,\n              'alpha':0.1 }\n\ntraining_args = {\n    'epochs': 3, \n    'batch_maxnum': 20,  # can be used to debugging to limit the number of batches per epoch\n#    'log_interval': 1,  # output a logging message every log_interval batches\n    'batch_size': 4\n}\n</pre> model_args = {'n_features': 28*28,               'n_classes' : 10,               'eta0':1e-6,               'random_state':1234,               'alpha':0.1 }  training_args = {     'epochs': 3,      'batch_maxnum': 20,  # can be used to debugging to limit the number of batches per epoch #    'log_interval': 1,  # output a logging message every log_interval batches     'batch_size': 4 } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MNIST', '#dataset']\nrounds = 3\n\n# select nodes participating in this experiment\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=SkLearnClassifierTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['#MNIST', '#dataset'] rounds = 3  # select nodes participating in this experiment exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=SkLearnClassifierTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) In\u00a0[\u00a0]: Copied! <pre>exp.run(increase=True)\n</pre> exp.run(increase=True) In\u00a0[\u00a0]: Copied! <pre>import tempfile\nimport os\nfrom fedbiomed.researcher.environ import environ\n\nfrom torchvision import datasets, transforms\nfrom sklearn.preprocessing import LabelBinarizer\nimport numpy as np\n\n\ntmp_dir_model = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+os.sep)\nmodel_file = os.path.join(tmp_dir_model.name, 'class_export_mnist.py')\n\n# collecting MNIST testing dataset: for that we are downloading the whole dataset on en temporary file\n\ntransform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\ntesting_MNIST_dataset = datasets.MNIST(root = os.path.join(environ['TMP_DIR'], 'local_mnist.tmp'),\n                                       download = True,\n                                       train = False,\n                                       transform = transform)\n\ntesting_MNIST_data = testing_MNIST_dataset.data.numpy().reshape(-1, 28*28)\ntesting_MNIST_targets = testing_MNIST_dataset.targets.numpy()\n</pre> import tempfile import os from fedbiomed.researcher.environ import environ  from torchvision import datasets, transforms from sklearn.preprocessing import LabelBinarizer import numpy as np   tmp_dir_model = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+os.sep) model_file = os.path.join(tmp_dir_model.name, 'class_export_mnist.py')  # collecting MNIST testing dataset: for that we are downloading the whole dataset on en temporary file  transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))]) testing_MNIST_dataset = datasets.MNIST(root = os.path.join(environ['TMP_DIR'], 'local_mnist.tmp'),                                        download = True,                                        train = False,                                        transform = transform)  testing_MNIST_data = testing_MNIST_dataset.data.numpy().reshape(-1, 28*28) testing_MNIST_targets = testing_MNIST_dataset.targets.numpy() In\u00a0[\u00a0]: Copied! <pre># retrieve Sklearn model and losses at the end of each round\n\nfrom sklearn.linear_model import  SGDClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, hinge_loss\n\nfed_perceptron_model = exp.training_plan().model()\nperceptron_args = {key: model_args[key] for key in model_args.keys() if key in fed_perceptron_model.get_params().keys()}\n\n\nlosses = []\naccuracies = []\n\nfor r in range(rounds):\n    fed_perceptron_model = fed_perceptron_model.set_params(**perceptron_args)\n    fed_perceptron_model.classes_ = np.unique(testing_MNIST_dataset.targets.numpy())\n    fed_perceptron_model.coef_ = exp.aggregated_params()[r]['params']['coef_'].copy()\n    fed_perceptron_model.intercept_ = exp.aggregated_params()[r]['params']['intercept_'].copy()  \n\n    prediction = fed_perceptron_model.decision_function(testing_MNIST_data)\n    losses.append(hinge_loss(testing_MNIST_targets, prediction))\n    accuracies.append(fed_perceptron_model.score(testing_MNIST_data,\n                                                testing_MNIST_targets))\n</pre> # retrieve Sklearn model and losses at the end of each round  from sklearn.linear_model import  SGDClassifier from sklearn.metrics import accuracy_score, confusion_matrix, hinge_loss  fed_perceptron_model = exp.training_plan().model() perceptron_args = {key: model_args[key] for key in model_args.keys() if key in fed_perceptron_model.get_params().keys()}   losses = [] accuracies = []  for r in range(rounds):     fed_perceptron_model = fed_perceptron_model.set_params(**perceptron_args)     fed_perceptron_model.classes_ = np.unique(testing_MNIST_dataset.targets.numpy())     fed_perceptron_model.coef_ = exp.aggregated_params()[r]['params']['coef_'].copy()     fed_perceptron_model.intercept_ = exp.aggregated_params()[r]['params']['intercept_'].copy()        prediction = fed_perceptron_model.decision_function(testing_MNIST_data)     losses.append(hinge_loss(testing_MNIST_targets, prediction))     accuracies.append(fed_perceptron_model.score(testing_MNIST_data,                                                 testing_MNIST_targets)) In\u00a0[\u00a0]: Copied! <pre># downloading MNIST dataset\ntraining_MNIST_dataset = datasets.MNIST(root = os.path.join(environ['TMP_DIR'], 'local_mnist.tmp'),\n                                       download = True,\n                                       train = True,\n                                       transform = transform)\n\ntraining_MNIST_data = training_MNIST_dataset.data.numpy().reshape(-1, 28*28)\ntraining_MNIST_targets = training_MNIST_dataset.targets.numpy()\n</pre> # downloading MNIST dataset training_MNIST_dataset = datasets.MNIST(root = os.path.join(environ['TMP_DIR'], 'local_mnist.tmp'),                                        download = True,                                        train = True,                                        transform = transform)  training_MNIST_data = training_MNIST_dataset.data.numpy().reshape(-1, 28*28) training_MNIST_targets = training_MNIST_dataset.targets.numpy() <p>Local Model training loop : a new model is trained locally, then compared with the remote <code>FedPerceptron</code> model</p> In\u00a0[\u00a0]: Copied! <pre>fed_perceptron_model.get_params()\n</pre> fed_perceptron_model.get_params() In\u00a0[\u00a0]: Copied! <pre>local_perceptron_losses = []\nlocal_perceptron_accuracies = []\nclasses = np.unique(training_MNIST_targets)\nbatch_size = training_args[\"batch_size\"]\n\n# model definition\nlocal_perceptron_model = SGDClassifier()\nperceptron_args = {key: model_args[key] for key in model_args.keys() if key in fed_perceptron_model.get_params().keys()}\nlocal_perceptron_model.set_params(**perceptron_args)\nmodel_param_list = ['coef_', 'intercept_']\n\n# Model initialization\nlocal_perceptron_model.intercept_ = np.zeros((model_args[\"n_classes\"],))\nlocal_perceptron_model.coef_ = np.zeros((model_args[\"n_classes\"], model_args[\"n_features\"]))\n</pre> local_perceptron_losses = [] local_perceptron_accuracies = [] classes = np.unique(training_MNIST_targets) batch_size = training_args[\"batch_size\"]  # model definition local_perceptron_model = SGDClassifier() perceptron_args = {key: model_args[key] for key in model_args.keys() if key in fed_perceptron_model.get_params().keys()} local_perceptron_model.set_params(**perceptron_args) model_param_list = ['coef_', 'intercept_']  # Model initialization local_perceptron_model.intercept_ = np.zeros((model_args[\"n_classes\"],)) local_perceptron_model.coef_ = np.zeros((model_args[\"n_classes\"], model_args[\"n_features\"])) <p>Implementation of mini-batch SGD</p> In\u00a0[\u00a0]: Copied! <pre>for r in range(rounds):\n    for e in range(training_args[\"epochs\"]):\n        \n        tot_samples_processed = 0\n        for idx_batch in range(training_args[\"batch_maxnum\"]):\n            param = {k: getattr(local_perceptron_model, k) for k in model_param_list}\n            grads = {k: np.zeros_like(v) for k, v in param.items()}\n            \n            # for each sample: 1) call partial_fit 2) accumulate the gradients 3) reset the model parameters\n            for sample_idx in range(tot_samples_processed, tot_samples_processed+batch_size):\n                local_perceptron_model.partial_fit(training_MNIST_data[sample_idx:sample_idx+1,:],\n                                                   training_MNIST_targets[sample_idx:sample_idx+1],\n                                                   classes=classes)\n                for key in model_param_list:\n                    grads[key] += getattr(local_perceptron_model, key)\n                    setattr(local_perceptron_model, key, param[key])\n                    \n            tot_samples_processed += batch_size\n\n            # after each epoch, we update the model with the averaged gradients over the batch\n            for key in model_param_list:\n                setattr(local_perceptron_model, key, grads[key] / batch_size)\n                \n    predictions = local_perceptron_model.decision_function(testing_MNIST_data)\n    local_perceptron_losses.append(hinge_loss(testing_MNIST_targets, predictions))\n    local_perceptron_accuracies.append(local_perceptron_model.score(testing_MNIST_data,\n                                                testing_MNIST_targets))\n</pre> for r in range(rounds):     for e in range(training_args[\"epochs\"]):                  tot_samples_processed = 0         for idx_batch in range(training_args[\"batch_maxnum\"]):             param = {k: getattr(local_perceptron_model, k) for k in model_param_list}             grads = {k: np.zeros_like(v) for k, v in param.items()}                          # for each sample: 1) call partial_fit 2) accumulate the gradients 3) reset the model parameters             for sample_idx in range(tot_samples_processed, tot_samples_processed+batch_size):                 local_perceptron_model.partial_fit(training_MNIST_data[sample_idx:sample_idx+1,:],                                                    training_MNIST_targets[sample_idx:sample_idx+1],                                                    classes=classes)                 for key in model_param_list:                     grads[key] += getattr(local_perceptron_model, key)                     setattr(local_perceptron_model, key, param[key])                                  tot_samples_processed += batch_size              # after each epoch, we update the model with the averaged gradients over the batch             for key in model_param_list:                 setattr(local_perceptron_model, key, grads[key] / batch_size)                      predictions = local_perceptron_model.decision_function(testing_MNIST_data)     local_perceptron_losses.append(hinge_loss(testing_MNIST_targets, predictions))     local_perceptron_accuracies.append(local_perceptron_model.score(testing_MNIST_data,                                                 testing_MNIST_targets)) <p>Compare the local and federated models. The two curves should overlap almost identically, although slight numerical errors are acceptable.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,5))\n\nplt.subplot(1,2,1)\nplt.plot(losses, label=\"federated Perceptron losses\")\nplt.plot(local_perceptron_losses, \"--\", color='r', label=\"local Perceptron losses\")\nplt.ylabel('Perceptron Cost Function (Hinge)')\nplt.xlabel('Number of Rounds')\nplt.title('Perceptron loss evolution on test dataset')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(accuracies, label=\"federated Perceptron accuracies\")\nplt.plot(local_perceptron_accuracies, \"--\", color='r',\n         label=\"local Perceptron accuracies\")\nplt.ylabel('Accuracy')\nplt.xlabel('Number of Rounds')\nplt.title('Perceptron accuracy over rounds (on test dataset)')\nplt.legend()\n</pre> import matplotlib.pyplot as plt  plt.figure(figsize=(10,5))  plt.subplot(1,2,1) plt.plot(losses, label=\"federated Perceptron losses\") plt.plot(local_perceptron_losses, \"--\", color='r', label=\"local Perceptron losses\") plt.ylabel('Perceptron Cost Function (Hinge)') plt.xlabel('Number of Rounds') plt.title('Perceptron loss evolution on test dataset') plt.legend()  plt.subplot(1,2,2) plt.plot(accuracies, label=\"federated Perceptron accuracies\") plt.plot(local_perceptron_accuracies, \"--\", color='r',          label=\"local Perceptron accuracies\") plt.ylabel('Accuracy') plt.xlabel('Number of Rounds') plt.title('Perceptron accuracy over rounds (on test dataset)') plt.legend()  <p>In this example, plots appear to be the same: this means that Federated and local <code>Perceptron</code> models are performing equivalently!</p> In\u00a0[\u00a0]: Copied! <pre># federated model predictions\nfed_prediction = fed_perceptron_model.predict(testing_MNIST_data)\nacc = accuracy_score(testing_MNIST_targets, fed_prediction)\nprint('Federated Perceptron Model accuracy :', acc)\n\n# local model predictions\nlocal_prediction = local_perceptron_model.predict(testing_MNIST_data)\nacc = accuracy_score(testing_MNIST_targets, local_prediction)\nprint('Local Perceptron Model accuracy :', acc)\n</pre> # federated model predictions fed_prediction = fed_perceptron_model.predict(testing_MNIST_data) acc = accuracy_score(testing_MNIST_targets, fed_prediction) print('Federated Perceptron Model accuracy :', acc)  # local model predictions local_prediction = local_perceptron_model.predict(testing_MNIST_data) acc = accuracy_score(testing_MNIST_targets, local_prediction) print('Local Perceptron Model accuracy :', acc) In\u00a0[\u00a0]: Copied! <pre>def plot_confusion_matrix(fig, ax, conf_matrix, title, xlabel, ylabel, n_image=0):\n    \n    im = ax[n_image].imshow(conf_matrix)\n\n    ax[n_image].set_xticks(np.arange(10))\n    ax[n_image].set_yticks(np.arange(10))\n\n    for i in range(conf_matrix.shape[0]):\n        for j in range(conf_matrix.shape[1]):\n            text = ax[n_image].text(j, i, conf_matrix[i, j],\n                           ha=\"center\", va=\"center\", color=\"w\")\n\n    ax[n_image].set_xlabel(xlabel)\n    ax[n_image].set_ylabel(ylabel)\n    ax[n_image].set_title(title)\n</pre> def plot_confusion_matrix(fig, ax, conf_matrix, title, xlabel, ylabel, n_image=0):          im = ax[n_image].imshow(conf_matrix)      ax[n_image].set_xticks(np.arange(10))     ax[n_image].set_yticks(np.arange(10))      for i in range(conf_matrix.shape[0]):         for j in range(conf_matrix.shape[1]):             text = ax[n_image].text(j, i, conf_matrix[i, j],                            ha=\"center\", va=\"center\", color=\"w\")      ax[n_image].set_xlabel(xlabel)     ax[n_image].set_ylabel(ylabel)     ax[n_image].set_title(title) In\u00a0[\u00a0]: Copied! <pre>fed_conf_matrix = confusion_matrix(testing_MNIST_targets, fed_prediction)\nlocal_conf_matrix = confusion_matrix(testing_MNIST_targets, local_prediction)\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2,figsize=(10,5))\n\n\n\nplot_confusion_matrix(fig, axs, fed_conf_matrix,\n                      \"Federated Perceptron Confusion Matrix\",\n                      \"Actual values\", \"Predicted values\", n_image=0)\n        \nplot_confusion_matrix(fig, axs, local_conf_matrix,\n                      \"Local Perceptron Confusion Matrix\",\n                      \"Actual values\", \"Predicted values\", n_image=1)\n</pre> fed_conf_matrix = confusion_matrix(testing_MNIST_targets, fed_prediction) local_conf_matrix = confusion_matrix(testing_MNIST_targets, local_prediction)   fig, axs = plt.subplots(nrows=1, ncols=2,figsize=(10,5))    plot_confusion_matrix(fig, axs, fed_conf_matrix,                       \"Federated Perceptron Confusion Matrix\",                       \"Actual values\", \"Predicted values\", n_image=0)          plot_confusion_matrix(fig, axs, local_conf_matrix,                       \"Local Perceptron Confusion Matrix\",                       \"Actual values\", \"Predicted values\", n_image=1)"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#mnist-classification-with-scikit-learn-classifier-perceptron","title":"MNIST classification with Scikit-Learn Classifier (Perceptron)\u00b6","text":"<p>Overview of the tutorial:</p> <p>In this tutorial, we are going to train Scikit-Learn <code>Perceptron</code> as a federated model over a <code>Node</code>.</p> <p>At the end of this tutorial, you will learn:</p> <ul> <li>how to define a Sklearn classifier in Fed-BioMed (especially <code>Perceptron</code> model)</li> <li>how to train it</li> <li>how to evaluate the resulting model</li> </ul> <p>HINT : to reload the notebook,  please click on the following button:</p> <p><code>Kernel</code> -&gt; <code>Restart and clear Output</code></p> <p></p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#1-clean-your-environments","title":"1. Clean your environments\u00b6","text":"<p>Before executing notebook and starting nodes, it is safer to remove all configuration scripts automatically generated by Fed-BioMed. To do so, enter the following in a terminal:</p> <pre><code>source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment clean\n</code></pre> <p>Note: <code>${FEDBIOMED_DIR}</code> is a path relative to based directory of the cloned Fed-BioMed repository. You can set it by running command <code>export FEDBIOMED_DIR=/path/to/fedbiomed</code>. This is not required for Fed-BioMed to work but enables you to run the tutorials more easily.</p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#2-setting-the-node-up","title":"2. Setting the node up\u00b6","text":"<p>It is necessary to previously configure a <code>Network</code>  and a <code>Node</code>before runnig this notebook:</p> <ol> <li><p><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run network</code></p> </li> <li><p><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node add</code></p> </li> </ol> <ul> <li>Select option 2 (default) to add MNIST to the node</li> <li>Confirm default tags by hitting \"y\" and ENTER</li> <li>Pick the folder where MNIST is downloaded (this is due torch issue https://github.com/pytorch/vision/issues/3549)</li> <li>Data must have been added (if you get a warning saying that data must be unique is because it has been already added)</li> </ul> <ol> <li>Check that your data has been added by executing <code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node list</code></li> <li>Run the node using <code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node start</code>. Wait until you get <code>Connected with result code 0</code>. it means your node is working and ready to participate to a Federated training.</li> </ol> <p>More details are given in tutorial : Installation/setting up environment </p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#3-create-sklearn-federated-perceptron-training-plan","title":"3. Create Sklearn Federated Perceptron Training Plan\u00b6","text":"<p>The class <code>FedPerceptron</code> constitutes the Fed-BioMed wrapper for executing Federated Learning using Scikit-Learn Perceptron model based on mini-batch Stochastic Gradient Descent (SGD). As we have done with Pytorch model in previous chapter, we create a new training plan class <code>SkLearnClassifierTrainingPlan</code> that inherits from it. For a refresher on how Training Plans work in Fed-BioMed, please refer to our Training Plan user guide.</p> <p>In scikit-learn Training Plans, you typically need to define only the <code>training_data</code> function, and optionally an <code>init_dependencies</code> function if your code requires additional module imports.</p> <p>The <code>training_data</code> function defines how datasets should be loaded in nodes to make them ready for training. It takes a <code>batch_size</code> argument and returns a <code>DataManager</code> class. For scikit-learn, the <code>DataManager</code> must be instantiated with a <code>dataset</code> and a <code>target</code> argument, both <code>np.ndarrays</code> of the same length.</p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#model-arguments","title":"Model arguments\u00b6","text":"<p><code>model_args</code> is a dictionary with the arguments related to the model, that will be passed to the <code>Perceptron</code> constructor.</p> <p>IMPORTANT For classification tasks, you are required to specify the following two fields:</p> <ul> <li><code>n_features</code>: the number of features in each input sample (in our case, the number of pixels in the images)</li> <li><code>n_classes</code>: the number of classes in the target data</li> </ul> <p>Furthermore, the classes may not be represented by arbitrary values: classes must be identified by integers in the range 0..n_classes</p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#training-arguments","title":"Training arguments\u00b6","text":"<p><code>training_args</code> is a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.</p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#4-train-your-model-on-mnist-dataset","title":"4. Train your model on MNIST dataset\u00b6","text":"<p>MNIST dataset is composed of handwritten digits images, from 0 to 9. The purpose of our classifier is to associate an image to the corresponding represented digit</p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#5-testing-on-mnist-test-dataset","title":"5. Testing on MNIST test dataset\u00b6","text":"<p>Let's assess performance of our classifier with  MNIST testing dataset</p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#6-getting-loss-function","title":"6. Getting Loss function\u00b6","text":"<p>Here we use the <code>aggregated_params()</code> getter to access all model weights at the end of each round to plot the evolution of Percpetron loss funciton, as well as its accuracy.</p>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#7-comparison-with-a-local-perceptron-model","title":"7. Comparison with a local <code>Perceptron</code> model\u00b6","text":"<p>In this section, we implement a local <code>Perceptron</code> model, so we can compare remote and local models accuracies.</p> <p>You can use this section as an insight on how things are implemented within the Fed-BioMed network. In particular, looking at the code in the next few cells you may learn how:</p> <ul> <li>we implement mini-batch gradient descent for scikit-learn models</li> <li>we implement <code>Perceptron</code> based on <code>SGDClassifier</code></li> </ul>"},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#8-getting-accuracy-and-confusion-matrix","title":"8. Getting accuracy and confusion matrix\u00b6","text":""},{"location":"tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/#congrats","title":"Congrats !\u00b6","text":"<p>You have figured out how to train your first Federated Sklearn classifier model !</p> <p>If you want to practise more, you can try to deploy such classifier on two or more nodes. As you can see, <code>Perceptron</code> is a limited model: its generalization is <code>SGDCLassifier</code>, provided by Fed-BioMed as a <code>FedSGDCLassifier</code> Training Plan. You can thus try to apply <code>SGDCLassifier</code>, providing more feature such as different cost functions, regularizations and learning rate decays.</p>"},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/","title":"Fed-BioMed to train a federated SGD regressor model","text":"In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.requests import Requests\nreq = Requests()\nreq.list(verbose=True)\n</pre> from fedbiomed.researcher.requests import Requests req = Requests() req.list(verbose=True) <p>The class <code>FedSGDRegressor</code> constitutes the Fed-BioMed wrapper for executing Federated Learning using Scikit-Learn <code>SGDRegressor</code> model based on mini-batch Stochastic Gradient Descent (SGD). As we have done with Pytorch model in previous chapter, we create a new training plan class <code>SGDRegressorTrainingPlan</code> that inherits from it. For a refresher on how Training Plans work in Fed-BioMed, please refer to our Training Plan user guide.</p> <p>In scikit-learn Training Plans, you typically need to define only the <code>training_data</code> function, and optionally an <code>init_dependencies</code> function if your code requires additional module imports.</p> <p>The <code>training_data</code> function defines how datasets should be loaded in nodes to make them ready for training. It takes a <code>batch_size</code> argument and returns a <code>DataManager</code> class. For scikit-learn, the <code>DataManager</code> must be instantiated with a <code>dataset</code> and a <code>target</code> argument, both <code>np.ndarrays</code> of the same length.</p> <p>We note that this model performs a common standardization across federated datasets by centering with respect to the same parameters.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom fedbiomed.common.training_plans import FedSGDRegressor\nfrom fedbiomed.common.data import DataManager\n\nclass SGDRegressorTrainingPlan(FedSGDRegressor):\n    def training_data(self, batch_size):\n        dataset = pd.read_csv(self.dataset_path,delimiter=';')\n        regressors_col = ['AGE', 'WholeBrain.bl',\n                          'Ventricles.bl', 'Hippocampus.bl', 'MidTemp.bl', 'Entorhinal.bl']\n        target_col = ['MMSE.bl']\n\n        # mean and standard deviation for normalizing dataset\n        # it has been computed over the whole dataset\n        scaling_mean = np.array([72.3, 0.7, 0.0, 0.0, 0.0, 0.0])\n        scaling_sd = np.array([7.3e+00, 5.0e-02, 1.1e-02, 1.0e-03, 2.0e-03, 1.0e-03])\n\n        X = (dataset[regressors_col].values-scaling_mean)/scaling_sd\n        y = dataset[target_col]\n        return DataManager(dataset=X, target=y.values.ravel(), batch_size=batch_size)\n</pre> import numpy as np from fedbiomed.common.training_plans import FedSGDRegressor from fedbiomed.common.data import DataManager  class SGDRegressorTrainingPlan(FedSGDRegressor):     def training_data(self, batch_size):         dataset = pd.read_csv(self.dataset_path,delimiter=';')         regressors_col = ['AGE', 'WholeBrain.bl',                           'Ventricles.bl', 'Hippocampus.bl', 'MidTemp.bl', 'Entorhinal.bl']         target_col = ['MMSE.bl']          # mean and standard deviation for normalizing dataset         # it has been computed over the whole dataset         scaling_mean = np.array([72.3, 0.7, 0.0, 0.0, 0.0, 0.0])         scaling_sd = np.array([7.3e+00, 5.0e-02, 1.1e-02, 1.0e-03, 2.0e-03, 1.0e-03])          X = (dataset[regressors_col].values-scaling_mean)/scaling_sd         y = dataset[target_col]         return DataManager(dataset=X, target=y.values.ravel(), batch_size=batch_size) <p>Provide dynamic arguments for the model and training. These may potentially be changed at every round.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.metrics import MetricTypes\nRANDOM_SEED = 1234\n\nmodel_args = {\n    'eta0':0.05,\n    'n_features': 6,\n    'random_state': RANDOM_SEED\n}\n\ntraining_args = {\n    'epochs': 1,\n    'test_ratio':.2,\n    'test_metric': MetricTypes.MEAN_SQUARE_ERROR,\n    'test_on_local_updates': True,\n    'test_on_global_updates': True,\n    'batch_size': 30,\n#    'batch_maxnum': 2,  # can be used to debugging to limit the number of batches per epoch\n#    'log_interval': 1,  # output a logging message every log_interval batches\n}\n</pre> from fedbiomed.common.metrics import MetricTypes RANDOM_SEED = 1234  model_args = {     'eta0':0.05,     'n_features': 6,     'random_state': RANDOM_SEED }  training_args = {     'epochs': 1,     'test_ratio':.2,     'test_metric': MetricTypes.MEAN_SQUARE_ERROR,     'test_on_local_updates': True,     'test_on_global_updates': True,     'batch_size': 30, #    'batch_maxnum': 2,  # can be used to debugging to limit the number of batches per epoch #    'log_interval': 1,  # output a logging message every log_interval batches } <p>The experiment can be now defined, by providing the <code>adni</code> tag, and running the local training on nodes with training plan defined in <code>training_plan_path</code>, standard <code>aggregator</code> (FedAvg) and <code>client_selection_strategy</code> (all nodes used). Federated learning is going to be performed through 10 optimization rounds.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['adni']\n\n# Add more rounds for results with better accuracy\n#\n#rounds = 40\nrounds = 10\n\n# select nodes participating in this experiment\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=SGDRegressorTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['adni']  # Add more rounds for results with better accuracy # #rounds = 40 rounds = 10  # select nodes participating in this experiment exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=SGDRegressorTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) In\u00a0[\u00a0]: Copied! <pre># start federated training\nexp.run(increase=True)\n</pre> # start federated training exp.run(increase=True) In\u00a0[\u00a0]: Copied! <pre>!pip install matplotlib\n!pip install gdown\n</pre> !pip install matplotlib !pip install gdown <p>Download the testing dataset on the local temporary folder.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport gdown\nimport tempfile\nimport zipfile\nimport pandas as pd\nimport numpy as np\n\nfrom fedbiomed.common.constants import ComponentType\nfrom fedbiomed.common.environ import Environ\n\nenviron = Environ(ComponentType.RESEARCHER)\n\nresource = \"https://drive.google.com/uc?id=19kxuI146WA2fhcOU2_AvF8dy-ppJkzW7\"\n\ntmpdir = tempfile.TemporaryDirectory(dir=environ['TMP_DIR'])\nbase_dir = tmpdir.name\n\ntest_file = os.path.join(base_dir, \"test_data.zip\")\ngdown.download(resource, test_file, quiet=False)\n\nzf = zipfile.ZipFile(test_file)\n\nfor file in zf.infolist():\n    zf.extract(file, base_dir)\n\n# loading testing dataset\ntest_data = pd.read_csv(os.path.join(base_dir,'adni_validation.csv'))\n</pre> import os import gdown import tempfile import zipfile import pandas as pd import numpy as np  from fedbiomed.common.constants import ComponentType from fedbiomed.common.environ import Environ  environ = Environ(ComponentType.RESEARCHER)  resource = \"https://drive.google.com/uc?id=19kxuI146WA2fhcOU2_AvF8dy-ppJkzW7\"  tmpdir = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']) base_dir = tmpdir.name  test_file = os.path.join(base_dir, \"test_data.zip\") gdown.download(resource, test_file, quiet=False)  zf = zipfile.ZipFile(test_file)  for file in zf.infolist():     zf.extract(file, base_dir)  # loading testing dataset test_data = pd.read_csv(os.path.join(base_dir,'adni_validation.csv')) In\u00a0[\u00a0]: Copied! <pre>from sklearn.linear_model import SGDRegressor\nimport matplotlib.pyplot as plt\n</pre> from sklearn.linear_model import SGDRegressor import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>Here we extract the relevant regressors and target from the testing data</p> In\u00a0[\u00a0]: Copied! <pre>regressors_col = ['AGE', 'WholeBrain.bl', 'Ventricles.bl', 'Hippocampus.bl', 'MidTemp.bl', 'Entorhinal.bl']\ntarget_col = ['MMSE.bl']\nX_test = test_data[regressors_col].values\ny_test = test_data[target_col].values\n</pre> regressors_col = ['AGE', 'WholeBrain.bl', 'Ventricles.bl', 'Hippocampus.bl', 'MidTemp.bl', 'Entorhinal.bl'] target_col = ['MMSE.bl'] X_test = test_data[regressors_col].values y_test = test_data[target_col].values <p>To inspect the model evolution across FL rounds, we export <code>exp.aggregated_params()</code> containing models parameters collected at the end of each round. The MSE should be decreasing at each iteration with the federated parameters.</p> In\u00a0[\u00a0]: Copied! <pre>scaling_mean = np.array([72.3, 0.7, 0.0, 0.0, 0.0, 0.0])\nscaling_sd = np.array([7.3e+00, 5.0e-02, 1.1e-02, 1.0e-03, 2.0e-03, 1.0e-03])\n\ntesting_error = []\n\n\n# we create here several instances of SGDRegressor using same sklearn arguments\n# we have used for Federated Learning training\nfed_model = exp.training_plan().model()\nregressor_args = {key: model_args[key] for key in model_args.keys() if key in fed_model.get_params().keys()}\n\nfor i in range(rounds):\n    fed_model.coef_ = exp.aggregated_params()[i]['params']['coef_'].copy()\n    fed_model.intercept_ = exp.aggregated_params()[i]['params']['intercept_'].copy()\n    mse = np.mean((fed_model.predict((X_test-scaling_mean)/scaling_sd) - y_test)**2)\n    testing_error.append(mse)\n\nplt.plot(testing_error)\nplt.title('FL testing loss')\nplt.xlabel('FL round')\nplt.ylabel('testing loss (MSE)')\n</pre> scaling_mean = np.array([72.3, 0.7, 0.0, 0.0, 0.0, 0.0]) scaling_sd = np.array([7.3e+00, 5.0e-02, 1.1e-02, 1.0e-03, 2.0e-03, 1.0e-03])  testing_error = []   # we create here several instances of SGDRegressor using same sklearn arguments # we have used for Federated Learning training fed_model = exp.training_plan().model() regressor_args = {key: model_args[key] for key in model_args.keys() if key in fed_model.get_params().keys()}  for i in range(rounds):     fed_model.coef_ = exp.aggregated_params()[i]['params']['coef_'].copy()     fed_model.intercept_ = exp.aggregated_params()[i]['params']['intercept_'].copy()     mse = np.mean((fed_model.predict((X_test-scaling_mean)/scaling_sd) - y_test)**2)     testing_error.append(mse)  plt.plot(testing_error) plt.title('FL testing loss') plt.xlabel('FL round') plt.ylabel('testing loss (MSE)') <p>We finally inspect the predictions of the final federated model on the testing data.</p> In\u00a0[\u00a0]: Copied! <pre>y_predicted = fed_model.predict((X_test-scaling_mean)/scaling_sd)\nplt.scatter(y_predicted, y_test, label='model prediction')\nplt.xlabel('predicted')\nplt.ylabel('target')\nplt.title('Federated model testing prediction')\n\nfirst_diag = np.arange(np.min(y_test.flatten()),\n                       np.max(y_test.flatten()+1))\nplt.scatter(first_diag, first_diag, label='correct Target')\nplt.legend()\n</pre> y_predicted = fed_model.predict((X_test-scaling_mean)/scaling_sd) plt.scatter(y_predicted, y_test, label='model prediction') plt.xlabel('predicted') plt.ylabel('target') plt.title('Federated model testing prediction')  first_diag = np.arange(np.min(y_test.flatten()),                        np.max(y_test.flatten()+1)) plt.scatter(first_diag, first_diag, label='correct Target') plt.legend()"},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#fed-biomed-to-train-a-federated-sgd-regressor-model","title":"Fed-BioMed to train a federated SGD regressor model\u00b6","text":""},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#data","title":"Data\u00b6","text":"<p>This tutorial shows how to deploy in Fed-BioMed to solve a federated regression problem with scikit-learn.</p> <p>In this tutorial we are using the wrapper of Fed-BioMed for the SGD regressor. The goal of the notebook is to train a model on a realistic dataset of (synthetic) medical information mimicking the ADNI dataset.</p>"},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#creating-nodes","title":"Creating nodes\u00b6","text":"<p>To proceed with the tutorial, we create 3 clients with corresponding dataframes of clinical information in .csv format. Each client has 300 data points composed by several features corresponding to clinical and medical imaging information. The data is entirely synthetic and randomly sampled to mimick the variability of the real ADNI dataset. The training partitions are available at the following link:</p> <p>https://drive.google.com/file/d/1R39Ir60oQi8ZnmHoPz5CoGCrVIglcO9l/view?usp=sharing</p> <p>The federated task we aim at solve is to predict a clinical variable (the mini-mental state examination, MMSE) from a combination of demographic and imaging features. The regressors variables are the following features:</p> <pre>['SEX', 'AGE', 'PTEDUCAT', 'WholeBrain.bl', 'Ventricles.bl', 'Hippocampus.bl', 'MidTemp.bl', 'Entorhinal.bl']\n</pre> <p>and the target variable is:</p> <pre>['MMSE.bl']\n</pre> <p>To create the federated dataset, we follow the standard procedure for node creation/population of Fed-BioMed. After activating the fedbiomed network with the commands</p> <pre>$ source ./scripts/fedbiomed_environment network`\n</pre> <p>and</p> <pre>$ ./scripts/fedbiomed_run network`\n</pre> <p>we create a first node by using the commands</p> <pre>$ source ./scripts/fedbiomed_environment node`\n</pre> <pre>$ ./scripts/fedbiomed_run node start\n</pre> <p>We then populate the node with the data of first client:</p> <pre>$ ./scripts/fedbiomed_run node add`\n</pre> <p>We select option 1 (csv) to add the .csv partition of client 1, by just picking the .csv of client 1. We use <code>adni</code> as tag to save the selected dataset. We can further check that the data has been added by executing <code>./scripts/fedbiomed_run node list</code></p> <p>Following the same procedure, we create the other two nodes with the datasets of client 2 and client 3 respectively.</p>"},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#fed-biomed-researcher","title":"Fed-BioMed Researcher\u00b6","text":"<p>We are now ready to start the researcher environment with the following command. This command will activate researcher environment and start Jupyter Notebook.</p> <pre>$ ./scripts/fedbiomed_run researcher start\n</pre> <p>We can first query the network for the <code>adni</code> dataset. In this case, the nodes are sharing the respective partitions using the same tag <code>adni</code>:</p>"},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#create-an-experiment-to-train-a-model-on-the-data-found","title":"Create an experiment to train a model on the data found\u00b6","text":""},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#model-arguments","title":"Model arguments\u00b6","text":"<p><code>model_args</code> is a dictionary with the arguments related to the model, that will be passed to the <code>SGDRegressor</code> constructor. In this case, these include <code>n_features</code>, <code>random_state</code> and <code>eta0</code>.</p> <p>IMPORTANT For regression tasks, you are required to specify the following field:</p> <ul> <li><code>n_features</code>: the number of features in each input sample (in our case, the number of pixels in the images)</li> </ul>"},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#training-arguments","title":"Training arguments\u00b6","text":"<p><code>training_args</code> is a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.</p>"},{"location":"tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/#testing","title":"Testing\u00b6","text":"<p>Once the federated model is obtained, it is possible to test it locally on an independent testing partition. The test dataset is available at this link:</p> <p>https://drive.google.com/file/d/1zNUGp6TMn6WSKYVC8FQiQ9lJAUdasxk1/</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/","title":"Implementing other Scikit Learn models for Federated Learning","text":"In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.common.training_plans import FedSGDRegressor, FedPerceptron, FedSGDClassifier\n\nSelectedTrainingPlan = FedPerceptron\n\n\nclass SkLearnTrainingPlan(SelectedTrainingPlan):\n    def init_dependencies(self):\n        # The method for declaring dependencies that are used generally in this training plan.\n        # E.g, `import numpy as np`should be added dependency array if it is used in the training_data method.\n        deps= [\"import numpy as np\",\n               \"import pandas as pd\"]\n        return deps\n\n    def training_data(self, batch_size):\n        # Define here how data are handled and /or shuffled\n        # First you need to instantiate the dataset. This will be typically something like\n        # raw_dataset = pd.read_csv(self.dataset_path)\n        # X = raw_dataset[feature_columns]\n        # y = raw_dataset[target_column(s)]\n\n        return DataManager(dataset=X.values, target=y.values, batch_size=batch_size, shuffle=True, drop_last=False)\n</pre> from fedbiomed.common.training_plans import FedSGDRegressor, FedPerceptron, FedSGDClassifier  SelectedTrainingPlan = FedPerceptron   class SkLearnTrainingPlan(SelectedTrainingPlan):     def init_dependencies(self):         # The method for declaring dependencies that are used generally in this training plan.         # E.g, `import numpy as np`should be added dependency array if it is used in the training_data method.         deps= [\"import numpy as np\",                \"import pandas as pd\"]         return deps      def training_data(self, batch_size):         # Define here how data are handled and /or shuffled         # First you need to instantiate the dataset. This will be typically something like         # raw_dataset = pd.read_csv(self.dataset_path)         # X = raw_dataset[feature_columns]         # y = raw_dataset[target_column(s)]          return DataManager(dataset=X.values, target=y.values, batch_size=batch_size, shuffle=True, drop_last=False) <p>Training a Scikit Learn model is pretty similar to training a Pytorch model. The only difference is the selection of model hyperparameters (contained in <code>model_args</code>) and training parameters (in <code>training_args</code>). Initializing the class <code>Experiment</code> will allow the <code>Researcher</code> to search for active nodes tagged with defined tags.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\n\ntags =  ['#MNIST', '#dataset']\n\n# select nodes participating to this experiment\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=SkLearnTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.experiment import Experiment  tags =  ['#MNIST', '#dataset']  # select nodes participating to this experiment exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=SkLearnTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre>  exp.run() In\u00a0[\u00a0]: Copied! <pre>exp.aggregated_params()\n</pre> exp.aggregated_params() <p>More algorithms from Scikit-Learn are coming soon ! Stay Tuned !</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#implementing-other-scikit-learn-models-for-federated-learning","title":"Implementing other Scikit Learn models for Federated Learning\u00b6","text":"<p>In this tutorial, you will learn how to define and run any Scikit Learn Supervised and Unsupervised model, as well as Data reduction methods, in Fed-BioMed.</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#1-introduction","title":"1. Introduction\u00b6","text":"<p>Like in previous tutorials with Pytorch, you can implement custom Scikit Learn models in Fed-BioMed. In this tutorial, we are summarizing all the steps to set up a Scikit Learn model in Fed-BioMed.</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#current-scikit-learn-methods-implemented-in-fed-biomed","title":"Current Scikit-Learn Methods implemented in Fed-BioMed\u00b6","text":"<ul> <li>Classifiers:<ul> <li>SGDClassifier</li> <li>Perceptron</li> </ul> </li> </ul> <ul> <li>Regressor:<ul> <li>SGDRegressor</li> </ul> </li> </ul> <ul> <li>Clustering:<ul> <li>Coming Soon!</li> </ul> </li> </ul> <p>Check out our User Guide for further information about Scikit Learn models available in Fed-BioMed.</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#2-scikit-learn-training-plan","title":"2. Scikit-Learn training plan\u00b6","text":"<p>As you could have seen in the previous tutorials concerning Scikit-Learn, you should define a \"Scikit-Learn training plan\". We provide here a template to create a TrainingPlan for Scikit Learn. As for PyTorch training plan, every Scikit-Learn Training Plan class should be inherited from one of the <code>\"FedPerceptron\", \"FedSGDRegressor\", \"FedSGDClassifier\"</code> classes.</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#training-plan-for-supervised-learning-regressor-and-classifier","title":"Training Plan for supervised Learning (Regressor and Classifier)\u00b6","text":"<p>A template of a Supervised Learning algorithm for Scikit-Learn models. Each supported SkLearn model can be imported from the module <code>fedbiomed.common.training_plan</code>. Currently Fed-BioMed support following SkLearn models <code> \"FedPerceptron\", \"FedSGDRegressor\", \"FedSGDClassifier\"</code>.</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#31-arguments-for-model-definition-and-model-training","title":"3.1 Arguments for model definition and model training:\u00b6","text":"<ul> <li><p><code>tags</code>: a list containing tags that will be used for finding models. Same as for PyTorch models.</p> </li> <li><p><code>model_args</code>: a Python dictionary containing all arguments related to the model (ie all Scikit Learn model parameters). In addition, it MUST include the following fields:</p> <ul> <li><code>n_features</code>: number of features in the dataset</li> <li><code>n_classes</code>: number of classes (for classification or clustering algorithms only, ignored if a Regression algorithm is used). </li> </ul> </li> <li><p><code>training_plan_class</code>: the Scikit-Learn training Plan class. Same as for Pytorch models.</p> </li> <li><p><code>training_args</code>: a dictionary containing training parameter. For the moment, it contains the following entries:</p> <ul> <li><code>epochs</code>: the number of epoch to be performed locally (ie on each node). </li> </ul> </li> <li><p><code>round_limit</code>: the number of rounds (ie global aggregations) to be performed. Same as for PyTorch models.</p> </li> <li><p><code>aggregator</code>: the aggregation strategy, here Federated Average. More information on User Guide/Aggregators. Same as for PyTorch models.</p> </li> <li><p><code>node_selection_startegy</code>: how to select/sample nodes among all available nodes. Same as for Pytorch models.</p> </li> </ul>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#32-training-the-model","title":"3.2 Training the model\u00b6","text":"<p>Calling the <code>run</code> method from <code>Experiment</code> will train the Federated Model.</p>"},{"location":"tutorials/scikit-learn/03-other-scikit-learn-models/#33-retrieve-model-weigths-for-each-federated-round","title":"3.3 Retrieve model weigths for each Federated round.\u00b6","text":"<p>The history of each round is accessed via <code>aggregated_params()</code> attribute of <code>Experiment</code> class. In fact, aggregated model at each round is contained in a dictionary, where each key corresponds to  a specific round. Each key is mapping an aggregated model obtained through the round.</p> <p>To extract all the history, enter :</p>"},{"location":"tutorials/security/differential-privacy-with-opacus-on-fedbiomed/","title":"Using Differential Privacy with OPACUS on Fed-BioMed","text":"<p>In this notebook we show how <code>opacus</code> (https://opacus.ai/) can be used in Fed-BioMed. Opacus is a library which allows to train PyTorch models with differential privacy. We will train the basic MNIST example using two nodes.</p> In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# Here we define the training plan to be used in the experiment. \nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\",\n                \"import torch.nn.functional as F\"]\n        \n        return deps\n    \n    def init_model(self):\n        model = nn.Sequential(nn.Conv2d(1, 32, 3, 1),\n                                  nn.ReLU(),\n                                  nn.Conv2d(32, 64, 3, 1),\n                                  nn.ReLU(),\n                                  nn.MaxPool2d(2),\n                                  nn.Dropout(0.25),\n                                  nn.Flatten(),\n                                  nn.Linear(9216, 128),\n                                  nn.ReLU(),\n                                  nn.Dropout(0.5),\n                                  nn.Linear(128, 10),\n                                  nn.LogSoftmax(dim=1))\n        return model\n    \n\n    \n    def training_data(self, batch_size = 48):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        loader_arguments = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset1, **loader_arguments)\n    \n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn import torch.nn.functional as F from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torch.utils.data import DataLoader from torchvision import datasets, transforms  # Here we define the training plan to be used in the experiment.  class MyTrainingPlan(TorchTrainingPlan):     def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\",                 \"import torch.nn.functional as F\"]                  return deps          def init_model(self):         model = nn.Sequential(nn.Conv2d(1, 32, 3, 1),                                   nn.ReLU(),                                   nn.Conv2d(32, 64, 3, 1),                                   nn.ReLU(),                                   nn.MaxPool2d(2),                                   nn.Dropout(0.25),                                   nn.Flatten(),                                   nn.Linear(9216, 128),                                   nn.ReLU(),                                   nn.Dropout(0.5),                                   nn.Linear(128, 10),                                   nn.LogSoftmax(dim=1))         return model                def training_data(self, batch_size = 48):         # Custom torch Dataloader for MNIST data         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         loader_arguments = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset1, **loader_arguments)          def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss  <p>This group of arguments correspond respectively:</p> <ul> <li><code>model_args</code>: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side. For instance, the privacy parameters should be passed here.</li> <li><code>training_args</code>: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.</li> </ul> <p>NOTE: typos and/or lack of positional (required) arguments will raise error. \ud83e\udd13</p> <p>In the cell below, we are going to define <code>dp_args</code> inside the <code>training_args</code> dictionary. Based on the given paremeters node will perform Opacus's differeantal privacy.</p> <ul> <li><p><code>noise_multiplier</code> - <code>sigma</code>: The ratio of the standard deviation of the Gaussian noise to the L2-sensitivity of the function to which the noise is added (How much noise to add)</p> </li> <li><p><code>max_grad_norm</code> - <code>clip</code>: The maximum norm of the per-sample gradients. Any gradient with norm higher than this will be clipped to this value.</p> </li> <li><p><code>type</code>: Differential privacy type as one of <code>local</code> or <code>central</code></p> </li> </ul> In\u00a0[2]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'batch_size': 48,\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 1, \n    'dry_run': False, \n    'dp_args': # DP Arguments for differential privacy\n        {\n            \"type\": \"local\", \n            \"sigma\": 0.4, \n            \"clip\": 0.005\n        },\n    'batch_maxnum': 50 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n</pre> model_args = {}  training_args = {     'batch_size': 48,     'optimizer_args': {         'lr': 1e-3     },     'epochs': 1,      'dry_run': False,      'dp_args': # DP Arguments for differential privacy         {             \"type\": \"local\",              \"sigma\": 0.4,              \"clip\": 0.005         },     'batch_maxnum': 50 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MNIST', '#dataset']\nrounds = 3\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['#MNIST', '#dataset'] rounds = 3  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) <p>Let's start the experiment.</p> <p>By default, this function doesn't stop until all the <code>rounds</code> are done for all the nodes</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() <p>Local training results for each round and each node are available in <code>exp.training_replies()</code> (index 0 to (<code>rounds</code> - 1) ).</p> <p>For example, you can view the training results for the last round below.</p> <p>Different timings (in seconds) are reported for each dataset of a node participating in a round :</p> <ul> <li><code>rtime_training</code> real time (clock time) spent in the training function on the node</li> <li><code>ptime_training</code> process time (user and system CPU) spent in the training function on the node</li> <li><code>rtime_total</code> real time (clock time) spent in the researcher between sending the request and handling the response, at the <code>Job()</code> layer</li> </ul> In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the training rounds : \", exp.training_replies().keys())\n\nprint(\"\\nList the nodes for the last training round and their timings : \")\nround_data = exp.training_replies()[rounds - 1].data()\nfor c in range(len(round_data)):\n    print(\"\\t- {id} :\\\n\\n\\t\\trtime_training={rtraining:.2f} seconds\\\n\\n\\t\\tptime_training={ptraining:.2f} seconds\\\n\\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n        rtraining = round_data[c]['timing']['rtime_training'],\n        ptraining = round_data[c]['timing']['ptime_training'],\n        rtotal = round_data[c]['timing']['rtime_total']))\nprint('\\n')\n    \nexp.training_replies()[rounds - 1].dataframe\n</pre> print(\"\\nList the training rounds : \", exp.training_replies().keys())  print(\"\\nList the nodes for the last training round and their timings : \") round_data = exp.training_replies()[rounds - 1].data() for c in range(len(round_data)):     print(\"\\t- {id} :\\     \\n\\t\\trtime_training={rtraining:.2f} seconds\\     \\n\\t\\tptime_training={ptraining:.2f} seconds\\     \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],         rtraining = round_data[c]['timing']['rtime_training'],         ptraining = round_data[c]['timing']['ptime_training'],         rtotal = round_data[c]['timing']['rtime_total'])) print('\\n')      exp.training_replies()[rounds - 1].dataframe <p>Federated parameters for each round are available in <code>exp.aggregated_params()</code> (index 0 to (<code>rounds</code> - 1) ).</p> <p>For example you can view the federated parameters for the last round of the experiment :</p> In\u00a0[\u00a0]: Copied! <pre>print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n\nprint(\"\\nAccess the federated params for the last training round :\")\nprint(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path'])\nprint(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n</pre> print(\"\\nList the training rounds : \", exp.aggregated_params().keys())  print(\"\\nAccess the federated params for the last training round :\") print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path']) print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys()) <p>We define a little testing routine to extract the accuracy metrics on the testing dataset</p> In\u00a0[7]: Copied! <pre>import torch\nimport torch.nn.functional as F\n\n\ndef testing_Accuracy(model, data_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    device = 'cpu'\n\n    correct = 0\n    \n    with torch.no_grad():\n        for data, target in data_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n        pred = output.argmax(dim=1, keepdim=True)\n\n    test_loss /= len(data_loader.dataset)\n    accuracy = 100* correct/len(data_loader.dataset)\n\n    return(test_loss, accuracy)\n</pre> import torch import torch.nn.functional as F   def testing_Accuracy(model, data_loader):     model.eval()     test_loss = 0     correct = 0     device = 'cpu'      correct = 0          with torch.no_grad():         for data, target in data_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()          pred = output.argmax(dim=1, keepdim=True)      test_loss /= len(data_loader.dataset)     accuracy = 100* correct/len(data_loader.dataset)      return(test_loss, accuracy) In\u00a0[\u00a0]: Copied! <pre>from torchvision import datasets, transforms\nfrom fedbiomed.researcher.environ import environ\nimport os\n\nlocal_mnist = os.path.join(environ['TMP_DIR'], 'local_mnist')\n\ntransform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\ntest_set = datasets.MNIST(root = local_mnist, download = True, train = False, transform = transform)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)\n</pre> from torchvision import datasets, transforms from fedbiomed.researcher.environ import environ import os  local_mnist = os.path.join(environ['TMP_DIR'], 'local_mnist')  transform = transforms.Compose([             transforms.ToTensor(),             transforms.Normalize((0.1307,), (0.3081,))         ])  test_set = datasets.MNIST(root = local_mnist, download = True, train = False, transform = transform) test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True) In\u00a0[9]: Copied! <pre>fed_model = exp.training_plan().model()\nfed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n\nacc_federated = testing_Accuracy(fed_model, test_loader)\n\nprint('\\nAccuracy federated training:  {:.4f}'.format(acc_federated[1]))\n\nprint('\\nError federated training:  {:.4f}'.format(acc_federated[0]))\n</pre> fed_model = exp.training_plan().model() fed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])  acc_federated = testing_Accuracy(fed_model, test_loader)  print('\\nAccuracy federated training:  {:.4f}'.format(acc_federated[1]))  print('\\nError federated training:  {:.4f}'.format(acc_federated[0])) <pre>\nAccuracy federated training:  68.3900\n\nError federated training:  1.0612\n</pre>"},{"location":"tutorials/security/differential-privacy-with-opacus-on-fedbiomed/#using-differential-privacy-with-opacus-on-fed-biomed","title":"Using Differential Privacy with OPACUS on Fed-BioMed\u00b6","text":""},{"location":"tutorials/security/differential-privacy-with-opacus-on-fedbiomed/#setting-up-fed-biomed-environment","title":"Setting up Fed-BioMed Environment\u00b6","text":""},{"location":"tutorials/security/differential-privacy-with-opacus-on-fedbiomed/#start-the-network","title":"Start the network\u00b6","text":"<p>Before running this notebook, start the network with <code>./scripts/fedbiomed_run network</code></p>"},{"location":"tutorials/security/differential-privacy-with-opacus-on-fedbiomed/#setting-the-node-up","title":"Setting the node up\u00b6","text":"<p>It is necessary to previously configure a node: 2. Check that your data has been added by executing <code>{FEDBIOMED_DIR}/scripts/fedbiomed_run node list</code></p> <ol> <li><code>{FEDBIOMED_DIR}/scripts/fedbiomed_run node add</code></li> </ol> <ul> <li>Select option 2 (default)</li> <li>Confirm default tags by hitting \"y\" and ENTER</li> <li>Pick the folder where MNIST is downloaded (this is due torch issue https://github.com/pytorch/vision/issues/3549)</li> <li>Data must have been added (if you get a warning saying that data must be unique is because it's been already added)</li> </ul> <ol> <li>Run the node using <code>{FEDBIOMED_DIR}/scripts/fedbiomed_run node start</code>. Wait until you get <code>Starting task manager</code>. it means you are online.</li> </ol>"},{"location":"tutorials/security/differential-privacy-with-opacus-on-fedbiomed/#defining-a-training-plan-and-parameters","title":"Defining a Training Plan and Parameters\u00b6","text":""},{"location":"tutorials/security/differential-privacy-with-opacus-on-fedbiomed/#declare-and-run-the-experiment","title":"Declare and run the experiment\u00b6","text":""},{"location":"tutorials/security/differential-privacy-with-opacus-on-fedbiomed/#testing","title":"Testing\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/","title":"Local and Central DP with Fed-BioMed: MONAI 2d image registration","text":"<p>We are now ready to start the researcher environment with the command <code>source {FEDBIOMED_DIR}/scripts/fedbiomed_environment researcher</code>, and open the Jupyter notebook.</p> <p>We can first quesry the network for the mednist dataset. In this case, the nodes are sharing the respective partitions unsing the same tag <code>mednist</code>:</p> In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.requests import Requests\nreq = Requests()\nreq.list(verbose=True)\n</pre> from fedbiomed.researcher.requests import Requests req = Requests() req.list(verbose=True) <p>The code for network and data loader of the MONAI tutorial can now be deployed in Fed-BioMed. We first import the necessary modules from <code>fedbiomed</code> and <code>monai</code> libraries:</p> <p>We can now define the training plan. Note that we use the standard <code>TorchTrainingPlan</code> natively provided in Fed-BioMed. We reuse the <code>MedNISTDataset</code> data loader defined in the original MONAI tutorial, which is returned by the method <code>training_data</code>, which also implements the data parsing from the nodes <code>dataset_path</code>. We should also properly define the <code>training_routine</code>, following the MONAI tutorial. According to the MONAI tutorial, the model is the <code>GlobalNet</code> and the loss is <code>MSELoss</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport torch\nfrom torch.nn import MSELoss\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\nfrom typing import Union, List\n\n#from torch.utils.data import Dataset, DataLoader\nimport monai\nfrom monai.utils import set_determinism, first\nfrom monai.transforms import (\n    EnsureChannelFirstD,\n    Compose,\n    LoadImageD,\n    RandRotateD,\n    RandZoomD,\n    ScaleIntensityRanged,\n    EnsureTypeD,\n)\nfrom monai.data import DataLoader, Dataset, CacheDataset\nfrom monai.config import print_config, USE_COMPILED\nfrom monai.networks.nets import GlobalNet\nfrom monai.networks.blocks import Warp\nfrom monai.apps import MedNISTDataset\n\n\n# Here we define the training plan to be used. \nclass MyTrainingPlan(TorchTrainingPlan):\n        \n    # Dependencies for training plan\n    def init_dependencies(self):\n        deps = [\"import numpy as np\",\n            \"import monai\",\n            \"from torch.nn import MSELoss\",\n            \"from monai.utils import set_determinism, first\",\n            \"from monai.transforms import (EnsureChannelFirstD,Compose,LoadImageD,RandRotateD,RandZoomD,ScaleIntensityRanged,EnsureTypeD,)\",\n            \"from monai.data import DataLoader, Dataset, CacheDataset\",\n            \"from monai.networks.nets import GlobalNet\",\n            \"from monai.config import USE_COMPILED\",\n            \"from monai.networks.blocks import Warp\",\n            \"from monai.apps import MedNISTDataset\" ]\n        return deps \n    \n    # Model for training\n    def init_model(self):\n        \n        # Define model related attributes \n        self.image_loss = MSELoss()\n        if USE_COMPILED:\n            self.warp_layer = Warp(3, \"border\")\n        else:\n            self.warp_layer = Warp(\"bilinear\", \"border\")\n        \n        # Define model \n        model = GlobalNet(image_size=(64, 64),\n                          spatial_dims=2,\n                          in_channels=2,  # moving and fixed\n                          num_channel_initial=16,\n                          depth=3)\n        \n        return model \n    \n    # Optimizer for training\n    def init_optimizer(self, optimizer_args):\n        optimizer = torch.optim.Adam(self.model().parameters(), lr=optimizer_args[\"lr\"])\n        \n        return optimizer\n\n\n    def training_data(self, batch_size = 20):\n        # Custom torch Dataloader for MedNIST data\n        data_path = self.dataset_path\n        # The following line is needed if client structure does not contain the \"/MedNIST\" folder\n        MedNISTDataset.dataset_folder_name = \"\"\n        train_data = MedNISTDataset(root_dir=data_path, section=\"training\", download=False, transform=None)\n        training_datadict = [\n            {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}\n            for item in train_data.data if item[\"label\"] == 4  # label 4 is for xray hands\n        ]\n        train_transforms = Compose(\n            [\n                LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],\n                                     a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),\n                RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),\n                RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1,\n                          monaiprob=1.0, mode=\"bicubic\", align_corners=False),\n                EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),\n            ]\n        )\n        train_ds = CacheDataset(data=training_datadict, transform=train_transforms,\n                                cache_rate=1.0, num_workers=0)\n        dl = self.MednistDataLoader(train_ds)\n        \n        return DataManager(dl, batch_size=batch_size, shuffle=True, num_workers=0)\n    \n    def training_step(self, moving, fixed):\n        ddf = self.model().forward(torch.cat((moving, fixed), dim=1))\n        pred_image = self.warp_layer(moving, ddf)\n        loss = self.image_loss(pred_image, fixed)\n        return loss\n    \n    class MednistDataLoader(monai.data.Dataset):\n        # Custom DataLoader that inherits from monai's Dataset object\n        def __init__(self, dataset):\n            self.dataset = dataset\n\n        def __len__(self):\n            return len(self.dataset)\n\n        def __getitem__(self, idx):\n            return (self.dataset[idx][\"moving_hand\"],\n                    self.dataset[idx][\"fixed_hand\"])\n</pre> import os import numpy as np import torch from torch.nn import MSELoss import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms from typing import Union, List  #from torch.utils.data import Dataset, DataLoader import monai from monai.utils import set_determinism, first from monai.transforms import (     EnsureChannelFirstD,     Compose,     LoadImageD,     RandRotateD,     RandZoomD,     ScaleIntensityRanged,     EnsureTypeD, ) from monai.data import DataLoader, Dataset, CacheDataset from monai.config import print_config, USE_COMPILED from monai.networks.nets import GlobalNet from monai.networks.blocks import Warp from monai.apps import MedNISTDataset   # Here we define the training plan to be used.  class MyTrainingPlan(TorchTrainingPlan):              # Dependencies for training plan     def init_dependencies(self):         deps = [\"import numpy as np\",             \"import monai\",             \"from torch.nn import MSELoss\",             \"from monai.utils import set_determinism, first\",             \"from monai.transforms import (EnsureChannelFirstD,Compose,LoadImageD,RandRotateD,RandZoomD,ScaleIntensityRanged,EnsureTypeD,)\",             \"from monai.data import DataLoader, Dataset, CacheDataset\",             \"from monai.networks.nets import GlobalNet\",             \"from monai.config import USE_COMPILED\",             \"from monai.networks.blocks import Warp\",             \"from monai.apps import MedNISTDataset\" ]         return deps           # Model for training     def init_model(self):                  # Define model related attributes          self.image_loss = MSELoss()         if USE_COMPILED:             self.warp_layer = Warp(3, \"border\")         else:             self.warp_layer = Warp(\"bilinear\", \"border\")                  # Define model          model = GlobalNet(image_size=(64, 64),                           spatial_dims=2,                           in_channels=2,  # moving and fixed                           num_channel_initial=16,                           depth=3)                  return model           # Optimizer for training     def init_optimizer(self, optimizer_args):         optimizer = torch.optim.Adam(self.model().parameters(), lr=optimizer_args[\"lr\"])                  return optimizer       def training_data(self, batch_size = 20):         # Custom torch Dataloader for MedNIST data         data_path = self.dataset_path         # The following line is needed if client structure does not contain the \"/MedNIST\" folder         MedNISTDataset.dataset_folder_name = \"\"         train_data = MedNISTDataset(root_dir=data_path, section=\"training\", download=False, transform=None)         training_datadict = [             {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}             for item in train_data.data if item[\"label\"] == 4  # label 4 is for xray hands         ]         train_transforms = Compose(             [                 LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),                 EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),                 ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],                                      a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),                 RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),                 RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1,                           monaiprob=1.0, mode=\"bicubic\", align_corners=False),                 EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),             ]         )         train_ds = CacheDataset(data=training_datadict, transform=train_transforms,                                 cache_rate=1.0, num_workers=0)         dl = self.MednistDataLoader(train_ds)                  return DataManager(dl, batch_size=batch_size, shuffle=True, num_workers=0)          def training_step(self, moving, fixed):         ddf = self.model().forward(torch.cat((moving, fixed), dim=1))         pred_image = self.warp_layer(moving, ddf)         loss = self.image_loss(pred_image, fixed)         return loss          class MednistDataLoader(monai.data.Dataset):         # Custom DataLoader that inherits from monai's Dataset object         def __init__(self, dataset):             self.dataset = dataset          def __len__(self):             return len(self.dataset)          def __getitem__(self, idx):             return (self.dataset[idx][\"moving_hand\"],                     self.dataset[idx][\"fixed_hand\"]) <p>Finally we import the required modules for running any experiment</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage <p>We first train our model in a non-private way. We set the model and training parameters. In particular, we are going to perform 2 epochs over 3 rounds for this experiment. Moreover the training is performed on ~26% of the locally available training data. We are also trying to use GPU if available.</p> In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'batch_size': 16, \n    'optimizer_args': {\n        'lr': 1e-5\n    },\n    'use_gpu': True,\n    'epochs': 4, \n    'dry_run': False\n#    'batch_maxnum': 2,  # can be used to debugging to limit the number of batches per epoch\n#    'log_interval': 1,  # output a logging message every log_interval batches\n}\n\ntags =  ['#MEDNIST', '#dataset']\nrounds = 5\n</pre> model_args = {}  training_args = {     'batch_size': 16,      'optimizer_args': {         'lr': 1e-5     },     'use_gpu': True,     'epochs': 4,      'dry_run': False #    'batch_maxnum': 2,  # can be used to debugging to limit the number of batches per epoch #    'log_interval': 1,  # output a logging message every log_interval batches }  tags =  ['#MEDNIST', '#dataset'] rounds = 5 <p>The experiment can be now defined, by providing the <code>mednist</code> tag, and running the local training on nodes with training plan defined in <code>training_plan_path</code>, standard <code>aggregator</code> (FedAvg) and <code>client_selection_strategy</code> (all nodes used). Federated learning is going to be perfomed through 3 optimization rounds.</p> In\u00a0[\u00a0]: Copied! <pre>exp = Experiment(tags=tags,\n                 training_plan_class=MyTrainingPlan,\n                 model_args=model_args,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None\n                )\n</pre> exp = Experiment(tags=tags,                  training_plan_class=MyTrainingPlan,                  model_args=model_args,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None                 ) <p>Let's start the experiment.</p> <p>By default, this function doesn't stop until all the <code>round_limit</code> rounds are done for all the clients</p> In\u00a0[\u00a0]: Copied! <pre>exp.run()\n</pre> exp.run() In\u00a0[\u00a0]: Copied! <pre>import urllib.request\nresponse = urllib.request.urlopen('https://raw.githubusercontent.com/tensorflow/privacy/7eea74a6a1cf15e2d2bd890722400edd0e470db8/research/hyperparameters_2022/rdp_accountant.py')\nrdp_accountant = response.read()\nexec(rdp_accountant)\n\ndef get_iterations(target_delta, sigma, q, max_epsilon, max_N):\n\"\"\"Computes max number of iterations given budget parameters\n\n    Args:\n        target_delta: If not `None`, the delta for which we compute the corresponding epsilon.\n        sigma: sigma to be used in Gaussian DP mechanism\n        q: training sample ratio\n        max_epsilon: Maximum budget allowed\n         max_N: Maximum number of iterations\n\n    Returns:\n        An integer number of iterations, and the evolution of the budget\n    Raises:\n        ValueError: If target_eps and target_delta are messed up.\n    \"\"\"\n\n    orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n    rdp = compute_rdp(q=q,\n                      noise_multiplier=sigma,\n                      steps=1,\n                      orders=orders)\n    epsilon_range = [get_privacy_spent(orders, i * rdp, target_delta=target_delta) for i in range(max_N)]\n    max_training_steps = int(np.sum(np.array([x[0] for x in epsilon_range]) &lt; max_epsilon))\n    return max_training_steps, [x[0] for x in epsilon_range][:max_training_steps]\n</pre> import urllib.request response = urllib.request.urlopen('https://raw.githubusercontent.com/tensorflow/privacy/7eea74a6a1cf15e2d2bd890722400edd0e470db8/research/hyperparameters_2022/rdp_accountant.py') rdp_accountant = response.read() exec(rdp_accountant)  def get_iterations(target_delta, sigma, q, max_epsilon, max_N):     \"\"\"Computes max number of iterations given budget parameters      Args:         target_delta: If not `None`, the delta for which we compute the corresponding epsilon.         sigma: sigma to be used in Gaussian DP mechanism         q: training sample ratio         max_epsilon: Maximum budget allowed          max_N: Maximum number of iterations      Returns:         An integer number of iterations, and the evolution of the budget     Raises:         ValueError: If target_eps and target_delta are messed up.     \"\"\"      orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))     rdp = compute_rdp(q=q,                       noise_multiplier=sigma,                       steps=1,                       orders=orders)     epsilon_range = [get_privacy_spent(orders, i * rdp, target_delta=target_delta) for i in range(max_N)]     max_training_steps = int(np.sum(np.array([x[0] for x in epsilon_range]) &lt; max_epsilon))     return max_training_steps, [x[0] for x in epsilon_range][:max_training_steps] <p>In order to perform DP training (both local and central) we need to provide to the model and training schemes:</p> <ul> <li><code>clip</code>: defining the maximal L2 norm of gradients</li> <li><code>sigma</code>: defining the strength of Gaussian noise to be added (either to gradients in case of LDP or to the final local model in case of CDP)</li> </ul> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.requests import Requests\nreq = Requests()\nquery_nodes = req.list()\n</pre> from fedbiomed.researcher.requests import Requests req = Requests() query_nodes = req.list() In\u00a0[\u00a0]: Copied! <pre>query_nodes\n</pre> query_nodes In\u00a0[\u00a0]: Copied! <pre>min_dataset_size = min([dataset['shape'][0] for i in query_nodes for dataset in query_nodes[i] if dataset['tags'] == ['#MEDNIST', '#dataset']]) #see training data in model\ntot_dataset_size = sum([dataset['shape'][0] for i in query_nodes for dataset in query_nodes[i] if dataset['tags'] == ['#MEDNIST', '#dataset']]) #see training data in model\n</pre> min_dataset_size = min([dataset['shape'][0] for i in query_nodes for dataset in query_nodes[i] if dataset['tags'] == ['#MEDNIST', '#dataset']]) #see training data in model tot_dataset_size = sum([dataset['shape'][0] for i in query_nodes for dataset in query_nodes[i] if dataset['tags'] == ['#MEDNIST', '#dataset']]) #see training data in model In\u00a0[\u00a0]: Copied! <pre>q = training_args['batch_size']/min_dataset_size\nsigma = 0.4\nclip = 0.005\ndelta = .1/min_dataset_size\nmax_epsilon = 10.\nmax_N = int(1e2)\n</pre> q = training_args['batch_size']/min_dataset_size sigma = 0.4 clip = 0.005 delta = .1/min_dataset_size max_epsilon = 10. max_N = int(1e2) In\u00a0[\u00a0]: Copied! <pre>N, eps_list = get_iterations(delta, sigma, q, max_epsilon, max_N)\n</pre> N, eps_list = get_iterations(delta, sigma, q, max_epsilon, max_N) In\u00a0[\u00a0]: Copied! <pre>max_rounds = N/(training_args['epochs'])\n</pre> max_rounds = N/(training_args['epochs']) In\u00a0[\u00a0]: Copied! <pre>assert training_args['epochs']*rounds&lt;=max_rounds, 'Number of rounds not compatible with privacy budget'\n\nprint(f'The maximal number of FL rounds for ({max_epsilon},{delta})-LDP training is {max_rounds}')\nprint('The selected number of FL rounds, '+str(rounds)+\n      ',implies ('+str(eps_list[training_args['epochs']*rounds-1])+','+str(delta)+',)-LDP')\n</pre> assert training_args['epochs']*rounds&lt;=max_rounds, 'Number of rounds not compatible with privacy budget'  print(f'The maximal number of FL rounds for ({max_epsilon},{delta})-LDP training is {max_rounds}') print('The selected number of FL rounds, '+str(rounds)+       ',implies ('+str(eps_list[training_args['epochs']*rounds-1])+','+str(delta)+',)-LDP') <p>We are now going to repeat the same training but with private SGD: at each epoch gradients are clipped and perturbed according to the provided privacy parameters.</p> <p>In order to perform DP-training we should provide an additional argument to training: the dictionalry <code>'DP_args'</code> containing necessary parameters for DP. If we want to perform LDP, we should specify: <code>'type' : 'local'</code>.</p> In\u00a0[\u00a0]: Copied! <pre>model_args = {}\nLDP = {'dp_args': {'type' : 'local', 'sigma': sigma, 'clip': clip}}\ntraining_args.update(LDP)\ntraining_args\n</pre> model_args = {} LDP = {'dp_args': {'type' : 'local', 'sigma': sigma, 'clip': clip}} training_args.update(LDP) training_args In\u00a0[\u00a0]: Copied! <pre>exp_LDP = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None\n                )\n</pre> exp_LDP = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None                 ) In\u00a0[\u00a0]: Copied! <pre>exp_LDP.run()\n</pre> exp_LDP.run() In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nnum_clients = len([dataset['shape'][0] for i in query_nodes for dataset in query_nodes[i] if dataset['tags'] == tags])\n\n# Here we use the same parameters as LDP to evaluate the number of rounds, \n# since we are performing record-level DP\n\nq = training_args['batch_size']/min_dataset_size \nsigma = 0.4#/(np.sqrt(num_clients)*training_args['batch_size'])\nclip = 0.005\ndelta = .1/min_dataset_size\nmax_epsilon = 10.\nmax_N = int(1e2)\n\nN, eps_list = get_iterations(delta, sigma, q, max_epsilon, max_N)\n</pre> import numpy as np num_clients = len([dataset['shape'][0] for i in query_nodes for dataset in query_nodes[i] if dataset['tags'] == tags])  # Here we use the same parameters as LDP to evaluate the number of rounds,  # since we are performing record-level DP  q = training_args['batch_size']/min_dataset_size  sigma = 0.4#/(np.sqrt(num_clients)*training_args['batch_size']) clip = 0.005 delta = .1/min_dataset_size max_epsilon = 10. max_N = int(1e2)  N, eps_list = get_iterations(delta, sigma, q, max_epsilon, max_N) In\u00a0[\u00a0]: Copied! <pre>max_rounds = N/(training_args['epochs'])\nprint(max_rounds)\n</pre> max_rounds = N/(training_args['epochs']) print(max_rounds) In\u00a0[\u00a0]: Copied! <pre>assert rounds&lt;=max_rounds, 'Number of rounds not compatible with privacy budget'\n\nprint(f'The maximal number of allowed rounds for ({max_epsilon},{delta})-CDP training is {max_rounds}')\nprint(f'The selected number of training rounds, '+str(rounds)+\n      ',implies ('+str(eps_list[rounds-1])+','+str(delta)+',)-CDP')\n</pre> assert rounds&lt;=max_rounds, 'Number of rounds not compatible with privacy budget'  print(f'The maximal number of allowed rounds for ({max_epsilon},{delta})-CDP training is {max_rounds}') print(f'The selected number of training rounds, '+str(rounds)+       ',implies ('+str(eps_list[rounds-1])+','+str(delta)+',)-CDP') <p>If we want to perform CDP, we should update the <code>'DP_args'</code> dictionary by setting:  <code>'type' : 'central'</code>. Otherwise we are going to keep the same privacy parameters.</p> In\u00a0[\u00a0]: Copied! <pre>CDP = {'dp_args': {'type' : 'central', 'sigma': sigma/np.sqrt(num_clients), 'clip': clip}}\ntraining_args.update(CDP)\ntraining_args\n</pre> CDP = {'dp_args': {'type' : 'central', 'sigma': sigma/np.sqrt(num_clients), 'clip': clip}} training_args.update(CDP) training_args In\u00a0[\u00a0]: Copied! <pre>exp_CDP = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None\n                )\n</pre> exp_CDP = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None                 ) In\u00a0[\u00a0]: Copied! <pre>exp_CDP.run()\n</pre> exp_CDP.run() <p>We are now going to test and compare locally the three final federated models on an independent testing partition. The test dataset is available at this link:</p> <p>https://drive.google.com/file/d/1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD/</p> In\u00a0[\u00a0]: Copied! <pre>!pip install matplotlib -q\n!pip install gdown -q\n</pre> !pip install matplotlib -q !pip install gdown -q In\u00a0[\u00a0]: Copied! <pre>import os\nimport tempfile\nimport PIL\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gdown\nimport zipfile\nimport matplotlib.pyplot as plt\n\nprint_config()\nset_determinism(42)\n</pre> import os import tempfile import PIL import torch import numpy as np import matplotlib.pyplot as plt import gdown import zipfile import matplotlib.pyplot as plt  print_config() set_determinism(42) <p>Download the testing dataset on the local temporary folder.</p> In\u00a0[\u00a0]: Copied! <pre>import gdown\nimport zipfile\nimport tempfile\nimport os\nfrom fedbiomed.researcher.environ import environ\n\ntmp_dir = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+os.sep)\n\nresource = \"https://drive.google.com/uc?id=1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD\"\nbase_dir = tmp_dir.name\ntest_file = os.path.join(base_dir, \"MedNIST_testing.zip\")\n\ngdown.download(resource, test_file, quiet=False)\n\nzf = zipfile.ZipFile(test_file)\n\nfor file in zf.infolist():\n    zf.extract(file, base_dir)\n    \ndata_dir = os.path.join(base_dir, \"MedNIST_testing\")\n</pre> import gdown import zipfile import tempfile import os from fedbiomed.researcher.environ import environ  tmp_dir = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+os.sep)  resource = \"https://drive.google.com/uc?id=1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD\" base_dir = tmp_dir.name test_file = os.path.join(base_dir, \"MedNIST_testing.zip\")  gdown.download(resource, test_file, quiet=False)  zf = zipfile.ZipFile(test_file)  for file in zf.infolist():     zf.extract(file, base_dir)      data_dir = os.path.join(base_dir, \"MedNIST_testing\") <p>We redefine our custom dataloader (defined previously in  the <code>TrainingPlan</code>):</p> In\u00a0[\u00a0]: Copied! <pre>from monai.data import DataLoader, Dataset, CacheDataset\nimport monai\n\nclass MednistDataLoader(monai.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        return (self.dataset[idx][\"moving_hand\"],\n                self.dataset[idx][\"fixed_hand\"])\n</pre> from monai.data import DataLoader, Dataset, CacheDataset import monai  class MednistDataLoader(monai.data.Dataset):     def __init__(self, dataset):         self.dataset = dataset      def __len__(self):         return len(self.dataset)      def __getitem__(self, idx):         return (self.dataset[idx][\"moving_hand\"],                 self.dataset[idx][\"fixed_hand\"]) <p>Create the testing data loader and pairs of moving vs fixed hands:</p> In\u00a0[\u00a0]: Copied! <pre># Use a GPU if you have one + enough memory available\n#\n#use_cuda = torch.cuda.is_available()\n#device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\ndevice = 'cpu'\n\n\n# recreate model\nmodel = GlobalNet(\n    image_size=(64, 64),\n    spatial_dims=2,\n    in_channels=2,  # moving and fixed\n    num_channel_initial=16,\n    depth=3).to(device)\n\nif USE_COMPILED:\n    warp_layer = Warp(3, \"border\").to(device)\nelse:\n    warp_layer = Warp(\"bilinear\", \"border\").to(device)\n\nMedNISTDataset.dataset_folder_name = \"\"\ntest_data = MedNISTDataset(root_dir=data_dir, section=\"test\", download=False, transform=None)\ntesting_datadict = [\n    {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}\n    for item in test_data.data if item[\"label\"] == 4  # label 4 is for xray hands\n]\ntest_transforms = Compose(\n            [\n                LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),\n                ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],\n                                     a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),\n                RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),\n                RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1, prob=1.0, mode=\"bicubic\", align_corners=False),\n                EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),\n            ]\n        )\nval_ds = CacheDataset(data=testing_datadict[:1000], transform=test_transforms,\n                      cache_rate=1.0, num_workers=0)\nval_dl = MednistDataLoader(val_ds)\nval_loader = DataLoader(val_dl, batch_size=16, num_workers=0)\n</pre> # Use a GPU if you have one + enough memory available # #use_cuda = torch.cuda.is_available() #device = torch.device(\"cuda:0\" if use_cuda else \"cpu\") device = 'cpu'   # recreate model model = GlobalNet(     image_size=(64, 64),     spatial_dims=2,     in_channels=2,  # moving and fixed     num_channel_initial=16,     depth=3).to(device)  if USE_COMPILED:     warp_layer = Warp(3, \"border\").to(device) else:     warp_layer = Warp(\"bilinear\", \"border\").to(device)  MedNISTDataset.dataset_folder_name = \"\" test_data = MedNISTDataset(root_dir=data_dir, section=\"test\", download=False, transform=None) testing_datadict = [     {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}     for item in test_data.data if item[\"label\"] == 4  # label 4 is for xray hands ] test_transforms = Compose(             [                 LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),                 EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),                 ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],                                      a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),                 RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),                 RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1, prob=1.0, mode=\"bicubic\", align_corners=False),                 EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),             ]         ) val_ds = CacheDataset(data=testing_datadict[:1000], transform=test_transforms,                       cache_rate=1.0, num_workers=0) val_dl = MednistDataLoader(val_ds) val_loader = DataLoader(val_dl, batch_size=16, num_workers=0) <p>To test the federated models we need to create model instances and assign to it the models parameters estimated at the last federated optimization rounds. Then, we generate predictions of the transformation between pairs. In addition, we evaluate the structural similarity index for each model.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install torchmetrics -q\n\nfrom torchmetrics.functional import structural_similarity_index_measure\n\n# Non private training\nmodel = exp.training_plan().model()\nmodel.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n\n# training with LDP\nmodel_LDP = exp_LDP.training_plan().model()\nmodel_LDP.load_state_dict(exp_LDP.aggregated_params()[rounds - 1]['params'])\n\n# training with CDP\nmodel_CDP = exp_CDP.training_plan().model()\nmodel_CDP.load_state_dict(exp_CDP.aggregated_params()[rounds - 1]['params'])\n\nfor moving, fixed in val_loader:\n    # Non private training\n    ddf = model(torch.cat((moving, fixed), dim=1))\n    pred_image = warp_layer(moving, ddf)\n    \n    # training with LDP\n    ddf_LDP = model_LDP(torch.cat((moving, fixed), dim=1))\n    pred_image_LDP = warp_layer(moving, ddf_LDP)\n    \n    # training with CDP\n    ddf_CDP = model_CDP(torch.cat((moving, fixed), dim=1))\n    pred_image_CDP = warp_layer(moving, ddf_CDP)\n    \n    # ssim predicted vs ground truth\n    # Non private training\n    SSIM = structural_similarity_index_measure(pred_image, fixed)\n    # training with LDP\n    SSIM_LDP = structural_similarity_index_measure(pred_image_LDP, fixed)\n    # training with CDP\n    SSIM_CDP = structural_similarity_index_measure(pred_image_CDP, fixed)\n    \n    break\n\nfixed_image = fixed.detach().cpu().numpy()[:, 0]\nmoving_image = moving.detach().cpu().numpy()[:, 0]\npred_image = pred_image.detach().cpu().numpy()[:, 0]\npred_image_LDP = pred_image_LDP.detach().cpu().numpy()[:, 0]\npred_image_CDP = pred_image_CDP.detach().cpu().numpy()[:, 0]\n</pre> !pip install torchmetrics -q  from torchmetrics.functional import structural_similarity_index_measure  # Non private training model = exp.training_plan().model() model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])  # training with LDP model_LDP = exp_LDP.training_plan().model() model_LDP.load_state_dict(exp_LDP.aggregated_params()[rounds - 1]['params'])  # training with CDP model_CDP = exp_CDP.training_plan().model() model_CDP.load_state_dict(exp_CDP.aggregated_params()[rounds - 1]['params'])  for moving, fixed in val_loader:     # Non private training     ddf = model(torch.cat((moving, fixed), dim=1))     pred_image = warp_layer(moving, ddf)          # training with LDP     ddf_LDP = model_LDP(torch.cat((moving, fixed), dim=1))     pred_image_LDP = warp_layer(moving, ddf_LDP)          # training with CDP     ddf_CDP = model_CDP(torch.cat((moving, fixed), dim=1))     pred_image_CDP = warp_layer(moving, ddf_CDP)          # ssim predicted vs ground truth     # Non private training     SSIM = structural_similarity_index_measure(pred_image, fixed)     # training with LDP     SSIM_LDP = structural_similarity_index_measure(pred_image_LDP, fixed)     # training with CDP     SSIM_CDP = structural_similarity_index_measure(pred_image_CDP, fixed)          break  fixed_image = fixed.detach().cpu().numpy()[:, 0] moving_image = moving.detach().cpu().numpy()[:, 0] pred_image = pred_image.detach().cpu().numpy()[:, 0] pred_image_LDP = pred_image_LDP.detach().cpu().numpy()[:, 0] pred_image_CDP = pred_image_CDP.detach().cpu().numpy()[:, 0] In\u00a0[\u00a0]: Copied! <pre>print('---&gt; Results for non-private training')\nprint(f'SSIM = {SSIM}')\n\nprint('---&gt; Results for training with LDP')\nprint(f'SSIM = {SSIM_LDP})')\n\nprint('---&gt; Results for training with CDP')\nprint(f'SSIM = {SSIM_CDP})')\n</pre> print('---&gt; Results for non-private training') print(f'SSIM = {SSIM}')  print('---&gt; Results for training with LDP') print(f'SSIM = {SSIM_LDP})')  print('---&gt; Results for training with CDP') print(f'SSIM = {SSIM_CDP})') <p>Finally, we can print some example of predictions of all models from the testing dataset.</p> In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\nbatch_size = 10\nplt.subplots(batch_size, 5, figsize=(12, 25))\nfor b in range(batch_size):\n    # moving image\n    plt.subplot(batch_size, 5, b * 5 + 1)\n    plt.axis('off')\n    plt.title(\"moving image\")\n    plt.imshow(moving_image[b], cmap=\"gray\")\n    # fixed image\n    plt.subplot(batch_size, 5, b * 5 + 2)\n    plt.axis('off')\n    plt.title(\"fixed image\")\n    plt.imshow(fixed_image[b], cmap=\"gray\")\n    # warped moving\n    plt.subplot(batch_size, 5, b * 5 + 3)\n    plt.axis('off')\n    plt.title(\"predicted image\")\n    plt.imshow(pred_image[b], cmap=\"gray\")\n    # warped moving LDP\n    plt.subplot(batch_size, 5, b * 5 + 4)\n    plt.axis('off')\n    plt.title(\"predicted image (LDP)\")\n    plt.imshow(pred_image_LDP[b], cmap=\"gray\")\n    # warped moving CDP\n    plt.subplot(batch_size, 5, b * 5 + 5)\n    plt.axis('off')\n    plt.title(\"predicted image (CDP)\")\n    plt.imshow(pred_image_CDP[b], cmap=\"gray\")\nplt.axis('off')\nplt.show()\n</pre> %matplotlib inline batch_size = 10 plt.subplots(batch_size, 5, figsize=(12, 25)) for b in range(batch_size):     # moving image     plt.subplot(batch_size, 5, b * 5 + 1)     plt.axis('off')     plt.title(\"moving image\")     plt.imshow(moving_image[b], cmap=\"gray\")     # fixed image     plt.subplot(batch_size, 5, b * 5 + 2)     plt.axis('off')     plt.title(\"fixed image\")     plt.imshow(fixed_image[b], cmap=\"gray\")     # warped moving     plt.subplot(batch_size, 5, b * 5 + 3)     plt.axis('off')     plt.title(\"predicted image\")     plt.imshow(pred_image[b], cmap=\"gray\")     # warped moving LDP     plt.subplot(batch_size, 5, b * 5 + 4)     plt.axis('off')     plt.title(\"predicted image (LDP)\")     plt.imshow(pred_image_LDP[b], cmap=\"gray\")     # warped moving CDP     plt.subplot(batch_size, 5, b * 5 + 5)     plt.axis('off')     plt.title(\"predicted image (CDP)\")     plt.imshow(pred_image_CDP[b], cmap=\"gray\") plt.axis('off') plt.show()"},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#local-and-central-dp-with-fed-biomed-monai-2d-image-registration","title":"Local and Central DP with Fed-BioMed: MONAI 2d image registration\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial shows how to deploy in Fed-BioMed the 2d image registration example provided in the project MONAI (https://monai.io/), trained with Differential Privacy (DP). We are going to compare results of:</p> <ul> <li>non private training</li> <li>train with Local Differential Privacy (LDP)</li> <li>train with Central Differential Privacy (CDP)</li> </ul> <p>In order to enforce differential privacy during training (both local and central) we will rely on the Opacus library (https://opacus.ai/).</p>"},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#image-registration","title":"Image Registration\u00b6","text":"<p>Image registration is the process of transforming and recalibrating different images into one coordinate system. It makes possible to compare several images captured with the same modality.</p> <p>In this tutorial, we are using a UNet-like registration network ( https://arxiv.org/abs/1711.01666 ). Goal of the notebook is to train a model given moving images and fixed images (recalibrated images).</p>"},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#start-the-network","title":"Start the network\u00b6","text":"<p>Before running this notebook, start the network with <code>./scripts/fedbiomed_run network</code></p>"},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#creating-mednist-nodes","title":"Creating MedNIST nodes\u00b6","text":"<p>MedNIST provides an artificial 2d classification dataset created by gathering different medical imaging datasets from TCIA, the RSNA Bone Age Challenge, and the NIH Chest X-ray dataset. The dataset is kindly made available by Dr. Bradley J. Erickson M.D., Ph.D. (Department of Radiology, Mayo Clinic) under the Creative Commons CC BY-SA 4.0 license.</p> <p>To proceed with the tutorial, we created an iid partitioning of the MedNIST dataset between 3 clients. Each client has 3000 image samples for each class. The training partitions are availables at the following link:</p> <p>https://drive.google.com/file/d/1vLIcBdtdAhh6K-vrgCFy_0Y55dxOWZwf/view</p> <p>The dataset owned by each client has structure:</p> <p>\u2514\u2500\u2500 client_*/</p> <pre><code>\u251c\u2500\u2500 AbdomenCT/\n\n\u2514\u2500\u2500 BreastMRI/\n\n\u2514\u2500\u2500 CXR/\n\n\u2514\u2500\u2500 ChestCT/\n\n\u2514\u2500\u2500 Hand/\n\n\u2514\u2500\u2500 HeadCT/   </code></pre> <p>To create the federated dataset, we follow the standard procedure for node creation/population of Fed-BioMed. After activating the fedbiomed network with the commands</p> <p><code>source {FEDBIOMED_DIR}/scripts/fedbiomed_environment network</code></p> <p>and</p> <p><code>{FEDBIOMED_DIR}/scripts/fedbiomed_run network</code></p> <p>we create a first node by using the commands</p> <p><code>source ./scripts/fedbiomed_environment node</code></p> <p><code>{FEDBIOMED_DIR}/scripts/fedbiomed_run node start</code></p> <p>We then poulate the node with the data of first client:</p> <p><code>{FEDBIOMED_DIR}/scripts/fedbiomed_run node add</code></p> <p>We select option 3 (images) to add MedNIST partition of client 1, by just picking the folder of client 1. Assign tag <code>mednist</code> to the data when asked.</p> <p>We can further check that the data has been added by executing <code>{FEDBIOMED_DIR}/scripts/fedbiomed_run node list</code></p> <p>Following the same procedure, we create the other two nodes with the datasets of client 2 and client 3 respectively.</p>"},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#running-fed-biomed-researcher","title":"Running Fed-BioMed Researcher\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#create-an-experiment-to-train-a-model-on-the-data-found","title":"Create an experiment to train a model on the data found\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#non-private-training","title":"Non-private training\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#training-with-dp","title":"Training with DP\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#download-and-execute-rdp-accountant-module","title":"Download and execute RDP Accountant Module\u00b6","text":"<p>Following actions will download RDP module to calculate privacy budget and create a function called <code>get_iterations</code> which is going to be used for calculating the number training iterations that respects the privacy budget. The result of the function will be used for finding max number of rounds for the experiment.</p>"},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#dp-parameters","title":"DP parameters\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#ldp","title":"LDP\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#dimensioning-the-training-parameters-with-ldp","title":"Dimensioning the training parameters with LDP\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#update-training-parameters-for-ldp","title":"Update training parameters for LDP\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#declare-and-run-the-ldp-training","title":"Declare and run the LDP training\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#cdp","title":"CDP\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#dimensioning-the-training-parameters-with-cdp","title":"Dimensioning the training parameters with CDP\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#update-training-parameters-for-cdp","title":"Update training parameters for CDP\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#declare-and-run-the-cdp-training","title":"Declare and run the CDP training\u00b6","text":""},{"location":"tutorials/security/non-private-local-central-dp-monai-2d-image-registration/#testing","title":"Testing\u00b6","text":""},{"location":"tutorials/security/secure-aggregation/","title":"Training with Secure Aggregation","text":"<p>Declare a torch training plan MyTrainingPlan class to send for training on the node</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\n\n# Here we define the model to be used. \n# You can use any class name (here 'Net')\nclass MyTrainingPlan(TorchTrainingPlan):\n    \n    # Defines and return model \n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n    \n    # Defines and return optimizer\n    def init_optimizer(self, optimizer_args):\n        return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n    \n    # Declares and return dependencies\n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\"]\n        return deps\n    \n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            self.fc1 = nn.Linear(9216, 128)\n            self.fc2 = nn.Linear(128, 10)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.dropout2(x)\n            x = self.fc2(x)\n\n\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_data(self, batch_size = 48):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset=dataset1, **train_kwargs)\n    \n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms   # Here we define the model to be used.  # You can use any class name (here 'Net') class MyTrainingPlan(TorchTrainingPlan):          # Defines and return model      def init_model(self, model_args):         return self.Net(model_args = model_args)          # Defines and return optimizer     def init_optimizer(self, optimizer_args):         return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])          # Declares and return dependencies     def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\"]         return deps          class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.conv1 = nn.Conv2d(1, 32, 3, 1)             self.conv2 = nn.Conv2d(32, 64, 3, 1)             self.dropout1 = nn.Dropout(0.25)             self.dropout2 = nn.Dropout(0.5)             self.fc1 = nn.Linear(9216, 128)             self.fc2 = nn.Linear(128, 10)          def forward(self, x):             x = self.conv1(x)             x = F.relu(x)             x = self.conv2(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = self.dropout1(x)             x = torch.flatten(x, 1)             x = self.fc1(x)             x = F.relu(x)             x = self.dropout2(x)             x = self.fc2(x)               output = F.log_softmax(x, dim=1)             return output      def training_data(self, batch_size = 48):         # Custom torch Dataloader for MNIST data         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         train_kwargs = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset=dataset1, **train_kwargs)          def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss  <p>This group of arguments correspond respectively:</p> <ul> <li><code>model_args</code>: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side.</li> <li><code>training_args</code>: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.</li> </ul> <p>NOTE: typos and/or lack of positional (required) arguments will raise error. \ud83e\udd13</p> In\u00a0[\u00a0]: Copied! <pre>model_args = {}\n\ntraining_args = {\n    'batch_size': 48, \n    'optimizer_args': {\n        \"lr\" : 1e-3\n    },\n    'epochs': 1, \n    'dry_run': False,  \n    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\n</pre> model_args = {}  training_args = {     'batch_size': 48,      'optimizer_args': {         \"lr\" : 1e-3     },     'epochs': 1,      'dry_run': False,       'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\nfrom fedbiomed.researcher.secagg import SecureAggregation\ntags =  ['#MNIST', '#dataset']\nrounds = 2\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None,\n                 secagg=True, # or custom SecureAggregation(active=&lt;bool&gt;, clipping_range=&lt;int&gt;, timeout=&lt;int&gt;)\n                 save_breakpoints=True)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage from fedbiomed.researcher.secagg import SecureAggregation tags =  ['#MNIST', '#dataset'] rounds = 2  exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None,                  secagg=True, # or custom SecureAggregation(active=, clipping_range=, timeout=)                  save_breakpoints=True) <p>Please use the attribute <code>secagg</code> to verify secure aggregation is set as active</p> In\u00a0[\u00a0]: Copied! <pre>print(\"Is using secagg: \", exp.secagg.active)\n</pre> print(\"Is using secagg: \", exp.secagg.active) <p>It is also possible to check secure aggregation context using <code>secagg</code> attribute. Since secure aggregation context negotiation will occur during experiment run, context and id should be <code>None</code> at this point.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"Secagg Biprime \", exp.secagg.biprime)\nprint(\"Secagg Servkey \", exp.secagg.servkey)\n</pre> print(\"Secagg Biprime \", exp.secagg.biprime) print(\"Secagg Servkey \", exp.secagg.servkey) <p>Run the experiment, using secure aggregation. Secure aggregation context will be created before the first training round, and it is going to be updated before each round when new nodes are added or removed to the experiment.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run(increase=True)\n</pre> exp.run(increase=True) <p>Display context after running one round of training.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"Secagg Biprime context: \", exp.secagg.biprime.context)\nprint(\"Secagg Servkey context: \", exp.secagg.servkey.context)\n</pre> print(\"Secagg Biprime context: \", exp.secagg.biprime.context) print(\"Secagg Servkey context: \", exp.secagg.servkey.context) In\u00a0[\u00a0]: Copied! <pre># sends new dataset search request\nfrom fedbiomed.researcher.strategies import DefaultStrategy\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\nexp.set_training_data(None, True)\nexp.set_strategy(DefaultStrategy)\nexp.set_aggregator(FedAverage)\nexp.set_job()\n</pre> # sends new dataset search request from fedbiomed.researcher.strategies import DefaultStrategy from fedbiomed.researcher.aggregators.fedavg import FedAverage exp.set_training_data(None, True) exp.set_strategy(DefaultStrategy) exp.set_aggregator(FedAverage) exp.set_job() In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) <p>Setting <code>secagg</code> argument <code>True</code> in <code>Experiment</code> creates a default <code>SecureAggregation</code> instance. Additionally, It is also possible to create <code>SecureAggregation</code> instance and pass it as an argument. Here are the arguments that can be set for the <code>SecureAggregation</code></p> <ul> <li><code>active</code>: <code>True</code> if the round will use secure aggregation. Default is <code>True</code></li> <li><code>clipping_range</code>: Clipping range that is going be use for quantization of model parameters. Default clipping range is <code>3</code>. However, some models can have model weigths greater than <code>3</code>. If clipping range is exceeded during the encryption on the nodes, <code>Experiment</code> will log a warning message. In such cases, you can provide a higher clipping range through the argument <code>clipping_range</code>.</li> <li><code>timeout</code>: Timeout is the maximum amount of time, in seconds, that the experiment will wait for responses from all parties during secure aggregation setup. Since secure aggregation context depends on network communication and multi-party computation, this argument allows setting higher timeout for larger context setups, or vice versa.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.secagg import SecureAggregation\nsecagg = SecureAggregation(\n    active=True, \n    clipping_range=100,\n    timeout=15\n    \n)\nexp.set_secagg(secagg=secagg)\n</pre> from fedbiomed.researcher.secagg import SecureAggregation secagg = SecureAggregation(     active=True,      clipping_range=100,     timeout=15      ) exp.set_secagg(secagg=secagg)  In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) In\u00a0[\u00a0]: Copied! <pre>loaded_exp = Experiment.load_breakpoint()\nloaded_exp.info()\n</pre> loaded_exp = Experiment.load_breakpoint() loaded_exp.info() In\u00a0[\u00a0]: Copied! <pre>loaded_exp.run_once(increase=True)\n</pre> loaded_exp.run_once(increase=True)"},{"location":"tutorials/security/secure-aggregation/#training-with-secure-aggregation","title":"Training with Secure Aggregation\u00b6","text":"<p>Secure aggregation is one of the security feature that is provided by Fed-BioMed. Please refer to secure aggregation user guide for more information regarding the methods and techniques that are used. This tutorial gives an example of secure aggregation usage in Fed-BioMed.</p>"},{"location":"tutorials/security/secure-aggregation/#setting-up-the-nodes","title":"Setting up the nodes\u00b6","text":"<p>During the tutorial, nodes and researcher will be launched locally using single clone of Fed-BioMed. However, it is also possible to execute notebook cells when the components are configured remotely by respecting following instruction.</p>"},{"location":"tutorials/security/secure-aggregation/#start-the-network","title":"Start the network\u00b6","text":"<p>Before running this notebook, start the network with <code>./scripts/fedbiomed_run network</code></p>"},{"location":"tutorials/security/secure-aggregation/#configuringinstalling-element-for-secure-aggregation","title":"Configuring/Installing  Element for Secure Aggregation\u00b6","text":"<p>You can follow the detailed instructions for configuring Fed-BioMed instance for secure aggregation or apply following shortened instructions for a basic setup.</p>"},{"location":"tutorials/security/secure-aggregation/#1-install-and-configure","title":"1. Install and configure\u00b6","text":"<p>Fed-BioMed uses MP-SPDZ for MPC. Therefore, please make sure that MP-SPDZ are installed and configured for Fed-BioMed by running following command.</p> <pre><code>${FEDBIOMED_DIR}/scripts/fedbiomed_confgiure_secagg node\n</code></pre> <p>Since node and researcher will be run in the same machine, single configuration for MP-SDPZ will enouhg </p>"},{"location":"tutorials/security/secure-aggregation/#2-create-node-and-researcher-instances","title":"2. Create node and researcher instances\u00b6","text":"<p>The setup for secure aggregation requires knowledge of the participating Fed-BioMed components in advance. Therefore, each component that will participate in the training should be created before starting them. Afterwards, participating components can be registered in every other component.</p>"},{"location":"tutorials/security/secure-aggregation/#21","title":"2.1\u00b6","text":"<p>It is mandatory to have at least two nodes for the experiment that requires secure aggregation. Please execute following commands to create two nodes.</p> <p>Node 1:</p> <pre><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini configuration create\n</code></pre> <p>Node 2:</p> <pre><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n2.ini configuration create\n</code></pre>"},{"location":"tutorials/security/secure-aggregation/#22-create-researcher","title":"2.2 Create researcher\u00b6","text":"<p>Please run the command below to create researcher component.</p> <pre><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run researcher configuration create\n</code></pre>"},{"location":"tutorials/security/secure-aggregation/#3-registering-participating-fed-biomed-instances","title":"3. Registering participating Fed-BioMed instances\u00b6","text":"<p>Normally, as it is mentioned in secure aggregation configuration each participating instance should register network credentials of others such as IP, port and SSL certificate. however, since this example will be run on single clone of Fed-BioMed, registration process can be done automaticity by running following command.</p> <pre><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run certicate-dev-setup\n</code></pre>"},{"location":"tutorials/security/secure-aggregation/#4-add-dataset-and-start-nodes","title":"4. Add dataset and start nodes\u00b6","text":"<p>The next step will be adding/deploying MNIST dataset in the nodes and starting them. For these step you can follow the instructions for adding dataset into nodes to add MNIST dataset. After the datasets are deployed you can start the nodes and researcher.</p>"},{"location":"tutorials/security/secure-aggregation/#define-an-experiment-model-and-parameters","title":"Define an experiment model and parameters\"\u00b6","text":""},{"location":"tutorials/security/secure-aggregation/#declare-and-run-the-experiment","title":"Declare and run the experiment\u00b6","text":""},{"location":"tutorials/security/secure-aggregation/#access-secure-aggregation-context","title":"Access secure aggregation context\u00b6","text":""},{"location":"tutorials/security/secure-aggregation/#changes-in-experiment-triggers-re-creation-of-secure-aggregation-context","title":"Changes in experiment triggers re-creation of secure aggregation context\u00b6","text":"<p>The changes that re-create jobs like adding new node to the experiment will trigger automatic secure aggregation re-setup for the next round.</p>"},{"location":"tutorials/security/secure-aggregation/#changing-arguments-of-secure-aggregation","title":"Changing arguments of secure aggregation\u00b6","text":""},{"location":"tutorials/security/secure-aggregation/#load-experiment-from-a-breakpoint","title":"Load experiment from a breakpoint\u00b6","text":"<p>Once a breakpoint is loadded if the context is already exsiting there won't be context setup.</p>"},{"location":"tutorials/security/training-with-approved-training-plans/","title":"Training Process with Training Plan Management","text":"<p>The following model is the model that will be sent to the node for training. Since the model files are processed by the Experiment to configure dependencies, import part of the final file might be different from this one.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\n\n# Here we define the training plan to be used. \nclass MyTrainingPlan(TorchTrainingPlan):\n    \n    # Defines and return model \n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n    \n    # Defines and return optimizer\n    def init_optimizer(self, optimizer_args):\n        return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n    \n    # Declares and return dependencies\n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\"]\n        return deps\n    \n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            self.fc1 = nn.Linear(9216, 128)\n            self.fc2 = nn.Linear(128, 10)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.dropout2(x)\n            x = self.fc2(x)\n\n\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_data(self, batch_size = 48):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        loader_arguments = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset=dataset1, **loader_arguments)\n    \n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms   # Here we define the training plan to be used.  class MyTrainingPlan(TorchTrainingPlan):          # Defines and return model      def init_model(self, model_args):         return self.Net(model_args = model_args)          # Defines and return optimizer     def init_optimizer(self, optimizer_args):         return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])          # Declares and return dependencies     def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\"]         return deps          class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.conv1 = nn.Conv2d(1, 32, 3, 1)             self.conv2 = nn.Conv2d(32, 64, 3, 1)             self.dropout1 = nn.Dropout(0.25)             self.dropout2 = nn.Dropout(0.5)             self.fc1 = nn.Linear(9216, 128)             self.fc2 = nn.Linear(128, 10)          def forward(self, x):             x = self.conv1(x)             x = F.relu(x)             x = self.conv2(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = self.dropout1(x)             x = torch.flatten(x, 1)             x = self.fc1(x)             x = F.relu(x)             x = self.dropout2(x)             x = self.fc2(x)               output = F.log_softmax(x, dim=1)             return output      def training_data(self, batch_size = 48):         # Custom torch Dataloader for MNIST data         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         loader_arguments = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset=dataset1, **loader_arguments)          def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss  <p>To be able to get/see the final model file we need to initialize the experiment.</p> In\u00a0[\u00a0]: Copied! <pre>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MNIST', '#dataset']\nrounds = 2\n\nmodel_args = {}\n\ntraining_args = {\n    'batch_size': 48, \n    'optimizer_args': {\n        \"lr\" : 1e-3\n    },\n    'epochs': 1, \n    'dry_run': False,  \n    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n}\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</pre> from fedbiomed.researcher.experiment import Experiment from fedbiomed.researcher.aggregators.fedavg import FedAverage  tags =  ['#MNIST', '#dataset'] rounds = 2  model_args = {}  training_args = {     'batch_size': 48,      'optimizer_args': {         \"lr\" : 1e-3     },     'epochs': 1,      'dry_run': False,       'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples } exp = Experiment(tags=tags,                  model_args=model_args,                  training_plan_class=MyTrainingPlan,                  training_args=training_args,                  round_limit=rounds,                  aggregator=FedAverage(),                  node_selection_strategy=None) In\u00a0[\u00a0]: Copied! <pre>exp.training_plan_file(display = True)\n</pre> exp.training_plan_file(display = True) <p>The <code>exp.check_training_plan_status()</code> sends request to the experiment's nodes to check whether the model is approved or not. The nodes that will receive the requests are the nodes that have been found after searching datasets.</p> In\u00a0[\u00a0]: Copied! <pre>status = exp.check_training_plan_status()\n</pre> status = exp.check_training_plan_status() In\u00a0[\u00a0]: Copied! <pre>status\n</pre> status In\u00a0[\u00a0]: Copied! <pre>exp.run_once()\n</pre> exp.run_once() <p>The logs should indicate that the training plan is approved. You can also get status object from the result of the <code>check_training_plan_status()</code>. It returns a list of status objects each for different node. Since we have only launched a single node, it returns only one status object.</p> <ul> <li><code>approval_obligation</code> : Indicates whether the training plan control is enabled in the node.</li> <li><code>status</code>         : Indicates training plan approval status.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\nfrom torchvision import datasets, transforms\n\n\n# Here we define the model to be used. \n# You can use any class name (here 'Net')\nclass MyTrainingPlan(TorchTrainingPlan):\n    \n    # Defines and return model \n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n    \n    # Defines and return optimizer\n    def init_optimizer(self, optimizer_args):\n        return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n    \n    # Declares and return dependencies\n    def init_dependencies(self):\n        deps = [\"from torchvision import datasets, transforms\"]\n        return deps\n    \n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 16, 5, 1, 2)\n            self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)\n            self.fc1 = nn.Linear(32 * 7 * 7, 10)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_data(self, batch_size = 48):\n        # Custom torch Dataloader for MNIST data\n        transform = transforms.Compose([transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))])\n        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset=dataset1, **train_kwargs)\n    \n    def training_step(self, data, target):\n        output = self.model().forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</pre> import torch import torch.nn as nn from fedbiomed.common.training_plans import TorchTrainingPlan from fedbiomed.common.data import DataManager from torchvision import datasets, transforms   # Here we define the model to be used.  # You can use any class name (here 'Net') class MyTrainingPlan(TorchTrainingPlan):          # Defines and return model      def init_model(self, model_args):         return self.Net(model_args = model_args)          # Defines and return optimizer     def init_optimizer(self, optimizer_args):         return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])          # Declares and return dependencies     def init_dependencies(self):         deps = [\"from torchvision import datasets, transforms\"]         return deps          class Net(nn.Module):         def __init__(self, model_args):             super().__init__()             self.conv1 = nn.Conv2d(1, 16, 5, 1, 2)             self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)             self.fc1 = nn.Linear(32 * 7 * 7, 10)          def forward(self, x):             x = self.conv1(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = self.conv2(x)             x = F.relu(x)             x = F.max_pool2d(x, 2)             x = torch.flatten(x, 1)             x = self.fc1(x)              output = F.log_softmax(x, dim=1)             return output      def training_data(self, batch_size = 48):         # Custom torch Dataloader for MNIST data         transform = transforms.Compose([transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,))])         dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)         train_kwargs = {'batch_size': batch_size, 'shuffle': True}         return DataManager(dataset=dataset1, **train_kwargs)          def training_step(self, data, target):         output = self.model().forward(data)         loss   = torch.nn.functional.nll_loss(output, target)         return loss <p>In the following cell, we update the training plan class using the setter <code>set_training_plan_class</code>. Afterwards, we need to update <code>job</code> by running <code>set_job</code> to update training plan that is going to be sent to the nodes.</p> In\u00a0[\u00a0]: Copied! <pre>exp.set_training_plan_class(MyTrainingPlan)\n# update job since model_path has been changed\nexp.set_job()\n</pre> exp.set_training_plan_class(MyTrainingPlan) # update job since model_path has been changed exp.set_job() <p>Since we changed the model/network structure (we removed dropouts and one dense layer <code>fc2</code>) in the experiment, the output of the following method should say that the training plan is not approved by the node and <code>is_approved</code> key of the result object should be equal to <code>False</code>.</p> In\u00a0[\u00a0]: Copied! <pre>status = exp.check_training_plan_status()\n</pre> status = exp.check_training_plan_status() In\u00a0[\u00a0]: Copied! <pre>exp.training_plan_file()\n</pre> exp.training_plan_file() In\u00a0[\u00a0]: Copied! <pre>status\n</pre> status <p>Since the training plan is not approved, you won't be able to train your model in the node. The following cell will return an error.</p> In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) In\u00a0[\u00a0]: Copied! <pre>exp.training_plan_approve(MyTrainingPlan, description=\"my new training plans\")\n</pre> exp.training_plan_approve(MyTrainingPlan, description=\"my new training plans\") <p>Once the training plan has been sent, we need to approve it (or reject it) on <code>Node</code> side.</p> <p>Before approving, optionally list models/training plans known to the node with their status (<code>Approved</code>, <code>Pending</code>, <code>Rejected</code>). Your new training plan should appear with <code>Pending</code> status and name <code>my new training plan</code>.</p> <pre>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini --list-training-plans\n</pre> <p>Then approve the training plan, using the following command on a new terminal:</p> <pre>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini --approve-training-plan\n</pre> <p>Training plans with both <code>Pending</code> or <code>Rejected</code> status will be displayed. Select the training plan you have sent to approve it. You might see a message explaining that training plan has successfully been approved.</p> <p>Optionally list again training plans known to the node with their status. Your training plan should now appear with <code>Approved</code> status.</p> <pre>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini --list-training-plans\n</pre> <p>Back on the <code>Researcher</code> side, let's check it status by running the <code>check_model_status</code> command:</p> In\u00a0[\u00a0]: Copied! <pre>exp.check_training_plan_status()\n</pre> exp.check_training_plan_status() <p>Model's status must have changed from <code>Pending</code> status to <code>Approved</code>, which means model can be trained from now on on the <code>Node</code>. <code>Researcher</code> can now run an <code>Experiment</code> on the <code>Node</code>!</p> In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) In\u00a0[\u00a0]: Copied! <pre>exp.training_plan_file()\n</pre> exp.training_plan_file() <p>The output of the <code>exp.training_plan_file()</code> is a file path that shows where the final training plan is saved. It also prints the content of the training plan file. You can either get the content of training plan from the output cell or the path where it is saved. Anyway, you need to create a new <code>txt</code> file and copy the training plan content in it. You can create new directory in Fed-BioMed called <code>training_plans</code> and inside it, you can create new <code>my-training-plan.txt</code> file and copy the training plan class content into it.</p> <pre>$ mkdir ${FEDBIOMED_DIR}/my_approved_training_plan\n$ cp &lt;training_plan_path_file&gt; ${FEDBIOMED_DIR}/my_approved_training_plan/my-training-plan.txt\n</pre> <p>Where <code>&lt;model_path_file&gt;</code> is the path of the model that is returned by <code>exp.training_plan_file(display=False)</code></p> <p>Afterward, please run following command in other terminal to register training plan file.</p> <pre>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini --register-training-plan\n</pre> <p>You should type a unique name for your training plan e.g. 'MyTestTP-1' and a description. The CLI will ask you select training plan file you want to register. Select the file that you saved and continue.</p> <p>Now, you should be able to train your model defined in the training plan.</p> <p>Back on the <code>Researcher</code> side, you should now be able to train your model.</p> In\u00a0[\u00a0]: Copied! <pre>exp.check_training_plan_status()\n</pre> exp.check_training_plan_status() In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True) In\u00a0[\u00a0]: Copied! <pre>exp.check_training_plan_status()\n</pre> exp.check_training_plan_status() In\u00a0[\u00a0]: Copied! <pre>exp.run_once(increase=True)\n</pre> exp.run_once(increase=True)"},{"location":"tutorials/security/training-with-approved-training-plans/#training-process-with-training-plan-management","title":"Training Process with Training Plan Management\u00b6","text":""},{"location":"tutorials/security/training-with-approved-training-plans/#introduction","title":"Introduction\u00b6","text":"<p>Fed-BioMed offers a feature to run only the pre-approved training plans on the nodes by default. The nodes which receive your training plan might require approved training plans. Therefore, if the node accepts only the approved training plan, the training plan files that are sent by a researcher with the training request should be approved by the node side in advance. In this workflow, the training plan approval process is done by a real user/person who reviews the code contained in the training plan file/class. The reviewer makes sure the model doesn't contain any code that might cause privacy issues or harm the node.</p> <p>In this tutorial, we will be creating a node with activated training plan control option.</p>"},{"location":"tutorials/security/training-with-approved-training-plans/#start-the-network","title":"Start the network\u00b6","text":"<p>Before running this notebook, start the network with <code>{FEDBIOMED_DIR}/scripts/fedbiomed_run network</code></p>"},{"location":"tutorials/security/training-with-approved-training-plans/#setting-up-a-node","title":"Setting Up a Node\u00b6","text":"<p>Enabling training plan control can be done both from config file or Fed-BioMed CLI while starting the node. The process of creating and starting a node with training plan control option is not so different from setting up a normal node. By default, if no option is specified in the CLI when the node is launched for the first time, the node disables training plan control in the security section of the config file. It then looks like the snippet below :</p> <pre>[security]\nhashing_algorithm = SHA256\nallow_default_training_plans = True\ntraining_plan_approval = False\n</pre> <p>The Fed-BioMed CLI has two optional extra parameters <code>--enable-training-plan-approval</code> and <code>--allow-default-training-plans</code> to activate model control. They choose the config file options, when the node is launched for the first time. They enable one-time override of the config file options at each launch of the node.</p> <ul> <li><code>--enable-training-plan-approval</code> : This parameter enables training plan control for the node. If there isn't a config file for the node while running CLI, it creates a new config file with enabled training plan approval mode <code>training_plan_approval = True</code>.</li> <li><code>--allow-default-training-plans</code>  : This parameter allows default training plans for train requests. These are the training plans that come for Fed-BioMed tutorials. For example, the training plan for MNIST dataset that we will be using for this tutorial. If the default training plans are enabled, node updates/registers training plan files which are located in <code>envs/common/default_training_plans</code> directory during the starting process of the node. This option has no effect if training plan control is not enabled.</li> </ul>"},{"location":"tutorials/security/training-with-approved-training-plans/#adding-mnist-dataset-to-the-node","title":"Adding MNIST Dataset to The Node.\u00b6","text":"<p>In this section we will add MNIST dataset to the node. While adding the dataset through CLI we'll also specify <code>--enable-training-plan-approval</code> and <code>--allow-default-training-plans</code> options. This will create new <code>config-n1.ini</code> file with following configuration.</p> <pre><code>[security]\nhashing_algorithm = SHA256\nallow_default_training_plans = True\ntraining_plan_approval = True\n\n</code></pre> <p>Now, let's run the following command.</p> <pre>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini --enable-training-plan-approval --allow-default-training-plans add\n</pre> <p>The CLI will ask you to select the dataset type. Since we will be working on MNIST dataset, please select <code>2</code> (default) and continue by typing <code>y</code> for the next prompt and select folder that you want to store MNIST dataset. Afterward, if you go to <code>etc</code> directory of fedbiomed, you can see <code>config-n1.ini</code> file.</p>"},{"location":"tutorials/security/training-with-approved-training-plans/#starting-the-node","title":"Starting the Node\u00b6","text":"<p>Now you can start your node by running following command;</p> <pre>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini start\n</pre> <p>Since config file has been configured to enable training plan control mode, you do not need to specify any extra parameter while starting the node. But it is also possible to start node with <code>--enable-training-plan-approval</code>, <code>--allow-default-training-plans</code> or <code>--disable-training-plan-approval</code>, <code>--disable-default-training-plans</code>. If you start your node with <code>--disable-model-approval</code> it will disable training plan control even it is enabled in the config file.</p>"},{"location":"tutorials/security/training-with-approved-training-plans/#creating-an-experiment","title":"Creating An Experiment\u00b6","text":"<p>In this section we will be using default MNIST model which has been already registered by the node.</p>"},{"location":"tutorials/security/training-with-approved-training-plans/#getting-final-training-plan-file-from-experiment","title":"Getting Final Training Plan File From Experiment\u00b6","text":"<p><code>training_plan_file()</code> displays the training plan file that will be sent to the nodes.</p>"},{"location":"tutorials/security/training-with-approved-training-plans/#changing-training-plan-and-testing-training-plan-approval-status","title":"Changing Training Plan And Testing Training Plan Approval Status\u00b6","text":"<p>Let's change the training plan network codes and test whether it is approved or not. We will be changing the network structure.</p>"},{"location":"tutorials/security/training-with-approved-training-plans/#registering-and-approving-the-training-plan","title":"Registering and Approving the Training Plan\u00b6","text":"<p>To register/approve the training plan that has been created in the previous section, we can use Fed-BioMed CLI. In Fed-Biomed, there are two ways of approving a model:</p> <ol> <li>By sending an <code>ApprovalRequest</code> from the researcher to the <code>Node</code></li> <li>By adding it directly to the <code>Node</code> through model registration facility</li> </ol>"},{"location":"tutorials/security/training-with-approved-training-plans/#1-approving-a-training-plan-through-an-approvalrequest","title":"1. Approving a Training Plan through an <code>ApprovalRequest</code>\u00b6","text":"<p>Fed-BioMed 's <code>Experiment</code> interface provides a method to submit a training plan to the <code>Node</code>, for approval. <code>Node</code> can then review the code and approve the training plan using CLI or GUI.</p> <p>The method of <code>Experiment</code> sending such request is <code>training_plan_approve</code></p>"},{"location":"tutorials/security/training-with-approved-training-plans/#2-registering-a-model-through-node-interface","title":"2. Registering a Model through Node interface\u00b6","text":"<p>Training plan status must have changed from <code>Pending</code> status to <code>Approved</code>, which means model can be trained from now on the <code>Node</code>. <code>Researcher</code> can now run an <code>Experiment</code> on the <code>Node</code>!</p>"},{"location":"tutorials/security/training-with-approved-training-plans/#rejecting-training-plans","title":"Rejecting training plans\u00b6","text":"<p>On <code>Node</code> side, it is possible to reject a Model using cli or GUI. Every type of training plan can be <code>Rejected</code>, even <code>Default</code> models. In Fed-BioMed, <code>Rejected</code> means that training plan cannot be trained/executed on the <code>Node</code> (but training plan is still <code>Registered</code> into the database).</p> <p>Using cli, <code>Node</code> can run:</p> <pre>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini --reject-training-plan\n</pre> <p>and select the training plan to be <code>Rejected</code>.</p>"},{"location":"user-guide/glossary/","title":"Glossary","text":"<p>Here below the glossary used for Fed-BioMed :</p> <ul> <li> <p>experiment : orchestrates the rounds during the federated learning, on the available nodes</p> <ul> <li>it includes : training plan, model, federated trainer, training parameters, model parameters, set of input data, results</li> <li>an experiment is unique (cannot be replayed) and is over when converged</li> <li>status : running and then done</li> </ul> </li> <li> <p>training : as commonly used in ML, process of feeding a model with data to improve its accuracy on some task.</p> </li> <li>validation : process of giving a heuristic information on the accuracy of a model during training.</li> <li> <p>testing : process of assessing the accuracy of a model after training, on holdout samples different from the one that were used for training. Not implemented yet in Fed-BioMed.</p> </li> <li> <p>job : not a researcher notion. Interface between the researcher and the nodes of an experiment. It triggers the local work for all sampled nodes at each round.</p> </li> <li>round : everything included in choice of the nodes, perform local work on the nodes, sending back whatever information is required, server performs the aggregation<ul> <li>current Round() class on node corresponds to local work</li> </ul> </li> <li>parameter update : an update of the ML model parameters during the training loop, which usually corresponds to the processing of one batch of data</li> <li> <p>epoch : a number of parameter updates equivalent to processing the entire dataset exactly once</p> </li> <li> <p>researcher (technical) : entity that defines and executes an experiment</p> </li> <li>node (technical) : entity with tagged datasets that replies to researcher queries and performs local work</li> </ul>"},{"location":"user-guide/deployment/deployment-vpn-node2/","title":"Fed-BioMed deployment with VPN/containers and two node instances on the same node machine","text":"<p>Most real-life deployments require protecting node data. Deployment using VPN/containers contributes to this goal by providing isolation of the Fed-BioMed instance from third parties.</p> <p>Deploying two nodes instance on the same node machine is not the normal real life VPN/containers deployment scenario, which consists of one node instance per node machine.</p> <p>Nevertheless, this scenario can be useful for testing purpose. For example, secure aggregation requires at least 2 nodes participating in the experiment. This scenario allows testing secure aggregation with VPN/containers while all components (researcher + 2 nodes) are running on the same machine.</p> <p>Operating a second node instance on the same node machine is mostly equivalent to operating the first node instance. Commands are adapted by replacing any occurrence of:</p> <ul> <li>node by node2</li> <li>gui by gui2</li> <li>NODETAG by NODE2TAG</li> <li>MPSPDZ_PORT=14001 by MPSPDZ_PORT=14002</li> <li><code>https://localhost:8443</code> by <code>https://localhost:8444</code></li> </ul>"},{"location":"user-guide/deployment/deployment-vpn-node2/#deploy-a-second-node-instance-on-the-same-node-machine","title":"Deploy a second node instance on the same node machine","text":"<p>This part of the tutorial is executed once on each node that runs a second node instance, after deploying the server. It covers the initial deployment, including build, configuration and launch of containers.</p> <p>Some commands are executed on the node side, while some commands are executed on the server side (pay attention to the prompt).</p> <p>For each node, choose a unique node tag (eg: NODE2TAG in this example) that represents this specific node instance for server side management commands.</p> <ul> <li> <p>build node-side containers</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn build node2 gui2\n</code></pre> </li> <li> <p>on the server side, generate a configuration for this node (known as NODE2TAG)</p> <pre><code>[user@server $] cd ${FEDBIOMED_DIR}/envs/vpn/docker\n[user@server $] docker-compose exec vpnserver bash -ci 'python ./vpn/bin/configure_peer.py genconf node NODE2TAG'\n</code></pre> <p>The configuration file is now available on the server side in path <code>${FEDBIOMED_DIR}/envs/vpn/docker/vpnserver/run_mounts/config/config_peers/node/NODE2TAG/config.env</code> or with command :</p> <pre><code>[user@server $] docker-compose exec vpnserver cat /config/config_peers/node/NODE2TAG/config.env\n</code></pre> </li> <li> <p>copy the configuration file from the server side to the node side via a secure channel, to path <code>/tmp/config2.env</code> on the node.</p> <p>In most real life deployments, one shouldn't have access to both server side and node side. Secure channel in an out-of-band secured exchange (outside of Fed-BioMed scope) between the server administrator and the node administrator that provides mutual authentication of the parties, integrity and privacy of the exchanged file.</p> <p>In a test deployment, one may be connected both on server side and node side. In this case, you just need to cut-paste or copy the file to the node.</p> <p>Use the node's copy of the configuration file:</p> <pre><code>[user@node $] cp /tmp/config2.env ./node2/run_mounts/config/config.env\n</code></pre> </li> <li> <p>start <code>node2</code> container</p> <pre><code>[user@node $] docker-compose up -d node2\n</code></pre> </li> <li> <p>retrieve the <code>node2</code>'s publickey</p> <pre><code>[user@node $] docker-compose exec node2 wg show wg0 public-key | tr -d '\\r' &gt;/tmp/publickey2-nodeside\n</code></pre> </li> <li> <p>copy the public key from the node side to the server side via a secure channel (see above), to path <code>/tmp/publickey2-serverside</code> on the server.</p> </li> <li> <p>on the server side finalize configuration of the VPN keys for this node (known as NODE2TAG)</p> <pre><code>[user@server $] cd ${FEDBIOMED_DIR}/envs/vpn/docker\n[user@server $] docker-compose exec vpnserver bash -ci \"python ./vpn/bin/configure_peer.py add node NODE2TAG $(cat /tmp/publickey2-serverside)\"\n</code></pre> </li> <li> <p>check containers running on the node side</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn status node2\n</code></pre> <p><code>node2</code> container should be up and able to ping the VPN server</p> <pre><code>** Checking docker VPN images &amp; VPN access: node2\n- container node2 is running\n- pinging VPN server from container node2 -&gt; OK\n</code></pre> <p><code>node2</code> container is now ready to be used.</p> </li> <li> <p>optionally force the use of secure aggregation by the node (node will refuse to train without the use of secure aggregation):</p> <pre><code>[user@node $] export FORCE_SECURE_AGGREGATION=True\n</code></pre> </li> <li> <p>do initial node configuration</p> <pre><code>[user@node $] docker-compose exec -u $(id -u) node2 bash -ci 'export FORCE_SECURE_AGGREGATION='${FORCE_SECURE_AGGREGATION}'&amp;&amp; export MPSPDZ_IP=$VPN_IP &amp;&amp; export MPSPDZ_PORT=14002 &amp;&amp; export MQTT_BROKER=10.220.0.2 &amp;&amp; export MQTT_BROKER_PORT=1883 &amp;&amp; export UPLOADS_URL=\"http://10.220.0.3:8000/upload/\" &amp;&amp; export PYTHONPATH=/fedbiomed &amp;&amp; export FEDBIOMED_NO_RESET=1 &amp;&amp; eval \"$(conda shell.bash hook)\" &amp;&amp; conda activate fedbiomed-node &amp;&amp; ENABLE_TRAINING_PLAN_APPROVAL=True ALLOW_DEFAULT_TRAINING_PLANS=True ./scripts/fedbiomed_run node configuration create'\n</code></pre> </li> </ul> <p>Optionally launch the node GUI :</p> <ul> <li> <p>start <code>gui2</code> container</p> <pre><code>[user@node $] docker-compose up -d gui2\n</code></pre> </li> <li> <p>check containers running on the node side</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn status node2 gui2\n</code></pre> <p>Node side containers should be up and able to ping the VPN server</p> <pre><code>** Checking docker VPN images &amp; VPN access: node2 gui2\n- container node2 is running\n- container gui2 is running\n- pinging VPN server from container node2 -&gt; OK\n</code></pre> <p><code>node2</code> and <code>gui2</code> containers are now ready to be used.</p> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn-node2/#use-the-second-node-instance-on-the-same-node-machine","title":"Use the second node instance on the same node machine","text":"<p>This part is executed at least once on each node that runs a second node instance, after deploying the node side containers.</p> <p>Setup the node by sharing datasets and by launching the Fed-BioMed node:</p> <ul> <li> <p>if node GUI is launched, it can be used to share datasets. On the node side machine, connect to <code>http://localhost:8485</code></p> </li> <li> <p>connect to the <code>node2</code> container and launch commands, for example :</p> <ul> <li> <p>connect to the container</p> <pre><code>[user@node $] docker-compose exec -u $(id -u) node2 bash -ci 'export MPSPDZ_IP=$VPN_IP &amp;&amp; export MPSPDZ_PORT=14002 &amp;&amp; export MQTT_BROKER=10.220.0.2 &amp;&amp; export MQTT_BROKER_PORT=1883 &amp;&amp; export UPLOADS_URL=\"http://10.220.0.3:8000/upload/\" &amp;&amp; export PYTHONPATH=/fedbiomed &amp;&amp; export FEDBIOMED_NO_RESET=1 &amp;&amp; eval \"$(conda shell.bash hook)\" &amp;&amp; conda activate fedbiomed-node &amp;&amp; bash'\n</code></pre> </li> <li> <p>start the Fed-BioMed node, for example in background:</p> <pre><code>[user@node2-container $] nohup ./scripts/fedbiomed_run node start &gt;./fedbiomed_node.out &amp;\n</code></pre> </li> <li> <p>share one or more datasets, for example a MNIST dataset or an interactively defined dataset (can also be done via the GUI):</p> <pre><code>[user@node2-container $] ./scripts/fedbiomed_run node -am /data\n[user@node2-container $] ./scripts/fedbiomed_run node add\n</code></pre> </li> </ul> </li> </ul> <p>Example of a few more possible commands:</p> <ul> <li> <p>optionally list shared datasets:</p> <pre><code>[user@node2-container $] ./scripts/fedbiomed_run node list\n</code></pre> </li> <li> <p>optionally register a new authorized training plan previously copied on the node side in <code>${FEDBIOMED_DIR}/envs/vpn/docker/node2/run_mounts/data/my_training_plan.txt</code></p> <p><pre><code>[user@node2-container $] ./scripts/fedbiomed_run node --register-training-plan\n</code></pre> Indicate <code>/data/my_training_plan.txt</code> as path of the training plan file.</p> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn-node2/#configure-secure-aggregation-for-the-server-side-for-a-second-node-instance","title":"Configure secure aggregation for the server side, for a second node instance","text":"<p>This part of the tutorial is optionally executed once on each node that runs a second node instance. It is necessary before this component can use secure aggregation in an experiment.</p> <ul> <li> <p>reuse the <code>/tmp/cert-secagg</code> created on this node for the first node instance</p> <p>Make the server secagg certificate available to the node:</p> <pre><code>[user@node $] cp /tmp/cert-secagg ./node2/run_mounts/etc/cert-secagg\n</code></pre> </li> <li> <p>connect to the <code>node2</code> container using the command line</p> <ul> <li> <p>register the researcher certificate using the researcher ID, IP address and port as indicated in the server registration instructions, in this example</p> <pre><code>[user@node2-container $] # this is an example, please cut-paste from your registration instructions\n[user@node2-container $] # ./scripts/fedbiomed_run node certificate register -pk ./etc/cert-secagg -pi researcher_2bd34852-830b-48f0-9f58-613f3e643d42  --ip 10.222.0.2 --port 14000\n</code></pre> </li> <li> <p>check the deployed certificate</p> <pre><code>[user@node2-container $] ./scripts/fedbiomed_run node certificate list\n</code></pre> </li> </ul> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn-node2/#configure-secure-aggregation-for-the-node-side-for-a-second-node-instance","title":"Configure secure aggregation for the node side, for a second node instance","text":"<p>This part of the tutorial is optionally executed once on each node that runs a second node instance. It is necessary before this component can use secure aggregation in an experiment.</p> <ul> <li> <p>reuse the <code>/tmp/cert-secagg</code> created on this node for the first node instance</p> <p>Make the node secagg certificate available to the other node:</p> <pre><code>[user@othernode $] cp /tmp/cert-secagg ./node2/run_mounts/etc/cert-secagg\n</code></pre> </li> <li> <p>connect to the <code>node2</code> container using the command line</p> <ul> <li> <p>register the node certificate using the node ID, IP address and port as indicated in the node registration instructions, in this example</p> <pre><code>[user@othernode2-container $] # this is an example, please cut-paste from your registration instructions\n[user@othernode2-container $] # ./scripts/fedbiomed_run node certificate register -pk ./etc/cert-secagg -pi node_964bdca9-809d-49b8-a9c4-8ba3d108c1ae  --ip 10.221.0.2 --port 14001\n</code></pre> </li> <li> <p>check the deployed certificate</p> <pre><code>[user@othernode2-container $] ./scripts/fedbiomed_run node certificate list\n</code></pre> </li> </ul> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn-secagg/","title":"Fed-BioMed deployment on multiple machines with VPN/containers using secure aggregation","text":"<p>Most real-life deployments require protecting node data. Deployment using VPN/containers contributes to this goal by providing isolation of the Fed-BioMed instance from third parties.</p> <p>Using secure aggregation for protecting model parameters from nodes' local training adds another layer of security.</p>"},{"location":"user-guide/deployment/deployment-vpn-secagg/#configure-secure-aggregation-for-the-server-side","title":"Configure secure aggregation for the server side","text":"<p>This part of the tutorial is optionally executed once for the server. It is necessary before this component can use secure aggregation in an experiment.</p> <ul> <li> <p>connect to the <code>researcher</code> container using the command line</p> </li> <li> <p>on the server side, generate the secagg setup instructions for the server</p> <pre><code>[user@server-container $] ./scripts/fedbiomed_run researcher certificate registration-instructions\n</code></pre> <p>Output contains a certificate of a public key for the server and instructions for registration:</p> <pre><code>Hi There! \n\n\nPlease find following certificate to register \n\n-----BEGIN CERTIFICATE-----\nMIIDBzCCAe+gAwIBAgIDASvKMA0GCSqGSIb3DQEBCwUAMEYxCjAIBgNVBAMMASox\n...\nqaX4EJXbAjS50P8=\n-----END CERTIFICATE-----\n\nPlease follow the instructions below to register this certificate:\n\n\n 1- Copy certificate content into a file e.g 'Hospital1.pem'\n2- Change your directory to 'fedbiomed' root\n 2- Run: \"scripts/fedbiomed_run [node | researcher] certificate register -pk [PATH WHERE CERTIFICATE IS SAVED] -pi researcher_2bd34852-830b-48f0-9f58-613f3e643d42  --ip 10.222.0.2 --port 14000\"\nExamples commands to use for VPN/docker mode:\n      ./scripts/fedbiomed_run node certificate register -pk ./etc/cert-secagg -pi researcher_2bd34852-830b-48f0-9f58-613f3e643d42 --ip 10.222.0.2 --port 14000\n./scripts/fedbiomed_run researcher certificate register -pk ./etc/cert-secagg -pi researcher_2bd34852-830b-48f0-9f58-613f3e643d42 --ip 10.222.0.2 --port 14000\n</code></pre> </li> <li> <p>then repeat for each node the propagation of the server's secagg configuration</p> <ul> <li> <p>transmit the registration instructions from the server side to the node side via a secure channel.</p> <p>In most real life deployments, one shouldn't have access to both server side and node side. Secure channel in an out-of-band secured exchange (outside of Fed-BioMed scope) between the server administrator and the node administrator that provides mutual authentication of the parties, integrity and privacy of the exchanged file.</p> <p>In a test deployment, one may be connected both on server side and node side. In this case, you just need to cut-paste or save to a file on the node.</p> <p>Copy certificate content to file <code>/tmp/cert-secagg</code> on the node.</p> <p>Make the server secagg certificate available to the node:</p> <pre><code>[user@node $] cp /tmp/cert-secagg ./node/run_mounts/etc/cert-secagg\n</code></pre> </li> <li> <p>connect to the <code>node</code> container using the command line</p> </li> <li> <p>register the server certificate using the researcher ID, IP address and port as indicated in the server registration instructions, in this example</p> <pre><code>[user@node-container $] # this is an example, please cut-paste from your registration instructions\n[user@node-container $] # ./scripts/fedbiomed_run node certificate register -pk ./etc/cert-secagg -pi researcher_2bd34852-830b-48f0-9f58-613f3e643d42  --ip 10.222.0.2 --port 14000\n</code></pre> </li> <li> <p>check the deployed certificate</p> <pre><code>[user@node-container $] ./scripts/fedbiomed_run node certificate list\n</code></pre> </li> <li> <p>optionally propagate to a second node instance on the same node, if a second node instance was previously deployed</p> </li> </ul> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn-secagg/#configure-secure-aggregation-for-the-node-side","title":"Configure secure aggregation for the node side","text":"<p>This part of the tutorial is optionally executed once for each node instance. It is necessary before this component can use secure aggregation in an experiment.</p> <ul> <li> <p>connect to the <code>node</code> container using the command line</p> </li> <li> <p>on the node side, generate the secagg setup instructions for the node (if running a second node instance on the node, repeat for that instance)</p> <pre><code>[user@node-container $] ./scripts/fedbiomed_run node certificate registration-instructions\n</code></pre> <p>Output contains a certificate of a public key for the node and instructions for registration:</p> <pre><code>Hi There!\n\n\nPlease find following certificate to register\n\n-----BEGIN CERTIFICATE-----\nMIIC+jCCAeKgAwIBAgICZRQwDQYJKoZIhvcNAQELBQAwQDEKMAgGA1UEAwwBKjEy\n...\nfG6KEo0KGnAKgmFpZxtftCBmAiLvZgvJ3LIfMbysfMXy1UFcnvVwoCQznqn1YQ==\n-----END CERTIFICATE-----\n\nPlease follow the instructions below to register this certificate:\n\n\n 1- Copy certificate content into a file e.g 'Hospital1.pem'\n2- Change your directory to 'fedbiomed' root\n 2- Run: \"scripts/fedbiomed_run [node | researcher] certificate register -pk [PATH WHERE CERTIFICATE IS SAVED] -pi node_964bdca9-809d-49b8-a9c4-8ba3d108c1ae  --ip 10.221.0.2 --port 14001\"\nExamples commands to use for VPN/docker mode:\n      ./scripts/fedbiomed_run node certificate register -pk ./etc/cert-secagg -pi node_964bdca9-809d-49b8-a9c4-8ba3d108c1ae --ip 10.221.0.2 --port 14001\n./scripts/fedbiomed_run researcher certificate register -pk ./etc/cert-secagg -pi node_964bdca9-809d-49b8-a9c4-8ba3d108c1ae --ip 10.221.0.2 --port 14001\n</code></pre> </li> <li> <p>then repeat for each other node the propagation of the node's secagg configuration</p> <ul> <li> <p>transmit the registration instructions from the node side to the other node side via a secure channel.</p> <p>In most real life deployments, one shouldn't have access to both nodes sides. Secure channel in an out-of-band secured exchange (outside of Fed-BioMed scope) between the nodes administrators that provides mutual authentication of the parties, integrity and privacy of the exchanged file.</p> <p>In a test deployment, one may be connected both nodes sides. In this case, you just need to cut-paste or save to a file on the other node.</p> <p>Copy certificate content to file <code>/tmp/cert-secagg</code> on the other node.</p> <p>Make the node secagg certificate available to the other node:</p> <pre><code>[user@othernode $] cp /tmp/cert-secagg ./node/run_mounts/etc/cert-secagg\n</code></pre> </li> <li> <p>connect to the other node's <code>node</code> container using the command line</p> </li> <li> <p>register the node certificate using the node ID, IP address and port as indicated in the node registration instructions, in this example</p> <pre><code>[user@othernode-container $] # this is an example, please cut-paste from your registration instructions\n[user@othernode-container $] # ./scripts/fedbiomed_run node certificate register -pk ./etc/cert-secagg -pi node_964bdca9-809d-49b8-a9c4-8ba3d108c1ae  --ip 10.221.0.2 --port 14001\n</code></pre> </li> <li> <p>check the deployed certificate</p> <pre><code>[user@othernode-container $] ./scripts/fedbiomed_run node certificate list\n</code></pre> </li> <li> <p>optionally propagate to a second node instance on the other node, if a second node instance was previously deployed</p> </li> </ul> </li> <li> <p>then propagate to the server the node's secagg configuration</p> <ul> <li> <p>transmit the registration instructions from the node side to the server side via a secure channel.</p> <p>In most real life deployments, one shouldn't have access to both server side and node side. Secure channel in an out-of-band secured exchange (outside of Fed-BioMed scope) between the server administrator and the node administrator that provides mutual authentication of the parties, integrity and privacy of the exchanged file.</p> <p>In a test deployment, one may be connected both on server side and node side. In this case, you just need to cut-paste or save to a file on the server.</p> <p>Copy certificate content to file <code>/tmp/cert-secagg</code> on the server.</p> <p>Make the node secagg certificate available to the node:</p> <pre><code>[user@server $] cp /tmp/cert-secagg ./researcher/run_mounts/etc/cert-secagg\n</code></pre> </li> <li> <p>connect to the <code>researcher</code> container using the command line</p> </li> <li> <p>register the node certificate using the node ID, IP address and port as indicated in the node registration instructions, in this example</p> <pre><code>[user@server-container $] # this is an example, please cut-paste from your registration instructions\n[user@server-container $] # ./scripts/fedbiomed_run researcher certificate register -pk ./etc/cert-secagg -pi node_964bdca9-809d-49b8-a9c4-8ba3d108c1ae  --ip 10.221.0.2 --port 14001\n</code></pre> </li> <li> <p>check the deployed certificate</p> <pre><code>[user@researcher-container $] ./scripts/fedbiomed_run researcher certificate list\n</code></pre> </li> </ul> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/","title":"Fed-BioMed deployment on multiple machines with VPN/containers","text":"<p>Most real-life deployments require protecting node data. Deployment using VPN/containers contributes to this goal by providing isolation of the Fed-BioMed instance from third parties. All communications between the components of a Fed-BioMed instance occur inside WireGuard VPN tunnels with mutual authentication of the VPN endpoints. Using containers can also ease installation on multiple sites.</p> <p>This tutorial details a deployment scenario where:</p> <ul> <li>Fed-BioMed network and researcher components run on the same machine (\"the server\") in the following <code>docker</code> containers<ul> <li><code>vpnserver</code> / <code>fedbiomed/vpn-vpnserver</code>: WireGuard server</li> <li><code>mqtt</code> / <code>fedbiomed/vpn-mqtt</code>: MQTT message broker server</li> <li><code>restful</code> / <code>fedbiomed/vpn-restful</code>: HTTP REST communication server</li> <li><code>researcher</code> / <code>fedbiomed/vpn-researcher</code>: a researcher jupyter notebooks</li> </ul> </li> <li>several Fed-BioMed node components run, one node per machine with the following containers<ul> <li><code>node</code> / <code>fedbiomed/vpn-node</code>: a node component</li> <li><code>gui</code> / <code>fedbiomed/vpn-gui</code>: a GUI for managing node component data (optional)</li> </ul> </li> <li>all communications between the components are tunneled through a VPN</li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#requirements","title":"Requirements","text":"<p>Supported operating systems and software requirements</p> <p>Supported operating systems for containers/VPN deployment include Fedora 35, Ubuntu 20.04, recent MacOS X, Windows 10 21H2 with WSL2 using Ubuntu-20.04 distribution. Also requires docker-compose &gt;= 1.27.0.</p> <p>Check here for detailed requirements.</p> <p>Account privileges</p> <p>Components deployment requires an account which can use docker (typically belonging to the <code>docker</code> group). Using a dedicated service account is a good practice. No access to the administrative account is needed, usage of <code>root</code> account for deploying components is discouraged to follow the principle of privilege minimization.</p> <p>Web proxy</p> <p>On sites where web access uses a proxy you need to configure web proxy for docker.</p> <p>User or group ID for containers</p> <p>By default, Fed-BioMed uses the current account's user and group ID for building and running containers.</p> <p>Avoid using low ID for user or group ( &lt; 500 for MacOS, &lt; 1000 for Linux ) inside containers. They often conflict with pre-existing user or group account in the container images. This results in unhandled failures when setting up or starting the containers. Check your account id with <code>id -a</code>.</p> <p>Use the <code>CONTAINER_USER</code>, <code>CONTAINER_UID</code>, <code>CONTAINER_GROUP</code> and <code>CONTAINER_GID</code> variables to use alternate values, eg for MacOS:</p> <p>MacOS commonly uses group <code>staff:20</code> for user accounts, which conflicts with Fed-BioMed VPN/containers mode. So a good configuration choice for MacOS can be:</p> <pre><code>export CONTAINER_GROUP=fedbiomed\nexport CONTAINER_GID=1111\n</code></pre> <p>More options for containers/VPN deployment are not covered in this tutorial but can be found here including:</p> <ul> <li>using GPU in <code>node</code> container</li> <li>building containers (eg: <code>node</code> and <code>gui</code>) on one machine, using this pre-built containers on the nodes</li> <li>using different identity (account) for building and launching a container</li> <li>deploying network and researcher on distinct machines</li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#notations","title":"Notations","text":"<p>In this tutorial we use the following notations:</p> <ul> <li><code>[user@server $]</code> means the command is launched on the server machine (outside containers)</li> <li><code>[user@node $]</code> means the command is launched on a node machine (outside containers)</li> <li>for commands typed inside containers, <code>[root@vpnserver-container #]</code> means the command is launched inside the <code>vpnserver</code> container as root, <code>[user@node-container $]</code> means the command is launched inside the <code>vpnserver</code> container with same user account as outside the container</li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#deploy-on-the-server-side","title":"Deploy on the server side","text":"<p>This part of the tutorial is executed once on the server side, before deploying the nodes. It covers the initial server deployment, including build, configuration and launch of containers.</p> <ul> <li> <p>download Fed-BioMed software by doing a local clone of the git repository: </p> <pre><code>[user@server $] git clone -b master https://github.com/fedbiomed/fedbiomed.git\n[user@server $] cd fedbiomed\n[user@server $] export FEDBIOMED_DIR=$PWD # use setenv for *csh\n[user@server $] cd envs/vpn/docker\n</code></pre> <p>For the rest of this tutorial <code>${FEDBIOMED_DIR}</code> represents the base directory of the clone.</p> <p><code>docker-compose</code> commands need to be launched from <code>${FEDBIOMED_DIR}/envs/vpn/docker directory</code>.</p> </li> <li> <p>clean running containers, containers files, temporary files</p> <pre><code>[user@server $] source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment clean\n[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean\n</code></pre> </li> <li> <p>optionally clean the container images to force build fresh new images</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean image\n</code></pre> </li> <li> <p>build server-side containers</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn build vpnserver mqtt restful researcher\n</code></pre> </li> <li> <p>configure the VPN keys for containers running on the server side, after starting the <code>vpnserver</code> container</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn configure mqtt restful researcher\n</code></pre> </li> <li> <p>start other server side containers</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn start mqtt restful researcher\n</code></pre> </li> <li> <p>check all containers are running as expected on the server side</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn status vpnserver mqtt restful researcher\n</code></pre> <p>Server side containers should be up and able to ping the VPN server</p> <pre><code>** Checking docker VPN images &amp; VPN access: vpnserver mqtt restful researcher\n- container vpnserver is running\n- container mqtt is running\n- container restful is running\n- container researcher is running\n- pinging VPN server from container vpnserver -&gt; OK\n- pinging VPN server from container mqtt -&gt; OK\n- pinging VPN server from container restful -&gt; OK\n- pinging VPN server from container researcher -&gt; OK\n</code></pre> <p>Server side containers are now ready for node side deployment.</p> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#deploy-on-the-node-side","title":"Deploy on the node side","text":"<p>This part of the tutorial is executed once on each node, after deploying the server. It covers the initial deployment, including build, configuration and launch of containers.</p> <p>Some commands are executed on the node side, while some commands are executed on the server side (pay attention to the prompt).</p> <p>For each node, choose a unique node tag (eg: NODETAG in this example) that represents this specific node instance for server side management commands.</p> <ul> <li> <p>download Fed-BioMed software by doing a local clone of the git repository: </p> <pre><code>[user@node $] git clone -b master https://github.com/fedbiomed/fedbiomed.git\n[user@node $] cd fedbiomed [user@node $] export FEDBIOMED_DIR=$PWD # use setenv for *csh\n[user@node $] cd envs/vpn/docker\n</code></pre> <p>For the rest of this tutorial <code>${FEDBIOMED_DIR}</code> represents the base directory of the clone.</p> <p><code>docker-compose</code> commands need to be launched from <code>${FEDBIOMED_DIR}/envs/vpn/docker directory</code>.</p> </li> <li> <p>clean running containers, containers files, temporary files (skip that step if node and server run on the same machine)</p> <pre><code>[user@node $] source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment clean\n[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean\n</code></pre> </li> <li> <p>optionally clean the container images to force build fresh new images</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean image\n</code></pre> </li> <li> <p>build node-side containers</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn build node gui\n</code></pre> </li> <li> <p>on the server side, generate a configuration for this node (known as NODETAG)</p> <pre><code>[user@server $] cd ${FEDBIOMED_DIR}/envs/vpn/docker\n[user@server $] docker-compose exec vpnserver bash -ci 'python ./vpn/bin/configure_peer.py genconf node NODETAG'\n</code></pre> <p>The configuration file is now available on the server side in path <code>${FEDBIOMED_DIR}/envs/vpn/docker/vpnserver/run_mounts/config/config_peers/node/NODETAG/config.env</code> or with command :</p> <pre><code>[user@server $] docker-compose exec vpnserver cat /config/config_peers/node/NODETAG/config.env\n</code></pre> </li> <li> <p>copy the configuration file from the server side to the node side via a secure channel, to path <code>/tmp/config.env</code> on the node.</p> <p>In most real life deployments, one shouldn't have access to both server side and node side. Secure channel in an out-of-band secured exchange (outside of Fed-BioMed scope) between the server administrator and the node administrator that provides mutual authentication of the parties, integrity and privacy of the exchanged file.</p> <p>In a test deployment, one may be connected both on server side and node side. In this case, you just need to cut-paste or copy the file to the node.</p> <p>Use the node's copy of the configuration file:</p> <pre><code>[user@node $] cp /tmp/config.env ./node/run_mounts/config/config.env\n</code></pre> </li> <li> <p>start <code>node</code> container</p> <pre><code>[user@node $] docker-compose up -d node\n</code></pre> </li> <li> <p>retrieve the <code>node</code>'s publickey</p> <pre><code>[user@node $] docker-compose exec node wg show wg0 public-key | tr -d '\\r' &gt;/tmp/publickey-nodeside\n</code></pre> </li> <li> <p>copy the public key from the node side to the server side via a secure channel (see above), to path <code>/tmp/publickey-serverside</code> on the server.</p> </li> <li> <p>on the server side finalize configuration of the VPN keys for this node (known as NODETAG)</p> <pre><code>[user@server $] cd ${FEDBIOMED_DIR}/envs/vpn/docker\n[user@server $] docker-compose exec vpnserver bash -ci \"python ./vpn/bin/configure_peer.py add node NODETAG $(cat /tmp/publickey-serverside)\"\n</code></pre> </li> <li> <p>check containers running on the node side</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn status node\n</code></pre> <p><code>node</code> container should be up and able to ping the VPN server</p> <pre><code>** Checking docker VPN images &amp; VPN access: node\n- container node is running\n- pinging VPN server from container node -&gt; OK\n</code></pre> <p><code>node</code> container is now ready to be used.</p> </li> <li> <p>optionally force the use of secure aggregation by the node (node will refuse to train without the use of secure aggregation):</p> <pre><code>[user@node $] export FORCE_SECURE_AGGREGATION=True\n</code></pre> </li> <li> <p>do initial node configuration</p> <pre><code>[user@node $] docker-compose exec -u $(id -u) node bash -ci 'export FORCE_SECURE_AGGREGATION='${FORCE_SECURE_AGGREGATION}'&amp;&amp; export MPSPDZ_IP=$VPN_IP &amp;&amp; export MPSPDZ_PORT=14001 &amp;&amp; export MQTT_BROKER=10.220.0.2 &amp;&amp; export MQTT_BROKER_PORT=1883 &amp;&amp; export UPLOADS_URL=\"http://10.220.0.3:8000/upload/\" &amp;&amp; export PYTHONPATH=/fedbiomed &amp;&amp; export FEDBIOMED_NO_RESET=1 &amp;&amp; eval \"$(conda shell.bash hook)\" &amp;&amp; conda activate fedbiomed-node &amp;&amp; ENABLE_TRAINING_PLAN_APPROVAL=True ALLOW_DEFAULT_TRAINING_PLANS=True ./scripts/fedbiomed_run node configuration create'\n</code></pre> </li> </ul> <p>Optionally launch the node GUI :</p> <ul> <li> <p>optionally authorize connection to node GUI from distant machines. By default, only connection from local machine (<code>localhost</code>) is authorized.</p> <pre><code>[user@node $] export GUI_SERVER_IP=0.0.0.0\n</code></pre> <p>To authorize distant connection to only one of the node machine's IP addresses use a command of the form <code>export GUI_SERVER_IP=a.b.c.d</code> where <code>a.b.c.d</code> is one of the IP addresses of the node machine.</p> <p>For security reasons, when authorizing connection from distant machines, it is strongly recommended to use a custom SSL certificate signed by a well-known authority.</p> <p>Custom SSL certificates  for GUI</p> <p>GUI will start serving on port 8443 with self-signed certificates. These certificates will be identified as risky by the  browsers, and users will have to approve them. However, it is also possible to set custom trusted SSL certificates by  adding <code>crt</code> and <code>key</code> files to the <code>${FEDBIOMED_DIR}/envs/vpn/docker/gui/run_mounts/certs</code> directory before starting the GUI.</p> <p>When adding these files, please ensure that:</p> <ul> <li>the certificate extension is <code>.crt</code> and the key file extension is <code>.key</code></li> <li>there is no more than one file for each certificate and key</li> </ul> </li> <li> <p>optionally restrict the HTTP host names that can be used to connect to the node GUI. By default all the host names (DNS CNAME) of the node machine can be used.</p> <p>For example, if the node machine has two host names <code>my.fqdn.com</code> and <code>other.alias.org</code>, use syntax like <code>export GUI_SERVER_NAME=my.fqdn.com</code> or <code>GUI_SERVER_NAME='*.fqdn.com'</code> (don't forget the enclosing single quotes) to authorize only requests using the first name (eg: <code>https://my.fqdn.com</code>) to reach the node GUI. Use the syntax of Nginx <code>server_name</code>.</p> </li> <li> <p>start <code>gui</code> container</p> <pre><code>[user@node $] docker-compose up -d gui\n</code></pre> </li> <li> <p>check containers running on the node side</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn status node gui\n</code></pre> <p>Node side containers should be up and able to ping the VPN server</p> <pre><code>** Checking docker VPN images &amp; VPN access: node gui\n- container node is running\n- container gui is running\n- pinging VPN server from container node -&gt; OK\n</code></pre> <p><code>node</code> and <code>gui</code> containers are now ready to be used.</p> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#optionally-deploy-a-second-node-instance-on-the-same-node","title":"Optionally deploy a second node instance on the same node","text":"<p>Optionally deploy a second node instance on the same node (useful for testing purpose, not a normal deployment scenario):</p> <ul> <li>deploy second node on the same machine</li> </ul> <p>This part of the tutorial is optionally executed on some nodes, after deploying the server.</p>"},{"location":"user-guide/deployment/deployment-vpn/#optionally-configure-secure-aggregation","title":"Optionally configure secure aggregation","text":"<p>Optionally configure secure aggregation for additional security:</p> <ul> <li>for the server side</li> <li>for the node side</li> </ul> <p>This part of the tutorial is optionally executed once on each node and once on the server. It is necessary before this component can use secure aggregation in an experiment.</p>"},{"location":"user-guide/deployment/deployment-vpn/#use-the-node","title":"Use the node","text":"<p>This part is executed at least once on each node, after deploying the node side containers.</p> <p>Setup the node by sharing datasets and by launching the Fed-BioMed node:</p> <ul> <li> <p>if node GUI is launched, it can be used to share datasets. On the node side machine, connect to <code>https://localhost:8443</code> (or <code>https://&lt;host_name_and_domain&gt;:8443</code> if connection from distant machine is authorized)</p> </li> <li> <p>connect to the <code>node</code> container and launch commands, for example :</p> <ul> <li> <p>connect to the container</p> <pre><code>[user@node $] cd ${FEDBIOMED_DIR}/envs/vpn/docker\n[user@node $] docker-compose exec -u $(id -u) node bash -ci 'export MPSPDZ_IP=$VPN_IP &amp;&amp; export MPSPDZ_PORT=14001 &amp;&amp; export MQTT_BROKER=10.220.0.2 &amp;&amp; export MQTT_BROKER_PORT=1883 &amp;&amp; export UPLOADS_URL=\"http://10.220.0.3:8000/upload/\" &amp;&amp; export PYTHONPATH=/fedbiomed &amp;&amp; export FEDBIOMED_NO_RESET=1 &amp;&amp; eval \"$(conda shell.bash hook)\" &amp;&amp; conda activate fedbiomed-node &amp;&amp; bash'\n</code></pre> </li> <li> <p>start the Fed-BioMed node, for example in background:</p> <pre><code>[user@node-container $] nohup ./scripts/fedbiomed_run node start &gt;./fedbiomed_node.out &amp;\n</code></pre> </li> <li> <p>share one or more datasets, for example a MNIST dataset or an interactively defined dataset (can also be done via the GUI):</p> <pre><code>[user@node-container $] ./scripts/fedbiomed_run node -am /data\n[user@node-container $] ./scripts/fedbiomed_run node add\n</code></pre> </li> </ul> </li> </ul> <p>Example of a few more possible commands:</p> <ul> <li> <p>optionally list shared datasets:</p> <pre><code>[user@node-container $] ./scripts/fedbiomed_run node list\n</code></pre> </li> <li> <p>optionally register a new authorized training plan previously copied on the node side in <code>${FEDBIOMED_DIR}/envs/vpn/docker/node/run_mounts/data/my_training_plan.txt</code></p> <p><pre><code>[user@node-container $] ./scripts/fedbiomed_run node --register-training-plan\n</code></pre> Indicate <code>/data/my_training_plan.txt</code> as path of the training plan file.</p> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#optionally-use-a-second-node-instance-on-the-same-node","title":"Optionally use a second node instance on the same node","text":"<p>This optional part is executed at least once on the nodes where a second node instance is deployed, after deploying the second node side containers:</p> <ul> <li>use second node on the same machine</li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#use-the-server","title":"Use the server","text":"<p>This part is executed at least once on the server after setting up the nodes:</p> <ul> <li> <p>on the server side machine, connect to <code>http://localhost:8888</code>, then choose and run a Jupyter notebook</p> <ul> <li> <p>make more notebooks available from the server side machine (eg: <code>/tmp/my_notebook.ipynb</code>) by copying them to the <code>samples</code> directory</p> <p><pre><code>[user@server $] cp /tmp/my_notebook.ipynb ${FEDBIOMED_DIR}/envs/vpn/docker/researcher/run_mounts/samples/\n</code></pre> The notebook is now available in the Jupyter GUI under the <code>samples</code> subdirectory of the Jupyter notebook interface.</p> </li> </ul> </li> <li> <p>if the notebook uses Tensorboard, it can be viewed</p> <ul> <li>either embedded inside the Jupyter notebook as explained in the Tensorboard documentation</li> <li>or by connecting to <code>http://localhost:6006</code></li> </ul> </li> </ul> <p>Optionally use the researcher container's command line instead of the Jupyter notebooks:</p> <ul> <li> <p>connect to the <code>researcher</code> container</p> <pre><code>[user@server $] cd ${FEDBIOMED_DIR}/envs/vpn/docker\n[user@server $] docker-compose exec -u $(id -u) researcher bash -ci 'export MPSPDZ_IP=$VPN_IP &amp;&amp; export MPSPDZ_PORT=14000 &amp;&amp; export MQTT_BROKER=10.220.0.2 &amp;&amp; export MQTT_BROKER_PORT=1883 &amp;&amp; export UPLOADS_URL=\"http://10.220.0.3:8000/upload/\" &amp;&amp; export PYTHONPATH=/fedbiomed &amp;&amp; export FEDBIOMED_NO_RESET=1 &amp;&amp; eval \"$(conda shell.bash hook)\" &amp;&amp; conda activate fedbiomed-researcher &amp;&amp; bash'\n</code></pre> </li> <li> <p>launch a command, for example a training:</p> <pre><code>[user@server-container $] ./notebooks/101_getting-started.py\n</code></pre> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#misc-server-management-commands","title":"Misc server management commands","text":"<p>Some possible management commands after initial deployment include:</p> <ul> <li> <p>check all containers running on the server side</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn status vpnserver mqtt restful researcher\n</code></pre> </li> <li> <p>check the VPN peers known from the VPN server</p> <pre><code>[user@server $] ( cd ${FEDBIOMED_DIR}/envs/vpn/docker ; docker-compose exec vpnserver bash -ci \"python ./vpn/bin/configure_peer.py list\" )\ntype        id           prefix         peers\n----------  -----------  -------------  ------------------------------------------------\nmanagement  mqtt         10.220.0.2/32  ['1exampleofdummykey12345abcdef6789ghijklmnop=']\nmanagement  restful      10.220.0.3/32  ['1exampleofdummykeyA79s0VsN5SFahT2fqxyooQAjQ=']\nresearcher  researcher1  10.222.0.2/32  ['1exampleofdummykeyVo+lj/ZfT/wYv+I9ddWYzohC0=']\nnode        NODETAG      10.221.0.2/32  ['1exampleofdummykey/Z1SKEzjsMkSe1qztF0uXglnA=']\n</code></pre> </li> <li> <p>restart all containers running on the server side</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn stop vpnserver mqtt restful researcher\n[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn start vpnserver mqtt restful researcher\n</code></pre> <p>VPN configurations and container files are kept unchanged when restarting containers.</p> </li> <li> <p>clean running containers, container files and temporary files on the server side. Requires to stop containers before.</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn stop vpnserver mqtt restful researcher\n[user@server $] source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment clean\n[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean\n</code></pre> <p>Warning: all VPN configurations, researcher configuration files,experiment files and results, etc. are deleted when cleaning.</p> <p>To clean also the container images:</p> <pre><code>[user@server $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean image\n</code></pre> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#misc-node-management-commands","title":"Misc node management commands","text":"<p>Some possible management commands after initial deployment include:</p> <ul> <li> <p>check all containers running on the node side</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn status node gui\n</code></pre> </li> <li> <p>restart all containers running on the node side</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn stop node gui\n[user@node $] ( cd ${FEDBIOMED_DIR}/envs/vpn/docker ; docker-compose up -d node gui )\n</code></pre> <p>VPN configurations and container files are kept unchanged when restarting containers.</p> </li> <li> <p>clean running containers, container files and temporary files on the node side. Requires to stop containers before.</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn stop node gui\n[user@node $] source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment clean\n[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean\n</code></pre> <p>Warning: all VPN configurations, node configuration files, node dataset sharing, etc. are deleted when cleaning.</p> <p>To clean also the container images:</p> <pre><code>[user@node $] ${FEDBIOMED_DIR}/scripts/fedbiomed_vpn clean image\n</code></pre> </li> </ul>"},{"location":"user-guide/deployment/deployment-vpn/#annex","title":"Annex","text":""},{"location":"user-guide/deployment/deployment-vpn/#proxy","title":"Proxy","text":"<p>On a site where access to an Internet web site requires using a proxy, configure web proxy for docker client in <code>~/.docker/config.json</code>.</p> <p>Prefix used by Fed-BioMed's communication inside the VPN (<code>10.220.0.0/14</code>) shall not be proxied. So your proxy configuration may look like (replace <code>mysiteproxy.domain</code> with your site proxy):</p> <pre><code>{\n\"proxies\":\n{\n\"default\":\n{\n\"httpProxy\": \"http://mysiteproxy.domain:3128\",\n\"httpsProxy\": \"http://mysiteproxy.domain:3128\",\n\"noProxy\": \"10.220.0.0/14\"\n}\n}\n}\n</code></pre>"},{"location":"user-guide/deployment/deployment/","title":"Fed-BioMed deployment scenarios","text":"<p>Fed-BioMed can be deployed in different ways:</p> <ul> <li>single-machine (nodes, network and researcher run on the same machine) or multiple-machine</li> <li>with or without containers (each component runs in its <code>docker</code> container) and VPN (all communications between components are tunneled in a WireGuard VPN with mutual authentication of the VPN endpoints)</li> </ul> <p>Choose a scenario depending on the context and requirements:</p> <ul> <li> <p>Single-machine without VPN/containers is the basic simple installation scenario described in the introduction tutorials. Use cases include: newcomer testing Fed-BioMed software ; FL researcher designing and testing FL methods with (non sensitive) data on the laptop ; software developer contributing to Fed-BioMed.</p> </li> <li> <p>Single-machine with VPN/containers deployment is briefly described here. The use case is the simplified testing of the VPN/containers facility (eg for testing purpose or integration tests).</p> </li> <li> <p>Multiple-machine without VPN/containers deployment is briefly described here. This should only be used when components are connected through a highly secure network.</p> </li> <li> <p>Multiple-machine with VPN/containers deployment is documented here. Most real-life deployments use this scenario to protect node data and communications between components. Typical deployment includes a secure federation server and one node for each data provider site.</p> </li> </ul> <p>Check the security model and network communications to understand which scenario fits your needs.</p>"},{"location":"user-guide/deployment/matrix/","title":"Network communication matrix","text":"<p>This page describes the network communications:</p> <ul> <li>between the Fed-BioMed components (<code>node</code>s, <code>network</code> and <code>researcher</code>), aka application internal / backend communications </li> <li>for user access to the Fed-BioMed components GUI</li> <li>for Fed-BioMed software installation and docker image build</li> </ul> <p>Communications between the components depend on the deployment scenario: with or without VPN/containers.</p> <p>Network communications for the setup of host machines and the installation of software requirements are out of the scope of this document. Network communications necessary for the host machines to run (access to DNS, LDAP, etc.) regardless of Fed-BioMed are also out of the scope of this document. They are specific to system configuration and installation processes.</p>"},{"location":"user-guide/deployment/matrix/#introduction","title":"Introduction","text":"<p>Fed-BioMed network communications basic principle is that all communications between components are outbound from one <code>node</code> or from the <code>researcher</code> to the <code>network</code>. There is no direct communication from a <code>node</code> to the <code>researcher</code>, or inbound communications to a <code>node</code> or the <code>researcher</code>.</p> <p>Nevertheless, future releases will introduce some optional direct communications between the components for using the secure aggregation feature (eg. <code>node</code>/<code>researcher</code> to <code>node</code>/<code>researcher</code> communication for cryptographic material negotiation).</p> <p>Fed-BioMed provides some optional GUI for the <code>node</code> (node configuration GUI) and the <code>researcher</code> (Jupyter notebook and Tensorboard). Currently, Fed-BioMed does not include secure communications to these GUI components. So they are configured by default to accept only communications from the same machine (localhost).</p>"},{"location":"user-guide/deployment/matrix/#software-installation","title":"Software installation","text":"<p>Network communications for software installation cover the Fed-BioMed software installation and setup until the software is ready to be used.</p> <p>They cover all deployment scenarios. If multiple machines are used, each machine needs to authorize these communications.</p> <ul> <li>direction is out (outbound communication from the component) or in (inbound communication to the component)</li> </ul> dir source machine destination machine destination port service out component Internet TCP/80 HTTP out component Internet TCP/443 HTTPS <p></p> <p>\"Component\" is the <code>node</code>, <code>network</code> or <code>researcher</code>. </p> <p>For destination machine, it is simpler to authorize outbound communications to all Internet addresses for the required ports during installation. Indeed, several packaging systems are used for installation, with no guarantee of stable IP address used by the packaging server:</p> <ul> <li>for all deployment scenarios: conda, pip (all components) and yarn/npm (node GUI component) packages</li> <li>plus for VPN/containers scenarios: dockerhub images, apt apk and cargo packages, git over https cloning, wget and curl download </li> </ul> <p>Note: when using a VPN/containers scenario, a site with very stringent requirements on <code>node</code>'s communication can avoid authorizing the above communications for installation of the node components (<code>node</code> and <code>gui</code>). For that, it needs to build the components docker image on another machine (with the above filter), save the image, copy it to the node machine, load it on the node machine. This scenario is not fully packaged and documented by Fed-BioMed but you can find some guidelines here.</p>"},{"location":"user-guide/deployment/matrix/#running-without-vpncontainers","title":"Running without VPN/containers","text":"<p>This part describes the communication matrix for running Fed-BioMed without VPN/containers:</p> <ul> <li>direction is out (outbound communication from the component) or in (inbound communication to the component)</li> <li>type of communication is either backend (between the application components) or user (user access to a component GUI). Command line user access to component from localhost are not noted here. GUI access are noted though recommended default configuration is to give access only from localhost</li> <li>status is either mandatory (needed to run Fed-BioMed) or optional (a Fed-BioMed experiment can run without this part)</li> </ul> <p>On the node component (<code>node</code> + <code>gui</code> ):</p> dir source machine destination machine destination port service type status comment out node restful TCP/8844 HTTP backend mandatory out node mqtt TCP/1883 MQTT backend mandatory out node other nodes + researcher TCP/14000+ MP-SPDZ backend optional secagg key negotation in other nodes + researcher node TCP/14000+ MP-SPDZ backend optional secagg key negotation in localhost gui TCP/8484 HTTP user optional node GUI <ul> <li><code>node</code> and <code>gui</code> also need a shared filesystem, so they are usually installed on the same machine.</li> <li>MP-SPDZ uses port TCP/14000 when one component runs on a machine. It also uses following port numbers when multiple components run on the same machine (one port per component).</li> </ul> <p>On the network component (<code>mqtt</code> + <code>restful</code>):</p> dir source machine destination machine destination port service type status comment in nodes + researcher mqtt TCP/1883 MQTT backend mandatory in nodes + researcher restful TCP/8844 HTTP backend mandatory <p></p> <p>On the researcher component (<code>researcher</code>):</p> dir source machine destination machine destination port service type status comment out researcher restful TCP/8844 HTTP backend mandatory out researcher mqtt TCP/1883 MQTT backend mandatory out researcher nodes TCP/14000+ MP-SPDZ backend optional secagg key negotation in nodes researcher TCP/14000+ MP-SPDZ backend optional secagg key negotation in localhost researcher TCP/8888 HTTP user optional Jupyter in localhost researcher TCP/6006 HTTP user optional Tensorboard"},{"location":"user-guide/deployment/matrix/#running-with-vpncontainers","title":"Running with VPN/containers","text":"<p>This part describes the communication matrix for running Fed-BioMed with VPN/containers:</p> <ul> <li>direction is out (outbound communication from the component) or in (inbound communication to the component)</li> <li>type of communication is either backend (between the application components) or user (user access to a component GUI). Command line user access to component from localhost are not noted here. GUI access are noted though recommended default configuration is to give access only from localhost</li> <li>status is either mandatory (needed to run Fed-BioMed) or optional (a Fed-BioMed experiment can run without this part)</li> </ul> <p>On the node component (<code>node</code> + <code>gui</code> ):</p> dir source machine destination machine destination port service type status comment out node vpnserver UDP/51820 WireGuard backend mandatory in localhost gui TCP/8443 HTTPS user optional node GUI <ul> <li><code>node</code> and <code>gui</code> also need a shared filesystem, so they are usually installed on the same machine.</li> </ul> <p>On the network component (<code>vpnserver</code> + <code>mqtt</code> + <code>restful</code>):</p> dir source machine destination machine destination port service type status comment in nodes + researcher vpnserver UDP/51820 WireGuard backend mandatory <ul> <li><code>mqtt</code> and <code>restful</code> also communicate with the <code>vpnserver</code> through a WireGuard tunnel</li> </ul> <p></p> <p>On the researcher component (<code>researcher</code>):</p> dir source machine destination machine destination port service type status comment out researcher vpnserver UDP/51820 WireGuard backend mandatory in localhost researcher TCP/8888 HTTP user optional Jupyter in localhost researcher TCP/6006 HTTP user optional Tensorboard <p>In this scenario, MQTT, restful and MP-SPDZ communications are tunneled within the WireGuard VPN.</p>"},{"location":"user-guide/deployment/security-model/","title":"Fed-BioMed security model","text":"<p>This page gives an overview of Fed-BioMed security model. A more complete and formal description of the security model underlying Fed-BioMed is currently work in progress.</p>"},{"location":"user-guide/deployment/security-model/#summary","title":"Summary","text":"<p>Fed-BioMed empowers the node sites</p> <p>In a Fed-BioMed instance, nodes have control. There is no notion of trusted party that has full control or full access to the other parties. The researcher can only train a model authorized by the node, on data explicitely shared by the node.</p> <p>Fed-BioMed minimizes firewall filters</p> <p>Fed-BioMed nodes and researcher only need one outbound VPN port to one server for running</p> <p>Fed-BioMed offers high protection against outsiders</p> <p>Fed-BioMed offers high protection against attacks coming from outside of the Fed-BioMed instance by isolating all communications between the components inside a VPN.</p> <p>Fed-BioMed protects from major attacks by insiders</p> <p>Fed-BioMed also identifies possible attacks coming from one of the Fed-BioMed instance components and already offers protection against major attack scenarios.</p>"},{"location":"user-guide/deployment/security-model/#assets-threats-vulnerabilities","title":"Assets, threats, vulnerabilities","text":"<p>Fed-BioMed security assets (as defined in ENISA glossary) and their assessed value are:</p> <ul> <li>node data: the primary goal of Fed-BioMed is to protect the data of the participating nodes </li> <li>host machines: they are an indirect asset (infrastructure asset) of Fed-BioMed as they host the other assets. Thus, the protection of the machines hosting the node, researcher and network components is at least as important as protecting the assets they host. Moreover, Fed-BioMed software should not be a vector to compromise the host machine or other machines/assets on the host site.</li> <li>experiment inputs (training plan source code, optional custom strategy/optimizer source code, training parameters and model hyper-parameters) and outputs (final trained model, experiment intermediate results, local training updates from nodes): the final trained model parameters are an important asset as they are the main output of the software. Experiment inputs and intermediate results are necessary to compute the final trained model, and the users may value intellectual property on them.</li> </ul> <p>Fed-BioMed identified threats are:</p> <ul> <li>outsiders: they include all the machines/people that do not belong to the Fed-BioMed instance (all but the Fed-BioMed components). They are considered to be the most likely adversaries, conducting active attacks (malicious). They mostly try to breach confidentiality of data, but may attempt any type of impact on assets.</li> <li>insiders: they are the members of the Fed-BioMed instance (node, researcher, network). They are considered to be less likely adversaries. Our current security model addresses in priority the case of honest but curious nodes and network (parties do not attempt at modifying the protocol for attacks), while the researcher may be malicious. Attacks are primarily aimed at breach data confidentiality, although they may also attempt to other kind of assets.</li> </ul> <p>Fed-BioMed main identified vulnerabilities are:</p> <ul> <li><code>1.</code> federated learning: honest but curious researcher can attempt privacy inference attacks on local model parameters sent by the nodes and try to gain some knowledge about the nodes' data</li> <li><code>2.</code> infrastructure: honest but curious node or network man in the middle (MITM) can listen to MQTT/restful exchanges between parties or access them directly on MQTT/restful. It then learns queries and results of the trainings performed by the nodes. The primary interest is to learn the local model parameters from other nodes and attempt attacks mentioned in <code>1.</code>. Network can also learn global model parameters and attempt attacks mentioned in <code>8.</code></li> <li><code>3.</code> federated learning: malicious researcher authorized to train on a node can send malicious training plan code to breach the assets. Typically, it tries to leak data from the nodes.</li> <li><code>4.</code> infrastructure: malicious insider man in the middle (MITM) can spoof the researcher or the network to execute training commands on the node (possibly <code>3.</code>)</li> </ul> <p>Fed-BioMed other vulnerabilities include:</p> <ul> <li><code>5.</code> federated learning: advanced attacks such as model poisoning, free-riding attacks, etc.</li> <li><code>6.</code> infrastructure: outsider may attempt penetration attacks on a VPN endpoint</li> <li><code>7.</code> infrastructure: insider may attempt penetration attacks on another component of the Fed-BioMed instance</li> <li><code>8.</code> federated learning: honest but curious nodes can attempt privacy inference attacks on global model parameters sent by the researcher and try to gain some knowledge about the other nodes' data</li> <li><code>9.</code> inference attacks on the final trained model: a malicious outsider that duly receives a copy of the final trained model for using it may try attacks from <code>1.</code>. This case is considered out of scope of this analysis, as it occurs outside of Fed-BioMed. Same precautions should be taken as for any machine learning model.</li> </ul>"},{"location":"user-guide/deployment/security-model/#addressing-the-vulnerabilities","title":"Addressing the vulnerabilities","text":"<p>Fed-BioMed addresses the above vulnerabilities in the following way:</p> <ul> <li>secure aggregation and differential privacy offer options to reduce the risk coming from <code>1.</code></li> <li>exploiting <code>2.</code> or <code>4.</code> would enable a node or the network component to execute same commands (training) or retrieve same information (local training updates from node) as the researcher, but no more. This is why implementation of secure communication inside the VPN was not prioritized by Fed-BioMed. Nevertheless, it is in the midterm roadmap.</li> <li>model approval functionality addresses <code>3.</code> by enabling each node site to review and authorize a training plan before it can train on the node.</li> <li>advanced federated learning attacks from <code>5.</code> will be further addressed in future releases with innovative functions. Stay tuned.</li> <li>Fed-BioMed seeks to offer minimal attack surface to penetration attacks from <code>6.</code>: the only network communication between the components is through the WireGuard VPN. Moreover, node sites that hold the data only have outbound connections to further reduce the attack surface on nodes.</li> <li>Fed-BioMed seeks to reduce the attack surface to penetration attacks from <code>7.</code> by design: nodes and researcher only have outbound communications to the MQTT/restful (except the temporary, authenticated and specialized MP-SPDZ inbound and outboung connections for negotiating secure aggregation keys). Also, the node only accepts a limited set of commands from a legitimate researcher. There is no notion of trusted party that have full control over the nodes or the node data in a Fed-BioMed instance. Finally, our future implementation of secure communication inside the VPN will further reduce this risk.</li> <li>attacks on global updates from <code>8.</code> are considered more complicated than attacks on local updates. Currently, implemented Local and Central Differential Privacy are valid mechanisms to protect against these attacks, and other specific defense strategies will be further addressed in future releases. </li> </ul>"},{"location":"user-guide/deployment/versions/","title":"Versions","text":"<p>Fed-BioMed stores and checks version numbers for several of its components.  The semantics of the versions are as follows:</p> <ul> <li>different major version: incompatibility that results in halting the execution immediately</li> <li>different minor or micro version: backward compatibility, provide a warning message if versions are different</li> </ul> <p>This page tracks the version changes for each component, to provide further information when incompatibilities are detected.</p>"},{"location":"user-guide/deployment/versions/#configuration-files","title":"Configuration files","text":""},{"location":"user-guide/deployment/versions/#researcher","title":"Researcher","text":"Version Changelog 0 Default version assigned prior to the introduction of versioning 1 Introduce default/version field tracking the version of this config file"},{"location":"user-guide/deployment/versions/#node","title":"Node","text":"Version Changelog 0 Default version assigned prior to the introduction of versioning 1 Introduce default/version field tracking the version of this config file"},{"location":"user-guide/deployment/versions/#breakpoints","title":"Breakpoints","text":"Version Changelog 0 Default version assigned prior to the introduction of versioning 1 Introduce <code>version</code> field in breakpoint.json file. In case of incompatible version, see the section below"},{"location":"user-guide/deployment/versions/#messaging-protocol","title":"Messaging protocol","text":"<p>Note that due to the two-sided nature of the communication, every change to the messaging protocol is equivalent to a major change.</p> <p>Incompatible versions</p> <p>In case of version mismatch, the only solution is to upgrade the software to have the same version on researcher and all nodes.</p> Version Changelog 0 Default version assigned prior to the introduction of versioning 1 Introduce <code>protocol_version</code> field in messages. In case of incompatibility see the warning message above."},{"location":"user-guide/installation/","title":"Installation guide","text":"<ul> <li>Installation guide for Windows</li> </ul>"},{"location":"user-guide/installation/windows-installation/","title":"Specific instructions for Windows installation","text":"<p>Fed-BioMed requires Windows 10 or 11, WSL2 and docker. It can run on a physical machine or a virtual machine.</p> <p>This documentation gives the steps for a typical Windows 10 installation, steps may vary depending on your system.</p>"},{"location":"user-guide/installation/windows-installation/#step-0-optional-virtual-machine","title":"Step 0: (optional) virtual machine","text":"<p>Skip this step if running Windows on a physical (native) machine, not a virtual machine.</p> <p>Requirement : choose an hypervisor compatible with other installation</p> <ul> <li>VMware Workstation 15.5.5 or above can be used, it is compatible with HyperV. We successfully ran Fed-BioMed with VMware Workstation 16.</li> <li>VirtualBox 6.1 cannot be used as it conflicts with Hyper-V \"Virtual Machine Platform\" which is needed for WSL2</li> </ul> <p>Tips :</p> <ul> <li>enable virtualization engine for your VM. For VMware Workstation 16, check the boxes in Virtual Machine Settings &gt; Hardware &gt; Processors &gt; Virtualization Engine</li> <li>allocate 8GB RAM or more to the VM, this is needed for conda PyTorch installation. For VMware Workstation 16 this can be done in Virtual Machine Settings &gt; Hardware &gt; Memory</li> </ul>"},{"location":"user-guide/installation/windows-installation/#step-1-windows","title":"Step 1: Windows","text":"<p>Requirement : Windows 10 version 2004 and higher (Build 19041 and higher) or Windows 11 is needed for WSL2 and Docker Desktop.</p> <ul> <li>update Windows</li> <li>reboot Windows</li> </ul> <p>Requirement: Windows Enterprise, Pro or Education edition (needed for Hyper-V functionality, which is not present in Home edition)</p> <p>Requirement : Hyper-V \"Virtual Machine Platform\" activation</p> <ul> <li>enable Hyper-V</li> <li>reboot Windows</li> </ul>"},{"location":"user-guide/installation/windows-installation/#step-2-wsl","title":"Step 2: WSL","text":"<p>WSL (Windows Subsystem for Linux) is a tool that allows to run Linux within a Windows system. Version 2 of WSL is needed for docker. We successfully tested Fed-BioMed with Ubuntu-20.04 distribution.</p> <p>Requirement : WSL version 2</p> <ul> <li>activate WSL with main menu &gt; enter 'Turn Windows Feature on or off' &gt; and click on 'Windows Subsystem for Linux' checkbox</li> <li>reboot Windows</li> <li>set version 2 in a Windows command tool <pre><code>cmd&gt; wsl --set-default-version 2\n</code></pre></li> <li>reboot Windows</li> </ul> <p>Requirement : a WSL distribution, eg Ubuntu</p> <ul> <li>install a distribution in a Windows command tool <pre><code>cmd&gt; wsl --install -d Ubuntu\n</code></pre></li> <li>if required by install, download and install Linux kernel update</li> <li>reboot Windows</li> </ul> <p>Check that WSL uses version 2 and Ubuntu is installed in a Windows command tool : <pre><code>cmd&gt; wsl -l -v\n  NAME                   STATE           VERSION\n* Ubuntu                 Running         2\n</code></pre></p> <p>Open a WSL session from a Windows command tool : <pre><code>cmd&gt; wsl\nuser@wsl-ubuntu$\n</code></pre></p>"},{"location":"user-guide/installation/windows-installation/#step-3-docker","title":"Step 3: docker","text":"<p>Requirement : docker and docker-compose</p> <p>Open an administrator session in WSL Ubuntu : <pre><code>user@wsl-ubuntu$ sudo bash\nroot@wsl-ubuntu#\n</code></pre></p> <p>Alternative 1 : Docker Desktop</p> <ul> <li>install Docker Desktop in Windows. Check the product license.</li> <li>reboot Windows</li> </ul> <p>Alternative 2 : docker engine</p> <ul> <li>install docker engine with as admin (root) account in WSL Ubuntu. Please note that <code>docker container run hello-world</code> will not work until we complete the steps below</li> <li>install docker compose <pre><code>root@wsl-ubuntu# apt install -y docker-compose\n</code></pre></li> <li>if you use an account named <code>USER</code> under Ubuntu, authorize it to use docker by typing under an admin (root) account in WSL Ubuntu : <pre><code>root@wsl-ubuntu# adduser USER docker\n</code></pre></li> <li>open a new WSL Ubuntu terminal so that it is authorized to use docker</li> <li>at each Ubuntu restart, launch docker daemon <pre><code>root@wsl-ubuntu# nohup dockerd &amp;\n</code></pre></li> </ul> <p>Check that you can use docker with your user account under Ubuntu : <pre><code>user@wsl-ubuntu$ docker container run hello-world\n</code></pre></p>"},{"location":"user-guide/installation/windows-installation/#step-4-conda","title":"Step 4: conda","text":"<p>Requirement : conda installed in Ubuntu and configured for your user account</p> <ul> <li>install Anaconda under Ubuntu, using your user account</li> <li>during installation, answer Yes to question \u201cDo you wish the installer to initialize Anaconda3 by running conda init?\u201d</li> <li>activate conda for your Ubuntu session <pre><code>user@wsl-ubuntu$ source ~/.bashrc\n</code></pre></li> </ul>"},{"location":"user-guide/installation/windows-installation/#step-5-fed-biomed","title":"Step 5: Fed-BioMed","text":"<p>Follow Fed-BioMed Linux installation tutorial from the git clone command</p> <p>When running <code>network</code> for the first time, a Windows defender pop up may appear (triggered by docker), choose \"authorize only on private network\".</p> <p>You may experience some differences when using Fed-BioMed on Windows in comparison to other systems : this is because WSL does not have a graphical interface. Everything happens as if you were running a headless Linux machine.</p>"},{"location":"user-guide/installation/windows-installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/installation/windows-installation/#step-2-troubleshooting","title":"Step 2 troubleshooting","text":""},{"location":"user-guide/installation/windows-installation/#error-1-the-virtual-machine-could-not-be-started-because-a-required-feature-is-not-installed","title":"Error 1: <code>The virtual machine could not be started because a required feature is not installed</code>","text":"<p>When installing Linux, if this error happens:</p> <pre><code>Installing, this may take a few minutes...\nWslRegisterDistribution failed with error: 0x80370102\nError: 0x80370102 The virtual machine could not be started because a required feature is not installed.\n</code></pre> <p>This means either you need to enable virtualisation on the bios of your computer or to enable Hyper-V : for the latter, go in</p> <pre><code>main menu &gt; enter 'Turn Windows Feature on or off' &gt; and click on Hyper-V checkbox\n</code></pre> <p>Then restart Linux distribution installation.</p>"},{"location":"user-guide/installation/windows-installation/#error-2-failed-with-error-0x80004005","title":"Error 2: <code>failed with error: 0x80004005</code>","text":"<p>If this error happen when installing a Linux disribution:</p> <pre><code>WSLRegisterDistribution failed with error: 0x80004005\n</code></pre> <ul> <li>Press <code>Win + R</code>: it will open a window named <code>Run tasks</code></li> <li>Enter <code>REGEDIT</code> and hit <code>OK</code></li> <li>Navigate to : <code>HKEY_LOCAL_MACHINE\\CurrentControlSet\\Services\\LxssManager</code></li> <li>Set the value Data to 2 and exit <code>REGEDIT</code>.</li> <li>reboot the machine and see if it is working</li> </ul>"},{"location":"user-guide/installation/windows-installation/#step-5-troubleshooting","title":"Step 5 troubleshooting:","text":"<p>If we encounter 'Operation not permitted' error on cloning git repository, you may follow below steps.</p> <p>Error --&gt; fatal: could not set 'core.filemode' to 'false'</p> <ol> <li> <p>Launch Ubuntu WSL.</p> </li> <li> <p>Create the file /etc/wsl.conf if it doesn't exist.</p> </li> <li> <p>Open the file (nano /etc/wsl.conf) and add the below lines:     [automount]     options = \"metadata\"</p> </li> <li> <p>Save the file and shoutdown WSL</p> </li> <li> <p>Ralaunch Ubuntu WSL</p> </li> </ol> <p>If the problem still persists, you may try restarting the machine and then execute git clone command.</p> <p>We detail here two common issues encountered when instlling and running <code>Fed-BioMed</code>.</p> <ul> <li> <p>If launching Fed-BioMed researcher fails with a message mentioning a display error, you may need to use an alternate IP address. There are two ways of fixing this:</p> <ol> <li>run jupyter <code>jupyter notebook --ip $(python3 -c \"import subprocess; subprocess.run(['hostname', \"-I\"], text=True).sdtout\")</code> and connect to the IP address given by jupyter-notebook.</li> <li>or you can just connect to the IP address given by the command <code>ip addr | grep eth0 | grep inet</code> instead of connecting to <code>localhost</code>.</li> </ol> </li> <li> <p>If Fed-BioMed fails with a message mentioning a <code>System.Management.Automation</code> error you may need to give WSL which browser to use. Set the <code>BROWSER</code> environment variable to the path to the browser. For example to use Microsoft Edge the path and command are commonly : <pre><code>user@wsl-ubuntu$ export BROWSER=export BROWSER=/mnt/c/Program\\ Files\\ \\(x86\\)/Microsoft/Edge/Application/msedge.exe\n</code></pre></p> </li> </ul>"},{"location":"user-guide/nodes/","title":"About Nodes specificities and configurations","text":"<ul> <li>Nodes configuration</li> </ul>"},{"location":"user-guide/nodes/configuring-nodes/","title":"Node Configuration","text":"<p>Fed-BioMed framework has 3 main components: <code>node</code>, <code>network</code>, and <code>researcher</code>. A <code>node</code> stores private datasets and  perform training in response to researcher's train requests. It communicates with the <code>researcher</code> component over MQTT (messaging server) that is running in the <code>network</code> component. It also uses file repository of the <code>network</code> component  to exchange model parameters and files with the researcher. </p> <p>A basic node configuration contains following settings;</p> <ul> <li>providing a python environment</li> <li>assigning a unique node id </li> <li>setting security parameters such as training plan approval mode, hashing algorithms </li> <li>providing the connection credentials for the network component</li> </ul> <p>Note</p> <p>These basic configurations are done automatically using default values by the scripts provided in Fed-FedBioMed.  However, it is possible to modify the configuration manually through configuration file. </p>"},{"location":"user-guide/nodes/configuring-nodes/#environment-for-nodes","title":"Environment for Nodes","text":"<p>A <code>node</code> requires a conda environment to be able to run. This environment provides necessary python modules for both the task management part and the model training part.  Thanks to Fed-BioMed Node CLI, this conda environment can be created easily. </p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/configure_conda\n</code></pre> <p>The command above creates three different environments including <code>fedbiomed-node</code>. You can see details in installation tutorial. This documentation will focus more on configuration steps.  </p> <p>Note: <code>FEDBIOMED_DIR</code> represents the path of the base Fed-BioMed project directory.</p>"},{"location":"user-guide/nodes/configuring-nodes/#config-files","title":"Config Files","text":"<p>Config files are the <code>ini</code> files including the following information.</p> <ul> <li> <p>Default Parameters: </p> <ul> <li><code>node_id</code>: This is the unique id which identifies the node. </li> <li><code>uploads_url</code>: The URL string that indicates upload request URL for the model parameters. </li> </ul> </li> <li> <p>MQTT Parameters: </p> <ul> <li><code>broker_ip</code>: The IP address for connecting MQTT to consume and publish messages with the researcher</li> <li><code>port</code>: Connection port for MQTT messaging server</li> <li><code>keep_alive</code>: Delay in seconds before sending an applicative MQTT ping if there is no MQTT exchange during this period.   </li> </ul> </li> <li> <p>Security Parameters:</p> </li> <li><code>hashing_algorithm</code>: The algorithm will be used for hashing training plan scripts to verify if the requestes    training plan matches the one approved on the node side.</li> <li><code>training_plan_approval</code>: Boolean value to switch training plan approval    to verify training plan scripts before the training.</li> <li><code>allow_default_training_plans</code>: Boolean value to enable automatic approval of example training plans provided by Fed-BioMed. </li> </ul> <p>An example for a config file is shown below;</p> <pre><code>[default]\nnode_id = node_7fd39224-4040-448f-8360-577e2066e2ce\nuploads_url = http://localhost:8844/upload/\n\n[mqtt]\nbroker_ip = localhost\nport = 1883\nkeep_alive = 60\n\n[security]\nhashing_algorithm = SHA256\nallow_default_training_plans = True\ntraining_plan_approval = False\n</code></pre>"},{"location":"user-guide/nodes/configuring-nodes/#starting-nodes-with-config-files","title":"Starting Nodes with Config Files","text":"<p>Currently, creating config files is done by the <code>fedbiomed_run</code> script. It automatically creates config files based on  default values assigned in the <code>fedbiomed.node.environ</code>.  Starting nodes with specific config creates a new  configuration file. The following command creates specific config file with default settings and starts the node. </p> <pre><code>$ ./scripts/fedbiomed_run node config config-n1.ini start\n</code></pre> <p>If you run this command, you can see a new config file created in the <code>etc/</code> directory of the Fed-BioMed.  Each node that runs in the same host should have a different node id and configuration file. Starting  another node that uses the same config file with another raises errors. Therefore, if you launch multiple nodes please make sure to use different configurations.</p> <p>Listing and adding datasets follows the same logic. If you want to list or add datasets in the nodes that is different  from the default one, you need to specify the config file.</p> <pre><code>$ ./scripts/fedbiomed_run node config config-n1.ini list\n$ ./scripts/fedbiomed_run node config config-n1.ini add\n</code></pre> <p>Configurations for deployment</p> <p>This is the process for the local development environment. Please see  deployment instructions with VPN for production.  </p>"},{"location":"user-guide/nodes/deploying-datasets/","title":"Deploying Datasets in Nodes","text":"<p>Deploying datasets in nodes makes the datasets ready for federated training with Fed-BioMed experiments. It is the process of indicating metadata of the dataset. Thanks to that, node can access data and perform training based on given arguments in the experiment. A node can deploy multiple datasets and train models on them. Currently, Fed-BioMed supports adding CSV and Image datasets into nodes. It adds data files from the file system and saves their locations into the database. Each dataset should have the following attributes;</p> <ul> <li>Database Name: The name of the dataset</li> <li>Description: The description of the dataset. Thanks to that researchers can understand what the dataset is about.</li> <li>Tags: This attribute identifies the dataset. It is important because when the experiment is initialized for training it searches the nodes based on given tags.</li> </ul>"},{"location":"user-guide/nodes/deploying-datasets/#adding-dataset-with-fed-biomed-cli","title":"Adding Dataset with Fed-BioMed CLI","text":"<p>You can use Fed-BioMed CLI to add a dataset into a node. You can either configure a new node by adding new data or you can use a node which has been already configured. The config files in the <code>etc</code> directory of Fed-BioMed corresponds to the nodes that have already been configured.</p> <p>The following code will add the dataset into the node configured using the <code>config-n1.ini</code> file. However, if there is no such config file, this command will automatically create a new one with a given file name which is <code>config-n1.ini</code>.</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini add\n</code></pre> <p>Another option is to add a dataset into the default node without addressing any config file. In this case, a default config file for the node is created.</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node add\n</code></pre> <p>Note: Adding dataset into nodes with config is better when you work with multiple nodes.</p> <p>Before adding a dataset into a node, please make sure that you already prepared your dataset and saved it on the file system. Adding a dataset doesn't mean that you can add it from remote sources. The data should be in the host machine.</p> <p>Please open a terminal and run one of the commands above. Afterward, you will see the following screen in your terminal.</p> <pre><code>Conda   env: fedbiomed-node\nPython  env: /path/to/your/fedbiomed/\nMQTT   host: localhost\nMQTT   port: 1883\nUPLOADS url: http://localhost:8844/upload/\n\n\n   __         _ _     _                          _                   _\n  / _|       | | |   (_)                        | |                 | |\n| |_ ___  __| | |__  _  ___  _ __ ___   ___  __| |  _ __   ___   __| | ___\n |  _/ _ \\/ _` | '_ \\| |/ _ \\| '_ ` _ \\ / _ \\/ _` | | '_ \\ / _ \\ / _` |/ _ \\\n | ||  __/ (_| | |_) | | (_) | | | | | |  __/ (_| | | | | | (_) | (_| |  __/\n |_| \\___|\\__,_|_.__/|_|\\___/|_| |_| |_|\\___|\\__,_| |_| |_|\\___/ \\__,_|\\___|\n\n        - \ud83c\udd94 Your node ID: node_&lt;id_of_the_node&gt;\n\nWelcome to the Fed-BioMed CLI data manager\nPlease select the data type that you're configuring:\n        1) CSV\n        2) default\n        3) mednist\n        4) images\nselect:\n</code></pre> <p>It asks you to select what kind of dataset you would like to add. As you can see, it offers four options, namely <code>csv</code>, <code>default</code>, <code>mednist</code>, and <code>images</code>.  The <code>default</code> and <code>mednist</code> option are configured to automatically downloading and adding the MNIST and MedNIST datasets respectively. To configure your data, you should select <code>csv</code> or <code>image</code> option according to your needs. Let's suppose that you are going to add a CSV dataset. To do that you should type 1 and press enter.</p> <pre><code>Name of the database: My Dataset\nTags (separate them by comma and no spaces): #my-csv-data,#csv-dummy-data\nDescription: Dummy CSV data\n</code></pre> <p>As you can see, it asks for the information which are necessary to save your data. For the <code>Tags</code> part, you can enter only one tag or more than one. Please make sure that you use a comma to separate multiple tags (without space). Afterward, it will open a browser window and ask you to select your dataset. After selecting, you will see the details of your dataset.</p> <pre><code>Great! Take a look at your data:\nname        data_type    tags                                 description     shape        path               dataset_id\n----------  -----------  -----------------------------------  ----------      ---------    ---------------    -----------\nMy Dataset  csv          ['#my-csv-data', '#csv-dummy-data']  Dummy CSV data  [1000, 15]   /pat/to/your.csv   dataset_&lt;id&gt;\n</code></pre> <p>You can also check the list of datasets by using the following command:</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini list\n</code></pre> <p>It will return the datasets saved into the node which use the <code>config-n1.ini</code> file as config.</p>"},{"location":"user-guide/nodes/deploying-datasets/#how-to-add-another-dataset-to-the-same-node","title":"How to Add Another Dataset to the Same Node","text":"<p>As mentioned, nodes can store multiple datasets. You can follow the previous steps to add another dataset.  While adding another dataset, the config file of the node should be indicated. Otherwise, CLI can create or use  default node config to deploy your dataset. It is allowed to add datasets  with the same file path, or name. As the tags are used as an identifier for the dataset, the CLI checks  they are not conflicting with another dataset on the same node.</p> <p>Conflicting tags between datasets</p> <p>Tags from two datasets on the same node need to respect some rules:</p> <ul> <li>tags from one dataset cannot be a subset of the tags of another dataset</li> <li>tags from one dataset cannot be a superset of the tags of another dataset</li> </ul> <p>As a consequence, two datasets on the same node cannot have exactly the same tags.</p> <p>For example, CLI on a node:</p> <ul> <li>accepts to register dataset1 with tags <code>[ 'tag1', 'tag3' ]</code>, dataset2 with tags <code>[ 'tag1', 'tag2' ]</code> and dataset3 with tags <code>[ 'tag2', 'tag3' ]</code></li> <li>refuses to register dataset1 with tags <code>[ 'tag1', 'tag2' ]</code> and dataset2 with tags <code>[ 'tag1' ]</code></li> <li>refuses to register dataset1 with tags <code>[ 'tag1', 'tag2' ]</code> and dataset2 with tags <code>[ 'tag1', 'tag2', 'tag3' ]</code></li> </ul>"},{"location":"user-guide/nodes/node-gui/","title":"Node GUI","text":"<p>Fed-BioMed provides a node user interface which currently allows node users to manage datasets and training plans easily. The GUI can be used as an alternative to command line interface (CLI).  Since the implementation of GUI is still in beta state it is only available for local access.</p>"},{"location":"user-guide/nodes/node-gui/#installing-the-node-gui-environment","title":"Installing the Node GUI Environment","text":"<p>The conda environment <code>fedbiomed-gui</code> should be installed on the machine where the node GUI will be running. Node and Node GUI should be running on same host/machine. This will allow GUI to access node's database and files.</p> <p>Technologies</p> <p>The back-end of GUI is developed using Flask and front-end with ReactJS.</p> <p>The following command will install GUI conda environment including <code>node.js</code> for <code>ReactJS</code> and necessary python libraries for back-end APIs.</p> <p>Please run the following command to install GUI conda environment.</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/configure_conda gui\n</code></pre>"},{"location":"user-guide/nodes/node-gui/#starting-node-gui","title":"Starting Node GUI","text":"<p>The option <code>gui</code> of the script <code>fedbiomed_run</code> is configured for starting Node GUI. You can run <code>${FEDBIOMED_DIR}/scripts/configure_conda gui --help</code> for usage and the description of the options, or you can follow the sections for more detailed information.</p> <p>Attention!</p> <p>By default <code>fedbiomed_run gui [OPTIONS]</code> starts Flask server accepting access only from <code>localhost</code>. It is not safe to open access from remote host machine since it is not a secured web server yet. We highly recommend to use <code>localhost</code> through SSH Tunnel for remote access.</p>"},{"location":"user-guide/nodes/node-gui/#options-to-start-the-gui","title":"Options to Start The GUI","text":"<p>Node GUI can be started through the script <code>fedbiomed_run</code> with various settings such as ip, port, folder where data files are stored or the configuration that specifies the node that the GUI will run for.</p> <p>The following command is the basic command to start Node GUI with default settings. This command assumes that the data files are stored in <code>${FEDBIOMED_DIR}/data</code> and the default node config is <code>config_node.ini</code> which is stored in <code>${FEDBIOMED_DIR}/etc</code>.</p> <p><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run gui start</code></p> <p>After running this command the GUI will start listening on <code>localhost</code> on port <code>8484</code>. You can access the GUI through browser <code>http://localhost:8484</code>. This page will redirect you to the login page. The credentials and possible configurations for log-in are explained in the default admin configuration.</p> <p>Important<p>If you are starting Fed-BioMed GUI with default settings please make sure that the <code>config_node.ini</code> is existing in <code>${FEDBIOMED_DIR}/etc</code>. Otherwise, starting operation will fail since it won't be able to find the node configuration. If you don't have <code>config_node.ini</code> yet please start the node before starting the GUI.</p> </p>"},{"location":"user-guide/nodes/node-gui/#using-different-port-and-host","title":"Using Different Port and Host","text":"<p>Custom ports and host IP address can be specified as long as the port in the specified IP isn't already in use.</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run gui --port &lt;port&gt; --host &lt;ip-address|localhost&gt; start\n</code></pre>"},{"location":"user-guide/nodes/node-gui/#specifying-data-folder","title":"Specifying Data Folder","text":"<p>You might want to store your data files in a different folder. In such cases you can use the option <code>--data-folder</code> to specify which folder is used that includes data files.</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run gui --data-folder &lt;path/to/data/folder&gt; start\n</code></pre> <p>Uploading data files through Fed-BioMed is not allowed.</p> <p>Fed-BioMed assumes that the datasets or the datafiles that will be deployed in the node are already present in the data folder that is specified. Fed-BioMed Node GUI will help you to use these stored datasets in node.</p>"},{"location":"user-guide/nodes/node-gui/#specifying-node-configuration","title":"Specifying Node Configuration","text":"<p>It is possible to specify the node that the user interface will be used for through the option <code>config</code>.</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run gui --config &lt;config-name&gt;.ini start\n</code></pre> <p>Thanks to this option it is possible to start multiple GUI for multiple nodes on the same machine as long as the ports are different.</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run gui --port 5001 --config config-node-1.ini start\n$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run gui --port 5002 --config config-node-2.ini start\n$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run gui --port 5003 --config config-name-3.ini start\n</code></pre> <p>If it is desired they can share the same data folder.</p>"},{"location":"user-guide/nodes/node-gui/#configuration-file","title":"Configuration file","text":"<p>Apart from <code>fedbiomed_run</code> command, some options can be configured through GUI configuration file and used without specifying each time the node is started. This file is located in <code>${FEDBIOMED_DIR}/gui/config_gui.ini</code>.</p>"},{"location":"user-guide/nodes/node-gui/#server-configuration","title":"Server Configuration","text":"<p>You can modify <code>HOST</code>, <code>IP</code> and <code>DATA_PATH</code> (equivalent of <code>--data-folder</code>) in the server section of the configuration.</p> <pre><code>; --------------------------------------------------------------------------------------------\n; Server configuration -----------------------------------------------------------------------\n; --------------------------------------------------------------------------------------------\n[server]\n\nHOST = localhost\nPORT = 8484\nDATA_PATH = /data\n</code></pre>"},{"location":"user-guide/nodes/node-gui/#default-admin-configuration","title":"Default Admin Configuration","text":"<p>When the Fed-BioMed GUI is started for the first time it will create a default admin with the credentials declared in the <code>[init_admin]</code> section of the configuration file.  By default, the email  will be <code>admin@fedbiomed.gui</code> and the password <code>admin</code>. You can modify the password either in configuration file or in GUI through User Panel but the e-mail can only be modified from the configuration file.</p> <pre><code>;---------------------------------------------------------------------------------------------\n; Initial admin credentials ------------------------------------------------------------------\n; --------------------------------------------------------------------------------------------\n[init_admin]\n\n; --------------------------------------------------------------------------------------------\n; - IMPORTANT!!! Please update initial admin credentials for production ----------------------\n; --------------------------------------------------------------------------------------------\nemail = admin@fedbiomed.gui\npassword = admin\n</code></pre> <p>Admin e-mail</p> <p>Please modify admin e-mail address before starting the node GUI for the first time. Otherwise, it will create an admin with default e-mail address. If the admin is already created it can only be changed manually through database file.</p> <p>e-mail addresses</p> <p>Currently, e-mail addresses are only used a login name by Fed-BioMed GUI. This is neither a user identity existing in the whole Fed-BioMed instance, nor used to send e-mails to the GUI user.</p>"},{"location":"user-guide/nodes/node-gui/#production-mode","title":"Production Mode","text":"<p>By default <code>fedbiomed_run gui</code> launches Node GUI in development mode that uses Flask web server.  However, using Flask server is not recommended for the production. Therefore, Node GUI has been configured to run on Gunicorn application server when the production mode is activated. Please type the following command to activate production mode.</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run gui --production start\n</code></pre> <p>It is also possible to activate production mode by setting  env variable <code>GUI_PRODUCTION=1</code> or <code>GUI_PRODUCTION=True</code></p> <pre><code>$ GUI_PRODUCTION=1 ${FEDBIOMED_DIR}/scripts/fedbiomed_run gui --production start\n</code></pre> <p>Please use a web server</p> <p>Gunicorn is an application server, and it is strongly recommended by Gunicorn to use proxy web server such as Nginx to  forward requests to Gunicorn using reverse proxy.</p>"},{"location":"user-guide/nodes/node-gui/#setting-an-ssl-certificate","title":"Setting an SSL Certificate","text":"<p>Gunicorn allows setting SSL certificate on application server layer. Please use following command to set SSL certificate for the application server.</p> <pre><code>$ GUI_PRODUCTION=1 ${FEDBIOMED_DIR}/scripts/fedbiomed_run gui --key-file &lt;path-to-key-file&gt; --cert-file &lt;path-to-cert-file&gt; start\n</code></pre> <p>SSL certificate can also be set through proxy server (e.g. Nginx) instead of application server.</p> <p>Nginx proxy server for GUI is provided in VPN/containers deployment mode</p> <p>Fed-BioMed provides a ready-to-deploy Node GUI container in VPN/containers deployment mode, that is configured to use Nginx as a proxy server and Gunicorn as an application server. This also allows for setting custom SSL certificates. Please refer to the  VPN deployment documentation.</p>"},{"location":"user-guide/nodes/training-plan-security-manager/","title":"Security: Training Plan Security Manager","text":"<p>Federated learning in Fed-BioMed is performed by sending training plans to each node and by requesting nodes to train model from the training plan. Hence, the content of training plan files plays an important role in terms of privacy and security. Therefore, Fed-BioMed offers a feature to run only pre-approved training plans. Training plan files that are sent by a researcher with the training request must be previously approved by the node side. This training plan approval process avoids possible attacks that come through the training plan code files. By running into nodes, they may access private data or jeopardize the node. The approval process should be done by a real user/person who reviews the training plan file. The reviewer should make sure the training plan doesn't contain any code that might cause privacy issues or harm the node. </p> <p>We talk about \"Training plan registration\" as it is a whole training plan that is registered, approved and controlled on a node, not only the training plan's model.</p>"},{"location":"user-guide/nodes/training-plan-security-manager/#how-the-training-plans-get-registered-approved-and-controlled","title":"How the Training Plans get Registered, Approved and Controlled","text":"<p>Training Plan registration and approval can be done through the Fed-BioMed node CLI (Command Line Interface) ou GUI (Graphical User Interface) before or after the node is started. Each training plan should have a unique name, a unique file path and a unique hash.</p> <ul> <li>During the registration, the <code>TrainingPlanSecurityManager</code> of the Fed-BioMed hashes the content of the training plan file and saves the hash into the persistent database.</li> <li>During the approval, the training plan is then marked with <code>Approved</code> status in the database (<code>Approved Training Plans</code> in Figure 1).</li> <li>Then when training begins, training plan files that are sent by a researcher are first hashed and compared to the saved hashes of approved training plans: this is training plan control. This process flow is presented in the following figure. </li> </ul> <p> Figure 1 - Controlling training plan file requested by researcher</p> <p>Details of <code>Figure 1</code> workflow:</p> <ol> <li><code>Researcher</code> creates a <code>TrainingPlan</code> to be trained on the <code>Node</code></li> <li><code>Researcher</code> submits a <code>TrainingPlan</code> to the <code>Node</code>: he/she sends a train request to the node ... </li> <li>... and sends a training plan file containing the <code>TrainingPlan</code></li> <li><code>Node</code> receives <code>TrainingPlan</code> sent by <code>Researcher</code></li> <li><code>Node</code> computes hash of the incoming <code>TrainingPlan</code>.</li> <li><code>Node</code> checks if <code>Researcher</code>'s training plan has already been approved by  comparing its hash to the existing pool of approved training plan hashes.</li> <li>If <code>TrainingPlan</code> has been approved by the <code>Node</code>, <code>Researcher</code> will be able to train his/her training plan on the <code>Node</code>.  </li> </ol>"},{"location":"user-guide/nodes/training-plan-security-manager/#hashing","title":"Hashing","text":"<p><code>TrainingPlanSecurityManager</code> of Fed-BioMed provides various hashing algorithms. These hashing algorithms are guaranteed by the Python <code>hashlib</code> built-in library. The hashing algorithm can be selected by configuring the configuration file of the node. Hashing algorithms provided by Fed-BioMed are: <code>SHA256</code>, <code>SHA384</code>, <code>SHA512</code>, <code>SHA3_256</code>, <code>SHA3_384</code>, <code>SHA3_512</code>, <code>BLAKE2B</code>, and <code>BLAKE2S</code>. You can have more information about these hashing algorithms in <code>hashlib</code> documentation page. </p>"},{"location":"user-guide/nodes/training-plan-security-manager/#checksum-operation","title":"Checksum Operation","text":"<p>Checksum control operation is done by querying database for hash of the requested training plan. Therefore, the training plan files that are used should be saved and approved into database by using Fed-BioMed CLI ou GUI before the checksum verification operation (train request). This operation controls whether any existing and approved training plan matches the training plan requesting to train on the node. </p> <p><code>TrainingPlanSecurityManager</code> minifies training plan files just before hashing the training plan file. The minification process removes spaces and comments from the training plan file. The purpose of using minified training plans is to avoid errors when the requested training plan file has the same code but more or less comments or empty spaces than the training plan which is approved. Since the spaces and the comments will have no effect when executing training plans, this process will not open a back door for attacks. Therefore, having more spaces or comments than the registered training plan will not affect the checksum result (and thus the hashing). </p>"},{"location":"user-guide/nodes/training-plan-security-manager/#managing-nodes-for-training-plan-control","title":"Managing Nodes for Training Plan Control","text":"<p>Training Plan Control activation on nodes can be managed either through configuration file or Fed-BioMed CLI. The configuration file of the node includes a section named <code>security</code> to control/switch options for selecting hashing algorithm, enabling/disabling training plan control, and accepting default training plans as approved. By default, Fed-BioMed does not enable training plan control. It means when you start or add data to the node for the first time, if the configuration file doesn't exist, it creates a new configuration file with training plan control disabled (<code>training_plan_approval = False</code>). </p>"},{"location":"user-guide/nodes/training-plan-security-manager/#default-training-plans","title":"Default Training Plans","text":"<p>Default training plans are a subset of the training plan files that are created for Fed-BioMed tutorials, i.e. some of the training plans contained in <code>/notebooks</code> folder. These training plans are saved into <code>envs/common/default_training_plans/</code> directory. If the node is configured to allow default training plans for training, it registers default training plans when the node is started. These training plans are saved for testing purposes, and they can be disabled in a production environment.</p> <p>Note<p>The hashes of the default training plans aren't updated while starting the node if the node is configured not to allow default training plans. However, default training plans might be already saved into database previously. Even if there are default training plans in the database, the node does not approve requests for the default training plans as long as this option is disabled.</p> </p>"},{"location":"user-guide/nodes/training-plan-security-manager/#config-files","title":"Config Files","text":"<p>When the new node is created without any specified configuration file or any options, the default configuration file is saved into the <code>etc</code> directory of Fed-BioMed as follows.</p> <p><pre><code>[default]\n# other parameters\n\n[mqtt]\n# parameters for mqtt \n\n[security]\nhashing_algorithm = SHA256\nallow_default_training_plans = True\ntraining_plan_approval = False\n</code></pre> As you can see, by default, training plan control (<code>training_plan_approval</code>) is disabled. For enabling or disabling this feature, you can change its value to <code>True</code> or <code>False</code>. Any values different from <code>True</code> or <code>False</code> will be counted as <code>False</code>. The node should be restarted to apply changes after updating the config file.</p> <p>Attention<p>When the training plan control is <code>False</code>, <code>allow_default_training_plans</code> has no effect because there is no  training plan control operation for train requests.  </p> </p>"},{"location":"user-guide/nodes/training-plan-security-manager/#changing-hashing-algorithm","title":"Changing Hashing Algorithm","text":"<p>By default, Fed-BioMed uses the <code>SHA256</code> hashing algorithm to hash training plan files both for registering and checking. It can be changed based on provided algorithms by Fed-BioMed. These algorithms are already presented in the \"Hashing\" section of this article. After the hashing algorithm is changed, the node should be restarted. When restarting the node, if the training plan control is enabled, the node updates hashes in the database by recreating them with the chosen hashing algorithm in the config file.</p>"},{"location":"user-guide/nodes/training-plan-security-manager/#using-fed-biomed-cli","title":"Using Fed-BioMed CLI","text":"<p>Fed-BioMed CLI can start nodes with options for tuning training plan management features. It is possible to change the default parameters of config file while starting a node for the first time. For instance, the following command enables training plan control and disables default training plans for the node. Let's assume we are working with a config file called <code>config-n1.ini</code>. If the <code>config-n1.ini</code> file doesn't exist, it creates the <code>config-n1.ini</code> file with the parameters <code>training_plan_approval = True</code> and <code>allow_default_training_plans = False</code>, under <code>[security]</code> sub-section. </p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini --enable-training-plan-approval --disable-default-training-plans start\n</code></pre> <p>It is also possible to start a node enabling <code>training_plan_approval</code> mode, even it is disabled in the configuration file. For instance, suppose that the <code>config-n1.ini</code> file is saved as follows, </p> <pre><code>[security]\nhashing_algorithm = SHA256\nallow_default_training_plans = False\ntraining_plan_approval = False\n</code></pre> <p>The command below forces the node to start with training plan control mode enabled and default training plans enabled. </p> <p><pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini --enable-training-plan-approval --allow-default-training-plans start\n</code></pre> or the following command enables training plan control while excluding default training plans;</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini --enable-training-plan-approval --disable-default-training-plans start\n</code></pre> <p>Note<p>Hashing algorithm should be changed directly from the configuration file. </p> </p>"},{"location":"user-guide/nodes/training-plan-security-manager/#training-plan-registration-and-approval","title":"Training Plan Registration and Approval","text":"<p>The training plan registration and approval process is done by the Fed-BioMed CLI or GUI tool.</p> <p>Fed-BioMed training plans have one of the following types on a node:</p> <ul> <li>requested training plans are sent by the researcher to nodes from inside the application (\"in band\"). This enables the researcher to easily submit a training plan to nodes for approval. This mode is the most commonly used for having an <code>Experiment</code>'s training plan approved on nodes.</li> <li>registered training plans are manually added on this node from a file (\"out of band\"). This enables the node to use a training plan from any source.</li> <li>default training plans are automatically registered and approved at node startup.</li> </ul> <p>Fed-BioMed training plans have one of the following status:</p> <ul> <li>Approved training plans are authorized to train and test on this node.</li> <li>Pending training plans are waiting for review and approval/rejection decision on this node.</li> <li>Rejected training plans are explicitly not authorized to run on this node.</li> </ul> <p>Training Plans are saved in the database with following details:</p> <pre><code>{\n'name' : '&lt;training-plan-name&gt;',\n'description' : '&lt;description&gt;',\n'training_plan_type' : 'registered',\n'training_plan_path' : '&lt;path/to/training-plan/f\u0131le&gt;',\n'training_plan_id' : '&lt;Unique id for the training plan&gt;',\n'researcher_id' : '&lt;The ID of the researcher that sent this training plan or None&gt;',\n'algorithm' : '&lt;algorithm used for the hash of the training plan file&gt;',\n'hash' : '&lt;hash of the training plan file&gt;',\n'date_registered' : '&lt;Registeration date&gt;',\n'date_created' : '&lt;The date file has been created&gt;',\n'date_modified' : '&lt;The date file has been modified&gt;',\n'date_last_action' : '&lt;The date file has been modified or hash recomputed&gt;'\n}\n</code></pre> <p>Note: training plan files are stored in the file system as <code>txt</code> files. Input training plan files used as default or registered training plans must have this format.</p>"},{"location":"user-guide/nodes/training-plan-security-manager/#using-requested-training-plans","title":"Using requested training plans","text":"<p>Requested training plans are training plans which are sent by a researcher to nodes, inside the Fed-BioMed software.</p> <p>After defining an <code>Experiment()</code> named <code>exp</code> in a researcher's notebook, the following command is typed in the notebook to send the requested training plan's training plan to the nodes of <code>exp</code>:</p> <pre><code>#exp = Experiment(...\n#   training_plan_class=MyTrainingPlan,\n#   ...)\n\nexp.training_plan_approve(MyTrainingPlan, description=\"A human readable description of the training plan\")\n</code></pre> <p>When receiving the training plan, <code>exp</code>'s nodes register the training plan in their persistent database with a type <code>registered</code>, a <code>Pending</code> status, and the <code>description</code> sent by the researcher. If a training plan with same hash already exists in the database, nothing is added and an error is returned to the researcher.</p> <p>A human reviewer then checks and decides whether the training plan should be authorized on the node, via the GUI or the CLI.</p> <p>Use the following command to view a training plan on the CLI, optionally indicating your preferred editor with <code>EDITOR</code>. All registered training plans are listed with their name and status, select the one you want to view:</p> <pre><code># EDITOR=emacs ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini view-training-plan\n\n$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini view-training-plan\n</code></pre> <p>After reviewing the training plan, use one of the following commands to either approve or reject the training plan:</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini approve-training-plan\n\n$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini reject-training-plan\n</code></pre> <p>The command returns the list of training plans that can be approved/rejected, choose the reviewed training plan from the list:</p> <pre><code>Select the training plan to approve:\n1) training_plan_5fa329d7-af27-4461-b7e7-87e5b8b5e7b6    Model ID training_plan_5fa329d7-af27-4461-b7e7-87e5b8b5e7b6  training-plan status Pending  date_last_action None\n2) training_plan_281464db-ab53-494a-bd58-951957eee762    Model ID training_plan_281464db-ab53-494a-bd58-951957eee762  training-plan status Pending  date_last_action None\nSelect: 1\n</code></pre> <p>Training Plan status on the node can later be changed again using the same <code>approve-training-plan</code> and <code>reject-training-plan</code> commands.</p> <p>When a node doesn't want to keep track of a registered training plan anymore, it can delete it from the database.  The command does not delete the file containing the modtraining planel, only the database entry for the training plan.</p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini delete-training-plan\n</code></pre> <p>The output of this command lists deletable training plans with their names and id. It asks you to select the training plan file you would like to remove. For example, in the following example, typing 1  removes the MyTrainingPlan from registered/approved  list of training plans. </p> <pre><code>Select the training plan to delete:\n1) MyTrainingPlan   Training Plan ID training_plan_98a1e68d-7938-4889-bc46-357e4ce8b6b5\n2) MyTrainingPlan2  Training Plan ID training_plan_18314625-2134-3334-vb35-123f3vbe7fu7\nSelect: </code></pre>"},{"location":"user-guide/nodes/training-plan-security-manager/#using-registered-training-plans","title":"Using Registered training plans","text":"<p>Registered training plans are training plans manually added to the node via the GUI or the CLI, from a file containing its training plan.</p> <p>The following command launches Fed-BioMed CLI for selecting a training plan file and entering a name and description for the training plan. The training plan name, its path and its hash should be unique. It means that you can not add the same training plan file multiple times. </p> <p><pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini register-training-plan\n</code></pre> After selecting the training plan file, the training plan manager computes a hash for the training plan file and saves it into the persistent database.</p> <p>The training plan type is <code>registered</code>, and status is <code>Approved</code> for the training plans that are saved through Fed-BioMed CLI.</p> <p>Each time that the node is started, training plan manager checks whether the training plan file still exists on the file system. If it is deleted,  training plan manager also deletes it from the database. Therefore, please make sure that the training plan file always exists in the path where it is stored. </p> <p>As for requested training plans, registered training plans can later be viewed (<code>view-training-plan</code>), changed status (<code>approve-training-plan</code> or <code>reject-training-plan</code>) or removed (<code>delete-training-plan</code>).</p> <p>It is also possible to update registered training plans with a different file or the same training plan file whose content has changed. This is useful when working on a training plan, and you want it to be updated without having to remove it and restore it in database. The following command launches the CLI to select the training plan that will be updated </p> <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini update-training-plan\n</code></pre> <p>The command lists registered training plans with their names and ids and asks you to select a training plan you would like to update. Afterward, it asks to select a training plan file from file system. You can either select a different or the same training plan file. It computes a hash for the specified training plan file and updates the database.   </p> <p>Note<p>You can update hashes only by providing a training plan file. This API does not allow you to update saved  hashes directly. </p> </p>"},{"location":"user-guide/nodes/training-plan-security-manager/#using-default-training-plans","title":"Using default training plans","text":"<p>Default training plans are training plans that are pre-authorized by Fed-BioMed, by default.</p> <p>Unlike the registered training plans, the Fed-BioMed GUI and CLI tools don't provide an option for adding new default training plans. Default training plans are already  stored in the <code>envs/common/default_training_plans</code> directory. They are automatically registered when the node is started with training plan type as <code>default</code> and status as <code>Approved</code>.</p> <p>If the default training plans already exists in the database at node start, training plan manager checks whether there is any modification. If any default training plan file is deleted from the filesystem, training plan manager also deletes it from the database. If the training plan file is modified, or the hashing algorithm is changed, training plan manager updates the hashes in the database. This checking/controlling operation is done while starting the node. </p> <p>As for requested training plans, default training plans can later be viewed (<code>view-training-plan</code>) or changed status (<code>approve-training-plan</code> or <code>reject-training-plan</code>).</p> <p>Note<p>Default training plans cannot be removed using Fed-BioMed CLI. They should be removed from the  <code>envs/common/default_training_plans</code> directory. After restarting the node, deleted training plan files are  also removed from the TrainingPlans table of the node database. </p> </p>"},{"location":"user-guide/nodes/using-gpu/","title":"Using GPU accelerator hardware","text":"<p>Federated learning often implies intensive computation for model training, aggregation or inference. Dedicated accelerator hardware such as Nvidia GPUs can help speed up these steps, with support from the libraries and frameworks.</p> <p>This page explains GPU support in Fed-BioMed.</p>"},{"location":"user-guide/nodes/using-gpu/#support-scope","title":"Support scope","text":"<p>Fed-BioMed supports accelerator hardware with the following requirements and limitations :</p> <ul> <li>Nvidia GPU hardware : basically all models supported by the CUDA interface can be used, but of course the hardware needs to have enough memory for the targeted model.</li> <li>single GPU on each node : if the host machine has multiple GPUs, only one is used by a Fed-BioMed node</li> <li>PyTorch framework using the <code>TorchTrainingPlan</code> Fed-BioMed training plan interface ; other frameworks / training plans (scikit-learn / <code>SGDSkLearnModel</code>) can run on a node with a hardware accelerator but don't use it.</li> <li>only training is accelerated (node side computation), not aggregation (server side computation)</li> <li>no VPN : VPN in Fed-BioMed is based on running each component in a docker container. Node container images provided by Fed-BioMed are not yet GPU-enabled.</li> </ul>"},{"location":"user-guide/nodes/using-gpu/#node-side","title":"Node side","text":"<p>To control GPU usage by Fed-BioMed training from the node side, use these options of the <code>fedbiomed_run node</code> command :</p> <ul> <li><code>--gpu</code> : Node offers to use a GPU for training, if a GPU is available on the node and if the researcher requests use of a GPU. If no GPU is available, or the training plan is not supported for GPU (scikit-learn), or the researcher does not request use of a GPU, then training occurs in CPU.</li> <li><code>--gpu-num GPU_NUM</code> : Node chooses the device with number GPU_NUM in CUDA instead of the default device. If this device does not exist, node fallbacks to default CUDA device. This option also implicitely sets <code>--gpu</code>. </li> <li><code>--gpu-only</code> : Node enforces use of a GPU for training, if a GPU is available on the node, even if the researcher doesn't request it. If no GPU is available, or the training plan is not supported for GPU (scikit-learn), then training occurs in CPU.</li> </ul> <p>By default (no options), Fed-BioMed training doesn't use GPU.</p> <p>The reason for not using GPU by default is that even if you have a GPU on a node, it may not have enough memory to train the given model. In this case, the training of a correct model fails with an error message (and you don't want a correct model to fail with default options) :</p> <pre><code>2022-01-13 08:07:28,737 fedbiomed ERROR - Cannot train model in round: CUDA error: out of memory\n</code></pre> <p>Example :</p> <ul> <li>launch a node that enforces use of GPU with CUDA device number 2 (the 3rd GPU on this host machine). If there is no GPU with device number 2, use the default GPU. If there is no GPU available or if not using <code>TorchTrainingPlan</code>, do the training in CPU : <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node start --gpu-only --gpu-num 2\n</code></pre></li> <li>if the researcher didn't request for GPU usage, and there is no GPU numbered 2 in CUDA on the node but another GPU is available, and the training plan supports GPU acceleration, then the following warning messages are emitted : <pre><code>2022-01-21 12:34:31,992 fedbiomed WARNING - Node enforces model training on GPU, though it is not requested by researcher\n2022-01-21 12:34:31,992 fedbiomed WARNING - Bad GPU number 2, using default GPU\n</code></pre></li> </ul>"},{"location":"user-guide/nodes/using-gpu/#researcher-side","title":"Researcher side","text":"<p>To control GPU usage for Fed-BioMed training from the researcher side, set the <code>'use_gpu': True</code> key of the <code>training_args</code> dict passed as argument to <code>Experiment</code>.</p> <p>In this example, the researcher requests the nodes participating in the <code>Experiment</code> to use GPU for training, if they have any GPU available and offer to use it : <pre><code># Researcher notebook or script code\ntraining_args = {\n'use_gpu': True #......\n}\n\nexp = Experiment(\n...\n    training_args=training_args,\n    ... )\n</code></pre></p> <p>Node requirements have precedence over researcher requests. For example, no GPU is used if the node requests it but the nodes does not offer it.</p>"},{"location":"user-guide/nodes/using-gpu/#how-to-enable-gpu-usage","title":"How to enable GPU usage","text":"<p>Fed-BioMed offers a simplified interface for training with GPU, as described above. This hides from the researcher the complexity of the specific resources and requirements of each node's. </p> <p>Warning<p>Fed-BioMed models and training plans should never try to directly access the CUDA resources on the node. For example don't use the <code>pytorch.cuda.is_available()</code>, <code>tensor.to()</code>, <code>tensor.cuda()</code> etc. methods. This is not  the supported way of using GPUs in Fed-BioMed.</p> </p>"},{"location":"user-guide/nodes/using-gpu/#option-1-enable-gpu-on-node-and-researcher","title":"Option 1 : enable GPU on node and researcher","text":"<p>In this scenario, the node proposes the use of a GPU and the researcher requests the use of a GPU :</p> <ul> <li> <p>launch node offering GPU usage <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node start --gpu\n</code></pre></p> </li> <li> <p>on the researcher, set the <code>training_args</code> of the notebook or script used for training <pre><code># Researcher notebook or script code\ntraining_args = {\n'use_gpu': True\n    # .....\n}\n\nexp = Experiment(\n...\n    training_args=training_args,\n    ... )\nexp.run()\n</code></pre></p> </li> </ul>"},{"location":"user-guide/nodes/using-gpu/#option-2-force-gpu-usage-on-node","title":"Option 2 : force GPU usage on node","text":"<p>In this scenario, no action is needed on the researcher side, no code modification is needed. The node enforces use of GPU :</p> <ul> <li> <p>launch node enforcing GPU usage <pre><code>$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node start --gpu-only\n</code></pre></p> </li> <li> <p>on the researcher, launch same notebook or script as when using CPU <pre><code># Unmodified researcher notebook or script code\nexp = Experiment( ... )\nexp.run()\n</code></pre></p> </li> </ul>"},{"location":"user-guide/nodes/using-gpu/#remarks","title":"Remarks","text":""},{"location":"user-guide/nodes/using-gpu/#heterogeneous-nodes","title":"Heterogeneous nodes","text":"<p>When using multiple nodes, they can have different GPU support and requirements. For example an <code>Experiment</code> can use 4 nodes with :</p> <ul> <li>1 node has no GPU available</li> <li>1 node has GPU available but does not offer GPU</li> <li>1 node has GPU available and offers GPU</li> <li>1 node has GPU available and enforces GPU usage</li> <li>etc.</li> </ul>"},{"location":"user-guide/nodes/using-gpu/#multiples-nodes-on-the-same-host","title":"Multiples nodes on the same host","text":"<p>When running multiples nodes on a same host machine that has multiple GPUs available, each node can use a different GPU. For example on a node with 2 GPUs numbered 0 and 1 : <pre><code># Launch node (in background) offering use of GPU 0\n$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node start --gpu-num 0 &amp;\n# Launch node offering use of GPU 1\n$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node start --gpu-num 1 </code></pre></p>"},{"location":"user-guide/nodes/using-gpu/#security","title":"Security","text":"<p>warning<p>Warning: from a security perspective, a malicious researcher can write a training plan that directly accesses  the node's GPU (eg: with <code>tensor.to()</code>) even if not offered by the node. This should be addressed  by using training plan approval and conducting proper training plan review.</p> </p>"},{"location":"user-guide/researcher/","title":"Researcher component user guide","text":"<ul> <li>Experiment</li> <li>Aggergation</li> <li>Training Plan</li> <li>Training Data</li> <li>Listing datasets and selecting nodes</li> <li>Training plan validation</li> <li>Tensorboard</li> </ul>"},{"location":"user-guide/researcher/aggregation/","title":"Parameter Aggregation in Fed-BioMed","text":"<p>Aggregation of model parameters plays an important role in federated learning, where we naturally deal with data  heterogeneity. Unlike the distributed learning datasets, model parameters are saved as same-sized data blocks for  each node training the same model. The number of samples, the quality of the samples cand their data distribution can vary in every <code>Node</code>.  In Fed-BioMed, we currently work on providing various solutions for this heterogeneity. Up to now, we support  <code>FedAverage</code> which performs the standard aggregation scheme in federated learning: federated averaging. We also provide <code>FedProx</code> and <code>SCAFFOLD</code> aggregation methods.</p>"},{"location":"user-guide/researcher/aggregation/#fed-biomed-aggregators","title":"Fed-BioMed <code>Aggregators</code>:","text":"<p>Fed-BioMed <code>Aggregators</code> are showcased in the following tutorial. </p>"},{"location":"user-guide/researcher/aggregation/#federated-averaging-fedaveraging","title":"Federated Averaging (FedAveraging)","text":"<p><code>FedAveraging</code> is the default <code>Aggregator</code> in Fed-BioMed, introduced by McMahan et al.. It  performs a weighted mean of local model parameters based on the size of node specific datasets. This operation  occurs after each round of training in the <code>Nodes</code>.</p> \\[w_{t+1} := \\sum_{k=1}^K\\frac{n_k}{n}w_{t+1}^k\\] <p>where \\( w_{t} \\) are the weights at round \\(t\\), \\(K\\) is the number of <code>Nodes</code> participating at round \\(t\\), and \\( n_k, n \\)  are the number of samples of the \\(k\\)-th node and of the total federation respectively. </p>"},{"location":"user-guide/researcher/aggregation/#fedprox","title":"FedProx","text":"<p>Similar to <code>FedAveraging</code>, <code>FedProx</code> performs a weighted sum of local model parameters.  <code>FedProx</code> however introduces a regularization operation, using \\(\\mathcal{L}_2\\) norm, in order to tackle statistical heterogeneity.  Basically, it reformulates the loss function by:</p> \\[F_k(w) + \\frac{\\mu}{2}|| w - w^t ||^2_2\\] <p>using the same notation as above, with \\(\\mu\\) the regularization parameter (we obtain <code>FedAveraging</code> by setting \\(\\mu=0\\)) and \\(F_k\\) the objective function.</p> <p>To use <code>FedProx</code>, use <code>FedAverage</code> from <code>fedbiomed.researcher.aggregators</code> and specify a value for \\(\\mu\\) in the training  arguments <code>training_args</code> using the argument name <code>fedprox_mu</code>.</p>"},{"location":"user-guide/researcher/aggregation/#scaffold","title":"SCAFFOLD","text":"<p><code>SCAFFOLD</code> stands for Stochastic Controlled Averaging for Federated Learning.  It introduces a correction state parameter in order to tackle the client drift, depicting the fact that when data  across each <code>Node</code> are heterogeneous, each of the <code>Nodes</code> pushes the model in a different direction in the optimization  space and the global model does not converge towards the true optima.  In Fed-BioMed, only option 2 of the <code>SCAFFOLD</code> paper has been implemented.  Additional details about the implementation can be found in the developer  API reference.</p> <p>The corrected loss function used to update the model is computed as follows:</p> \\[F_k(w) + c \\cdot w - c_k \\cdot w\\] <p>where \\(c_k\\) is the <code>Node</code> correction term,  \\(c = \\frac{1}{K}\\sum_{k=1}^K{c_k}\\) is the server's correction term, and \\(K\\) is the total number of participating <code>Nodes</code> as above. </p> <p>On the <code>Researcher</code> side, the global model is updated by performing gradient descent.</p> <p>Additional parameters are needed when working with <code>SCAFFOLD</code>: </p> <ul> <li><code>server_lr</code>: <code>Researcher</code>'s learning rate for performing a gradient step</li> <li><code>num_updates</code>: the number of updates (ie gradient descent optimizer steps) to be performed on each <code>Node</code>. Relying only on <code>epochs</code> could lead to some inconsistencies in the computation of the correction term: thus, in Fed-BioMed, <code>SCAFFOLD</code> aggregator cannot be used with <code>epochs</code>.</li> </ul> <p>Please note that:</p> <ul> <li><code>SCAFFOLD</code> should be used only with <code>SGD</code> optimizer. Using other <code>Optimizers</code> in Fed-BioMed is possible, but without any convergence guarantees.</li> <li><code>SCAFFOLD</code> can only be used with the <code>PyTorch</code> framework at the moment.</li> <li><code>SCAFFOLD</code> requires using the <code>num_updates</code> training argument to control the number of training iterations. Using only <code>epochs</code> will raise an error.</li> </ul>"},{"location":"user-guide/researcher/aggregation/#how-to-create-your-custom-aggregator","title":"How to Create Your Custom Aggregator","text":""},{"location":"user-guide/researcher/aggregation/#designing-your-own-aggregator-class-the-aggregation-method","title":"Designing your own <code>Aggregator</code> class: the <code>aggregation</code> method","text":"<p>Before designing your custom aggregation algorithm we recommend you to see default <code>FedAverage</code> aggregate method</p> <p><code>aggregate</code> method is expecting at least <code>model_params</code> and <code>weights</code> arguments. Additional argument can be passed  through <code>*args</code> and <code>kwargs</code> depending, on the values needed for your <code>Aggregator</code>.</p> <p>It is possible to create your custom aggregator by creating a new class which inherits from the Aggregator class  defined in <code>fedbiomed.researcher.aggregators.aggregator.Aggregator</code>.</p> <pre><code>class Aggregator:\n\"\"\"\n    Defines methods for aggregating strategy\n    (eg FedAvg, FedProx, SCAFFOLD, ...).\n    \"\"\"\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def normalize_weights(weights) -&gt; list:\n        # Load list of weights assigned to each node and\n        # normalize these weights so they sum up to 1\n        norm = [w/sum(weights) for w in weights]\n        return norm\n\n    def aggregate(self,  model_params: list, weights: list, *args, **kwargs) -&gt; Dict: # pragma: no cover\n\"\"\"Strategy to aggregate models\"\"\"\n        pass\n</code></pre> <p>Your child class should extend the method <code>aggregate</code> that gets model parameters and weights as arguments. The model  parameters are those which have been locally updated in each node during the last round. The weights represent the  ratio of the number of samples in each node and the total number of samples. Your custom aggregator class should return  aggregated parameters.</p> <p>You should also pay attention to the way the parameters are loaded. For example, it may be a dictionary that contains  tensor data types or just an array. As you can see from the following example, the aggregator first checks the data  type of the parameters, and then it does the averaging.</p> <pre><code>    if t == 'tensor':\n        for model, weight in zip(model_params, proportions):\n            for key in avg_params.keys():\n                avg_params[key] += weight * model[key]\n\n    if t == 'array':\n        for key in avg_params.keys():\n            matr = np.array([ d[key] for d in model_params ])\n            avg_params[key] = np.average(matr,weights=np.array(weights),axis=0)\n</code></pre>"},{"location":"user-guide/researcher/aggregation/#desinging-your-own-aggregator-class-the-create_aggregator_args-method","title":"Desinging your own <code>Aggregator</code> class: the <code>create_aggregator_args</code> method","text":"<p>For some advanced <code>Aggregators</code>, you may need to send some argument to <code>Nodes</code> in order to update the local model. For instance, <code>SCAFFOLD</code> <code>Aggregator</code> sends specific correction terms for each of the <code>Nodes</code> involved in the training. </p> <p>The method that has this responsability is <code>create_aggregator_args</code>, and is designed as follow (in the <code>fedbiomed.researcher.aggregators.aggregator.Aggregator</code> class):</p> <pre><code>def create_aggregator_args(self, *args, **kwargs) -&gt; Tuple[dict, dict]:\n\"\"\"Returns aggregator arguments that are expecting by the nodes\n\n    Returns:\n    dict: contains `Aggregator` parameters that will be sent through MQTT message\n            service\n    dict: contains parameters that will be sent through file exchange message.\n            Both dictionaries are mapping node_id to `Aggregator` parameters specific \n            to each Node.\n\n    \"\"\"\n    return self._aggregator_args or {}, {}\n</code></pre> <p><code>create_aggregator_args</code> returns two dictionaries, the first one containing <code>Aggregator</code> parameters that will be sent through MQTT message service, and the second one <code>Aggregator</code> parameters exchanged through file exchange service. The latter is designed for the transmission of large amount of data, e.g., in <code>SCAFFOLD</code> the correction terms parameters. Each of the dictionary is mapping <code>Node</code> ids to a dictionary of parameter that will be sent to the corresponding <code>Node</code>.</p>"},{"location":"user-guide/researcher/aggregation/#conclusions","title":"Conclusions","text":"<p>In this article, the aggregation process has been explained. Currently, Fed-BioMed only supports the vanilla federated  averaging scheme for the aggregation operation called <code>FedAverage</code>, as well as <code>FedProx</code> and <code>SCAFFOLD</code>. However, Fed-BioMed also allows you to create  your custom aggregator using the <code>Aggregator</code> parent class. It means that you can define your custom aggregator based  on your use case(s). You can define it in your notebook or python script and passed into the  Experiment as an argument.</p>"},{"location":"user-guide/researcher/client-selection-strategies/","title":"Node Selection Strategies in Fed-BioMed","text":"<p>Node selection might play an important role to increase the performance of the model. Currently, in the development,  Fed-BioMed doesn't provide any node selection strategy. This means that, by default, all nodes disposing of the  dataset that is required for training are selected.   </p>"},{"location":"user-guide/researcher/experiment/","title":"Experiment Class of Fed-BioMed","text":""},{"location":"user-guide/researcher/experiment/#introduction","title":"Introduction","text":"<p>Fed-BioMed provides a way to perform Federated Learning, that is a model training process over multiple nodes where the datasets are stored and models get trained. The experiment is in charge of orchestrating the training process on available nodes. Orchestrating means;</p> <ul> <li>Searching the datasets on active nodes, based on specific tags given by a researcher and used by the nodes to identify the dataset.</li> <li>Uploading the training plan file created by the researcher and sending the file URL to the nodes.</li> <li>Sending model and training arguments to the nodes.</li> <li>Tracking training process in the nodes during all training rounds.</li> <li>Checking the nodes responses to make sure that each round is successfully completed in every node.</li> <li>Downloading the local model parameters after every round of training.</li> <li>Aggregating the local model parameters based on the specified federated approach, and eventually sending the aggregated parameters to the selected nodes for the next round.</li> </ul> <p>Please see the following Figure 1 to understand what experiment does after its declaration.</p> <p> Figure 1 - Experiment workflow on the researcher component</p>"},{"location":"user-guide/researcher/experiment/#defining-an-experiment","title":"Defining an experiment","text":"<p>The code snippet below shows a basic experiment initialization for federated training. These arguments have to be passed to the experiment to tune the experiment based on user preferences.</p> <pre><code>exp = Experiment(tags=tags,\n                 nodes=None,\n                 training_plan_class=Net,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</code></pre> <p>It is also possible to define an empty experiment and set the arguments afterwards, using the setters of the experiment object. Please visit the tutorial In depth experiment configuration to find out more about declaring an experiment step by step</p> <p>When you first initialize your experiment by providing all the arguments as it is shown in the code snippet above, it creates a <code>FederatedDataSet</code> by searching the datasets in nodes based on the given list of <code>tags</code>. Afterwards, it initializes a <code>Job</code> object with <code>training_plan_class</code>, <code>training_arguments</code>, <code>model_args</code>, and the training plan code to manage federated training rounds on all participating nodes. However, it also creates the strategy to select the nodes for each training round. When the <code>node_selection_strategy</code> is set to <code>None</code>, the experiment uses the default strategy which is <code>DefaultStrategy</code>.</p>"},{"location":"user-guide/researcher/experiment/#looking-for-a-specific-dataset-using-tags","title":"Looking for a specific dataset using Tags","text":"<p>Nodes can deploy multiple datasets. It means that on each node, each dataset should have a unique dataset id. Since the dataset ids might change from one node to another, there should be another identifier which will be global for the datasets which are deployed for a specific training plan. This identifier is called \"Tags\". Tags allow researchers to select the same dataset registered under a given tag (or list of tags) on each node for the training.</p> <p>The argument <code>tags</code> of the experiment is used for dataset search request. It can be a list of tags which are of type <code>string</code> or single tag as of type <code>string</code>. It can be declared at the first definition of experiment or using the setter of the experiment, as illustrated in the code snippet below: in this example, each stanza can be used to retrieve default dataset (tagged using <code>#MNIST</code> and <code>#dataset</code>).</p> <pre><code>exp = Experiment()\nexp.set_tags(tags=['#MNIST', '#dataset'])\n#or\nexp.set_tags(tags='#MNIST')\n</code></pre> <p>You can check your tags in your experiment as follows:</p> <pre><code>tags = exp.tags()\nprint(tags)\n# &gt; OUTPUT:\n# &gt; ['#MNIST', '#dataset']\n</code></pre> <p>     An <code>Experiment</code> object must have one unique dataset per node. Object creation fails if this is not the case when trying to instantiate the <code>FederatedDataSet</code> object. This is done to ensure that training for an <code>Experiment</code> uses only a single dataset for each node.     </p> <p>As a consequence, <code>tags</code> specified for an <code>Experiment</code> should not be ambiguous, which means they cannot match multiple datasets on one node.</p> <p>For example if you instantiate <code>Experiment(tags='#dataset')</code> and a node has registered one dataset with tags <code>['#dataset', '#MNIST']</code> and another dataset with tags <code>['#dataset', '#foo']</code> then experiment creation fails.</p>"},{"location":"user-guide/researcher/experiment/#selecting-specific-nodes-for-the-training","title":"Selecting specific Nodes for the training","text":"<p>The argument <code>nodes</code> stands for filtering the nodes that are going to be used for federated training. It is useful when there are too many nodes on the network, and you want to perform federated training on specific ones. <code>nodes</code> argument is a list that contains node ids. When it is set, the experiment queries only the nodes that are in the list for a dataset matching tags. You can visit listing datasets and selecting nodes documentation to get more information about this feature.</p> <p><pre><code>nodes = ['node-id-1', 'node-id-2']\nexp.set_nodes(nodes=nodes)\n</code></pre> By default, <code>nodes</code> argument is <code>None</code> which means that each node that a has registered dataset matching all the tags will be part of the federated training.</p> <pre><code>exp.set_nodes(nodes=None)\n</code></pre> <p>Note<p>Setting nodes doesn't mean sending another dataset search request to the nodes. If the training data has been already set for the experiment, you need to run <code>exp.set_training_data(training_data=None, from_tags=True)</code> to update <code>FederatedDataset</code> after changing the nodes. This command will send search request to specified nodes (to all if <code>nodes</code> is <code>None</code>) and update training data (<code>FederatedDataset</code>)</p> </p>"},{"location":"user-guide/researcher/experiment/#load-your-training-plan-training-plan-class-and-training-plan-path","title":"Load your Training Plan: Training Plan Class and Training Plan Path","text":"<p>The <code>training_plan_class</code> is  the class where the model, training data and training step are defined. Although not required, optimizers and dependencies can also be defined in this class. The experiment will extract source of your training plan, save as a python module (script), and upload to the file repository during every round of training. Thanks to that, each node can construct the model and perform the training.</p> <pre><code>class MyTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args):\n        # Builds the model and returns it\n        return model\n\n    def init_optimizer(self, optimizer_args):\n        # Builds the optimizer and returns it\n        return optimizer\n\n    def training_step(self):\n        # Training step at each iteration of training\n        return loss\n\n    def training_data(self):\n        # Loads the dat and returns Fed-BioMed DataManager\n        return DataManager()\n\nexp.set_training_plan_class(training_plan_class=MyTrainingPlan)\n\n# Retrieving training plan class from experiment object\ntraining_plan_class = exp.training_plan_class()\n</code></pre> <p>There is also another optional argument called <code>training_plan_path</code>. This argument can be used when the training plan class is not created in current python workspace but in a different python script. In such scenarios, <code>training_plan_path</code> should indicate the path where your python script is saved, and the argument <code>training_plan_class</code> should be the name of the class that defined in the python script.</p> <p><pre><code>exp.set_training_plan_path(training_plan_path='path/to/your/python/training-plan/script.py')\n# Let's assume that class name in `script.py` is `Net` as: `class Net:`\nexp.set_training_plan_class(training_plan_class='Net')\n</code></pre> After setting your training plan path if you haven't set training plan class or name of the training plan class has been changed you need to update/set your training plan class;</p>"},{"location":"user-guide/researcher/experiment/#model-arguments","title":"Model Arguments","text":"<p>The <code>model_args</code> is a dictionary with the arguments related to the model (e.g. number of layers, layer arguments and dimensions, etc.). This will be passed to the <code>init_model</code> method during model setup. For example, the number of features that are going to be used in network layers can be passed with <code>model_args</code>. An example is shown below.</p> <pre><code>{\n    \"in_features\"   : 15\n    \"out_features\"  : 1\n}\n</code></pre> <p>These parameters can then be used within a <code>TrainingPlan</code> as in the example below,</p> <pre><code>class MyTrainingPlan(TorchTrainingPlan):\n\n    def init_model(self, model_args):\n        model = self.Net(model_args)\n        return model\n\n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n            self.in_features = model_args['in_features']\n            self.out_features = model_args['out_features']\n            self.fc1 = nn.Linear(self.in_features, 5)\n            self.fc2 = nn.Linear(5, self.out_features)\n</code></pre> <p>Special model arguments for scikit-learn experiments</p> <p>In scikit-learn experiments, you are required to provide additional special arguments in the <code>model_args</code> dictionary. For classification tasks, you must provide both a <code>n_features</code> and an <code>n_classes</code> field, while for regression tasks you are only required to provide a <code>n_features</code> field.</p> <ul> <li><code>n_features</code>: an integer indicating the number of input features for your model</li> <li><code>n_classes</code>: an integer indicating the number of classes in your task</li> </ul> <p>Note that, as an additional requirement for classification tasks, classes must be identified by integers in the range <code>0..n_classes</code></p>"},{"location":"user-guide/researcher/experiment/#training-arguments","title":"Training Arguments","text":"<p><code>training_args</code> is a dictionary, containing the arguments for the training on the node side (e.g. batch size, optimizer_args, epochs, etc.). These arguments are dynamic, in the sense that you may change them between two rounds of the same experiment, and the updated changes will be taken into account (provided you also update the  experiment using the <code>set_training_args</code> method).</p> <p>A list of valid arguments is given in the TrainingArgs.default_scheme documentation.</p> <p>Always specify batch size</p> <p>Although not a strict requirement, it is good practice to always specify <code>batch_size</code> in your training arguments</p> <p>To set the training arguments you may either pass them to the <code>Experiment</code> constructor, or set them on an instance with the <code>set_training_arguments</code> method:</p> <p><pre><code>exp.set_training_arguments(training_args=training_args)\n</code></pre> To get the current training arguments that are used for the experiment, you can write:</p> <pre><code>exp.training_args()\n</code></pre>"},{"location":"user-guide/researcher/experiment/#controlling-the-number-of-training-loop-iterations","title":"Controlling the number of training loop iterations","text":"<p>The preferred way is to set the <code>num_updates</code> training argument. This argument is equal to the number of iterations to be performed in the training loop. In mini-batch based scenarios, this corresponds to the number of updates to the model parameters, hence the name. In PyTorch notation, this is equivalent to the number of calls to  <code>optimizer.step</code>.</p> <p>Another way to determine the number of training loop iterations is to set <code>epochs</code> in the training arguments. In this case, you may optionally set a <code>batch_maxnum</code> argument, in order to exit the training loop before a full epoch is completed. If <code>batch_maxnum</code> is set and is greater than 0, then only <code>batch_maxnum</code> iterations will be performed per  epoch.</p> <p><code>num_updates</code> is the same for all nodes</p> <p>If you specify <code>num_updates</code> in your <code>training_args</code>, then every node will perform the same number of updates. Conversely, if you specify <code>epochs</code>, then each node may perform a different number of iterations if their local dataset sizes differ.</p> <p>Note that if you set both <code>num_updates</code> and <code>epochs</code> by mistake, the value of <code>num_updates</code> takes precedence, and  <code>epochs</code> will effectively be ignored. </p> <p>Why num updates?</p> <p>In a federated scenario, different nodes may have datasets with very different sizes. By performing the same number of epochs on each node, we would be biasing the global model towards the larger nodes, because we would be performing more gradient updates based on their data. Instead, we fix the number of gradient updates regardless of the dataset size with the <code>num_updates</code> parameter.</p>"},{"location":"user-guide/researcher/experiment/#compatibility","title":"Compatibility","text":"<p>Not all configurations are compatible with all types of aggregators and experiments.  We list here the known constraints: </p> <ul> <li>the Scaffold aggregator requires using <code>num_updates</code></li> </ul>"},{"location":"user-guide/researcher/experiment/#sub-arguments-for-optimizer-and-differential-privacy","title":"Sub-arguments for optimizer and differential privacy","text":"<p>In Pytorch experiments, you may include sub arguments such as <code>optimizer_args</code>  and <code>dp_args</code>. Optimizer arguments represents the arguments that are going to be passed to <code>def init_optimizer(self, o_args)</code> method as dictionary and (<code>dp_args</code>) represents the arguments  of differential privacy.</p> <p>         Optimizer arguments and differential privacy arguments are valid only in PyTorch base training plan.     </p> <pre><code>training_args = {\n    'batch_size': 20,\n    'num_updates': 100,\n    'optimizer_args': {\n        'lr': 1e-3,\n    },\n    'dp_args': {\n        'type': 'local',\n        'sigma': 0.4,\n        'clip': 0.005\n    },\n    'dry_run': False,\n}\n</code></pre>"},{"location":"user-guide/researcher/experiment/#aggregator","title":"Aggregator","text":"<p>An aggregator is one of the required arguments for the experiment. It is used on the researcher for aggregating model parameters that are received from the nodes after every round. By default, when the experiment is initialized without passing any aggregator, it will automatically use the default <code>FedAverage</code> aggregator class. However, it is also possible to set a different aggregation algorithm with the method <code>set_aggregator</code>. Currently, Fed-BioMed has only <code>FedAverage</code> class, but it is possible to create a custom aggregator class. You can see the current aggregator by running <code>exp.aggregator()</code>. It will return the aggregator object that will be used for aggregation.</p> <p>When you pass the aggregator argument as <code>None</code> it will use <code>FedAverage</code> aggregator (performing a Federated Averaging aggregation) by default.</p> <pre><code>exp.set_aggregator(aggregator=None)\n</code></pre> <p>or you can directly pass an aggregator class</p> <pre><code>from fedbiomed.researcher.aggregators.fedavg import FedAverage\nexp.set_aggregator(aggregator=FedAverage)\n</code></pre> <p>Custom aggregator classes should inherit from the base class <code>Aggregator</code> of Fed-BioMed. Please visit user guide for  aggregators for more information.</p>"},{"location":"user-guide/researcher/experiment/#node-selection-strategy","title":"Node Selection Strategy","text":"<p>Node selection Strategy is also one of the required arguments for the experiment. It is used for selecting nodes before each round of training. Since the strategy will be used for selecting nodes, thus, training data should be already set before setting any strategies. Then, strategy will be able to select among training nodes that are currently available regarding their dataset.</p> <p>By default, <code>set_strategy(node_selection_strategy=None)</code> will use the default <code>DefaultStrategy</code> strategy. It is the default strategy in Fed-BioMed that selects for the training all the nodes available regardless their datasets. However, it is also possible to set different strategies. Currently, Fed-BioMed only provides <code>DefaultStrategy</code> but you can create your custom strategy classes.</p>"},{"location":"user-guide/researcher/experiment/#round-limit","title":"Round Limit","text":"<p>The experiment should have a round limit that specifies the max number of training round. By default, it is <code>None</code>, and it needs to be created either declaring/building experiment class or using setter method for round limit. Setting round limit doesn't mean that it is going to be permanent. It can be changed after running the experiment once or more.</p> <pre><code>exp.set_round_limit(round_limit=4)\n</code></pre> <p>To see current round limit of the experiment:</p> <pre><code>exp.round_limit()\n</code></pre> <p>You might also wonder how many rounds have been completed in the experiment. The method <code>round_current()</code> will return the last round that has been completed.</p> <pre><code>exp.round_currrent()\n</code></pre>"},{"location":"user-guide/researcher/experiment/#displaying-training-loss-values-through-tensorboard","title":"Displaying training loss values through Tensorboard","text":"<p>The argument <code>tensorboard</code> is of type boolean, and it is used for activating tensorboard during the training. When it is <code>True</code> the loss values received from each node will be written into tensorboard event files in order to display training loss function on the tensorboard interface.</p> <p>Tensorboard events are controlled by the class called <code>Monitor</code>. To enable tensorboard after the experiment has already been initialized, you can use the method <code>set_monitor()</code> of the experiment object.</p> <pre><code>exp.set_monitor(tensorboard=True)\n</code></pre> <p>You can visit tensorboard documentation page to get more information about how to use tensorboard with Fed-BioMed</p>"},{"location":"user-guide/researcher/experiment/#saving-breakpoints","title":"Saving Breakpoints","text":"<p>Breakpoint is a researcher side function that saves an intermediate status and training results of an experiment to disk files. The argument <code>save_breakpoints</code> is of type boolean, and it indicates whether breakpoints of the experiment should be saved after each round of training or not. <code>save_breakpoints</code> can be declared while creating the experiment or after using its setter method.</p> <pre><code>exp.set_save_breakpoints(True)\n</code></pre> <p>Info<p>Setting <code>save_breakpoints</code> to <code>True</code> after the experiment has performed several rounds of training will only save the breakpoints for remaining rounds.</p> </p> <p>Please visit the tutorial \"Breakpoints (experiment saving facility)\" to find out more about breakpoints.</p>"},{"location":"user-guide/researcher/experiment/#experimentation-folder","title":"Experimentation Folder","text":"<p>Experimentation folder indicates the name of the folder in which all the experiment results will be stored/saved. By default, it will be <code>Experiment_XXXX</code>, and <code>XXXX</code> part stands for the auto increment (hence, first folder will be named <code>Experiment_0001</code>, the second one <code>Experiment_0002</code> and so on). However, you can also define your custom experimentation folder name.</p> <p>Passing experimentation folder while creating the experiment; <pre><code>exp = Experiment(\n    #....\n    experimentation_folder='MyExperiment'\n    #...\n)\n</code></pre></p> <p>Setting experimentation folder using setter; <pre><code>exp.set_experimentation_folder(experimentation_folder='MyExperiment')\n</code></pre></p> <p>Using custom folder name for your experimentation might be useful for identifying different types of experiment. Experiment folders will be located at <code>${FEDBIOMED_DIR}/var/experiments</code>. However, you can always get exact path to your experiment folder using the getter method <code>experimentation_path()</code>. Below is presented a way to retrieve all the files from the folder using <code>os</code> builtin package.</p> <pre><code>import os\n\nexp_path = exp.experimentation_path()\nos.listdir(exp_path)\n</code></pre>"},{"location":"user-guide/researcher/experiment/#running-an-experiment","title":"Running an Experiment","text":""},{"location":"user-guide/researcher/experiment/#train_request-and-train_reply-messages","title":"<code>train_request</code> and <code>train_reply</code> messages","text":"<p>Running an experiment means starting the training process by sending train request to nodes. It publishes training commands as JSON string on the MQTT topics (separate topics for each node) that are subscribed by each live node. After sending training commands it waits for the responses that will be sent by the nodes. The following code snippet represents an example of train request.</p> <pre><code>{\n\"researcher_id\": \"researcher id that sends training command\",\n\"job_id\": \"created job id by experiment\",\n\"training_args\": {\n\"batch_size\": 32,\n\"optimizer_args\": {\n\"lr\": 0.001\n},\n\"epochs\": 1,\n\"dry_run\": false,\n\"batch_maxnum\": 100\n},\n\"model_args\": &lt;args&gt;,\n\"command\": \"train\",\n\"training_plan_url\": \"&lt;training plan url&gt;\",\n\"params_url\": \"&lt;model_parameter_url&gt;\",\n\"training_plan_class\": \"Net\",\n\"training_data\": {\n\"node_id\": [\n\"dataset_id\"\n]\n}\n}\n</code></pre> <p>After sending train requests, Experiment waits for the replies that are going to be published by each node once every round of training is completed. These replies are called training replies, and they include information about the training and the URL from which to download model parameters that have been uploaded by the nodes to the file repository. The following code snippet shows an example of <code>training_reply</code> from a node.</p> <p><pre><code>{\n   \"researcher_id\":\"researcher id that sends the training command\",\n   \"job_id\":\"job id that creates training job\",\n   \"success\":True,\n   \"node_id\":\"ID of the node that completes the training \",\n   \"dataset_id\":\"dataset_dcf88a68-7f66-4b60-9b65-db09c6d970ee\",\n   \"params_url\":\"URL of the model parameters' file obtained after training\",\n   \"timing\":{\n      \"rtime_training\":87.74385611899197,\n      \"ptime_training\":330.388954968\n   },\n   \"msg\":\"\",\n   \"command\":\"train\"\n}\n</code></pre> <code>training_reply</code> always results of a <code>training_request</code> sent by the <code>Researcher</code> to the <code>Node</code>.</p> <p>To complete one round of training, the experiment waits until receiving each reply from nodes. At the end of the round, it downloads the model parameters that are indicated in the training replies. It aggregates the model parameters based on a given aggregation class/algorithm. This process is repeated until every round is completed. Please see Figure 1 to understand how federated training is performed between the nodes and the researcher (<code>Experiment</code>) component.</p> <p> Figure 2 - Federated training workflow among the components of Fed-BioMed. It illustrates the messages exchanged between <code>Researcher</code> and 2 <code>Nodes</code> during a Federated Training</p>"},{"location":"user-guide/researcher/experiment/#the-methods-runand-run_once","title":"The Methods <code>run()</code>and <code>run_once()</code>","text":"<p>In order to provide more control over the training rounds, <code>Experiment</code> class has two methods as <code>run</code> and <code>run_once</code> to run training rounds.</p> <ul> <li><code>run()</code> runs the experiment rounds from current round to round limit. If the round limit is reached it will indicate that the round limit has been reached. However, the method <code>run</code> takes 2 arguments as <code>rounds</code> and <code>increase</code>.<ul> <li><code>rounds</code> is an integer that indicates number of rounds that are going to be run. If the experiment is at round <code>0</code>,  the round limit is <code>4</code>, and if you pass <code>rounds</code> as 3, it will run the experiment only for <code>3</code> rounds.</li> <li><code>increase</code> is a boolean that indicates whether round limit should be increased if the given <code>rounds</code> pass over the  round limit. For example, if the current round is <code>3</code>, the round limit is <code>4</code>, and the <code>rounds</code> argument is <code>2</code>, the experiment will increase round limit to <code>5</code></li> </ul> </li> <li><code>run_once()</code> runs the experiment for single round of training. If the round limit is reached it will indicate that the round limit has been reached. This command is the same as <code>run(rounds=1, increase=False)</code>. However, if <code>run_once</code> is executed as <code>run_once(increase=True)</code>, then, when the round limit is reached, it increases the round limit for one extra round.</li> </ul> <p>To run your experiment until the round limit;</p> <pre><code>exp.run()\n</code></pre> <p>To run your experiment for given number of rounds (while not passing the round limit):</p> <pre><code>exp.run(rounds=2)\n</code></pre> <p>To run your experiment for given number of rounds and increase the round limit accordingly if needed:</p> <pre><code>exp.run(rounds=2, increase=True)\n</code></pre> <p>To run your experiment only once (while not passing the round limit):</p> <pre><code>exp.run_once()\n</code></pre> <p>To run your experiment only once even round limit is reached (and increase the round limit accordingly if needed):</p> <pre><code>exp.run_once(increase=True)\n</code></pre> <p>Running experiment with both <code>run(rounds=rounds, increase=True)</code> and <code>run_once(increase=True)</code> will automatically increase/update round limit if it is exceeded.</p>"},{"location":"user-guide/researcher/listing-datasets-and-selecting-nodes/","title":"Listing Datasets and Selecting Nodes","text":"<p>In this article, you will learn how to list datasets that are deployed in nodes and select specific nodes to conduct your experiment. </p>"},{"location":"user-guide/researcher/listing-datasets-and-selecting-nodes/#listing-datasets","title":"Listing Datasets","text":"<p>The <code>list()</code> method of the <code>Requests</code> class has been created for listing datasets on the active nodes. It sends <code>list</code> request to the nodes and waits for the reply. It gets two arguments as <code>nodes</code> and <code>verbose</code>;</p> <ul> <li><code>verbose</code>: If it is <code>True</code>, it will print the dataset lists in table format for each node. Default is <code>False</code> </li> <li><code>nodes</code>: It is a list that includes the node ids to send list requests. Default is <code>None</code> and it means that it sends list requests to all activate nodes.  </li> </ul> <p>It returns a python <code>dict</code> that includes datasets for each node. </p> <pre><code>from fedbiomed.researcher.requests import Requests\nreq = Requests()\nreq.list(verbose=True)\n</code></pre> <p>If you set <code>verbose=True</code> you will get the following output that shows datasets on nodes up and running. </p> <pre><code> Node: node_481d9ec3-79e5-49d1-96a2-9f4928d3ecf4 | Number of Datasets: 1 \n+--------+-------------+--------+---------------+---------+\n| name   | data_type   | tags   | description   | shape   |\n+========+=============+========+===============+=========+\n| sk     | csv         | ['sk'] | sk            | [20, 6] |\n+--------+-------------+--------+---------------+---------+\n\n2021-10-19 16:51:59,699 fedbiomed INFO - \n Node: node_e289dfdc-4635-4c3c-938a-9548dbb85c92 | Number of Datasets: 2 \n+--------+-------------+------------------------+----------------+--------------------+\n| name   | data_type   | tags                   | description    | shape              |\n+========+=============+========================+================+====================+\n| MNIST  | default     | ['#MNIST', '#dataset'] | MNIST database | [60000, 1, 28, 28] |\n+--------+-------------+------------------------+----------------+--------------------+\n| sk     | csv         | ['sk']                 | sk             | [20, 6]            |\n+--------+-------------+------------------------+----------------+--------------------+\n</code></pre> <p>Listing datasets technically lists active nodes in the network. When the <code>verbose</code> argument is <code>True</code> it also  prints nodes that don't have any dataset and indicates that the node has no dataset. </p> <p>You can also list datasets in specific nodes;</p> <pre><code>req.list(nodes=['node_e289dfdc-4635-4c3c-938a-9548dbb85c92'], verbose=True)\n</code></pre> <p>It will return the datasets deployed only in the node: <code>node_e289dfdc-4635-4c3c-938a-9548dbb85c92</code></p> <pre><code> Node: node_e289dfdc-4635-4c3c-938a-9548dbb85c92 | Number of Datasets: 2 \n+--------+-------------+------------------------+----------------+--------------------+\n| name   | data_type   | tags                   | description    | shape              |\n+========+=============+========================+================+====================+\n| MNIST  | default     | ['#MNIST', '#dataset'] | MNIST database | [60000, 1, 28, 28] |\n+--------+-------------+------------------------+----------------+--------------------+\n| sk     | csv         | ['sk']                 | sk             | [20, 6]            |\n+--------+-------------+------------------------+----------------+--------------------+\n</code></pre>"},{"location":"user-guide/researcher/listing-datasets-and-selecting-nodes/#selecting-nodes-for-the-experiment","title":"Selecting Nodes for the Experiment","text":"<p>The experiment class has <code>nodes</code> arguments to optionally select specific nodes on which the federated training will be performed.  If you pass a non-empty list of node ids, then only the nodes that have a matching dataset and belong to the <code>nodes</code> list are selected.</p> <p>Let's assume that you want to perform training only in the node  <code>node_e289dfdc-4635-4c3c-938a-9548dbb85c92</code></p> <pre><code>nodes = ['node_e289dfdc-4635-4c3c-938a-9548dbb85c92']\n</code></pre> <p>Afterwards, you need to pass the <code>nodes</code> list while you are initializing the experiment class. The experiment will send a search request to the nodes in the <code>nodes</code> list for datasets deployed with the given tags.</p> <pre><code>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.aggregators.fedavg import FedAverage\n\ntags =  ['#MNIST', '#dataset']\nrounds = 2\n\nexp = Experiment(tags=tags,\n                 nodes=nodes,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None)\n</code></pre> <p>The output of the initialization will be similar to the following output.</p> <pre><code>2021-10-19 17:06:16,599 fedbiomed INFO - Searching dataset with data tags: ['#MNIST', '#dataset'] on specified nodes: ['node_e289dfdc-4635-4c3c-938a-9548dbb85c92']\n2021-10-19 17:06:16,631 fedbiomed INFO - log from: node_e289dfdc-4635-4c3c-938a-9548dbb85c92 - DEBUG Message received: {'researcher_id': 'researcher_1c4fc722-02c8-41b2-b9ed-b85d97968ba9', 'tags': ['#MNIST', '#dataset'], 'command': 'search'}\n2021-10-19 17:06:26,612 fedbiomed INFO - Node selected for training -&gt; node_e289dfdc-4635-4c3c-938a-9548dbb85c92\n</code></pre>"},{"location":"user-guide/researcher/model-testing-during-federated-training/","title":"Model Validation During Federated Training on the Nodes","text":"<p>Model validation is critical to discover how the model performs during the training rounds, when no dedicated holdout dataset is available for testing. In federated training, models are refined on the different nodes with different datasets. Therefore, model validation should be implemented on each node separately to compare model performances after its parameters are updated. Fed-BioMed provides a validation routine on the datasets that are randomly split at each round of training.</p> <p>In the federated learning concept, two validation types can be applied at each round of training:</p> <ul> <li>Validation on globally updated parameters (<code>test_on_global_updates</code>): It is the validation applied on the aggregated parameters before performing the training for the current round on a node.</li> <li>Validation on locally updated parameters (<code>test_on_local_updates</code>): It is the validation applied after the local training for the current round is performed on a node, and model parameters have been locally updated.</li> </ul> <p>These two validations allow the users to compare how training on the node has improved the model performance.</p> <p>Validation on the node side shouldn't be confused with the model testing</p> <p>Currently, nodes do not provide completely separated datasets for validating model performance. Since the samples for validation and training are picked randomly at each round of training, the same samples could be used for training in one round and for validation in another round. It is assumed that the testing should be done by the user using a local dataset that contains samples that are not used for the training.</p> <p>Figure 1 illustrates the phases of validation and training during 2 rounds of federated training. As it can be seen in the figure, after the last round of training, one last validation on global updates is performed on the last aggregated parameters by each node. Therefore, the number of validation on globally updated parameters, if it is activated, will be equal to the number of rounds + 1</p> <p> Figure 1 - Validation on global and local updates</p>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#default-metrics-provided-by-fed-biomed","title":"Default Metrics Provided by Fed-BioMed","text":"<p>Fed-BioMed provides several test metrics to perform validation that can be used without defining a validation function in the training plan. It allows the user to launch an experiment by providing as less code as possible. You can display all the metric provided by Fed-BioMed as shown in the following code snippet.</p> <pre><code>from fedbiomed.common.metrics import MetricTypes\nMetricTypes.get_all_metrics()\n\n# &gt; Output:\n# &gt; ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL',\n# &gt; 'MEAN_SQUARE_ERROR', 'MEAN_ABSOLUTE_ERROR',\n# &gt; 'EXPLAINED_VARIANCE']\n</code></pre> <p>Note</p> <p>By default, <code>ACCURACY</code> metric is used as a test metric if there isn't a metric defined by the researcher. Therefore, please pay attention to whether <code>ACCURACY</code> is relevant for the model that is going to be trained. Otherwise, metric results might be inconsistent.</p>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#validation-arguments","title":"Validation Arguments","text":"<p>Validation during the training is an optional process, and validation arguments should be configured in order to activate it. Here is the list of validation arguments that can be configured.</p> <ul> <li><code>test_ratio</code>: Ratio of the validation partition of the dataset.  The remaining samples will be used for training. By                  default, it is <code>0.0</code>.</li> <li><code>test_on_global_updates</code>: Boolean value that indicates whether validation will be applied to globally updated (aggregated) parameters (see Figure 1). Default is <code>False</code></li> <li><code>test_on_local_updates</code>: Boolean value that indicates whether validation will be applied to locally updated (trained) parameters (see Figure 1). Default is <code>False</code></li> <li><code>test_metric</code>: One of <code>MetricTypes</code> that indicates which metric will be used for validation. It can be <code>str</code> or     an instance of <code>MetricTypes</code> (e.g. <code>MetricTypes.RECALL</code> or <code>RECALL</code> ). If it is <code>None</code> and there isn't <code>testing_step</code>     defined in the training plan (see section: Define Custom Validation Step) default metric will be     <code>ACCURACY</code>.</li> <li><code>test_metric_args</code>: A dictionary that contains the arguments that will be used for the metric function.</li> </ul> <p>Info</p> <p>Validation functions for each default metric executes functions from scikit-learn framework. Therefore, <code>test_metric_args</code> should be coherent with the arguments of \"scikit-learn\" metrics functions. Please visit here to see API documentation of scikit-learn metrics.</p> <p>To activate validation on the node side, the arguments <code>test_ratio</code> and at least one of <code>test_on_local_updates</code> or <code>test_on_global_updates</code> should be set. Since the default values of <code>test_on_local_updates</code> and <code>test_on_global_updates</code> are <code>False</code>, setting <code>test_ratio</code> will only split dataset as validation and train sets but won't perform validation.</p>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#setting-validation-arguments-in-training-arguments","title":"Setting Validation Arguments in Training Arguments","text":"<p>Validation arguments are considered a part of the training on the node side. Therefore, it is possible to define validation arguments in the training arguments and pass them to the experiment.</p> <pre><code>from fedbiomed.common.metrics import MetricTypes\nfrom fedbiomed.researcher.experiment import Experiment\ntraining_args = {\n    #....\n    'optimizer_args': {\n        'lr': 1e-3\n    },\n    'epochs': 2,\n    'batch_maxnum': 100,\n    #...\n    'test_ratio' : 0.25,\n    'test_metric': MetricTypes.F1_SCORE,\n    'test_on_global_updates': True,\n    'test_on_local_updates': True,\n    'test_metric_args': {'average': 'macro'}\n}\n\nexp = Experiment(# ....\n                 training_args=training_args)\n</code></pre>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#setting-validation-arguments-using-setters-of-the-experiment-class","title":"Setting Validation Arguments using Setters of the Experiment Class","text":"<p>Each validation argument has its own setter method in the experiment class where federated training is managed. Therefore, validation arguments can be set, modified, or reset using the setters. To enable setters for validation arguments, the experiment should be created in advance.</p> <pre><code>from fedbiomed.common.metrics import MetricTypes\nfrom fedbiomed.researcher.experiment import Experiment\n\ntraining_args = {\n    'optimizer_args': {\n        'lr': 1e-3,\n    },\n    'epochs': 2,\n    'batch_maxnum': 100,\n}\nexp = Experiment(training_args=training_args)\n\nexp.set_test_ratio(0.25)\nexp.set_test_on_local_updates(True)\nexp.set_test_on_global_updates(True)\nexp.set_test_metric(MetricTypes.F1_SCORE) # or exp.set_test_metric('F1_SCORE')\nexp.set_test_metric_args({'average': 'macro'})\n</code></pre> <p>Setters allow updating validation arguments from one round to others.</p> <pre><code>exp.run(rounds=2)\nexp.set_test_ratio(0.35)\nexp.set_set_test_metric(MetricTypes.ACCURACY)\nexp.run(rounds=2)\n</code></pre>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#define-custom-validation-step","title":"Define Custom Validation Step","text":"<p>Fed-BioMed training plans allow defining custom validation steps for model evaluation on the node side. The name of the method that should be defined in the training plan is <code>testing_step</code>. It should take two input arguments as data/inputs and target/actual that are provided on the node side. The validation step can calculate and return multiple testing metrics as long as the return value of the method is supported. The method should return:</p> <ul> <li>Single <code>float</code> or <code>int</code> value that represents a single validation result. The name of the metric will be displayed as <code>Custom</code>.</li> </ul> <pre><code>def testing_step(self, data, target):\n    # Validation actions ...\n    value = 1.001\n\n    return value\n</code></pre> <ul> <li>List of multiple validation results. Metrics names will be displayed as <code>Custom_1</code>, <code>Custom_2</code>, <code>Custom_n</code> .</li> </ul> <pre><code>def testing_step(self, data, target):\n    # Validation actions ...\n    value_1 = 1.001\n    value_2 = 1.002\n    return [value_1, value_2]\n</code></pre> <ul> <li>Dictionary of multiple metric results as <code>int</code> or <code>float</code>. Metrics names will be displayed as the keys of dictionary.</li> </ul> <pre><code>def testing_step(self, data, target):\n    # Validation actions ...\n    result = {'metric-1' : 0.01, 'metric-2': 0.02}\n    return result\n</code></pre> <p>Info</p> <p><code>testing_step</code> has a higher priority than default test metrics. It means that if both <code>testing_step</code> in training plan and <code>test_metric</code><code>argument in the validation arguments are defined, node will only execute the method</code>testing_step`</p> <p>The modules, functions, and methods that are going to be used in the validation method should be added as dependencies in the training plan (see PyTorch and Sklearn). Please also make sure that the modules whose functions will be used in the validation step do exist in the Fed-BioMed node environment.</p>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#pytorch","title":"PyTorch","text":"<p>The validation method in PyTorch-based training plans takes two arguments respectively for input (<code>X</code>) and target (<code>y</code>). These arguments are instances of <code>torch.Tensor</code>. The validation mode of PyTorch will be already activated on the node side before running <code>testing_step</code> with <code>self.eval()</code>. Therefore, there is no need to configure it again in the validation step method.</p> <p>The following code snippet shows an example <code>testing_step</code> that calculates negative log-likelihood, cross-entropy and accuracy.</p> <pre><code>import torch\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\n\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    # Other necessary methods e.g. `def init_model`\n    # .......\n\n    def testing_step(self, data, target):\n\n        pred = self.model().forward(data)\n        nll   = torch.nn.functional.nll_loss(pred, target)  # negative log likelihood loss\n        ce = torch.nn.functional.cross_entropy(pred,target) # cross entropy\n\n\n        _, predicted = torch.max(pred.data,1)\n        acc = torch.sum(predicted==target)\n        accuracy = acc/len(target)  # accuracy\n\n        return { 'NLL': nll, 'CE': ce, 'ACCURACY': accuracy}\n</code></pre> <p>Info</p> <p>Datasets for validation (<code>data</code> and <code>target</code>) are not a batch iterator. They contain all the samples in one block. However, it is possible to define batch iterator in the validation method as long as the method returns a single value for each metric that is calculated.</p>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#sklearn","title":"SkLearn","text":"<p>The validation method in scikit-learn based training plans also takes two arguments respectively for input/data (<code>X</code>) and target (<code>y</code>). These arguments are instances of <code>np.ndarray</code>.</p> <p>The following code snippet shows an example <code>testing_step</code> that calculates hinge loss and accuracy.</p> <pre><code>from fedbiomed.common.training_plans import FedPerceptron\nfrom sklearn.metrics import hinge_loss\nimport numpy as np\n\n\nclass SkLearnClassifierTrainingPlan(FedPerceptron):\n    def init_dependencies(self):\n        return ['import torch',\n                \"from sklearn.linear_model import Perceptron\",\n                \"from torchvision import datasets, transforms\",\n                \"from torch.utils.data import DataLoader\",\n                \"from sklearn.metrics import hinge_loss\"]\n\n\n    def compute_accuracy_for_specific_digit(self, data, target, digit: int):\n        idx_data_equal_to_digit = target == digit\n\n        predicted = self.model().predict(data[idx_data_equal_to_digit])\n        well_predicted_label = np.sum(predicted == digit) / np.sum(idx_data_equal_to_digit)\n        return well_predicted_label\n\n    def testing_step(self, data, target):\n        # hinge loss\n        distance_from_hyperplan = self.model().decision_function(data)\n        loss = hinge_loss(target, distance_from_hyperplan)\n\n        # get the accuracy only on images representing digit 1\n        well_predicted_label_1 = self.compute_accuracy_for_specific_digit(data, target, 1)\n\n        # Returning results as dict\n        return {'Hinge Loss': loss, 'Well Predicted Label 1' : well_predicted_label_1}\n</code></pre>"},{"location":"user-guide/researcher/model-testing-during-federated-training/#conclusion","title":"Conclusion","text":"<p>The validation part in FL plays an important role in evaluating model performance that is trained on different nodes with different datasets. Applying a validation on the node side for each training round allows comparing the impacts of particular nodes on the trained model. Understanding and comparing different impacts will be clearer thanks to two types of validations: validation on aggregated parameters and validation on locally trained parameters.</p>"},{"location":"user-guide/researcher/tensorboard/","title":"Displaying Loss Values on Tensorboard","text":"<p>Tensorboard is one of the most useful tools to display various metrics during the training. Fed-BioMed offers an option to display loss values on the tensorboard interface. This article focuses on how to use tensorboard on Fed-BioMed to display loss values during the training rounds in each node. This section is presented as follows:</p> <ul> <li>Running experiment with tensorboard option</li> <li>Launching tensorboard</li> <li>Using tensorboard</li> </ul> <p>The tensorboard logs of an experiment is saved in a directory, by default <code>TENSORBOARD_RESULTS_DIR</code>. Thus, if you re-use the same directory for another experiment, the previous experiment's tensorboard logs are cleared. See below to learn how to specify per-experiment directory.</p> <p>Breakpoints currently do not save tensorboard monitoring status and tensorboard logs. If you continue from a breakpoint, tensorboard monitoring is not restarted and logs from pre-breakpoint run are not restored.</p>"},{"location":"user-guide/researcher/tensorboard/#running-experiment-with-tensorboard-option","title":"Running Experiment with Tensorboard Option","text":"<p>During the training of each round, scalar values are sent by each node through the <code>monitoring</code> channel. The experiment does not write scalar values to the event file as long as it has been specified. To do that you need to set <code>tensorboard=True</code> while you are initializing an experiment (see below).  Afterward, the <code>Monitor</code> will be activated, and it will write the loss values coming from each node into a new log file. Thanks to that, it is possible to display and compare on the tensorboard loss evolution (and model performances) trained on each node. By default, losses are saved in files under  the <code>runs</code> directory.</p> <pre><code>from fedbiomed.researcher.experiment import Experiment\n\n\nexp = Experiment(tags=tags,\n                 model_args=model_args,\n                 training_plan_class=MyTrainingPlan,\n                 training_args=training_args,\n                 round_limit=rounds,\n                 aggregator=FedAverage(),\n                 node_selection_strategy=None,\n                 tensorboard=True\n                )\n</code></pre>"},{"location":"user-guide/researcher/tensorboard/#launching-tensorboard","title":"Launching Tensorboard","text":""},{"location":"user-guide/researcher/tensorboard/#1-from-terminal","title":"1. From Terminal","text":"<p>Tensorboard comes with the <code>fedbiomed-researcher</code> conda environment. Therefore, please make sure that you have activated the conda <code>fedbiomed-researcher</code> environment in your new terminal window before launching the tensorboard. You can either activate the conda environment using <code>conda activate fedbiomed-researcher</code> or <code>${FEDBIOMED_DIR}/scripts/fedbiomed-environment researcher</code>.</p> <p>You can launch the tensorboard while your experiment is running or not. If you launch the tensorboard before running your experiment, it won't show any result at first. After running the experiment, it will save tensorboard event logs into the <code>runs</code> directory during the training. Afterward, you can refresh tensorboard page to see the current results.</p> <p>While you are launching the tensorboard, you need to pass the correct logs directory with <code>--logdir</code> parameter. You can either change your directory to Fed-BioMed's base directory or use the <code>FEDBIOMED_DIR</code> environment variable if you set it while you were installing Fed-BioMed.</p> <p>Option 1: <pre><code>$ cd path/to/fedbiomed\n$ tensorboard --logdir runs\n</code></pre></p> <p>Option 2: <pre><code>$ tensorboard --logdir $FEDBIOMED_DIR/runs\n</code></pre></p>"},{"location":"user-guide/researcher/tensorboard/#2-from-jupyter-notebook","title":"2. From Jupyter Notebook","text":"<p>It is also possible to launch a tensorboard inside the Jupyter notebook with the tensorboard extension.  Therefore, before launching the tensorboard from the notebook you need to load the tensorboard extension. It is important to launch the tensorboard before running the experiment in the notebook. Otherwise, you will have to wait until the experiment is done to be able to launch the tensorboard because the notebook kernel will be busy running the model training.</p> <p>First, please run the following command in another cell of your notebook to load the tensorboard extension.</p> <pre><code>%load_ext tensorboard\n</code></pre> <p>Afterward, you will be able to start the tensorboard. It is important to pass the correct path to the <code>runs</code> directory. You can use <code>ROOT_DIR</code> from the<code>fedbiomed.researcher.environ</code> to set the correct logs directory. This is the base directory of the Fed-BioMed that <code>runs</code> directory is located.</p> <p>First please import the <code>TENSORBOARD_RESULTS_DIR</code> global variable in a different cell.</p> <pre><code>from fedbiomed.researcher.environ import environ\ntensorboard_dir = environ['TENSORBOARD_RESULTS_DIR']\n</code></pre> <p>Then, you can pass <code>TENSORBOARD_RESULTS_DIR</code> to <code>--logdir</code> parameter of the <code>tensorboard</code> command. Please create a new cell and run the following command to start the tensorboard.</p> <pre><code>tensorboard --logdir \"$tensorboard_dir\"\n</code></pre> <p>Afterward, the tensorboard interface will be displayed inside the notebook cell.</p>"},{"location":"user-guide/researcher/tensorboard/#using-tensorboard","title":"Using Tensorboard","text":"<p>After launching the tensorboard it will open a interface. If you haven't run your experiment yet, tensorboard will say <code>No dashboard is active for the current data set.</code> as seen in the following image. It is because there is no logs file that has been written in the <code>runs</code> directory.</p> <p></p> <p>After running your experiment, it will start to write loss values into tensorboard log files. You can refresh the tensorboard by clicking the refresh button located at the right top menu of the tensorboard view.</p> <p></p> <p>By default, the tensorboard doesn't set a time period to refresh the scalar values on the interface. You can click the gear button at the top right corner to set reload period to update the interface. The minimum reload period is 30 seconds. It means that the tensorboard interface will refresh itself every 30 seconds.</p> <p></p>"},{"location":"user-guide/researcher/tensorboard/#conclusions","title":"Conclusions","text":"<p>You can visit tensorboard documentation page to have more information about using tensorboard. Tensorboard can be used for all training plans provided by Fed-BioMed (including Pytorch an scikit-Learn training plans). Currently, in Fed-BioMed, tensorboard has been configured to display only loss values during training in each node. In the future, there might be extra indicators / statistics such as accuracy. Stay tuned!</p>"},{"location":"user-guide/researcher/training-data/","title":"Loading Dataset for Training","text":"<p>Datasets in the nodes are saved on the disk. Therefore, before the training, each node should load these datasets from  the file system. Since the type of datasets (image, tabular, etc.) and the way of loading might vary from one to  another, the user (researcher) should define a method called <code>training_data</code>. The method <code>training_data</code> is mandatory  for each training plan (<code>TrochTrainingPlan</code> and <code>SkLearnSGDModel</code>). If it is not defined nodes will return an error at  the very beginning of the first round. </p>"},{"location":"user-guide/researcher/training-data/#defining-the-method-training-data","title":"Defining The Method Training Data","text":"<p>The method <code>training_data</code> is the method that should be defined while creating the training plan class. This method can  take one input argument which is the <code>batch_size</code>. This argument represents the batch size that is going to be used for the  data loader. Although this argument is not mandatory for <code>training_data</code>, if it exists,  each node will pass the value  <code>batch_size</code> defined in the <code>training_args</code> while calling the <code>training_data</code> method. </p> <pre><code>from fedbiomed.common.training_plans import TorchTrainingPlan\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def __init__(self):\n        pass\n        # ....\n    def training_data(self, batch_size):\n        pass\n</code></pre> <p>You can also remove the <code>batch_size</code> argument and define it inside the method. In this case, the <code>batch_size</code>defined in  the training arguments will no longer have an impact. </p> <pre><code>def training_data(self):\n    batch_size = 48\n    pass\n</code></pre> <p><code>training_data</code> can have a maximum of one argument which is <code>batch_size</code>. Since <code>training_data</code> is executed by the node and the nodes are aware of only one argument,   adding extra argument to this method will raise errors on the node side.</p>"},{"location":"user-guide/researcher/training-data/#reading-datafiles","title":"Reading Datafiles","text":"<p>As mentioned before, a node can store <code>IMAGE</code> and <code>CSV</code> dataset types. In this case, the way of loading these datasets  might be different. The only information necessary for loading will be the file path where data files are stored.  This path is accessible through <code>self.dataset_path</code>. If the dataset is a tabular <code>CSV</code> dataset  <code>self.dataset_path</code>  will address to a file. Otherwise, if it is an <code>IMAGE</code> dataset, <code>self.dataset_path</code> will address to a directory where  all the images are stored. Therefore, before loading the dataset it is important to know what type of dataset is going  to be loaded. It is possible to send list request to the nodes to get meta-data of the dataset there are  deployed.</p> <p>The following snippet shows an example of loading operation for a dataset of <code>CSV</code> type. </p> <pre><code>import pandas as pd\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_model(self):\n        # ....\n    def init_dependencies(self):\n        # ....\n    def init_optimizer(self):\n        # ....\n\n    def training_data(self, batch_size):\n        dataset = pd.read_csv(self.dataset_path, header=None, delimiter=',')\n        X = dataset.iloc[:,0:15].values\n        y = dataset.iloc[:,15]\n        return DataManager(dataset=X, target=y.values, batch_size=batch_size)\n</code></pre> <p>It is also possible to use some model arguments in the training data method. For example, if  the following model argument is passed to the model by the experiment. </p> <p><pre><code>model_args = {\n    'feature_cols' : 15\n}\n</code></pre> <code>training_data</code> can be configured as follows:</p> <pre><code>import pandas as pd\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_model(self):\n        # ....\n    def init_dependencies(self):\n        # ....\n    def init_optimizer(self):\n        # ....\n\n    def training_data(self, batch_size):\n        feature_cols = self.model_args()[\"feature_cols\"]\n        dataset = pd.read_csv(self.dataset_path, header=None, delimiter=',')\n        X = dataset.iloc[:,0:feature_cols].values\n        y = dataset.iloc[:,feature_cols]\n        return DataManager(dataset=X, target=y.values, batch_size=batch_size)\n</code></pre>"},{"location":"user-guide/researcher/training-data/#what-training_data-should-return","title":"What <code>training_data</code> Should Return?","text":"<p>The method <code>training_data</code> should always return <code>DataManager</code> of Fed-BioMed defined in the module  <code>fedbiomed.common.data.DataManager</code>. <code>DataManager</code> has been designed for managing different types of data objects for  different types of training plans. It is also responsible for splitting a given dataset into training and validation if  model validation is activated in the experiment. </p> <p><code>DataManager</code> takes two main input arguments as <code>dataset</code> and <code>target</code>. <code>dataset</code> should be an instance of one of PyTorch <code>Dataset</code>, Numpy <code>ndarray</code>, <code>pd.DataFrame</code> or <code>pd.Series</code>. The argument <code>target</code> should be an instance of one of Numpy <code>ndarray</code>,  <code>pd.DataFrame</code> or <code>pd.Series</code>. By default, the argument <code>target</code> is <code>None</code>. If <code>target</code> is <code>None</code> the data manager  considers that the <code>dataset</code> is an object that includes both input and target variables. This is the case where  the dataset is an instance of the PyTorch dataset. If <code>dataset</code> is an instance of Numpy <code>Array</code> or Pandas <code>DataFrame</code>,  it is mandatory to provide the <code>target</code> variable. </p> <p>As it is mentioned, <code>DataManager</code> is capable of managing/configuring datasets/data-loaders based on the training plans  that are going to be used for training. This configuration is necessary since each training plan requires different  types of data loader/batch iterator. </p>"},{"location":"user-guide/researcher/training-data/#defining-training-data-in-different-training-plans","title":"Defining Training Data in Different Training Plans","text":""},{"location":"user-guide/researcher/training-data/#defining-training-data-for-pytorch-based-training-plans","title":"Defining Training Data for PyTorch Based Training Plans","text":"<p>In the following code snippet, <code>training_data</code> of PyTorch-based training plan returns a <code>DataManager</code> object instantiated  with <code>dataset</code> and <code>target</code> as <code>pd.Series</code>. Since PyTorch-based training requires a PyTorch <code>DataLoader</code>, <code>DataManager</code>  converts <code>pd.Series</code> to a proper <code>torch.utils.data.Dataset</code> object and create a PyTorch <code>DataLoader</code> to pass it to the  training loop on the node side. </p> <pre><code>import pandas as pd\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_model(self):\n        # ....\n    def init_dependencies(self):\n        # ....\n    def init_optimizer(self):\n        # ....\n\n    def training_data(self, batch_size):\n        feature_cols = self.model_args()[\"feature_cols\"]\n        dataset = pd.read_csv(self.dataset_path, header=None, delimiter=',')\n        X = dataset.iloc[:,0:feature_cols].values\n        y = dataset.iloc[:,feature_cols]\n        return DataManager(dataset=X, target=y.values, batch_size=batch_size)\n</code></pre> <p>It is also possible to define a custom PyTorch <code>Dataset</code> and use it in the <code>DataManager</code> without declaring the argument <code>target</code>. </p> <p><pre><code>import pandas as pd\nfrom torch.utils.data import Dataset\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\n\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    class CSVDataset(Dataset):\n\"\"\" Cusotm PyTorch Dataset \"\"\"\n        def __init__(self, dataset_path, features):\n            self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)\n            x_train = self.input_file.iloc[:,0:features].values\n            y_train = self.input_file.iloc[:,features].values\n            self.X_train = torch.from_numpy(x_train).float()\n            self.Y_train = torch.from_numpy(y_train).float()\n\n        def __len__(self):            \n            return len(self.Y_train)\n\n        def __getitem__(self, idx):\n            return self.X_train[idx], self.Y_train[idx]\n\n    def training_data(self, batch_size): \n        feature_cols = self.model_args()[\"feature_cols\"]    \n        dataset = self.CSVDataset(self.dataset_path, feature_cols)\n        loader_kwargs = {'batch_size': batch_size, 'shuffle': True}\n        return DataManager(dataset=dataset, **loader_kwargs)\n</code></pre> In the code snippet above, <code>loader_kwargs</code> contains the arguments that are going to be used while creating a  PyTorch <code>DataLoader</code>.</p>"},{"location":"user-guide/researcher/training-data/#defining-training-data-for-sklearn-based-training-plans","title":"Defining Training Data for SkLearn Based Training Plans","text":"<p>The operations in the <code>training_data</code> for SkLearn based training plans are not much different than<code>TorchTrainingPlan</code>.  Currently, SkLearn based training plans do not require a data loader for training. This means that all samples will be  used for fitting the model. That's why passing <code>**loader_args</code> does not make sense for SkLearn based training plans. These arguments will be ignored even if they are set. </p> <pre><code>import pandas as pd\nfrom fedbiomed.common.training_plans import FedPerceptron\nfrom fedbiomed.common.data import DataManager\n\nclass SGDRegressorTrainingPlan(FedPerceptron):\n\n    def training_data(self, batch_size):\n        num_cols = self.model_args()[\"number_cols\"]\n        dataset = pd.read_csv(self.dataset_path, header=None, delimiter=',')\n        X = dataset.iloc[:,0:num_cols].values\n        y = dataset.iloc[:,num_cols]\n        return DataManager(dataset=X, target=y.values, batch_size)\n</code></pre>"},{"location":"user-guide/researcher/training-data/#preprocessing-for-data","title":"Preprocessing for Data","text":"<p>Since the method <code>training_data</code> is defined by the user, it is possible to do preprocessing before creating the  <code>DataManager</code> object. In the code snippet below, a preprocess for normalization is shown for the dataset MNIST.</p> <pre><code>def training_data(self, batch_size = 48):\n    # Custom torch Dataloader for MNIST data\n    transform = transforms.Compose([transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))])\n    dataset_mnist = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n    train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n    return DataManager(dataset=dataset_mnist, **train_kwargs)\n</code></pre> <p>Training and validation partitions are created on the node side using returned <code>DataManager</code> object. Therefore,     preprocessing in <code>training_data</code> will be applied for both validation and training data.</p>"},{"location":"user-guide/researcher/training-data/#data-loaders","title":"Data Loaders","text":"<p>A <code>DataLoader</code> is a class that takes care of handling the logic of iterating over a certain dataset.  Thus, while a <code>Dataset</code> is concerned with loading and preprocessing samples one-by-one, the <code>DataLoader</code> is responsible for:</p> <ul> <li>calling the dataset's <code>__getitem__</code> method when needed</li> <li>collating samples in a batch</li> <li>shuffling the data at every epoch</li> <li>in general, managing the actions related to iterating over a certain dataset</li> </ul>"},{"location":"user-guide/researcher/training-data/#passing-arguments-to-data-loaders","title":"Passing arguments to Data Loaders","text":"<p>Any keyword argument, except <code>dataset</code> and <code>target</code>, that was provided to the <code>DataManager</code> constructor in the  <code>training_data</code> function will be passed as a keyword argument to the <code>DataLoader</code>.  For PyTorch and scikit-learn experiments, the <code>DataLoaders</code> have been heavily inspired by the <code>torch.utils.data.DataLoader</code>  class, so please refer to that documentation for the meaning of the supported keyword arguments. For example:</p> <pre><code>class MyTrainingPlan(SKLearnTrainingPlan):\n    def training_data(self, batch_size):\n        return DataManager(dataset=X, target=Y, \n                           # The following arguments will be passed to the DataLoader:\n                           batch_size=batch_size, shuffle=True, drop_last=False)\n</code></pre>"},{"location":"user-guide/researcher/training-data/#the-special-case-of-batch-size","title":"The special case of batch size","text":"<p>As you may have noticed below, <code>batch_size</code> represents a special case since it can be provided as argument to the  <code>training_data</code> function. </p> <p>Recommended approach</p> <p>For <code>batch_size</code>, we recommend that you always include it is argument to the <code>training_data</code> function, and forward that value as keyworkd argument to the <code>DataManager</code>, as such: <pre><code>class MyTrainingPlan(SKLearnTrainingPlan):\n    def training_data(self, batch_size):\n        return DataManager(dataset=X, target=Y, batch_size=batch_size)\n</code></pre></p> <p>It is not useful to set a default value for the <code>batch_size</code> argument, as it will be ignored in favour of Fed-BioMed's  internal default value set in the <code>TrainingArgs</code> class. </p>"},{"location":"user-guide/researcher/training-data/#conclusion","title":"Conclusion","text":"<p><code>training_data</code> should be provided in each training plan. The way it is defined is almost the same for each framework's  training plan as long as the structure of the datasets is simple. Since the method is defined by users, it provides  flexibility to load and pre-process complex datasets distributed on the nodes. However, this method will be executed  on the node side. Therefore, typos and lack of arguments may cause errors in the nodes even if it does not create any  errors on the researcher side.</p>"},{"location":"user-guide/researcher/training-plan/","title":"The Training Plan","text":"<p>A training plan is a class that defines the four main components of federated model training: the data, the model, the loss and the optimizer. It is responsible for providing custom methods allowing every node to perform the training.  In Fed-BioMed, you will be required to define a training plan class before submitting a federated training experiment.  You will do so by sub-classing one of the base training plan classes provided by the library,  and overriding certain methods to suit your needs as explained below. The code of the whole training plan class is shipped to the nodes, meaning that you may define custom classes and functions inside it, and re-use them within the training routine. </p> <p>Training Plans</p> <p>A Training Plan contains the recipe for executing the training loop on the nodes. It defines: the data, the model, the loss function, and the optimizer. The code in the training plan is shipped in its entirety to the nodes, where its different parts are executed at different times during the training loop.</p>"},{"location":"user-guide/researcher/training-plan/#the-trainingplan-class","title":"The <code>TrainingPlan</code> class","text":"<p>Fed-BioMed provides a base training plan class for two commonly-used ML frameworks: PyTorch (<code>fedbiomed.common.training_plans.TorchTrainingPlan</code>)  and scikit-learn (<code>fedbiomed.common.training_plans.SKLearnTrainingPlan</code>). Therefore, the first step of the definition of your federated training experiment will be to define a new training plan class that inherits from one of these. </p>"},{"location":"user-guide/researcher/training-plan/#pytorch-training-plan","title":"Pytorch Training Plan","text":"<p>The interfaces for the two frameworks differ quite a bit, so let's start by taking the example of PyTorch:</p> <pre><code>from fedbiomed.common.training_plans import TorchTrainingPlan\n\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    pass\n</code></pre> <p>The above example will not lead to a meaningful experiment, because we need to provide at least the following information to complete our training plan:</p> <ul> <li>a model instance</li> <li>an optimizer instance</li> <li>a list of dependencies (i.e. modules to be imported before instantiating the model and optimizer)</li> <li>how to load the training data (and potential preprocessing)</li> <li>a loss function</li> </ul> <p>Following the PyTorch example, here is what the prototype of your training plan would look like: <pre><code>from fedbiomed.common.training_plans import TorchTrainingPlan\n\nclass MyTrainingPlan(TorchTrainingPlan):\n    def init_model(self, model_args):\n        # defines and returns a model\n        pass\n\n    def init_optimizer(self, optimizer_args):\n        # defines and returns an optimizer\n        pass\n\n    def init_dependencies(self):\n        # returns a list of dependencies\n        pass\n\n    def training_data(self, batch_size=1):\n        # returns a Fed-BioMed DataManager object\n        pass\n\n    def training_step(self, data, target):\n        # returns the loss\n        pass\n</code></pre></p>"},{"location":"user-guide/researcher/training-plan/#scikit-learn-training-plan","title":"Scikit-learn Training Plan","text":"<p>In the case of scikit-learn, Fed-BioMed already does a lot of the heavy lifting for you by providing the <code>FedPerceptron</code>, <code>FedSGDClassifier</code> and <code>FedSGDRegressor</code> classes as training plans. These classes already take care of the model, optimizer, loss function and related dependencies for you, so you only need to define how the data will be loaded. For example, in the case of <code>FedSGDClassifier</code>:</p> <pre><code>from fedbiomed.common.training_plans import FedSGDClassifier\n\nclass MyTrainingPlan(FedSGDClassifier):\n    def training_data(self):\n        # returns a Fed-BioMed DataManager object\n        pass\n</code></pre>"},{"location":"user-guide/researcher/training-plan/#_1","title":"The Training Plan of Fed-BioMed","text":"<p>Definition of <code>__init__</code> is discouraged for all training plans</p> <p>As you may have noticed, none of the examples defined an <code>__init__</code> function for the training plan. This is on purpose! Overriding <code>__init__</code> is not required, and is actually discouraged, as it is reserved for the library's internal use. If you decide to override it, you do it at your own risk!</p>"},{"location":"user-guide/researcher/training-plan/#accessing-the-training-plan-attributes","title":"Accessing the Training Plan attributes","text":"<p>Fed-BioMed provides the following getter functions to access Training Plan attributes:</p> attribute function TorchTrainingPlan SKLearnTrainingPlan notes model <code>model()</code> you may not dynamically reassign a model. The instance of the model is created at initialization by storing the output of the <code>init_model</code> function. optimizer <code>optimizer()</code> you may not dynamically reassign an optimizer. The instance of the optimizer is created at initialization by storing the output of the <code>init_optimizer</code> function. model arguments <code>model_args()</code> training arguments <code>training_args()</code> optimizer arguments <code>optimizer_args()</code>"},{"location":"user-guide/researcher/training-plan/#_2","title":"The Training Plan of Fed-BioMed","text":"<p>Lifecycle of Training Plan Attributes</p> <p>The attributes in the table above will not be available during the <code>init_model</code>, <code>init_optimizer</code> and  <code>init_dependencies</code> functions, as they are set just after initialization. You may however use them in the definition of <code>training_data</code>, <code>training_step</code> or <code>training_routine</code>.</p>"},{"location":"user-guide/researcher/training-plan/#defining-the-training-data","title":"Defining the training data","text":"<p>The method <code>training_data</code> defines how datasets should be loaded in nodes to make them ready for training. In both PyTorch and scikit-learn training plans, you are required to define a <code>training_data</code> method with the following specs:</p> <ol> <li>takes as input a <code>batch_size</code> parameter</li> <li>returns a <code>fedbiomed.common.data.DataManager</code> object</li> <li>inside the method, a dataset is instantiated according to the data type that you wish to use (one of <code>torch.Dataset</code>,    <code>numpy.ndarray</code> or a <code>*Dataset</code> class from the <code>fedbiomed.common.data</code> module)</li> <li>the dataset is used to initialize a <code>DataManager</code> class to be returned</li> </ol> <p>The signature of the <code>training_data</code> function is then: <pre><code>def training_data(self, batch_size) -&gt; DataManager:\n</code></pre></p> <p>You can read the documentation for training data to learn more about the <code>DataManager</code> class and various use cases.</p>"},{"location":"user-guide/researcher/training-plan/#initializing-the-model","title":"Initializing the model","text":"<p>In Pytorch training plans, you must also define a <code>init_model</code> function with the following signature: <pre><code>def init_model(self, model_args: Dict[str, Any]) -&gt; torch.nn.Module:\n</code></pre></p> <p>The purpose of <code>init_model</code> is to return an instance of a trainable PyTorch model. Since the definition of such models  can be quite large, a common pattern is to define the neural network class inside the training plan namespace,  and simply instantiate it within <code>init_model</code>. This also allows to minimize the amount of adjustments needed to go from local PyTorch code to its federated version. Remember that only the code defined inside the training plan namespace will be shipped to the nodes for execution, so you may not use classes that are defined outside of it.</p> <p>The Pytorch neural network class that you define must satisfy the following constraints: 1. it should inherit from <code>torch.nn.Module</code> 2. it should implement a <code>forward</code> method that takes a <code>torch.Tensor</code> as input and returns a <code>torch.Tensor</code>  Note that inheriting from <code>torch.nn.Sequential</code> and using the default <code>forward</code> method would also respect the conditions above. </p> <p>The <code>model_args</code> argument is a dictionary of model arguments that you may provide to the <code>Experiment</code> class and that will be automatically passed to the <code>init_model</code> function internally. If you followed the suggested pattern of defining the model class within the training plan namespace, you can easily adapt the model's constructor to make use of any model arguments that you wish to define.</p> <p>The example below, adapted from our getting started notebook, shows the suggested pattern, the use of <code>init_model</code>, and the use of <code>model_args</code>.</p> <pre><code>import torch.nn as nn\nfrom fedbiomed.common.training_plans import TorchTrainingPlan\nfrom fedbiomed.common.data import DataManager\n\n\n# Here we define the model to be used. \n# You can use any class name (here 'Net')\nclass MyTrainingPlan(TorchTrainingPlan):\n\n    # Defines and return model \n    def init_model(self, model_args):\n        return self.Net(model_args = model_args)\n\n    class Net(nn.Module):\n        def __init__(self, model_args):\n            super().__init__()\n\n            fc_hidden_layer_size = model_args.get('fc_hidden_size', 128)\n\n            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n            self.dropout1 = nn.Dropout(0.25)\n            self.dropout2 = nn.Dropout(0.5)\n            self.fc1 = nn.Linear(9216, fc_hidden_layer_size)\n            self.fc2 = nn.Linear(fc_hidden_layer_size, 10)\n\n        def forward(self, x):\n            x = self.conv1(x)\n            x = F.relu(x)\n            x = self.conv2(x)\n            x = F.relu(x)\n            x = F.max_pool2d(x, 2)\n            x = self.dropout1(x)\n            x = torch.flatten(x, 1)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.dropout2(x)\n            x = self.fc2(x)\n\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    def training_data(self, batch_size = 48):\n        pass    \n\n    def training_step(self, data, target):\n        pass\n\n    def init_optimizer(self, optimizer_args):\n        pass\n\n    def init_dependencies(self):\n        pass\n</code></pre>"},{"location":"user-guide/researcher/training-plan/#defining-the-optimizer","title":"Defining the optimizer","text":"<p>In Pytorch training plans, you must also define a <code>init_optimizer</code> function with the following signature: <pre><code>def init_optimizer(self, optimizer_args: Dict[str, Any]) -&gt; torch.optim.Optimizer:\n</code></pre></p> <p>The purpose of <code>init_optimizer</code> is to return an instance of a PyTorch optimizer. You may instantiate a \"vanilla\"  optimizer directly from <code>torch.optim</code>, or follow a similar pattern to <code>init_model</code> by defining a custom optimizer class  within the training plan namespace. </p>"},{"location":"user-guide/researcher/training-plan/#_3","title":"The Training Plan of Fed-BioMed","text":"<p>The output of <code>init_optimizer</code> must be a <code>torch.optim</code> type</p> <p>The output of <code>init_optimizer</code> must be either a vanilla optimizer provided by the <code>torch.optim</code> module, or a class that inherits from <code>torch.optim.Optimizer</code></p> <p>Similarly, the <code>optimizer_args</code> follow the same pattern as <code>model_args</code> described above.  Note that the learning rate will always be included in the optimizer arguments with the key <code>lr</code>.</p> <p>A pretty straightforward example can be again found in the getting started notebook <pre><code>def init_optimizer(self, optimizer_args):\n    return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n</code></pre></p>"},{"location":"user-guide/researcher/training-plan/#defining-the-loss-function","title":"Defining the loss function","text":"<p>The PyTorch training plan requires you to define the loss function via the <code>training_step</code> method, with the following signature: <pre><code>def training_step(self, data, target) -&gt; float:\n</code></pre></p> <p>The <code>training_step</code> method of the training class defines how the cost is computed by forwarding input values through the  network and using the loss function. It should return the loss value. By default, it is not defined in the parent  <code>TrainingPlan</code> class: it should be defined by the researcher in his/her model class, same as the <code>forward</code> method. An example of training step for PyTorch is shown below.</p> <pre><code>    def training_step(self, data, target):\n        output = self.forward(data)\n        loss   = torch.nn.functional.nll_loss(output, target)\n        return loss\n</code></pre>"},{"location":"user-guide/researcher/training-plan/#type-of-data-and-target","title":"Type of <code>data</code> and <code>target</code>","text":"<p>The <code>training_step</code> function takes as input two arguments, <code>data</code> and <code>target</code>, which are obtained by cycling through the dataset defined in the <code>training_data</code> function. There is some flexibility concerning what type of variables they might be. </p> <p>In a Pytorch training plan, the following data types are supported: </p> <ul> <li>a <code>torch.Tensor</code></li> <li>a collection (a <code>dict</code>, <code>tuple</code> or <code>list</code>) of <code>torch.Tensor</code></li> <li>a recursive collection of collections, arbitrarily nested, that ultimately contain <code>torch.Tensor</code> objects</li> </ul> <p>Be aware of the data types in your dataset</p> <p>It is ultimately your responsibility to write the code for <code>training_step</code> that correctly handles the data types returned by the <code>__getitem__</code> function of the dataset you are targeting. Be aware of the specifics of your dataset when writing this function.</p>"},{"location":"user-guide/researcher/training-plan/#adding-dependencies","title":"Adding Dependencies","text":"<p>By dependencies we mean here the python modules that are necessary to build all the various elements of your training plan on the node side. The method <code>init_dependencies</code> allows you to indicate modules that are needed by your model class, with the following  signature: <pre><code>def init_dependencies(self) -&gt; List[str]:\n</code></pre></p> <p>Each dependency should be defined as valid import statement in a string, for example <code>from torch.optim import Adam</code> or  <code>import torch</code>. You must specify dependencies for any python module that you wish to use, regardless of whether it  is for the data, optimizer, model, etc...</p>"},{"location":"user-guide/researcher/training-plan/#training_routine","title":"<code>training_routine</code>","text":"<p>The training routine is the heart of the training plan. This method performs the model training loop, based on given  model and training arguments. For example, if the model is a neural network based on the PyTorch framework, the training routine is in charge of performing the training  part over looping epochs and batches. If the model is a Scikit-Learn model, it fits the model by the given ML method  and Scikit-Learn does the rest. The training routine is executed by the nodes after they have received a train request from the researcher and downloaded the training plan file.</p> <p>Overriding <code>training_routine</code> is discouraged</p> <p>Both PyTorch and scikit-learn training plans already implement a <code>training_routine</code>, that internally uses the <code>training_step</code> provided by you to compute the loss function (only in the PyTorch case). Overriding this default routine is strongly discouraged, and you may do so only at your own risk.</p> <p>As you can see from the following code snippet, the training routine requires some training arguments such  as <code>epochs</code>, <code>lr</code>, <code>batch_size</code> etc. Since the <code>training_routine</code> is already defined by Fed-BioMed, you are only allowed  to control the training process by changing these arguments. Modifying the training routine from the training plan class might raise unexpected errors. These training arguments are passed to the node by the experiment through the network component.</p> <pre><code> def training_routine(self,\n                         epochs: int = 2,\n                         log_interval: int = 10,\n                         lr: Union[int, float] = 1e-3,\n                         batch_size: int = 48,\n                         batch_maxnum: int = 0,\n                         dry_run: bool = False,\n                         ... ):\n\n        # You can see details from `fedbiomed.common.torchnn`\n        # .....\n\n        for epoch in range(1, epochs + 1):\n            training_data = self.training_data(batch_size=batch_size)\n            for batch_idx, (data, target) in enumerate(training_data):\n                self.train() \n                data, target = data.to(self.device), target.to(self.device)\n                self.optimizer.zero_grad()\n                res = self.training_step(data, target)\n                res.backward()\n                self.optimizer.step()\n\n                #.....\n</code></pre>"},{"location":"user-guide/researcher/training-plan/#saving-and-loading-model","title":"Saving and Loading Model","text":"<p>Each training plan provides save and load functionality. These are required for loading and saving model parameters  into the file system after or before the training in the nodes and the researcher part. Consequently,  experiment can upload and download the model parameters. Indeed, each framework has its own way to load and save models.</p> <p>You can access these classes from the <code>fedbiomed/common</code> directory to see them in more detail. </p> <p>Overriding <code>load</code> and <code>save</code> is discouraged</p> <p>Both PyTorch and scikit-learn training plans already implement <code>load</code> and <code>save</code>. Overriding this default routines is strongly discouraged, and you may do so only at your own risk.</p>"},{"location":"user-guide/secagg/certificate-registration/","title":"Registration of Certificate and Network Parameters of FL Parties","text":"<p>Fed-BioMed relies on the MP-SPDZ library for conducting multi-party computation and also uses the MP-SPDZ network infrastructure for MPC, where each party runs an MP-SPDZ instance listening on an IP and port. In order to proceed with Multi-Party Computation (MPC) in federated experiments, each participating party is required to register the network parameters of all the other participating parties. The registration must be done before the experiment.</p>"},{"location":"user-guide/secagg/certificate-registration/#registration-through-cli","title":"Registration Through CLI","text":"<p>Fed-BioMed CLI provide options to manage network parameters registration of other parties easily. Registration process involves two steps as:</p> <ol> <li>Getting the certificate and network details of current component and send it to other parties.</li> <li>Registering certificates and network details received from other parties.</li> </ol>"},{"location":"user-guide/secagg/certificate-registration/#retrieve-certificate-and-registration-instructions","title":"Retrieve certificate and registration instructions","text":"<p>The option <code>registration-instructions</code> facilitates the certificate registration process by providing the necessary details and commands that must be executed by other parties to register the component.</p> <p>The command below generates registration instructions to assist in the process of registering the researcher component in the participating parties. This command must be executed by the researcher component, and the instructions sent to other parties for registration.</p> <pre><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run researcher certificate registration-instructions\n</code></pre> <p>the output is:</p> <pre><code>Hi There!\n\n\nPlease find following certificate to register\n\n-----BEGIN CERTIFICATE-----\nMIIDBzCCAe+gAwIBAgIDAPQiMA0GCSqGSIb3DQEBCwUAMEYxCjAIBgNVBAMMASox\nODA2BgNVBAoML3Jlc2VhcmNoZXJfZTFjNWMxMDEtMGM3OS00M2IxLThiZjEtZDcw\nYjA2YjkxODMwMB4XDTIzMDQwNDEwMTcxN1oXDTI4MDQwMjEwMTcxN1owRjEKMAgG\nA1UEAwwBKjE4MDYGA1UECgwvcmVzZWFyY2hlcl9lMWM1YzEwMS0wYzc5LTQzYjEt\nOGJmMS1kNzBiMDZiOTE4MzAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIB\nAQC+NQU1HzoNJFWguQY8W97oNWWpkZOtXQE/C63JStZoepbos5nsHpMTZ67Qihfu\nBdCe7XNBaZwzTxO3xjKByWocnw+UaadSvNK5zZZNqGiAj3P9F2q1duaWXMldtK/Y\nl9bRAW6pp4ri/lnAU5gADDcV7M79pVxhfhMI3xKFP03CA0OqQnXABgZheMCWmtll\nx8DVEsKj4jCZSaUqMUHDpxX3l1eUPeDryG3kpcWT28dElBSAynRQznq3StTNghC8\nNPMWUQR8uU5HG13n9Xv8+TBZ33b4iXE5Ei24IleFeTJG0PjtRGY6KvEkFKxGvqYs\noKAwpc7u5v0QeDjDeNDrSUhJAgMBAAEwDQYJKoZIhvcNAQELBQADggEBAB5WoUo2\nq4VSExJoIpIDEwCimcEKz/pHX9IYBgLGluzGUPfFfN+cjUsmKjzXtIqTRau+LtVO\nV/TZ7jRbhTZ7A3FZDrmsE/FOENjUQjFeHIW1Ombqso8BmBfgmn84UF/i1q9rieqZ\njMd+0WppJGp0JNV33mV+veuVbZFaFadRznQ/yUflBcYp0Hfji9/ZU74ivaTdl6vF\nLlSIEKmPyHGx+dHub4uzUyfAHlCTxsaOaZzhc8BCR+qbJ499WvKIO5x02r5+mwqN\nIe5FpFt8M14gC+YEfE/KRSOsRlhKHE+wThdNqEC9UpePpkHdS1/9vNs3ql+PojI8\nojZqtVij//Fp8S4=\n-----END CERTIFICATE-----\n\nPlease follow the instructions below to register this certificate:\n\n\n 1- Copy certificate content into a file e.g 'Hospital1.pem'\n 2- Change your directory to 'fedbiomed' root\n 2- Run: \"./scripts/fedbiomed_run [node | researcher] certificate register -pk [PATH WHERE CERTIFICATE IS SAVED] -pi researcher_e1c5c101-0c79-43b1-8bf1-d70b06b91830  --ip 193.0.0.1 --port 14002\"\n    Examples commands to use for VPN/docker mode:\n      ./scripts/fedbiomed_run node certificate register -pk ./etc/cert-secagg -pi researcher_e1c5c101-0c79-43b1-8bf1-d70b06b91830 --ip 193.0.0.1 --port 14002\n      ./scripts/fedbiomed_run researcher certificate register -pk ./etc/cert-secagg -pi researcher_e1c5c101-0c79-43b1-8bf1-d70b06b91830 --ip 193.0.0.1 --port 14002\n</code></pre> <p>The aforementioned instructions provide essential details for registering a party among the other participants in a federated experiment. The information includes the certificate, component identification number (<code>-pi</code>), IP address (<code>--ip</code>), and port (<code>--port</code>).</p> <p>Certificates should be shared outside Fed-BioMed through a trusted channel.</p> <p>Fed-BioMed does not provide a way to exchange certificate and network parameters internally. Therefore, parameters should be shared using third party trusted channels such as e-mail or other messaging channels.</p>"},{"location":"user-guide/secagg/certificate-registration/#registering-the-certificate","title":"Registering the certificate","text":"<p>Certificates of other parties should be registered with their component ID, IP and port information. Certificates must be copied and saved in a file. Then, the file path is given with the option <code>-pk</code>.</p> <pre><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run [node | researcher] certificate register -pk &lt;certificate-file-path&gt; -pi &lt;component-id&gt; --ip  &lt;IP&gt; --port &lt;PORT&gt;\"\n</code></pre> <p>One of <code>[node | researcher]</code> must be chosen according to component type that registers the certificate.</p>"},{"location":"user-guide/secagg/certificate-registration/#registration-through-gui","title":"Registration Through GUI","text":"<p>Currently, certificate registration is not supported through GUI.</p>"},{"location":"user-guide/secagg/certificate-registration/#certificate-registration-in-developmenttesting-mode","title":"Certificate registration in development/testing mode","text":"<p>Certificate registration is a lengthy procedure, as every network parameter must be registered by every other participating component. This process can be time-consuming when components are launched locally for testing or development purposes.</p> <p>However, Fed-BioMed CLI provides a magic script when all components run in development mode in the same clone. The script parses every configuration file created in the <code>etc</code> directory and registers all available parties automatically in every component.</p> <p>After all the components are created, please run the following command to complete certificate registration for development environment.</p> <pre><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run certificate-dev-setup\n</code></pre> <p>Important</p> <p>Secure aggregation setup requires at least 2 nodes and 1 researcher in the FL experiment.</p>"},{"location":"user-guide/secagg/configuration/","title":"Secure Aggregation Configuration","text":"<p>Secure aggregation is implemented in Fed-BioMed and can be activated or deactivated as an option through the configuration. Even if secure aggregation is not configured during the initial installation, Fed-BioMed still works as long as the researcher or node component does not activate it. However, if secure aggregation is activated in an instance, the infrastructure must have been post-configured for secure aggregation beforehand, after the Fed-BioMed instance  is installed.</p> <p>To configure an instance for secure aggregation, you need to install MP-SPDZ for multi-party computation and provide communication parameters, such as IP address, port, and SSL certificate. While Fed-BioMed provides magic scripts for configuring communication parameters in development mode, where all instances are launched in the same local environment, these parameters must be manually configured for other Fed-BioMed deployment cases.</p>"},{"location":"user-guide/secagg/configuration/#mp-spdz-installation","title":"MP-SPDZ Installation","text":"<p>Fed-BioMed provides pre-compiled MP-SPDZ protocols and scripts for Linux-based operating systems. However, for Darwin-based operating systems, MP-SPDZ must be compiled from its source code. Fed-BioMed provides a script that eases this process by distinguishing the operating system and performing the installation.</p> <p>To install or configure existing pre-compiled MP-SPDZ scripts for a <code>Node compoent</code>, please run following command.</p> <p><pre><code>${FEDBIOMED_DIR}/scripts/fedbiomed_configure_secagg node\n</code></pre> For researcher component;</p> <pre><code>${FEDBIOMED_DIR}/scripts/fedbiomed_configure_secagg researcher\n</code></pre> <p>Development environment</p> <p>In a development environment where researcher and nodes are located in the same local environment, it is sufficient to run the installation once equally as node or researcher.</p> <p>Specifying environment</p> <p>Specifying the environment is necessary to make available the required modules for installation. Declaring either node or researcher environments does not make any significant difference, except that the node instance would not have the researcher environment and the researcher instance would not have the node environment in a deployment scenario.</p>"},{"location":"user-guide/secagg/configuration/#creating-a-component","title":"Creating a component","text":"<p>Usually, a Fed-BioMed instance (either node or researcher) is created once it is started or a dataset is added. However, when using secure aggregation, creating Fed-BioMed instances before starting the node gives the opportunity to modify the configuration.</p>"},{"location":"user-guide/secagg/configuration/#using-cli-with-default-option","title":"Using CLI with default option","text":"<p>The Fed-BioMed CLI provides an option to generate or create a component. It creates a configuration file, a database file and certificates.</p> <p>The command below creates a component with a configuration file named <code>config-n1.ini</code>.</p> <pre><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini configuration create\n</code></pre> <p>Other node components can be created by replacing <code>config-n1.in</code>i` with any desired unique name.</p> <p>Fed-BioMed currently allows only a single researcher. Therefore, the following command creates a researcher component with default configuration file name.</p> <pre><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run researcher configuration create\n</code></pre>"},{"location":"user-guide/secagg/configuration/#updating-existing-configuration","title":"Updating existing configuration","text":"<p>Depending on the version of Fed-BioMed for which the configuration file is created, some variables may not be available in the current configuration file after upgrading Fed-BioMed. In such cases, it is always better to back up the old configuration file and create a new one using the CLI. Afterwards, the values of the backed-up configuration file can be replaced by the default ones that are generated.</p>"},{"location":"user-guide/secagg/configuration/#mp-spdz-and-secure-aggregation-configuration","title":"MP-SPDZ and Secure Aggregation Configuration","text":"<p>After the installation and component creation are completed, configuration files of Fed-BioMed instances can be modified. Here are the sections that can be configured.</p> <pre><code>[mpspdz]\nprivate_key = certs/cert_node_e0394bff-4684-4f84-9c84-6c5e3c683dcb/MPSPDZ_certificate.key\npublic_key = certs/cert_node_e0394bff-4684-4f84-9c84-6c5e3c683dcb/MPSPDZ_certificate.pem\nmpspdz_ip = localhost\nmpspdz_port = 14000\nallow_default_biprimes = True\n</code></pre> <p>Node configuration file has extra variables under <code>security</code> section regarding secure aggregation configuration as <code>secure_aggregation</code> and <code>force_secure_aggregation</code>.  These parameters allow node owners to manage activate, deactivate and force secure aggregation by default.</p> <pre><code>[security]\nhashing_algorithm = SHA256\nallow_default_training_plans = True\ntraining_plan_approval = False\nsecure_aggregation = True\nforce_secure_aggregation = False\n</code></pre>"},{"location":"user-guide/secagg/configuration/#activating-deactivating-and-forcing-secure-aggregation","title":"Activating, Deactivating and Forcing Secure Aggregation","text":"<p>Nodes have the privilege of activating, deactivating, and enforcing secure aggregation. This means that model parameters can either be encrypted (required), optionally encrypted, or unencrypted. If a node requires model encryption and a training request from a researcher does not include secure aggregation context, the node will refuse training. If secure aggregation is allowed but not forced by the node, end-users are able to send training requests with or without secure aggregation.</p> <p>In a federated setup, if one of the nodes requires secure aggregation but the researcher does not activate it, the FL round fails. Please refer to the researcher secure aggregation interface for more details.</p> <p>Researcher</p> <p>Researcher configuration file does not have parameter regarding secure aggregation activation. However, secure aggregation context is managed through Experiment interface (class).</p>"},{"location":"user-guide/secagg/configuration/#mp-spdz-network-parameters","title":"MP-SPDZ Network Parameters","text":""},{"location":"user-guide/secagg/configuration/#certificates","title":"Certificates","text":"<p>When a component is created, Fed-BioMed CLI automatically generates SSL certificates. These certificates are self-signed certificates, and they can be re-generated or replaced by certificates signed by third party authorities. Certificate files should be located in <code>${FEDBIOMED_DIR}/etc/certs</code> and path should be relative to <code>${FEDBIOMED_DIR}</code>.</p> <p>Fed-BioMed generates 2048 bits RSA key and signs it using <code>SHA256</code>. Generated private keys and certificates located in <code>${FEDBIOMED_DIR}/etc/certs/cert_&lt;component_id&gt;</code>, and named as <code>MPSPDZ_certificate.key</code> and <code>MPSPDZ_certificate.pem</code>. <code>ORGANIZATION_NAME</code> attribute of the certificate is set to the component id.</p>"},{"location":"user-guide/secagg/configuration/#re-generating-certificates","title":"Re-generating Certificates","text":"<p>Auto-generated certificates expire after 5 years. However, they can be renewed through the Fed-BioMed CLI at any desired time. If a certificate has already been generated, the command should be executed with the --force or -f option.</p> <p><pre><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini certificate generate -f\n</code></pre> or</p> <pre><code>${FEDBIOMED_DIR}/scripts/fedbiomed_run researcher certificate generate -f\n</code></pre> <p>Important</p> <p>After SSL certificates are re-generated, other parties that participate and common FL experiment should register the new certificate. Please see certificate registration</p>"},{"location":"user-guide/secagg/configuration/#mp-spdz-port-and-ip","title":"MP-SPDZ Port and IP","text":"<p>Each component of Fed-BioMed in a FL experiment must have IP address and port number assigned for MP-SPDZ. IP address and port can be set through configuration file or environment variables.</p> <pre><code>[mpspdz]\nprivate_key = certs/cert_node_e0394bff-4684-4f84-9c84-6c5e3c683dcb/MPSPDZ_certificate.key\npublic_key = certs/cert_node_e0394bff-4684-4f84-9c84-6c5e3c683dcb/MPSPDZ_certificate.pem\nmpspdz_ip = localhost\nmpspdz_port = 14000\nallow_default_biprimes = True\n</code></pre>"},{"location":"user-guide/secagg/configuration/#setting-ip-and-port-through-environment-variables","title":"Setting IP and PORT through environment variables","text":"<p>Setting <code>MPSDPZ_IP</code> and <code>MPSDPZ_PORT</code> environment variables can be set dynamically through environment variables while starting a Fed-BioMed component. This option does not re-write configuration file but uses the environment variable value during the component lifetime.</p> <pre><code>MPSDPZ_IP=&lt;ip-address&gt; MPSDPZ_PORT=&lt;port&gt; ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini start\n</code></pre> <p>or for researcher</p> <pre><code>MPSDPZ_IP=&lt;ip-address&gt; MPSDPZ_PORT=&lt;port&gt; ${FEDBIOMED_DIR}/scripts/fedbiomed_run researcher start\n</code></pre> <p>Important</p> <p>IP and port information of each party must be known by every other parties before using secure aggregation. Therefore, changing them without getting them updated on the other parties causes connection problems during the MPC.</p>"},{"location":"user-guide/secagg/configuration/#auto-increment-port-in-dev-mode","title":"Auto-increment Port in Dev-Mode","text":"<p>Fed-BioMed configuration automatically assigns <code>localhost</code> as MP-SPDZ IP by default. The port is incremented automatically starting from <code>14000</code>. The port number for every other generated component in a single clone of Fed-BioMed equals to the previously assigned port number plus one.</p>"},{"location":"user-guide/secagg/configuration/#certificate-and-ipport-registration-of-fl-parties","title":"Certificate and IP/PORT Registration of FL Parties","text":"<p>Each party must register the network parameters of every other party in order to establish communication for MPC. This is because MP-SPDZ has its own communication procedure that differs from the one natively provided by Fed-BioMed. Registration process involves providing network details such as IP, port, and SSL certificate for establishing secure connections. Fortunately, Fed-BioMed provides scripts to simplify the registration process. For more details, please refer to the certificate registration documentation.</p>"},{"location":"user-guide/secagg/introduction/","title":"Secure Aggregation","text":"<p>Fed-BioMed offers a secure aggregation framework where local model parameters of each node are encrypted before sending them to the researcher/aggregator for aggregation. Parameters are encrypted using homomorphic encryption that ties the decryption of the parameters to the execution of fixed computations. This guarantees that the model parameters will remain secure on aggregator level, and researcher (component) or/and end-user will only have the final decrypted aggregated parameters.</p>"},{"location":"user-guide/secagg/introduction/#technologies","title":"Technologies","text":""},{"location":"user-guide/secagg/introduction/#mp-spdz","title":"MP-SPDZ","text":"<p>Fed-BioMed uses MP-SPDZ library for multi-party computation by launching MP-SPDZ protocols at each secure aggregation context setup request. MP-SPDZ processes are started at each request and stopped after context setup computation is completed.</p>"},{"location":"user-guide/secagg/introduction/#fault-tolerant-secure-aggregation","title":"Fault Tolerant Secure Aggregation","text":"<p>Fed-BioMed uses a modified version of  Joye-Libert scheme implementation from repository fault-tolerant-secure-agg.</p>"},{"location":"user-guide/secagg/introduction/#methods-and-techniques","title":"Methods and Techniques","text":""},{"location":"user-guide/secagg/introduction/#joye-libert-secure-aggregation-scheme","title":"Joye-Libert Secure Aggregation Scheme","text":"<p>Secure aggregation in Fed-BioMed is achieved through the use of a mix of the Joye-Libert (JL) aggregation scheme and the Shamir multi-party computation (MPC) protocol. JL is an additively homomorphic encryption (AHE) technique that encrypts model parameters locally using private and unique 2048-bit keys. The sum of encrypted model parameters can only be decrypted using the sum of private keys from each node that participate in the federated learning (FL) experiment. However, the encryption key used on each node is private and not shared with other parties or the central aggregator. Therefore, server key is calculated using MPC without revealing user (node) keys (server-key shares).</p>"},{"location":"user-guide/secagg/introduction/#shamir-mpc-protocol","title":"Shamir MPC Protocol","text":"<p>Shamir multi-party computation protocol is used to compute the server key, which is equal to the negative sum of nodes' keys and is used for decryption on the researcher component. Thanks to MPC, the server key generation does not reveal nodes' private keys to the aggregator.</p>"},{"location":"user-guide/secagg/introduction/#process-flow","title":"Process-flow","text":"<p>Since FL experiments are launched through researcher component, activating secure aggregation and setting up necessary context is done through <code>Experiment</code> class of researcher component. However, the status of the secure aggregation can be managed by node as well: node owner can disable, enable or force secure aggregation (see secure aggregation node configuration for more details).</p>"},{"location":"user-guide/secagg/introduction/#1-generating-public-parameter-biprime","title":"1. Generating Public Parameter Biprime","text":"<p>At the beginning of the FL experiment, researcher sends secure aggregation context setup request to every node that participates to this experiment. The first request is for generating public parameter <code>Biprime</code>. <code>Biprime</code> is multiplication of two prime numbers that are generated using multi party computation. Final prime number is public while prime shares are private and used for <code>Biprime</code> calculation. <code>Biprime</code> should be calculated using at least two different parties. It is used for encrypting and decrypting aggregated parameters in Joye-Libert AHE.</p> <p>Current implementation</p> <p>Since <code>Biprime</code> is public parameter, Fed-BioMed currently uses a default pre-generated 1024-bits biprime. Dynamic biprime generation on our road-map of future releases.</p>"},{"location":"user-guide/secagg/introduction/#2-generating-random-key-that-are-double-the-length-of-biprime","title":"2. Generating random key that are double the length of biprime","text":"<p>After biprime is generated or a default one is loaded, researcher sends another request for generating private key of each node and the corresponding server key for researcher component. Each node generates random private keys.</p> <p>Key-size</p> <p>Key size depends on biprime number that is used for secure aggregation. Maximum key-size should be less or equal the double of biprime key-size.</p>"},{"location":"user-guide/secagg/introduction/#3-execute-shamir","title":"3. Execute Shamir","text":"<p>Once the local key generation is completed, each node launches Shamir protocol to calculate negative sum of keys. The result is only revealed it to the researcher. This protocol is launched using MP-SPDZ library.</p>"},{"location":"user-guide/secagg/introduction/#4-encrypting-model-parameters","title":"4. Encrypting model parameters","text":"<p>If secure aggregation is activated for the <code>Experiment</code>, the training request contains information about which secure aggregation context will be used for encryption. Once training is completed, model parameters are encrypted using biprime and the user (node)-key.</p>"},{"location":"user-guide/secagg/introduction/#5-decrypting-sum-of-encrypted-model-parameters","title":"5. Decrypting sum of encrypted model parameters","text":"<p>After the encryption is done and the encrypted model parameters are received by the researcher, all model parameters are aggregated using JL scheme. Aggregated parameters are clear text. Aggregation and decryption can not be performed independently. It is an atomic operation.</p> <p>Important</p> <p>Secure aggregation requires at least 3 parties in FL experiment with one researcher and 2 nodes.</p>"},{"location":"user-guide/secagg/introduction/#conclusions","title":"Conclusions","text":"<p>In Fed-BioMed, Joye-Libert secure aggregation has been chosen because it provides fast encryption and it does not require the re-generation of server-key and user-key shares for every training round, making it faster. Although it is not tolerant to drop-out during one round of training, the implementation allows the re-configuration of Joye-Libert elements for the next round if a node is dropped or a new node is added.</p> <p>The security model implemented in Fed-BioMed's secure aggregation primarily targets the case of honest but curious parties.</p>"},{"location":"user-guide/secagg/introduction/#next-steps","title":"Next steps","text":"<ul> <li> <p>Configuration documentation for more detail about configuring Fed-BioMed   instances for secure aggregation.</p> </li> <li> <p>Certificate and party registration for MPC.</p> </li> <li> <p>Activating secure aggregation for the training through researcher component.</p> </li> </ul>"},{"location":"user-guide/secagg/researcher-interface/","title":"Managing Secure Aggregation on Researcher Side","text":"<p>Researcher component is responsible for managing secure aggregation context setup that prepares necessary elements to apply secure aggregation over encrypted model parameters. Some nodes might require secure aggregation while some of them don't, and some others don't support secure aggregation. Therefore, end-user (researcher) should activate secure aggregation depending on all participating nodes configuration.</p>"},{"location":"user-guide/secagg/researcher-interface/#managing-secure-aggregation-through-experiment","title":"Managing secure aggregation through Experiment","text":""},{"location":"user-guide/secagg/researcher-interface/#activation","title":"Activation","text":"<p>By default, secure aggregation is deactivated in <code>Experiment</code> class. It can be activated by setting the <code>secagg</code> as <code>True</code>.</p> <pre><code>from fedbiomed.researcher.experiment import Experiment\nExperiment(\n    secagg=True\n)\n</code></pre> <p>Setting secagg <code>True</code> instantiates a <code>SecureAggregation</code> with default arguments as <code>timeout</code> and <code>clipping_range</code>.  However, it is also possible to create a secure aggregation instance by providing desired argument values.</p> <pre><code>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.secagg import SecureAggregation\nExperiment(\n    #...\n    secagg=SecureAggregation(timeout=5, clipping_range=30),\n    #....\n)\n</code></pre> <p>Federated averaging</p> <p>Once the secure aggregation is activated, experiment doesn't use the <code>aggregator</code> parameter of the <code>Experiment</code> (eg <code>FedAverage</code>) for aggregation. Secure aggregation aggregates model parameters with its own federated average, but without weighting them. Therefore, using <code>num_updates</code> instead of <code>epochs</code> in <code>training_args</code> is strongly recommended for secure aggregation.</p>"},{"location":"user-guide/secagg/researcher-interface/#timeout","title":"Timeout","text":"<p>Secure aggregation setup launches MP-SPDZ process in each Fed-BioMed component that participates in the federated training. However, these processes and communication delay might be longer or shorter than expected depending on number of nodes and communication bandwidth. The argument <code>timeout</code> allows increasing or decreasing the timeout for secure aggregation context setup.</p>"},{"location":"user-guide/secagg/researcher-interface/#clipping-range","title":"Clipping Range","text":"<p>Encryption on the node-side is performed after the quantization of model weights/parameters. However, the maximum and minimum values of model parameters may vary depending on the technique used. Therefore, the clipping range of quantization depends on the model, data, or technique. The clipping range should always be greater than or equal to the maximum model weight value, but kept reasonably low.</p> <p>By default, the clipping range is set to 3. If the clipping range is exceeded while encrypting model parameters, a warning is raised instead of failing. Therefore, the end-user is aware that the clipping range should be increased for the next rounds.</p> <p>Setting clipping range</p> <p>The optimal clipping range depends on the specific scenario and the models being used. In some cases, using too high of a clipping range can result in a loss of information and lead to decreased performance. Therefore, it is important to carefully choose the appropriate clipping range based on the specific situation and the characteristics of the models being used.</p>"},{"location":"user-guide/secagg/researcher-interface/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/secagg/researcher-interface/#can-not-set-secure-aggregation-context-on-the-researcher-side","title":"Can not set secure aggregation context on the researcher side","text":"<p>This may be because of the timeout on the researcher side. If you have low bandwidth, connection latency or many nodes, please try to increase timeout.</p>"},{"location":"user-guide/secagg/researcher-interface/#context-is-set-on-the-nodes-but-not-on-the-researcher","title":"Context is set on the nodes but not on the researcher","text":"<p>This is also because of the timeout issue. It happens when MP-SPDZ completes multi-party computation but can not send success status back to researcher in time. Therefore, researcher assumes that the secure aggregation is context is not set properly. Please increase secure aggregation timeout and re-run training round.</p>"},{"location":"user-guide/secagg/researcher-interface/#model-encryption-takes-too-much-time","title":"Model encryption takes too much time","text":"<p>The time of encryption depends on model size. If the model is larger, it is normal that the encryption takes longer.</p>"},{"location":"user-guide/secagg/researcher-interface/#i-want-to-set-secure-aggregation-context-without-re-running-a-round","title":"I want to set secure aggregation context without re-running a round.","text":"<p>It is possible to access the secagg instance through the experiment object in order to reset the secure aggregation context by providing a list of parties and the experiment <code>job_id</code>.</p> <p><pre><code>from fedbiomed.researcher.experiment import Experiment\nfrom fedbiomed.researcher.environ import environ\n\nexp = Experiment(secagg=True,\n                 #....\n                 )\n\nexp.secagg.setup(\n    parties= parties=[environ[\"ID\"]] + exp.job().nodes,\n    job_id=exp.job().id\n)\n</code></pre> If a context has already been set, you can use the force argument to forcefully recreate the context. <pre><code>exp.secagg.setup(\n    parties= parties=[environ[\"ID\"]] + exp.job().nodes,\n    job_id=exp.job().id,\n    force=True\n)\n</code></pre></p>"}]}