<!DOCTYPE html><html> <head><meta charset=UTF-8><meta name=viewport content="width=device-width, initial-scale=1"><meta name=robots context="index, archive"><!--  --><script>
      const versions_json = "../../../../../versions.json";
      const search_worker_js = "../../../../assets/javascript/search-worker.js";
      const search_index_json = "../../../../search/search_index.json";
      const base_url = '../../../..';
    </script><!-- Site title --><title>Optimizers - Fed-BioMed</title><link rel=icon type=image/x-icon href=../../../../favicon.ico><!-- Page description --><meta name=description content="Open, Transparent and Trusted Collaborative Learning for Real-world Healthcare Applications"><!-- Page keywords --><!-- Page author --><!-- Latest compiled and minified CSS --><link rel=stylesheet href=https://stackpath.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css integrity=sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu crossorigin=anonymous><!-- Bootstrap Icons --><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css><link href=../../../../assets/_mkdocstrings.css rel=stylesheet><link href=../../../../assets/css/style.css rel=stylesheet></head> <body> <header> <div class="container-fluid top-bar"> <div class=brand> <a href=/ > <img src=../../../../assets/img/fedbiomed-logo-small.png> </a> </div> <nav class=top> <ul> <li class> <a href=../../../..>Home</a> </li> <li class> <a href=../../../../getting-started/what-is-fedbiomed/ >User Documentation</a> </li> <li class> <a href=../../../../pages/about-us/ >About</a> </li> <li id=clicker class=has-sub> More <i class="bi bi-chevron-down"></i> <ul class=sub-nav-menu> <li class> <a href=../../../../#funding>Funding</a> </li> <li class> <a href=../../../../news/ >News</a> </li> <li class> <a href=../../../../#contributors>Contributors</a> </li> <li class> <a href=../../../../#users>Users</a> </li> <li class> <a href=../../../../pages/roadmap/ >Roadmap</a> </li> <li class> <a href=../../../../#contact-us>Contact Us</a> </li> </ul> </li> </ul> </nav> </div> <div class="container-fluid top-bar-mobile"> <div class=mobile-bar> <div class=brand> <a href=../../../../ > <img src=../../../../assets/img/fedbiomed-logo-small.png> </a> </div> <div class=hum-menu> <img class=open src=../../../../assets/img/menu.svg> <img class=close style=display:none src=../../../../assets/img/cancel.svg> </div> </div> <nav class=top-mobile> <ul> <li class> <a href=../../../..>Home</a> </li> <li class> <a href=../../../../getting-started/what-is-fedbiomed/ >User Documentation</a> </li> <li class> <a href=../../../../pages/about-us/ >About</a> </li> <li id=clicker class=has-sub> More <i class="bi bi-chevron-down"></i> <ul class=sub-nav-menu> <li class> <a href=../../../../#funding>Funding</a> </li> <li class> <a href=../../../../news/ >News</a> </li> <li class> <a href=../../../../#contributors>Contributors</a> </li> <li class> <a href=../../../../#users>Users</a> </li> <li class> <a href=../../../../pages/roadmap/ >Roadmap</a> </li> <li class> <a href=../../../../#contact-us>Contact Us</a> </li> </ul> </li> </ul> </nav> </div> </header> <div class=main> <!-- Home page --> <div class=container-fluid> <div class=doc-row> <div class=left-col> <div class=sidebar-doc> <nav class=sidebar-inner> <ul class="sidebar-menu-left sub"> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Getting Started <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../getting-started/what-is-fedbiomed/ class>What's Fed-BioMed</a> </li> <li class> <a href=../../../../getting-started/getting-started/ class>Basic Example</a> </li> <li class> <a href=../../../../getting-started/fedbiomed-architecture/ class>Fedbiomed Architecture</a> </li> <li class> <a href=../../../../getting-started/fedbiomed-workflow/ class>Fedbiomed Workflow</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Tutorials <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Installation <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../tutorials/installation/0-basic-software-installation/ class>Software Installation</a> </li> <li class> <a href=../../../../tutorials/installation/1-setting-up-environment/ class>Setting Up Environment</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> PyTorch <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/ class>PyTorch MNIST Basic Example</a> </li> <li class> <a href=../../../../tutorials/pytorch/02_Create_Your_Custom_Training_Plan/ class>How to Create Your Custom PyTorch Training Plan</a> </li> <li class> <a href=../../../../tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/ class>PyTorch Used Cars Dataset Example</a> </li> <li class> <a href=../../../../tutorials/pytorch/04-Aggregation_in_Fed-BioMed/ class>PyTorch aggregation methods in Fed-BioMed</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> MONAI <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../tutorials/monai/01_monai-2d-image-classification/ class>Federated 2d image classification with MONAI</a> </li> <li class> <a href=../../../../tutorials/monai/02_monai-2d-image-registration/ class>Federated 2d XRay registration with MONAI</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Scikit-Learn <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/ class>MNIST classification with Scikit-Learn Classifier (Perceptron)</a> </li> <li class> <a href=../../../../tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/ class>Fed-BioMed to train a federated SGD regressor model</a> </li> <li class> <a href=../../../../tutorials/scikit-learn/03-other-scikit-learn-models/ class>Implementing other Scikit Learn models for Federated Learning</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Optimizers <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../tutorials/optimizers/01-fedopt-and-scaffold/ class>Advanced optimizers in Fed-BioMed</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> FLamby <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../tutorials/flamby/flamby/ class>General Concepts</a> </li> <li class> <a href=../../../../tutorials/flamby/flamby-integration-into-fedbiomed/ class>FLamby integration in Fed-BioMed</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Advanced <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../tutorials/advanced/in-depth-experiment-configuration/ class>In Depth Experiment Configuration</a> </li> <li class> <a href=../../../../tutorials/advanced/training-with-gpu/ class>PyTorch model training using a GPU</a> </li> <li class> <a href=../../../../tutorials/advanced/breakpoints/ class>Breakpoints</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Security <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../tutorials/security/differential-privacy-with-opacus-on-fedbiomed/ class>Using Differential Privacy with OPACUS on Fed-BioMed</a> </li> <li class> <a href=../../../../tutorials/security/non-private-local-central-dp-monai-2d-image-registration/ class>Local and Central DP with Fed-BioMed: MONAI 2d image registration</a> </li> <li class> <a href=../../../../tutorials/security/training-with-approved-training-plans/ class>Training Process with Training Plan Management</a> </li> <li class> <a href=../../../../tutorials/security/secure-aggregation/ class>Training with Secure Aggregation</a> </li> <li class> <a href=../../../../tutorials/concrete-ml/concrete-ml/ class>End-to-end Privacy Preserving Training and Inference on Medical Data</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Biomedical data <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../tutorials/medical/medical-image-segmentation-unet-library/ class>Brain Segmentation</a> </li> </ul> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> User Guide <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../user-guide/glossary/ class>Glossary</a> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Deployment <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../user-guide/deployment/deployment/ class>Introduction</a> </li> <li class> <a href=../../../../user-guide/deployment/deployment-vpn/ class>VPN Deployment</a> </li> <li class> <a href=../../../../user-guide/deployment/matrix/ class>Network matrix</a> </li> <li class> <a href=../../../../user-guide/deployment/security-model/ class>Security model</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Node <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../user-guide/nodes/configuring-nodes/ class>Configuring Nodes</a> </li> <li class> <a href=../../../../user-guide/nodes/deploying-datasets/ class>Deploying Datasets</a> </li> <li class> <a href=../../../../user-guide/nodes/training-plan-security-manager/ class>Training Plan Management</a> </li> <li class> <a href=../../../../user-guide/nodes/using-gpu/ class>Using GPU</a> </li> <li class> <a href=../../../../user-guide/nodes/node-gui/ class>Node GUI</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Researcher <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../user-guide/researcher/training-plan/ class>Training Plan</a> </li> <li class> <a href=../../../../user-guide/researcher/training-data/ class>Training Data</a> </li> <li class> <a href=../../../../user-guide/researcher/experiment/ class>Experiment</a> </li> <li class> <a href=../../../../user-guide/researcher/aggregation/ class>Aggregation</a> </li> <li class> <a href=../../../../user-guide/researcher/listing-datasets-and-selecting-nodes/ class>Listing Datasets and Selecting Nodes</a> </li> <li class> <a href=../../../../user-guide/researcher/model-testing-during-federated-training/ class>Model Validation on the Node Side</a> </li> <li class> <a href=../../../../user-guide/researcher/tensorboard/ class>Tensorboard</a> </li> </ul> </li> <li class> <a href=../../../../user-guide/advanced-optimization/ class>Optimization</a> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Secure Aggregation <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../../user-guide/secagg/introduction/ class>Introduction</a> </li> <li class> <a href=../../../../user-guide/secagg/configuration/ class>Configuration</a> </li> <li class> <a href=../../../../user-guide/secagg/certificate-registration/ class>Certificate Registration</a> </li> <li class> <a href=../../../../user-guide/secagg/researcher-interface/ class>Managing Secure Aggregation in Researcher</a> </li> </ul> </li> </ul> </li> <li data-adress=sub-1 class="current has-sub-side"> <div href class="parent-list current "> Developer <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub active "> <li data-adress=sub-1 class="current has-sub-side"> <div href class="parent-list current "> API Reference <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub active "> <li data-adress=sub-1 class="current has-sub-side"> <div href class="parent-list current "> Common <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub active "> <li class> <a href=../constants/ class>Constants</a> </li> <li class> <a href=../data/ class>Data</a> </li> <li class> <a href=../environ/ class>Environ</a> </li> <li class> <a href=../exceptions/ class>Exceptions</a> </li> <li class> <a class href=../json.md>Json</a> </li> <li class> <a href=../logger/ class>Logger</a> </li> <li class> <a href=../message/ class>Message</a> </li> <li class> <a class href=../messaging.md>Messaging</a> </li> <li class> <a href=../models/ class>Model</a> </li> <li class=current> <a href=./ class="link current">Optimizers</a> </li> <li class> <a class href=../repository.md>Repository</a> </li> <li class> <a href=../tasks_queue/ class>TasksQueue</a> </li> <li class> <a href=../training_plans/ class>TrainingPlans</a> </li> <li class> <a href=../training_args/ class>TrainingArgs</a> </li> <li class> <a href=../utils/ class>Utils</a> </li> <li class> <a href=../validator/ class>Validator</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Node <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../node/cli/ class>CLI</a> </li> <li class> <a href=../../node/dataset_manager/ class>DatasetManager</a> </li> <li class> <a href=../../node/node/ class>Node</a> </li> <li class> <a href=../../node/node_state_manager/ class>NodeStateManager</a> </li> <li class> <a href=../../node/training_plan_security_manager/ class>TrainingPlanSecurityManager</a> </li> <li class> <a href=../../node/history_monitor/ class>HistoryMonitor</a> </li> <li class> <a href=../../node/round/ class>Round</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Researcher <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../researcher/aggregators/ class>Aggregators</a> </li> <li class> <a href=../../researcher/datasets/ class>Datasets</a> </li> <li class> <a href=../../researcher/experiment/ class>Experiment</a> </li> <li class> <a href=../../researcher/filetools/ class>Filetools</a> </li> <li class> <a href=../../researcher/jobs/ class>Jobs</a> </li> <li class> <a href=../../researcher/monitor/ class>Monitor</a> </li> <li class> <a href=../../researcher/node_state_agent/ class>NodeStateAgent</a> </li> <li class> <a href=../../researcher/requests/ class>Requests</a> </li> <li class> <a href=../../researcher/strategies/ class>Strategies</a> </li> <li class> <a href=../../researcher/secagg/ class>Secagg</a> </li> </ul> </li> </ul> </li> <li class> <a href=../../../usage_and_tools/ class>Usage and Tools</a> </li> <li class> <a href=../../../ci/ class>Continuous Integration</a> </li> <li class> <a href=../../../definition-of-done/ class>Definition of Done</a> </li> <li class> <a href=../../../testing-in-fedbiomed/ class>Testing in Fed-BioMed</a> </li> <li class> <a href=../../../messaging/ class>RPC Protocol and Messages</a> </li> </ul> </li> </ul> </nav> </div> </div> <div class=main-col> <main class=main-docs> <h1>Optimizers</h1> <article> <div class="doc doc-object doc-module"> <div class="doc doc-contents first"> <p>Optimizer is an interface that enables the use of <a href=https://gitlab.inria.fr/magnet/declearn/declearn2>declearn</a> 's optimizers for Federated Learning inside Fed-BioMed</p> <div class="doc doc-children"> <h2 id=fedbiomed.common.optimizers-classes>Classes</h2> <div class="doc doc-object doc-class"> <h3 id=fedbiomed.common.optimizers.BaseOptimizer class="doc doc-heading"> <span class="doc doc-object-name doc-class-name">BaseOptimizer</span> </h3> <div class="doc-signature highlight"><pre><span></span><code><span class=n>BaseOptimizer</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
</code></pre></div> <div class="doc doc-contents "> <p class="doc doc-class-bases"> Bases: <code><a class="autorefs autorefs-external" title="typing.Generic" href="https://docs.python.org/3/library/typing.html#typing.Generic">Generic</a>[<span title="fedbiomed.common.optimizers.generic_optimizers.OT">OT</span>]</code></p> <p>Abstract base class for Optimizer and Model wrappers.</p> <p><strong>Parameters:</strong></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>model</code></td> <td> <code><a class="autorefs autorefs-internal" title="fedbiomed.common.models.Model" href="../models/#fedbiomed.common.models.Model">Model</a></code> </td> <td> <div class=doc-md-description> <p>model to train, interfaced via a framework-specific Model.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr> <td><code>optimizer</code></td> <td> <code><span title="fedbiomed.common.optimizers.generic_optimizers.OT">OT</span></code> </td> <td> <div class=doc-md-description> <p>optimizer that will be used for optimizing the model.</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <p><strong>Raises:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><a class="autorefs autorefs-internal" title="fedbiomed.common.exceptions.FedbiomedOptimizerError" href="../exceptions/#fedbiomed.common.exceptions.FedbiomedOptimizerError">FedbiomedOptimizerError</a></code> </td> <td> <div class=doc-md-description> <p>Raised if model is not an instance of <code>_model_cls</code> (which may be a subset of the generic Model type).</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="75 "></span><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>:</span> <span class=n>Model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>:</span> <span class=n>OT</span><span class=p>):</span>
<span class=linenos data-linenos="76 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Constuctor of the optimizer wrapper that sets a reference to model and optimizer.</span>
<span class=linenos data-linenos="77 "></span>
<span class=linenos data-linenos="78 "></span><span class=sd>    Args:</span>
<span class=linenos data-linenos="79 "></span><span class=sd>        model: model to train, interfaced via a framework-specific Model.</span>
<span class=linenos data-linenos="80 "></span><span class=sd>        optimizer: optimizer that will be used for optimizing the model.</span>
<span class=linenos data-linenos="81 "></span>
<span class=linenos data-linenos="82 "></span><span class=sd>    Raises:</span>
<span class=linenos data-linenos="83 "></span><span class=sd>        FedbiomedOptimizerError:</span>
<span class=linenos data-linenos="84 "></span><span class=sd>            Raised if model is not an instance of `_model_cls` (which may</span>
<span class=linenos data-linenos="85 "></span><span class=sd>            be a subset of the generic Model type).</span>
<span class=linenos data-linenos="86 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="87 "></span>    <span class=k>if</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model_cls</span><span class=p>):</span>
<span class=linenos data-linenos="88 "></span>        <span class=k>raise</span> <span class=n>FedbiomedOptimizerError</span><span class=p>(</span>
<span class=linenos data-linenos="89 "></span>            <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>ErrorNumbers</span><span class=o>.</span><span class=n>FB626</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2>, in `model` argument, expected an instance &quot;</span>
<span class=linenos data-linenos="90 "></span>            <span class=sa>f</span><span class=s2>&quot;of </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_model_cls</span><span class=si>}</span><span class=s2> but got an object of type </span><span class=si>{</span><span class=nb>type</span><span class=p>(</span><span class=n>model</span><span class=p>)</span><span class=si>}</span><span class=s2>.&quot;</span>
<span class=linenos data-linenos="91 "></span>        <span class=p>)</span>
<span class=linenos data-linenos="92 "></span>    <span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=p>:</span> <span class=n>Model</span> <span class=o>=</span> <span class=n>model</span>
<span class=linenos data-linenos="93 "></span>    <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=p>:</span> <span class=n>OT</span> <span class=o>=</span> <span class=n>optimizer</span>
</code></pre></div> </details> <div class="doc doc-children"> <h4 id=fedbiomed.common.optimizers.BaseOptimizer-attributes>Attributes</h4> <div class="doc doc-object doc-attribute"> <h5 id=fedbiomed.common.optimizers.BaseOptimizer.optimizer class="doc doc-heading"> <span class="doc doc-object-name doc-attribute-name">optimizer</span> <span class="doc doc-labels"> <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small> </span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optimizer</span>
</code></pre></div> <div class="doc doc-contents "> </div> </div> <h4 id=fedbiomed.common.optimizers.BaseOptimizer-functions>Functions</h4> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.BaseOptimizer.init_training class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">init_training</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>init_training</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <p>Sets up training and misceallenous parameters so the model is ready for training</p> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="95 "></span><span class=k>def</span> <span class=nf>init_training</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
<span class=linenos data-linenos="96 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Sets up training and misceallenous parameters so the model is ready for training</span>
<span class=linenos data-linenos="97 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="98 "></span>    <span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=o>.</span><span class=n>init_training</span><span class=p>()</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.BaseOptimizer.load_state class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">load_state</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>load_state</span><span class=p>(</span><span class=n>optim_state</span><span class=p>,</span> <span class=n>load_from_state</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</code></pre></div> <div class="doc doc-contents "> <p>Reconfigures optimizer from a given state.</p> <p>This is the default method for optimizers that don't support state. Does nothing.</p> <p><strong>Parameters:</strong></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>optim_state</code></td> <td> <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a></code> </td> <td> <div class=doc-md-description> <p>not used</p> </div> </td> <td> <em>required</em> </td> </tr> <tr> <td><code>load_from_state</code></td> <td> <code>optional</code> </td> <td> <div class=doc-md-description> <p>not used</p> </div> </td> <td> <code>False</code> </td> </tr> </tbody> </table> <p><strong>Returns:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><a class="autorefs autorefs-external" title="typing.Union" href="https://docs.python.org/3/library/typing.html#typing.Union">Union</a>[<a class="autorefs autorefs-internal" title="fedbiomed.common.optimizers.generic_optimizers.BaseOptimizer" href="#fedbiomed.common.optimizers.BaseOptimizer">BaseOptimizer</a>, None]</code> </td> <td> <div class=doc-md-description> <p>None</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="105 "></span><span class=k>def</span> <span class=nf>load_state</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>optim_state</span><span class=p>:</span> <span class=n>Dict</span><span class=p>,</span> <span class=n>load_from_state</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Union</span><span class=p>[</span><span class=s1>&#39;BaseOptimizer&#39;</span><span class=p>,</span> <span class=kc>None</span><span class=p>]:</span>
<span class=linenos data-linenos="106 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Reconfigures optimizer from a given state.</span>
<span class=linenos data-linenos="107 "></span>
<span class=linenos data-linenos="108 "></span><span class=sd>    This is the default method for optimizers that don&#39;t support state. Does nothing.</span>
<span class=linenos data-linenos="109 "></span>
<span class=linenos data-linenos="110 "></span><span class=sd>    Args:</span>
<span class=linenos data-linenos="111 "></span><span class=sd>        optim_state: not used</span>
<span class=linenos data-linenos="112 "></span><span class=sd>        load_from_state (optional): not used</span>
<span class=linenos data-linenos="113 "></span>
<span class=linenos data-linenos="114 "></span><span class=sd>    Returns:</span>
<span class=linenos data-linenos="115 "></span><span class=sd>        None</span>
<span class=linenos data-linenos="116 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="117 "></span>    <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=s2>&quot;load_state method of optimizer not implemented, cannot load optimizer status&quot;</span><span class=p>)</span>
<span class=linenos data-linenos="118 "></span>    <span class=k>return</span> <span class=kc>None</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.BaseOptimizer.save_state class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">save_state</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>save_state</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <p>Gets optimizer state.</p> <p>This is the default method for optimizers that don't support state. Does nothing.</p> <p><strong>Returns:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><a class="autorefs autorefs-external" title="typing.Union" href="https://docs.python.org/3/library/typing.html#typing.Union">Union</a>[<a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>, None]</code> </td> <td> <div class=doc-md-description> <p>None</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="120 "></span><span class=k>def</span> <span class=nf>save_state</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Union</span><span class=p>[</span><span class=n>Dict</span><span class=p>,</span> <span class=kc>None</span><span class=p>]:</span>
<span class=linenos data-linenos="121 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Gets optimizer state.</span>
<span class=linenos data-linenos="122 "></span>
<span class=linenos data-linenos="123 "></span><span class=sd>    This is the default method for optimizers that don&#39;t support state. Does nothing.</span>
<span class=linenos data-linenos="124 "></span>
<span class=linenos data-linenos="125 "></span><span class=sd>    Returns:</span>
<span class=linenos data-linenos="126 "></span><span class=sd>        None</span>
<span class=linenos data-linenos="127 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="128 "></span>    <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=s2>&quot;save_state method of optimizer not implemented, cannot save optimizer status&quot;</span><span class=p>)</span>
<span class=linenos data-linenos="129 "></span>    <span class=k>return</span> <span class=kc>None</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.BaseOptimizer.step class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">step</span> <span class="doc doc-labels"> <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small> </span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>step</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <p>Performs an optimisation step and updates model weights.</p> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="100 "></span><span class=nd>@abstractmethod</span>
<span class=linenos data-linenos="101 "></span><span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
<span class=linenos data-linenos="102 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Performs an optimisation step and updates model weights.</span>
<span class=linenos data-linenos="103 "></span><span class=sd>    &quot;&quot;&quot;</span>
</code></pre></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h3 id=fedbiomed.common.optimizers.DeclearnOptimizer class="doc doc-heading"> <span class="doc doc-object-name doc-class-name">DeclearnOptimizer</span> </h3> <div class="doc-signature highlight"><pre><span></span><code><span class=n>DeclearnOptimizer</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
</code></pre></div> <div class="doc doc-contents "> <p class="doc doc-class-bases"> Bases: <code><a class="autorefs autorefs-internal" title="fedbiomed.common.optimizers.generic_optimizers.BaseOptimizer" href="#fedbiomed.common.optimizers.BaseOptimizer">BaseOptimizer</a></code></p> <p>Base Optimizer subclass to use a declearn-backed Optimizer.</p> <p><strong>Parameters:</strong></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>model</code></td> <td> <code><a class="autorefs autorefs-internal" title="fedbiomed.common.models.Model" href="../models/#fedbiomed.common.models.Model">Model</a></code> </td> <td> <div class=doc-md-description> <p>Model that wraps the actual model</p> </div> </td> <td> <em>required</em> </td> </tr> <tr> <td><code>optimizer</code></td> <td> <code><a class="autorefs autorefs-external" title="typing.Union" href="https://docs.python.org/3/library/typing.html#typing.Union">Union</a>[<a class="autorefs autorefs-internal" title="fedbiomed.common.optimizers.optimizer.Optimizer" href="#fedbiomed.common.optimizers.Optimizer">Optimizer</a>, <span title="declearn.optimizer.Optimizer">Optimizer</span>]</code> </td> <td> <div class=doc-md-description> <p>declearn optimizer, or fedbiomed optimizer (that wraps declearn optimizer)</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="139 "></span><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>:</span> <span class=n>Model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=n>FedOptimizer</span><span class=p>,</span> <span class=n>declearn</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>Optimizer</span><span class=p>]):</span>
<span class=linenos data-linenos="140 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Constructor of Optimizer wrapper for declearn&#39;s optimizers</span>
<span class=linenos data-linenos="141 "></span>
<span class=linenos data-linenos="142 "></span><span class=sd>    Args:</span>
<span class=linenos data-linenos="143 "></span><span class=sd>        model: Model that wraps the actual model</span>
<span class=linenos data-linenos="144 "></span><span class=sd>        optimizer: declearn optimizer,</span>
<span class=linenos data-linenos="145 "></span><span class=sd>            or fedbiomed optimizer (that wraps declearn optimizer)</span>
<span class=linenos data-linenos="146 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="147 "></span>    <span class=n>logger</span><span class=o>.</span><span class=n>debug</span><span class=p>(</span><span class=s2>&quot;Using declearn optimizer&quot;</span><span class=p>)</span>
<span class=linenos data-linenos="148 "></span>    <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>declearn</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>Optimizer</span><span class=p>):</span>
<span class=linenos data-linenos="149 "></span>        <span class=c1># convert declearn optimizer into a fedbiomed optimizer wrapper</span>
<span class=linenos data-linenos="150 "></span>        <span class=n>optimizer</span> <span class=o>=</span> <span class=n>FedOptimizer</span><span class=o>.</span><span class=n>from_declearn_optimizer</span><span class=p>(</span><span class=n>optimizer</span><span class=p>)</span>
<span class=linenos data-linenos="151 "></span>    <span class=k>elif</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>FedOptimizer</span><span class=p>):</span>
<span class=linenos data-linenos="152 "></span>        <span class=k>raise</span> <span class=n>FedbiomedOptimizerError</span><span class=p>(</span>
<span class=linenos data-linenos="153 "></span>            <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>ErrorNumbers</span><span class=o>.</span><span class=n>FB626</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2>: expected a declearn optimizer,&quot;</span>
<span class=linenos data-linenos="154 "></span>            <span class=sa>f</span><span class=s2>&quot; but got an object with type </span><span class=si>{</span><span class=nb>type</span><span class=p>(</span><span class=n>optimizer</span><span class=p>)</span><span class=si>}</span><span class=s2>.&quot;</span>
<span class=linenos data-linenos="155 "></span>        <span class=p>)</span>
<span class=linenos data-linenos="156 "></span>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
<span class=linenos data-linenos="157 "></span>    <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>init_round</span><span class=p>()</span>
</code></pre></div> </details> <div class="doc doc-children"> <h4 id=fedbiomed.common.optimizers.DeclearnOptimizer-attributes>Attributes</h4> <div class="doc doc-object doc-attribute"> <h5 id=fedbiomed.common.optimizers.DeclearnOptimizer.model class="doc doc-heading"> <span class="doc doc-object-name doc-attribute-name">model</span> <span class="doc doc-labels"> <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small> <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small> </span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>model</span> <span class=o>=</span> <span class=kc>None</span>
</code></pre></div> <div class="doc doc-contents "> </div> </div> <div class="doc doc-object doc-attribute"> <h5 id=fedbiomed.common.optimizers.DeclearnOptimizer.optimizer class="doc doc-heading"> <span class="doc doc-object-name doc-attribute-name">optimizer</span> <span class="doc doc-labels"> <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small> <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small> </span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>optimizer</span> <span class=o>=</span> <span class=kc>None</span>
</code></pre></div> <div class="doc doc-contents "> </div> </div> <h4 id=fedbiomed.common.optimizers.DeclearnOptimizer-functions>Functions</h4> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.DeclearnOptimizer.get_aux class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">get_aux</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>get_aux</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="177 "></span><span class=k>def</span> <span class=nf>get_aux</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]]:</span>
<span class=linenos data-linenos="178 "></span>    <span class=n>aux</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>get_aux</span><span class=p>()</span>
<span class=linenos data-linenos="179 "></span>    <span class=k>return</span> <span class=n>aux</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.DeclearnOptimizer.load_state class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">load_state</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>load_state</span><span class=p>(</span><span class=n>optim_state</span><span class=p>,</span> <span class=n>load_from_state</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</code></pre></div> <div class="doc doc-contents "> <p>Reconfigures optimizer from a given state (contained in <code>optim_state</code> argument). Usage: <div class=highlight><pre><span></span><code><span class=o>&gt;&gt;&gt;</span> <span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=o>&gt;&gt;&gt;</span> <span class=kn>from</span> <span class=nn>fedbiomed.common.optimizers</span> <span class=kn>import</span> <span class=n>Optimizer</span>
<span class=o>&gt;&gt;&gt;</span> <span class=kn>from</span> <span class=nn>fedbiomed.common.models</span> <span class=kn>import</span> <span class=n>TorchModel</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>model</span> <span class=o>=</span> <span class=n>TorchModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>optimizer</span> <span class=o>=</span> <span class=n>Optimizer</span><span class=p>(</span><span class=n>lr</span><span class=o>=</span><span class=mf>.1</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>optim</span> <span class=o>=</span> <span class=n>DeclearnOptimizer</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>optim</span><span class=o>.</span><span class=n>load_state</span><span class=p>(</span><span class=n>state</span><span class=p>)</span>  <span class=c1># provided state contains the state one wants to load the optimizer with</span>
</code></pre></div> If <code>load_from_state</code> argument is True, it completes the current optimizer state with <code>optim_state</code> argument</p> <p><div class=highlight><pre><span></span><code><span class=o>&gt;&gt;&gt;</span> <span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=o>&gt;&gt;&gt;</span> <span class=kn>from</span> <span class=nn>fedbiomed.common.optimizers</span> <span class=kn>import</span> <span class=n>Optimizer</span>
<span class=o>&gt;&gt;&gt;</span> <span class=kn>from</span> <span class=nn>fedbiomed.common.optimizers.declearn</span> <span class=kn>import</span> <span class=n>MomentumModule</span><span class=p>,</span> <span class=n>AdamModule</span>
<span class=o>&gt;&gt;&gt;</span> <span class=kn>from</span> <span class=nn>fedbiomed.common.models</span> <span class=kn>import</span> <span class=n>TorchModel</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>model</span> <span class=o>=</span> <span class=n>TorchModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>optimizer</span> <span class=o>=</span> <span class=n>Optimizer</span><span class=p>(</span><span class=n>lr</span><span class=o>=</span><span class=mf>.1</span><span class=p>,</span> <span class=n>modules</span><span class=o>=</span><span class=p>[</span><span class=n>MomentumModule</span><span class=p>(),</span> <span class=n>AdamModule</span><span class=p>()])</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>optim_1</span> <span class=o>=</span> <span class=n>DeclearnOptimizer</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>optimizer</span> <span class=o>=</span> <span class=n>Optimizer</span><span class=p>(</span><span class=n>lr</span><span class=o>=</span><span class=mf>.1</span><span class=p>,</span> <span class=n>modules</span><span class=o>=</span><span class=p>[</span><span class=n>AdamModule</span><span class=p>(),</span> <span class=n>MomentumModule</span><span class=p>()])</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>optim_2</span> <span class=o>=</span> <span class=n>DeclearnOptimizer</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>optim_2</span><span class=o>.</span><span class=n>load_state</span><span class=p>(</span><span class=n>optim_1</span><span class=o>.</span><span class=n>save_state</span><span class=p>())</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>optim_2</span><span class=o>.</span><span class=n>save_state</span><span class=p>()[</span><span class=s1>&#39;states&#39;</span><span class=p>]</span>
<span class=p>{</span><span class=s1>&#39;modules&#39;</span><span class=p>:</span> <span class=p>[(</span><span class=s1>&#39;momentum&#39;</span><span class=p>,</span> <span class=p>{</span><span class=s1>&#39;velocity&#39;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>}),</span>
        <span class=p>(</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span>
        <span class=p>{</span><span class=s1>&#39;steps&#39;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
            <span class=s1>&#39;vmax&#39;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
            <span class=s1>&#39;momentum&#39;</span><span class=p>:</span> <span class=p>{</span><span class=s1>&#39;state&#39;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>},</span>
            <span class=s1>&#39;velocity&#39;</span><span class=p>:</span> <span class=p>{</span><span class=s1>&#39;state&#39;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>}})]}</span>
</code></pre></div> Modules of DeclearnOptimizer will be reloaded provided that Module is the same and occupying the same index. Eg if the state contains following modules: <code>modules=[AdamModule(), AdagradModule(), MomemtumModule()]</code> And the Optimizer contained in the TrainingPlan has the following modules: <code>modules=[AdamModule(), MomemtumModule()]</code> Then only <code>AdamModule</code> module will be reloaded, <code>MomentumModule</code> will be set with default argument (they don't share the same index in the modules list).</p> <p><strong>Parameters:</strong></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>optim_state</code></td> <td> <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="typing.Any" href="https://docs.python.org/3/library/typing.html#typing.Any">Any</a>]</code> </td> <td> <div class=doc-md-description> <p>state of the Optimizer to be loaded. It will change the current state of the optimizer with the one loaded</p> </div> </td> <td> <em>required</em> </td> </tr> <tr> <td><code>load_from_state</code></td> <td> <code>optional</code> </td> <td> <div class=doc-md-description> <p>strategy for loading states: whether to load from saved states (True) or from breakpoint (False). If set to True, loading is done partially in the sense that if some of the OptimModules is different in the optim_state and the original state of the optimizer, it loads only the OptiModule(s) from the latest state that both state has in common. Defaults to False.</p> </div> </td> <td> <code>False</code> </td> </tr> </tbody> </table> <p><strong>Raises:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><a class="autorefs autorefs-internal" title="fedbiomed.common.exceptions.FedbiomedOptimizerError" href="../exceptions/#fedbiomed.common.exceptions.FedbiomedOptimizerError">FedbiomedOptimizerError</a></code> </td> <td> <div class=doc-md-description> <p>raised if state is not of dict type.</p> </div> </td> </tr> </tbody> </table> <p><strong>Returns:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><a class="autorefs autorefs-internal" title="fedbiomed.common.optimizers.generic_optimizers.DeclearnOptimizer" href="#fedbiomed.common.optimizers.DeclearnOptimizer">DeclearnOptimizer</a></code> </td> <td> <div class=doc-md-description> <p>Optimizer wrapper reloaded from <code>optim_state</code> argument.</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="181 "></span><span class=k>def</span> <span class=nf>load_state</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>optim_state</span><span class=p>:</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>],</span> <span class=n>load_from_state</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=s1>&#39;DeclearnOptimizer&#39;</span><span class=p>:</span>
<span class=linenos data-linenos="182 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Reconfigures optimizer from a given state (contained in `optim_state` argument).</span>
<span class=linenos data-linenos="183 "></span><span class=sd>    Usage:</span>
<span class=linenos data-linenos="184 "></span><span class=sd>    ```python</span>
<span class=linenos data-linenos="185 "></span><span class=sd>    &gt;&gt;&gt; import torch.nn as nn</span>
<span class=linenos data-linenos="186 "></span><span class=sd>    &gt;&gt;&gt; from fedbiomed.common.optimizers import Optimizer</span>
<span class=linenos data-linenos="187 "></span><span class=sd>    &gt;&gt;&gt; from fedbiomed.common.models import TorchModel</span>
<span class=linenos data-linenos="188 "></span><span class=sd>    &gt;&gt;&gt; model = TorchModel(nn.Linear(4, 2))</span>
<span class=linenos data-linenos="189 "></span><span class=sd>    &gt;&gt;&gt; optimizer = Optimizer(lr=.1)</span>
<span class=linenos data-linenos="190 "></span><span class=sd>    &gt;&gt;&gt; optim = DeclearnOptimizer(model, optimizer)</span>
<span class=linenos data-linenos="191 "></span>
<span class=linenos data-linenos="192 "></span><span class=sd>    &gt;&gt;&gt; optim.load_state(state)  # provided state contains the state one wants to load the optimizer with</span>
<span class=linenos data-linenos="193 "></span><span class=sd>    ```</span>
<span class=linenos data-linenos="194 "></span><span class=sd>    If `load_from_state` argument is True, it completes the current optimizer state with `optim_state` argument</span>
<span class=linenos data-linenos="195 "></span>
<span class=linenos data-linenos="196 "></span><span class=sd>    ```python</span>
<span class=linenos data-linenos="197 "></span><span class=sd>    &gt;&gt;&gt; import torch.nn as nn</span>
<span class=linenos data-linenos="198 "></span><span class=sd>    &gt;&gt;&gt; from fedbiomed.common.optimizers import Optimizer</span>
<span class=linenos data-linenos="199 "></span><span class=sd>    &gt;&gt;&gt; from fedbiomed.common.optimizers.declearn import MomentumModule, AdamModule</span>
<span class=linenos data-linenos="200 "></span><span class=sd>    &gt;&gt;&gt; from fedbiomed.common.models import TorchModel</span>
<span class=linenos data-linenos="201 "></span><span class=sd>    &gt;&gt;&gt; model = TorchModel(nn.Linear(4, 2))</span>
<span class=linenos data-linenos="202 "></span><span class=sd>    &gt;&gt;&gt; optimizer = Optimizer(lr=.1, modules=[MomentumModule(), AdamModule()])</span>
<span class=linenos data-linenos="203 "></span><span class=sd>    &gt;&gt;&gt; optim_1 = DeclearnOptimizer(model, optimizer)</span>
<span class=linenos data-linenos="204 "></span>
<span class=linenos data-linenos="205 "></span><span class=sd>    &gt;&gt;&gt; optimizer = Optimizer(lr=.1, modules=[AdamModule(), MomentumModule()])</span>
<span class=linenos data-linenos="206 "></span><span class=sd>    &gt;&gt;&gt; optim_2 = DeclearnOptimizer(model, optimizer)</span>
<span class=linenos data-linenos="207 "></span><span class=sd>    &gt;&gt;&gt; optim_2.load_state(optim_1.save_state())</span>
<span class=linenos data-linenos="208 "></span><span class=sd>    &gt;&gt;&gt; optim_2.save_state()[&#39;states&#39;]</span>
<span class=linenos data-linenos="209 "></span><span class=sd>    {&#39;modules&#39;: [(&#39;momentum&#39;, {&#39;velocity&#39;: 0.0}),</span>
<span class=linenos data-linenos="210 "></span><span class=sd>            (&#39;adam&#39;,</span>
<span class=linenos data-linenos="211 "></span><span class=sd>            {&#39;steps&#39;: 0,</span>
<span class=linenos data-linenos="212 "></span><span class=sd>                &#39;vmax&#39;: None,</span>
<span class=linenos data-linenos="213 "></span><span class=sd>                &#39;momentum&#39;: {&#39;state&#39;: 0.0},</span>
<span class=linenos data-linenos="214 "></span><span class=sd>                &#39;velocity&#39;: {&#39;state&#39;: 0.0}})]}</span>
<span class=linenos data-linenos="215 "></span><span class=sd>    ```</span>
<span class=linenos data-linenos="216 "></span><span class=sd>    Modules of DeclearnOptimizer will be reloaded provided that Module is the same and occupying the same index.</span>
<span class=linenos data-linenos="217 "></span><span class=sd>    Eg if the state contains following modules: </span>
<span class=linenos data-linenos="218 "></span><span class=sd>    ```modules=[AdamModule(), AdagradModule(), MomemtumModule()]```</span>
<span class=linenos data-linenos="219 "></span><span class=sd>     And the Optimizer contained in the TrainingPlan has the following modules:</span>
<span class=linenos data-linenos="220 "></span><span class=sd>    ```modules=[AdamModule(), MomemtumModule()]```</span>
<span class=linenos data-linenos="221 "></span><span class=sd>    Then only `AdamModule` module will be reloaded, `MomentumModule` will be set with default argument (they don&#39;t</span>
<span class=linenos data-linenos="222 "></span><span class=sd>    share the same index in the modules list).</span>
<span class=linenos data-linenos="223 "></span>
<span class=linenos data-linenos="224 "></span><span class=sd>    Args:</span>
<span class=linenos data-linenos="225 "></span><span class=sd>        optim_state: state of the Optimizer to be loaded. It will change the current state of the optimizer</span>
<span class=linenos data-linenos="226 "></span><span class=sd>            with the one loaded</span>
<span class=linenos data-linenos="227 "></span><span class=sd>        load_from_state (optional): strategy for loading states: whether to load from saved states (True) or</span>
<span class=linenos data-linenos="228 "></span><span class=sd>            from breakpoint (False).</span>
<span class=linenos data-linenos="229 "></span><span class=sd>            If set to True, loading is done partially in the sense that if some of the OptimModules is different in</span>
<span class=linenos data-linenos="230 "></span><span class=sd>            the optim_state and the original state of the optimizer, it loads only the OptiModule(s) from the</span>
<span class=linenos data-linenos="231 "></span><span class=sd>            latest state that both state has in common. Defaults to False.</span>
<span class=linenos data-linenos="232 "></span>
<span class=linenos data-linenos="233 "></span><span class=sd>    Raises:</span>
<span class=linenos data-linenos="234 "></span><span class=sd>        FedbiomedOptimizerError: raised if state is not of dict type.</span>
<span class=linenos data-linenos="235 "></span>
<span class=linenos data-linenos="236 "></span><span class=sd>    Returns:</span>
<span class=linenos data-linenos="237 "></span><span class=sd>        Optimizer wrapper reloaded from `optim_state` argument.</span>
<span class=linenos data-linenos="238 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="239 "></span>    <span class=c1># state: breakpoint content for optimizer</span>
<span class=linenos data-linenos="240 "></span>    <span class=k>if</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>optim_state</span><span class=p>,</span> <span class=n>Dict</span><span class=p>):</span>
<span class=linenos data-linenos="241 "></span>        <span class=k>raise</span> <span class=n>FedbiomedOptimizerError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>ErrorNumbers</span><span class=o>.</span><span class=n>FB626</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2>, incorrect type of argument `optim_state`: &quot;</span>
<span class=linenos data-linenos="242 "></span>                                      <span class=sa>f</span><span class=s2>&quot;expecting a dict, but got </span><span class=si>{</span><span class=nb>type</span><span class=p>(</span><span class=n>optim_state</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=linenos data-linenos="243 "></span>
<span class=linenos data-linenos="244 "></span>    <span class=k>if</span> <span class=n>load_from_state</span><span class=p>:</span>
<span class=linenos data-linenos="245 "></span>        <span class=c1># first get Optimizer detailed in the TrainingPlan.</span>
<span class=linenos data-linenos="246 "></span>
<span class=linenos data-linenos="247 "></span>        <span class=n>init_optim_state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>get_state</span><span class=p>()</span>  <span class=c1># we have to get states since it is the only way we can </span>
<span class=linenos data-linenos="248 "></span>        <span class=c1># gather modules (other methods of `Optimizer are private`)</span>
<span class=linenos data-linenos="249 "></span>
<span class=linenos data-linenos="250 "></span>        <span class=n>optim_state_copy</span> <span class=o>=</span> <span class=n>copy</span><span class=o>.</span><span class=n>deepcopy</span><span class=p>(</span><span class=n>optim_state</span><span class=p>)</span>
<span class=linenos data-linenos="251 "></span>        <span class=n>optim_state</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>init_optim_state</span><span class=p>)</span>  <span class=c1># optim_state will be updated with current optimizer state</span>
<span class=linenos data-linenos="252 "></span>        <span class=c1># check if opimizer state has changed from last optimizer to the current one</span>
<span class=linenos data-linenos="253 "></span>        <span class=c1># if it has changed, find common modules and update common states</span>
<span class=linenos data-linenos="254 "></span>        <span class=k>for</span> <span class=n>component</span> <span class=ow>in</span> <span class=p>(</span> <span class=s1>&#39;modules&#39;</span><span class=p>,</span> <span class=s1>&#39;regularizers&#39;</span><span class=p>,):</span>
<span class=linenos data-linenos="255 "></span>            <span class=n>components_to_keep</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=nb>int</span><span class=p>]]</span> <span class=o>=</span> <span class=p>[]</span>  <span class=c1># we store here common Module between current Optimizer</span>
<span class=linenos data-linenos="256 "></span>            <span class=c1># and the ones in the `optim_state` tuple (common Module name, index in List)</span>
<span class=linenos data-linenos="257 "></span>
<span class=linenos data-linenos="258 "></span>            <span class=k>if</span> <span class=ow>not</span> <span class=n>init_optim_state</span><span class=p>[</span><span class=s1>&#39;states&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>component</span><span class=p>)</span> <span class=ow>or</span> <span class=ow>not</span> <span class=n>optim_state_copy</span><span class=p>[</span><span class=s1>&#39;states&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>component</span><span class=p>):</span>
<span class=linenos data-linenos="259 "></span>                <span class=k>continue</span>
<span class=linenos data-linenos="260 "></span>            <span class=bp>self</span><span class=o>.</span><span class=n>_collect_common_optimodules</span><span class=p>(</span>
<span class=linenos data-linenos="261 "></span>                <span class=n>init_optim_state</span><span class=p>,</span>
<span class=linenos data-linenos="262 "></span>                <span class=n>optim_state_copy</span><span class=p>,</span>
<span class=linenos data-linenos="263 "></span>                <span class=n>component</span><span class=p>,</span>
<span class=linenos data-linenos="264 "></span>                <span class=n>components_to_keep</span>
<span class=linenos data-linenos="265 "></span>            <span class=p>)</span>
<span class=linenos data-linenos="266 "></span>
<span class=linenos data-linenos="267 "></span>            <span class=k>for</span> <span class=n>mod</span> <span class=ow>in</span> <span class=n>components_to_keep</span><span class=p>:</span>
<span class=linenos data-linenos="268 "></span>                <span class=k>for</span> <span class=n>mod_state</span> <span class=ow>in</span> <span class=n>optim_state_copy</span><span class=p>[</span><span class=s1>&#39;states&#39;</span><span class=p>][</span><span class=n>component</span><span class=p>]:</span>
<span class=linenos data-linenos="269 "></span>                    <span class=k>if</span> <span class=n>mod</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>==</span> <span class=n>mod_state</span><span class=p>[</span><span class=mi>0</span><span class=p>]:</span>
<span class=linenos data-linenos="270 "></span>                        <span class=c1># if we do find same module in the current optimizer than the previous one,</span>
<span class=linenos data-linenos="271 "></span>                        <span class=c1># we load the previous optimizer module state into the current one</span>
<span class=linenos data-linenos="272 "></span>                        <span class=n>optim_state</span><span class=p>[</span><span class=s1>&#39;states&#39;</span><span class=p>][</span><span class=n>component</span><span class=p>][</span><span class=n>mod</span><span class=p>[</span><span class=mi>1</span><span class=p>]]</span> <span class=o>=</span> <span class=n>mod_state</span>
<span class=linenos data-linenos="273 "></span>
<span class=linenos data-linenos="274 "></span>        <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&quot;Loading optimizer state from saved state&quot;</span><span class=p>)</span>
<span class=linenos data-linenos="275 "></span>
<span class=linenos data-linenos="276 "></span>    <span class=n>reloaded_optim</span> <span class=o>=</span> <span class=n>FedOptimizer</span><span class=o>.</span><span class=n>load_state</span><span class=p>(</span><span class=n>optim_state</span><span class=p>)</span>
<span class=linenos data-linenos="277 "></span>    <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span> <span class=o>=</span> <span class=n>reloaded_optim</span>
<span class=linenos data-linenos="278 "></span>    <span class=k>return</span> <span class=bp>self</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.DeclearnOptimizer.optimizer_processing class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">optimizer_processing</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>optimizer_processing</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <p>Provides a context manager able to do some actions before and after setting up an Optimizer, mainly disabling scikit-learn internal optimizer.</p> <p>Also, checks if <code>model_args</code> dictionary contains training parameters that won't be used or have any effect on the training, because of disabling the scikit-learn optimizer ( such as initial learning rate, learnig rate scheduler, ...). If disabling the internal optimizer leads to such changes, displays a warning.</p> <p><strong>Returns:</strong></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>SklearnOptimizerProcessing</code></td> <td> <code><a class="autorefs autorefs-internal" title="fedbiomed.common.optimizers.generic_optimizers.SklearnOptimizerProcessing" href="#fedbiomed.common.optimizers.SklearnOptimizerProcessing">SklearnOptimizerProcessing</a></code> </td> <td> <div class=doc-md-description> <p>context manager providing extra logic</p> </div> </td> </tr> </tbody> </table> <p>Usage: <div class=highlight><pre><span></span><code>    <span class=o>&gt;&gt;&gt;</span> <span class=n>dlo</span> <span class=o>=</span> <span class=n>DeclearnSklearnOptimizer</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
    <span class=o>&gt;&gt;&gt;</span> <span class=k>with</span> <span class=n>dlo</span><span class=o>.</span><span class=n>optimizer_processing</span><span class=p>():</span>
            <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span><span class=n>targets</span><span class=p>)</span>
</code></pre></div></p> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="330 "></span><span class=k>def</span> <span class=nf>optimizer_processing</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>SklearnOptimizerProcessing</span><span class=p>:</span>
<span class=linenos data-linenos="331 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Provides a context manager able to do some actions before and after setting up an Optimizer, mainly</span>
<span class=linenos data-linenos="332 "></span><span class=sd>    disabling scikit-learn internal optimizer.</span>
<span class=linenos data-linenos="333 "></span>
<span class=linenos data-linenos="334 "></span><span class=sd>    Also, checks if `model_args` dictionary contains training parameters that</span>
<span class=linenos data-linenos="335 "></span><span class=sd>    won&#39;t be used or have any effect on the training, because of disabling the scikit-learn optimizer (</span>
<span class=linenos data-linenos="336 "></span><span class=sd>    such as initial learning rate, learnig rate scheduler, ...). If disabling the internal optimizer leads</span>
<span class=linenos data-linenos="337 "></span><span class=sd>    to such changes, displays a warning.</span>
<span class=linenos data-linenos="338 "></span>
<span class=linenos data-linenos="339 "></span><span class=sd>    Returns:</span>
<span class=linenos data-linenos="340 "></span><span class=sd>        SklearnOptimizerProcessing: context manager providing extra logic</span>
<span class=linenos data-linenos="341 "></span>
<span class=linenos data-linenos="342 "></span><span class=sd>    Usage:</span>
<span class=linenos data-linenos="343 "></span><span class=sd>    ```python</span>
<span class=linenos data-linenos="344 "></span><span class=sd>        &gt;&gt;&gt; dlo = DeclearnSklearnOptimizer(model, optimizer)</span>
<span class=linenos data-linenos="345 "></span><span class=sd>        &gt;&gt;&gt; with dlo.optimizer_processing():</span>
<span class=linenos data-linenos="346 "></span><span class=sd>                model.train(inputs,targets)</span>
<span class=linenos data-linenos="347 "></span><span class=sd>    ```</span>
<span class=linenos data-linenos="348 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="349 "></span>    <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=p>,</span> <span class=n>SkLearnModel</span><span class=p>):</span>
<span class=linenos data-linenos="350 "></span>        <span class=k>return</span> <span class=n>SklearnOptimizerProcessing</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=p>,</span> <span class=n>disable_internal_optimizer</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=linenos data-linenos="351 "></span>    <span class=k>else</span><span class=p>:</span>
<span class=linenos data-linenos="352 "></span>        <span class=k>raise</span> <span class=n>FedbiomedOptimizerError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>ErrorNumbers</span><span class=o>.</span><span class=n>FB626</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2>: Method optimizer_processing should be used &quot;</span>
<span class=linenos data-linenos="353 "></span>                                      <span class=sa>f</span><span class=s2>&quot;only with SkLearnModel, but model is </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.DeclearnOptimizer.save_state class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">save_state</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>save_state</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <p>Gets optimizer state.</p> <p><strong>Returns:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a></code> </td> <td> <div class=doc-md-description> <p>optimizer state</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="306 "></span><span class=k>def</span> <span class=nf>save_state</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Dict</span><span class=p>:</span>
<span class=linenos data-linenos="307 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Gets optimizer state.</span>
<span class=linenos data-linenos="308 "></span>
<span class=linenos data-linenos="309 "></span><span class=sd>    Returns:</span>
<span class=linenos data-linenos="310 "></span><span class=sd>        optimizer state</span>
<span class=linenos data-linenos="311 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="312 "></span>    <span class=n>optim_state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>get_state</span><span class=p>()</span>
<span class=linenos data-linenos="313 "></span>    <span class=k>return</span> <span class=n>optim_state</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.DeclearnOptimizer.set_aux class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">set_aux</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>set_aux</span><span class=p>(</span><span class=n>aux</span><span class=p>)</span>
</code></pre></div> <div class="doc doc-contents "> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="172 "></span><span class=k>def</span> <span class=nf>set_aux</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>aux</span><span class=p>:</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]):</span>
<span class=linenos data-linenos="173 "></span>    <span class=c1># FIXME: for imported tensors in PyTorch sent as auxiliary variables,</span>
<span class=linenos data-linenos="174 "></span>    <span class=c1># we should push it on the appropriate device (ie cpu/gpu)</span>
<span class=linenos data-linenos="175 "></span>    <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>set_aux</span><span class=p>(</span><span class=n>aux</span><span class=p>)</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.DeclearnOptimizer.step class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">step</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>step</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <p>Performs one optimization step</p> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="159 "></span><span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
<span class=linenos data-linenos="160 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Performs one optimization step&quot;&quot;&quot;</span>
<span class=linenos data-linenos="161 "></span>    <span class=c1># NOTA: for sklearn, gradients retrieved are unscaled because we are using learning rate equal to 1.</span>
<span class=linenos data-linenos="162 "></span>    <span class=c1># Therefore, it is necessary to disable the sklearn internal optimizer beforehand</span>
<span class=linenos data-linenos="163 "></span>    <span class=c1># otherwise, computation will be incorrect</span>
<span class=linenos data-linenos="164 "></span>    <span class=n>grad</span> <span class=o>=</span> <span class=n>declearn</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>api</span><span class=o>.</span><span class=n>Vector</span><span class=o>.</span><span class=n>build</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=o>.</span><span class=n>get_gradients</span><span class=p>())</span>
<span class=linenos data-linenos="165 "></span>    <span class=n>weights</span> <span class=o>=</span> <span class=n>declearn</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>api</span><span class=o>.</span><span class=n>Vector</span><span class=o>.</span><span class=n>build</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=o>.</span><span class=n>get_weights</span><span class=p>(</span>
<span class=linenos data-linenos="166 "></span>        <span class=n>only_trainable</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
<span class=linenos data-linenos="167 "></span>        <span class=n>exclude_buffers</span><span class=o>=</span><span class=kc>True</span>
<span class=linenos data-linenos="168 "></span>    <span class=p>))</span>
<span class=linenos data-linenos="169 "></span>    <span class=n>updates</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>grad</span><span class=p>,</span> <span class=n>weights</span><span class=p>)</span>
<span class=linenos data-linenos="170 "></span>    <span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=o>.</span><span class=n>apply_updates</span><span class=p>(</span><span class=n>updates</span><span class=o>.</span><span class=n>coefs</span><span class=p>)</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.DeclearnOptimizer.zero_grad class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">zero_grad</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>zero_grad</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <p>Zeroes gradients of the Pytorch model. Basically calls the <code>zero_grad</code> method of the model.</p> <p><strong>Raises:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><a class="autorefs autorefs-internal" title="fedbiomed.common.exceptions.FedbiomedOptimizerError" href="../exceptions/#fedbiomed.common.exceptions.FedbiomedOptimizerError">FedbiomedOptimizerError</a></code> </td> <td> <div class=doc-md-description> <p>triggered if model has no method called <code>zero_grad</code></p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="317 "></span><span class=k>def</span> <span class=nf>zero_grad</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
<span class=linenos data-linenos="318 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Zeroes gradients of the Pytorch model. Basically calls the `zero_grad`</span>
<span class=linenos data-linenos="319 "></span><span class=sd>    method of the model.</span>
<span class=linenos data-linenos="320 "></span>
<span class=linenos data-linenos="321 "></span><span class=sd>    Raises:</span>
<span class=linenos data-linenos="322 "></span><span class=sd>        FedbiomedOptimizerError: triggered if model has no method called `zero_grad`</span>
<span class=linenos data-linenos="323 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="324 "></span>    <span class=c1># warning: specific for pytorch</span>
<span class=linenos data-linenos="325 "></span>    <span class=k>if</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=p>,</span> <span class=n>TorchModel</span><span class=p>):</span>
<span class=linenos data-linenos="326 "></span>        <span class=k>raise</span> <span class=n>FedbiomedOptimizerError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>ErrorNumbers</span><span class=o>.</span><span class=n>FB626</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2>. This method can only be used for TorchModel, &quot;</span>
<span class=linenos data-linenos="327 "></span>                                      <span class=sa>f</span><span class=s2>&quot;but got </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=linenos data-linenos="328 "></span>    <span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</code></pre></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h3 id=fedbiomed.common.optimizers.NativeSkLearnOptimizer class="doc doc-heading"> <span class="doc doc-object-name doc-class-name">NativeSkLearnOptimizer</span> </h3> <div class="doc-signature highlight"><pre><span></span><code><span class=n>NativeSkLearnOptimizer</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</code></pre></div> <div class="doc doc-contents "> <p class="doc doc-class-bases"> Bases: <code><a class="autorefs autorefs-internal" title="fedbiomed.common.optimizers.generic_optimizers.BaseOptimizer" href="#fedbiomed.common.optimizers.BaseOptimizer">BaseOptimizer</a></code></p> <p>Optimizer wrapper for scikit-learn native models.</p> <p><strong>Parameters:</strong></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>model</code></td> <td> <code><a class="autorefs autorefs-internal" title="fedbiomed.common.models.SkLearnModel" href="../models/#fedbiomed.common.models.SkLearnModel">SkLearnModel</a></code> </td> <td> <div class=doc-md-description> <p>SkLearnModel model that builds a scikit-learn model.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr> <td><code>optimizer</code></td> <td> <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[None]</code> </td> <td> <div class=doc-md-description> <p>unused. Defaults to None.</p> </div> </td> <td> <code>None</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="426 "></span><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>:</span> <span class=n>SkLearnModel</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=kc>None</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>):</span>
<span class=linenos data-linenos="427 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Constructor of the Optimizer wrapper for scikit-learn native models.</span>
<span class=linenos data-linenos="428 "></span>
<span class=linenos data-linenos="429 "></span><span class=sd>    Args:</span>
<span class=linenos data-linenos="430 "></span><span class=sd>        model: SkLearnModel model that builds a scikit-learn model.</span>
<span class=linenos data-linenos="431 "></span><span class=sd>        optimizer: unused. Defaults to None.</span>
<span class=linenos data-linenos="432 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="433 "></span>
<span class=linenos data-linenos="434 "></span>    <span class=k>if</span> <span class=n>optimizer</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
<span class=linenos data-linenos="435 "></span>        <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Passed Optimizer </span><span class=si>{</span><span class=n>optimizer</span><span class=si>}</span><span class=s2> won&#39;t be used (using only native scikit learn optimization)&quot;</span><span class=p>)</span>
<span class=linenos data-linenos="436 "></span>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
<span class=linenos data-linenos="437 "></span>    <span class=n>logger</span><span class=o>.</span><span class=n>debug</span><span class=p>(</span><span class=s2>&quot;Using native Sklearn Optimizer&quot;</span><span class=p>)</span>
</code></pre></div> </details> <div class="doc doc-children"> <h4 id=fedbiomed.common.optimizers.NativeSkLearnOptimizer-functions>Functions</h4> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.NativeSkLearnOptimizer.optimizer_processing class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">optimizer_processing</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>optimizer_processing</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="445 "></span><span class=k>def</span> <span class=nf>optimizer_processing</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>SklearnOptimizerProcessing</span><span class=p>:</span>
<span class=linenos data-linenos="446 "></span>    <span class=k>return</span> <span class=n>SklearnOptimizerProcessing</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=p>,</span> <span class=n>disable_internal_optimizer</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.NativeSkLearnOptimizer.step class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">step</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>step</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <p>Performs an optimization step and updates model weights.</p> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="439 "></span><span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
<span class=linenos data-linenos="440 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Performs an optimization step and updates model weights.&quot;&quot;&quot;</span>
<span class=linenos data-linenos="441 "></span>    <span class=n>gradients</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=o>.</span><span class=n>get_gradients</span><span class=p>()</span>
<span class=linenos data-linenos="442 "></span>    <span class=n>updates</span> <span class=o>=</span> <span class=p>{</span><span class=n>k</span><span class=p>:</span> <span class=o>-</span><span class=n>v</span> <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>gradients</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
<span class=linenos data-linenos="443 "></span>    <span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=o>.</span><span class=n>apply_updates</span><span class=p>(</span><span class=n>updates</span><span class=p>)</span>
</code></pre></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h3 id=fedbiomed.common.optimizers.NativeTorchOptimizer class="doc doc-heading"> <span class="doc doc-object-name doc-class-name">NativeTorchOptimizer</span> </h3> <div class="doc-signature highlight"><pre><span></span><code><span class=n>NativeTorchOptimizer</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
</code></pre></div> <div class="doc doc-contents "> <p class="doc doc-class-bases"> Bases: <code><a class="autorefs autorefs-internal" title="fedbiomed.common.optimizers.generic_optimizers.BaseOptimizer" href="#fedbiomed.common.optimizers.BaseOptimizer">BaseOptimizer</a></code></p> <p>Optimizer wrapper for pytorch native optimizers and models.</p> <p><strong>Parameters:</strong></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>model</code></td> <td> <code><a class="autorefs autorefs-internal" title="fedbiomed.common.models.TorchModel" href="../models/#fedbiomed.common.models.TorchModel">TorchModel</a></code> </td> <td> <div class=doc-md-description> <p>fedbiomed model wrapper that warps the pytorch model</p> </div> </td> <td> <em>required</em> </td> </tr> <tr> <td><code>optimizer</code></td> <td> <code><a class="autorefs autorefs-external" title="torch.optim.Optimizer" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer">Optimizer</a></code> </td> <td> <div class=doc-md-description> <p>pytorch native optimizers (inhereting from <code>torch.optim.Optimizer</code>)</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <p><strong>Raises:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><a class="autorefs autorefs-internal" title="fedbiomed.common.exceptions.FedbiomedOptimizerError" href="../exceptions/#fedbiomed.common.exceptions.FedbiomedOptimizerError">FedbiomedOptimizerError</a></code> </td> <td> <div class=doc-md-description> <p>raised if optimizer is not a pytorch native optimizer ie a <code>torch.optim.Optimizer</code> object.</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="361 "></span><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>:</span> <span class=n>TorchModel</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Optimizer</span><span class=p>):</span>
<span class=linenos data-linenos="362 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Constructor of the optimizer wrapper</span>
<span class=linenos data-linenos="363 "></span>
<span class=linenos data-linenos="364 "></span><span class=sd>    Args:</span>
<span class=linenos data-linenos="365 "></span><span class=sd>        model: fedbiomed model wrapper that warps the pytorch model</span>
<span class=linenos data-linenos="366 "></span><span class=sd>        optimizer: pytorch native optimizers (inhereting from `torch.optim.Optimizer`)</span>
<span class=linenos data-linenos="367 "></span>
<span class=linenos data-linenos="368 "></span><span class=sd>    Raises:</span>
<span class=linenos data-linenos="369 "></span><span class=sd>        FedbiomedOptimizerError: raised if optimizer is not a pytorch native optimizer ie a `torch.optim.Optimizer`</span>
<span class=linenos data-linenos="370 "></span><span class=sd>            object.</span>
<span class=linenos data-linenos="371 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="372 "></span>    <span class=k>if</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Optimizer</span><span class=p>):</span>
<span class=linenos data-linenos="373 "></span>        <span class=k>raise</span> <span class=n>FedbiomedOptimizerError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>ErrorNumbers</span><span class=o>.</span><span class=n>FB626</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2> Expected a native pytorch `torch.optim` &quot;</span>
<span class=linenos data-linenos="374 "></span>                                      <span class=sa>f</span><span class=s2>&quot;optimizer, but got </span><span class=si>{</span><span class=nb>type</span><span class=p>(</span><span class=n>optimizer</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=linenos data-linenos="375 "></span>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
<span class=linenos data-linenos="376 "></span>    <span class=n>logger</span><span class=o>.</span><span class=n>debug</span><span class=p>(</span><span class=s2>&quot;using native torch optimizer&quot;</span><span class=p>)</span>
</code></pre></div> </details> <div class="doc doc-children"> <h4 id=fedbiomed.common.optimizers.NativeTorchOptimizer-functions>Functions</h4> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.NativeTorchOptimizer.get_learning_rate class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">get_learning_rate</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>get_learning_rate</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <p>Gets learning rates from param groups in Pytorch optimizer.</p> <p>For each optimizer param group, it iterates over all parameters in that parameter group and searches for the " corresponding parameter of the model by iterating over all model parameters. If it finds a correspondence, it saves the learning rate value. This function assumes that the parameters in the optimizer and the model have the same reference.</p> <div class="admonition warning"> <p class=admonition-title>Warning</p> <p>This function gathers the base learning rate applied to the model weights, including alterations due to any LR scheduler. However, it does not catch any adaptive component, e.g. due to RMSProp, Adam or such.</p> </div> <p><strong>Returns:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a>]</code> </td> <td> <div class=doc-md-description> <p>List[float]: list of single learning rate or multiple learning rates (as many as the number of the layers contained in the model)</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="389 "></span><span class=k>def</span> <span class=nf>get_learning_rate</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=nb>float</span><span class=p>]:</span>
<span class=linenos data-linenos="390 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Gets learning rates from param groups in Pytorch optimizer.</span>
<span class=linenos data-linenos="391 "></span>
<span class=linenos data-linenos="392 "></span><span class=sd>    For each optimizer param group, it iterates over all parameters in that parameter group and searches for the &quot;</span>
<span class=linenos data-linenos="393 "></span><span class=sd>    corresponding parameter of the model by iterating over all model parameters. If it finds a correspondence,</span>
<span class=linenos data-linenos="394 "></span><span class=sd>    it saves the learning rate value. This function assumes that the parameters in the optimizer and the model</span>
<span class=linenos data-linenos="395 "></span><span class=sd>    have the same reference.</span>
<span class=linenos data-linenos="396 "></span>
<span class=linenos data-linenos="397 "></span>
<span class=linenos data-linenos="398 "></span><span class=sd>    !!! warning</span>
<span class=linenos data-linenos="399 "></span><span class=sd>        This function gathers the base learning rate applied to the model weights,</span>
<span class=linenos data-linenos="400 "></span><span class=sd>        including alterations due to any LR scheduler. However, it does not catch</span>
<span class=linenos data-linenos="401 "></span><span class=sd>        any adaptive component, e.g. due to RMSProp, Adam or such.</span>
<span class=linenos data-linenos="402 "></span>
<span class=linenos data-linenos="403 "></span><span class=sd>    Returns:</span>
<span class=linenos data-linenos="404 "></span><span class=sd>        List[float]: list of single learning rate or multiple learning rates</span>
<span class=linenos data-linenos="405 "></span><span class=sd>            (as many as the number of the layers contained in the model)</span>
<span class=linenos data-linenos="406 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="407 "></span>    <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span>
<span class=linenos data-linenos="408 "></span>        <span class=s2>&quot;`get_learning_rate` is deprecated and will be removed in future Fed-BioMed releases&quot;</span><span class=p>,</span>
<span class=linenos data-linenos="409 "></span>        <span class=n>broadcast</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=linenos data-linenos="410 "></span>
<span class=linenos data-linenos="411 "></span>    <span class=n>mapping_lr_layer_name</span><span class=p>:</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=nb>float</span><span class=p>]</span> <span class=o>=</span> <span class=p>{}</span>
<span class=linenos data-linenos="412 "></span>
<span class=linenos data-linenos="413 "></span>    <span class=k>for</span> <span class=n>param_group</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>param_groups</span><span class=p>:</span>
<span class=linenos data-linenos="414 "></span>        <span class=k>for</span> <span class=n>layer_params</span> <span class=ow>in</span> <span class=n>param_group</span><span class=p>[</span><span class=s1>&#39;params&#39;</span><span class=p>]:</span>
<span class=linenos data-linenos="415 "></span>            <span class=k>for</span> <span class=n>layer_name</span><span class=p>,</span> <span class=n>tensor</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>():</span>
<span class=linenos data-linenos="416 "></span>                <span class=k>if</span> <span class=n>layer_params</span> <span class=ow>is</span> <span class=n>tensor</span><span class=p>:</span>
<span class=linenos data-linenos="417 "></span>                    <span class=n>mapping_lr_layer_name</span><span class=p>[</span><span class=n>layer_name</span><span class=p>]</span> <span class=o>=</span> <span class=n>param_group</span><span class=p>[</span><span class=s1>&#39;lr&#39;</span><span class=p>]</span>
<span class=linenos data-linenos="418 "></span>    <span class=k>return</span> <span class=n>mapping_lr_layer_name</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.NativeTorchOptimizer.step class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">step</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>step</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <p>Performs an optimization step and updates model weights</p> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="378 "></span><span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
<span class=linenos data-linenos="379 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Performs an optimization step and updates model weights</span>
<span class=linenos data-linenos="380 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="381 "></span>    <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.NativeTorchOptimizer.zero_grad class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">zero_grad</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>zero_grad</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <p>Zeroes gradients of the Pytorch model. Basically calls the <code>zero_grad</code> method of the optimizer.</p> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="383 "></span><span class=k>def</span> <span class=nf>zero_grad</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
<span class=linenos data-linenos="384 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Zeroes gradients of the Pytorch model. Basically calls the `zero_grad`</span>
<span class=linenos data-linenos="385 "></span><span class=sd>    method of the optimizer.</span>
<span class=linenos data-linenos="386 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="387 "></span>    <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</code></pre></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h3 id=fedbiomed.common.optimizers.Optimizer class="doc doc-heading"> <span class="doc doc-object-name doc-class-name">Optimizer</span> </h3> <div class="doc-signature highlight"><pre><span></span><code><span class=n>Optimizer</span><span class=p>(</span><span class=n>lr</span><span class=p>,</span> <span class=n>decay</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span> <span class=n>modules</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>regularizers</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</code></pre></div> <div class="doc doc-contents "> <p>Optimizer class with a declearn-backed modular SGD-core algorithm.</p> <p><strong>Parameters:</strong></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>lr</code></td> <td> <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code> </td> <td> <div class=doc-md-description> <p>Base learning rate (i.e. step size) applied to gradients-based updates upon applying them to a model's weights.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr> <td><code>decay</code></td> <td> <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code> </td> <td> <div class=doc-md-description> <p>Optional weight decay parameter, used to parameterize a decoupled weight decay regularization term (see [1]) added to the updates right before the learning rate is applied and model weights are effectively updated.</p> </div> </td> <td> <code>0.0</code> </td> </tr> <tr> <td><code>modules</code></td> <td> <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="typing.Sequence" href="https://docs.python.org/3/library/typing.html#typing.Sequence">Sequence</a>[<a class="autorefs autorefs-external" title="typing.Union" href="https://docs.python.org/3/library/typing.html#typing.Union">Union</a>[<span title="declearn.optimizer.modules.OptiModule">OptiModule</span>, <a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="typing.Tuple" href="https://docs.python.org/3/library/typing.html#typing.Tuple">Tuple</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="typing.Any" href="https://docs.python.org/3/library/typing.html#typing.Any">Any</a>]]]]]</code> </td> <td> <div class=doc-md-description> <p>Optional list of plug-in modules implementing gradients' alteration into model weights' udpates. Modules will be applied to gradients following this list's ordering. See <code>declearn.optimizer.modules.OptiModule</code> for details. See Notes section below for details on the "specs" format.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr> <td><code>regularizers</code></td> <td> <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="typing.Sequence" href="https://docs.python.org/3/library/typing.html#typing.Sequence">Sequence</a>[<a class="autorefs autorefs-external" title="typing.Union" href="https://docs.python.org/3/library/typing.html#typing.Union">Union</a>[<span title="declearn.optimizer.regularizers.Regularizer">Regularizer</span>, <a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="typing.Tuple" href="https://docs.python.org/3/library/typing.html#typing.Tuple">Tuple</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="typing.Any" href="https://docs.python.org/3/library/typing.html#typing.Any">Any</a>]]]]]</code> </td> <td> <div class=doc-md-description> <p>Optional list of plug-in loss regularizers. Regularizers will be applied to gradients following this list's order, prior to any other alteration (see <code>modules</code> above). See <code>declearn.optimizer.regularizers.Regularizer</code> for details. See Notes section below for details on the "specs" format.</p> </div> </td> <td> <code>None</code> </td> </tr> </tbody> </table> <div class="admonition info"> <p class=admonition-title>Note</p> <p><code>Regularizer</code> and <code>OptiModule</code> to be used by this optimizer, specified using the <code>regularizers</code> and <code>modules</code> parameters, may be passed as ready-for-use instances, or be instantiated from specs, consisting either of a single string (the <code>name</code> attribute of the class to build) or a tuple grouping this name and a config dict (to specify some hyper-parameters).</p> </div> <div class="admonition info"> <p class=admonition-title>References</p> <p>[1] Loshchilov &amp; Hutter, 2019. Decoupled Weight Decay Regularization. <a href=https://arxiv.org/abs/1711.05101>https://arxiv.org/abs/1711.05101</a></p> </div> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/optimizer.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="21 "></span><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
<span class=linenos data-linenos="22 "></span>    <span class=bp>self</span><span class=p>,</span>
<span class=linenos data-linenos="23 "></span>    <span class=n>lr</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span>
<span class=linenos data-linenos="24 "></span>    <span class=n>decay</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.0</span><span class=p>,</span>
<span class=linenos data-linenos="25 "></span>    <span class=n>modules</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span>
<span class=linenos data-linenos="26 "></span>        <span class=n>Sequence</span><span class=p>[</span><span class=n>Union</span><span class=p>[</span><span class=n>OptiModule</span><span class=p>,</span> <span class=nb>str</span><span class=p>,</span> <span class=n>Tuple</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]]]]</span>
<span class=linenos data-linenos="27 "></span>    <span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
<span class=linenos data-linenos="28 "></span>    <span class=n>regularizers</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span>
<span class=linenos data-linenos="29 "></span>        <span class=n>Sequence</span><span class=p>[</span><span class=n>Union</span><span class=p>[</span><span class=n>Regularizer</span><span class=p>,</span> <span class=nb>str</span><span class=p>,</span> <span class=n>Tuple</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]]]]</span>
<span class=linenos data-linenos="30 "></span>    <span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
<span class=linenos data-linenos="31 "></span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
<span class=linenos data-linenos="32 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Instantiate the declearn-issued gradient-descent optimizer.</span>
<span class=linenos data-linenos="33 "></span>
<span class=linenos data-linenos="34 "></span><span class=sd>    Args:</span>
<span class=linenos data-linenos="35 "></span><span class=sd>        lr: Base learning rate (i.e. step size) applied to gradients-based</span>
<span class=linenos data-linenos="36 "></span><span class=sd>            updates upon applying them to a model&#39;s weights.</span>
<span class=linenos data-linenos="37 "></span><span class=sd>        decay: Optional weight decay parameter, used to parameterize a</span>
<span class=linenos data-linenos="38 "></span><span class=sd>            decoupled weight decay regularization term (see [1]) added to</span>
<span class=linenos data-linenos="39 "></span><span class=sd>            the updates right before the learning rate is applied and model</span>
<span class=linenos data-linenos="40 "></span><span class=sd>            weights are effectively updated.</span>
<span class=linenos data-linenos="41 "></span><span class=sd>        modules: Optional list of plug-in modules implementing gradients&#39;</span>
<span class=linenos data-linenos="42 "></span><span class=sd>            alteration into model weights&#39; udpates. Modules will be applied</span>
<span class=linenos data-linenos="43 "></span><span class=sd>            to gradients following this list&#39;s ordering.</span>
<span class=linenos data-linenos="44 "></span><span class=sd>            See `declearn.optimizer.modules.OptiModule` for details.</span>
<span class=linenos data-linenos="45 "></span><span class=sd>            See Notes section below for details on the &quot;specs&quot; format.</span>
<span class=linenos data-linenos="46 "></span><span class=sd>        regularizers: Optional list of plug-in loss regularizers.</span>
<span class=linenos data-linenos="47 "></span><span class=sd>            Regularizers will be applied to gradients following this list&#39;s</span>
<span class=linenos data-linenos="48 "></span><span class=sd>            order, prior to any other alteration (see `modules` above).</span>
<span class=linenos data-linenos="49 "></span><span class=sd>            See `declearn.optimizer.regularizers.Regularizer` for details.</span>
<span class=linenos data-linenos="50 "></span><span class=sd>            See Notes section below for details on the &quot;specs&quot; format.</span>
<span class=linenos data-linenos="51 "></span>
<span class=linenos data-linenos="52 "></span><span class=sd>    !!! info &quot;Note&quot;</span>
<span class=linenos data-linenos="53 "></span><span class=sd>        `Regularizer` and `OptiModule` to be used by this optimizer,</span>
<span class=linenos data-linenos="54 "></span><span class=sd>        specified using the `regularizers` and `modules` parameters,</span>
<span class=linenos data-linenos="55 "></span><span class=sd>        may be passed as ready-for-use instances, or be instantiated</span>
<span class=linenos data-linenos="56 "></span><span class=sd>        from specs, consisting either of a single string (the `name`</span>
<span class=linenos data-linenos="57 "></span><span class=sd>        attribute of the class to build) or a tuple grouping this</span>
<span class=linenos data-linenos="58 "></span><span class=sd>        name and a config dict (to specify some hyper-parameters).</span>
<span class=linenos data-linenos="59 "></span>
<span class=linenos data-linenos="60 "></span><span class=sd>    !!! info &quot;References&quot;</span>
<span class=linenos data-linenos="61 "></span><span class=sd>        [1] Loshchilov &amp; Hutter, 2019.</span>
<span class=linenos data-linenos="62 "></span><span class=sd>            Decoupled Weight Decay Regularization.</span>
<span class=linenos data-linenos="63 "></span><span class=sd>            https://arxiv.org/abs/1711.05101</span>
<span class=linenos data-linenos="64 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="65 "></span>    <span class=k>try</span><span class=p>:</span>
<span class=linenos data-linenos="66 "></span>        <span class=bp>self</span><span class=o>.</span><span class=n>_optimizer</span> <span class=o>=</span> <span class=n>DeclearnOptimizer</span><span class=p>(</span>
<span class=linenos data-linenos="67 "></span>            <span class=n>lrate</span><span class=o>=</span><span class=n>lr</span><span class=p>,</span>
<span class=linenos data-linenos="68 "></span>            <span class=n>w_decay</span><span class=o>=</span><span class=n>decay</span><span class=p>,</span>
<span class=linenos data-linenos="69 "></span>            <span class=n>modules</span><span class=o>=</span><span class=n>modules</span><span class=p>,</span>
<span class=linenos data-linenos="70 "></span>            <span class=n>regularizers</span><span class=o>=</span><span class=n>regularizers</span><span class=p>,</span>
<span class=linenos data-linenos="71 "></span>        <span class=p>)</span>
<span class=linenos data-linenos="72 "></span>    <span class=k>except</span> <span class=p>(</span><span class=ne>KeyError</span><span class=p>,</span> <span class=ne>TypeError</span><span class=p>)</span> <span class=k>as</span> <span class=n>exc</span><span class=p>:</span>
<span class=linenos data-linenos="73 "></span>        <span class=k>raise</span> <span class=n>FedbiomedOptimizerError</span><span class=p>(</span>
<span class=linenos data-linenos="74 "></span>            <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>ErrorNumbers</span><span class=o>.</span><span class=n>FB621</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2>: declearn Optimizer instantiation&quot;</span>
<span class=linenos data-linenos="75 "></span>            <span class=sa>f</span><span class=s2>&quot; raised the following exception: </span><span class=si>{</span><span class=nb>repr</span><span class=p>(</span><span class=n>exc</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span>
<span class=linenos data-linenos="76 "></span>        <span class=p>)</span> <span class=kn>from</span> <span class=nn>exc</span>
</code></pre></div> </details> <div class="doc doc-children"> <h4 id=fedbiomed.common.optimizers.Optimizer-functions>Functions</h4> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.Optimizer.from_declearn_optimizer class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">from_declearn_optimizer</span> <span class="doc doc-labels"> <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small> </span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>from_declearn_optimizer</span><span class=p>(</span><span class=n>declearn_optimizer</span><span class=p>)</span>
</code></pre></div> <div class="doc doc-contents "> <p>Wrap a declearn Optimizer into a fed-biomed one.</p> <p><strong>Parameters:</strong></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>declearn_optimizer</code></td> <td> <code><span title="declearn.optimizer.Optimizer">Optimizer</span></code> </td> <td> <div class=doc-md-description> <p>[declearn.optimizer.Optimizer][] instance that needs to be wrapped.</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <p><strong>Returns:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><span title="typing_extensions.Self">Self</span></code> </td> <td> <div class=doc-md-description> <p>Fed-BioMed <code>Optimizer</code> instance wrapping a copy of the input declearn optimizer.</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/optimizer.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos=" 78 "></span><span class=nd>@classmethod</span>
<span class=linenos data-linenos=" 79 "></span><span class=k>def</span> <span class=nf>from_declearn_optimizer</span><span class=p>(</span>
<span class=linenos data-linenos=" 80 "></span>    <span class=bp>cls</span><span class=p>,</span>
<span class=linenos data-linenos=" 81 "></span>    <span class=n>declearn_optimizer</span><span class=p>:</span> <span class=n>DeclearnOptimizer</span><span class=p>,</span>
<span class=linenos data-linenos=" 82 "></span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Self</span><span class=p>:</span>
<span class=linenos data-linenos=" 83 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Wrap a declearn Optimizer into a fed-biomed one.</span>
<span class=linenos data-linenos=" 84 "></span>
<span class=linenos data-linenos=" 85 "></span><span class=sd>    Args:</span>
<span class=linenos data-linenos=" 86 "></span><span class=sd>        declearn_optimizer: [declearn.optimizer.Optimizer][] instance that</span>
<span class=linenos data-linenos=" 87 "></span><span class=sd>            needs to be wrapped.</span>
<span class=linenos data-linenos=" 88 "></span>
<span class=linenos data-linenos=" 89 "></span><span class=sd>    Returns:</span>
<span class=linenos data-linenos=" 90 "></span><span class=sd>        Fed-BioMed `Optimizer` instance wrapping a copy of the input</span>
<span class=linenos data-linenos=" 91 "></span><span class=sd>            declearn optimizer.</span>
<span class=linenos data-linenos=" 92 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos=" 93 "></span>    <span class=n>config</span> <span class=o>=</span> <span class=n>declearn_optimizer</span><span class=o>.</span><span class=n>get_config</span><span class=p>()</span>
<span class=linenos data-linenos=" 94 "></span>    <span class=n>optim</span> <span class=o>=</span> <span class=bp>cls</span><span class=p>(</span>
<span class=linenos data-linenos=" 95 "></span>        <span class=n>lr</span><span class=o>=</span><span class=n>config</span><span class=p>[</span><span class=s2>&quot;lrate&quot;</span><span class=p>],</span>
<span class=linenos data-linenos=" 96 "></span>        <span class=n>decay</span><span class=o>=</span><span class=n>config</span><span class=p>[</span><span class=s2>&quot;w_decay&quot;</span><span class=p>],</span>
<span class=linenos data-linenos=" 97 "></span>        <span class=n>modules</span><span class=o>=</span><span class=n>config</span><span class=p>[</span><span class=s2>&quot;modules&quot;</span><span class=p>],</span>
<span class=linenos data-linenos=" 98 "></span>        <span class=n>regularizers</span><span class=o>=</span><span class=n>config</span><span class=p>[</span><span class=s2>&quot;regularizers&quot;</span><span class=p>],</span>
<span class=linenos data-linenos=" 99 "></span>    <span class=p>)</span>
<span class=linenos data-linenos="100 "></span>    <span class=n>optim</span><span class=o>.</span><span class=n>_optimizer</span><span class=o>.</span><span class=n>set_state</span><span class=p>(</span><span class=n>declearn_optimizer</span><span class=o>.</span><span class=n>get_state</span><span class=p>())</span>
<span class=linenos data-linenos="101 "></span>    <span class=k>return</span> <span class=n>optim</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.Optimizer.get_aux class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">get_aux</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>get_aux</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <p>Return auxiliary variables that need to be shared across network.</p> <p><strong>Returns:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="typing.Union" href="https://docs.python.org/3/library/typing.html#typing.Union">Union</a>[<a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="typing.Any" href="https://docs.python.org/3/library/typing.html#typing.Any">Any</a>], <a class="autorefs autorefs-external" title="typing.Any" href="https://docs.python.org/3/library/typing.html#typing.Any">Any</a>]]</code> </td> <td> <div class=doc-md-description> <p>Aux-var dict that associates <code>module.collect_aux_var()</code> values to <code>module.name</code> keys for each and every module plugged in this Optimizer that has some auxiliary variables to share.</p> </div> </td> </tr> </tbody> </table> <div class="admonition info"> <p class=admonition-title>Note</p> <p>"Auxiliary variables" are information that needs to be shared between the nodes and the researcher between training rounds, to synchronize some optimizer plug-ins that work by pair. Their production via this method can have internal side effects; <code>get_aux</code> should therefore be called sparingly.</p> </div> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/optimizer.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="158 "></span><span class=k>def</span> <span class=nf>get_aux</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Union</span><span class=p>[</span><span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>],</span> <span class=n>Any</span><span class=p>]]:</span>
<span class=linenos data-linenos="159 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Return auxiliary variables that need to be shared across network.</span>
<span class=linenos data-linenos="160 "></span>
<span class=linenos data-linenos="161 "></span><span class=sd>    Returns:</span>
<span class=linenos data-linenos="162 "></span><span class=sd>        Aux-var dict that associates `module.collect_aux_var()` values to</span>
<span class=linenos data-linenos="163 "></span><span class=sd>            `module.name` keys for each and every module plugged in this</span>
<span class=linenos data-linenos="164 "></span><span class=sd>            Optimizer that has some auxiliary variables to share.</span>
<span class=linenos data-linenos="165 "></span>
<span class=linenos data-linenos="166 "></span><span class=sd>    !!! info &quot;Note&quot;</span>
<span class=linenos data-linenos="167 "></span><span class=sd>        &quot;Auxiliary variables&quot; are information that needs to be shared</span>
<span class=linenos data-linenos="168 "></span><span class=sd>        between the nodes and the researcher between training rounds, to</span>
<span class=linenos data-linenos="169 "></span><span class=sd>        synchronize some optimizer plug-ins that work by pair. Their</span>
<span class=linenos data-linenos="170 "></span><span class=sd>        production via this method can have internal side effects;</span>
<span class=linenos data-linenos="171 "></span><span class=sd>        `get_aux` should therefore be called sparingly.</span>
<span class=linenos data-linenos="172 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="173 "></span>    <span class=k>try</span><span class=p>:</span>
<span class=linenos data-linenos="174 "></span>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_optimizer</span><span class=o>.</span><span class=n>collect_aux_var</span><span class=p>()</span>
<span class=linenos data-linenos="175 "></span>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>exc</span><span class=p>:</span>
<span class=linenos data-linenos="176 "></span>        <span class=k>raise</span> <span class=n>FedbiomedOptimizerError</span><span class=p>(</span>
<span class=linenos data-linenos="177 "></span>            <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>ErrorNumbers</span><span class=o>.</span><span class=n>FB621</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2>: error in &#39;get_aux&#39;: </span><span class=si>{</span><span class=n>exc</span><span class=si>}</span><span class=s2>&quot;</span>
<span class=linenos data-linenos="178 "></span>        <span class=p>)</span> <span class=kn>from</span> <span class=nn>exc</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.Optimizer.get_state class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">get_state</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>get_state</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <p>Return the configuration and current states of this Optimizer.</p> <p>This method is to be used for creating breakpoints.</p> <p><strong>Returns:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="typing.Any" href="https://docs.python.org/3/library/typing.html#typing.Any">Any</a>]</code> </td> <td> <div class=doc-md-description> <p>State-and-config dict that may be saved as part of a breakpoint file, and used to re-create this Optimizer using the <code>Optimizer.load_state</code> classmethod constructor.</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/optimizer.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="209 "></span><span class=k>def</span> <span class=nf>get_state</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]:</span>
<span class=linenos data-linenos="210 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Return the configuration and current states of this Optimizer.</span>
<span class=linenos data-linenos="211 "></span>
<span class=linenos data-linenos="212 "></span><span class=sd>    This method is to be used for creating breakpoints.</span>
<span class=linenos data-linenos="213 "></span>
<span class=linenos data-linenos="214 "></span><span class=sd>    Returns:</span>
<span class=linenos data-linenos="215 "></span><span class=sd>        State-and-config dict that may be saved as part of a breakpoint</span>
<span class=linenos data-linenos="216 "></span><span class=sd>            file, and used to re-create this Optimizer using the</span>
<span class=linenos data-linenos="217 "></span><span class=sd>            `Optimizer.load_state` classmethod constructor.</span>
<span class=linenos data-linenos="218 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="219 "></span>    <span class=k>try</span><span class=p>:</span>
<span class=linenos data-linenos="220 "></span>        <span class=n>config</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_optimizer</span><span class=o>.</span><span class=n>get_config</span><span class=p>()</span>
<span class=linenos data-linenos="221 "></span>        <span class=n>states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_optimizer</span><span class=o>.</span><span class=n>get_state</span><span class=p>()</span>
<span class=linenos data-linenos="222 "></span>        <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;config&quot;</span><span class=p>:</span> <span class=n>config</span><span class=p>,</span> <span class=s2>&quot;states&quot;</span><span class=p>:</span> <span class=n>states</span><span class=p>}</span>
<span class=linenos data-linenos="223 "></span>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>exc</span><span class=p>:</span>
<span class=linenos data-linenos="224 "></span>        <span class=k>raise</span> <span class=n>FedbiomedOptimizerError</span><span class=p>(</span>
<span class=linenos data-linenos="225 "></span>            <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>ErrorNumbers</span><span class=o>.</span><span class=n>FB621</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2>: error in &#39;get_state&#39;: </span><span class=si>{</span><span class=n>exc</span><span class=si>}</span><span class=s2>&quot;</span>
<span class=linenos data-linenos="226 "></span>        <span class=p>)</span> <span class=kn>from</span> <span class=nn>exc</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.Optimizer.init_round class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">init_round</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>init_round</span><span class=p>()</span>
</code></pre></div> <div class="doc doc-contents "> <p>Trigger start-of-training-round behavior of wrapped regularizers.</p> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/optimizer.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="103 "></span><span class=k>def</span> <span class=nf>init_round</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
<span class=linenos data-linenos="104 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Trigger start-of-training-round behavior of wrapped regularizers.&quot;&quot;&quot;</span>
<span class=linenos data-linenos="105 "></span>    <span class=k>try</span><span class=p>:</span>
<span class=linenos data-linenos="106 "></span>        <span class=bp>self</span><span class=o>.</span><span class=n>_optimizer</span><span class=o>.</span><span class=n>start_round</span><span class=p>()</span>
<span class=linenos data-linenos="107 "></span>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>exc</span><span class=p>:</span>
<span class=linenos data-linenos="108 "></span>        <span class=k>raise</span> <span class=n>FedbiomedOptimizerError</span><span class=p>(</span>
<span class=linenos data-linenos="109 "></span>            <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>ErrorNumbers</span><span class=o>.</span><span class=n>FB621</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2>: error in &#39;init_round&#39;: </span><span class=si>{</span><span class=n>exc</span><span class=si>}</span><span class=s2>&quot;</span>
<span class=linenos data-linenos="110 "></span>        <span class=p>)</span> <span class=kn>from</span> <span class=nn>exc</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.Optimizer.load_state class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">load_state</span> <span class="doc doc-labels"> <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small> </span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>load_state</span><span class=p>(</span><span class=n>state</span><span class=p>)</span>
</code></pre></div> <div class="doc doc-contents "> <p>Instantiate an Optimizer from its breakpoint state dict.</p> <p><strong>Parameters:</strong></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>state</code></td> <td> <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="typing.Any" href="https://docs.python.org/3/library/typing.html#typing.Any">Any</a>]</code> </td> <td> <div class=doc-md-description> <p>state-and-config dict created using the <code>get_state</code> method.</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <p><strong>Returns:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><span title="typing_extensions.Self">Self</span></code> </td> <td> <div class=doc-md-description> <p>Optimizer instance re-created from the <code>state</code> dict.</p> </div> </td> </tr> </tbody> </table> <p><strong>Raises:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><a class="autorefs autorefs-internal" title="fedbiomed.common.exceptions.FedbiomedOptimizerError" href="../exceptions/#fedbiomed.common.exceptions.FedbiomedOptimizerError">FedbiomedOptimizerError</a></code> </td> <td> <div class=doc-md-description> <p>If the input <code>state</code> dict has improper keys or fails to set up a declearn Optimizer and set back its state.</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/optimizer.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="228 "></span><span class=nd>@classmethod</span>
<span class=linenos data-linenos="229 "></span><span class=k>def</span> <span class=nf>load_state</span><span class=p>(</span><span class=bp>cls</span><span class=p>,</span> <span class=n>state</span><span class=p>:</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=n>Self</span><span class=p>:</span>
<span class=linenos data-linenos="230 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Instantiate an Optimizer from its breakpoint state dict.</span>
<span class=linenos data-linenos="231 "></span>
<span class=linenos data-linenos="232 "></span><span class=sd>    Args:</span>
<span class=linenos data-linenos="233 "></span><span class=sd>        state: state-and-config dict created using the `get_state` method.</span>
<span class=linenos data-linenos="234 "></span>
<span class=linenos data-linenos="235 "></span><span class=sd>    Returns:</span>
<span class=linenos data-linenos="236 "></span><span class=sd>        Optimizer instance re-created from the `state` dict.</span>
<span class=linenos data-linenos="237 "></span>
<span class=linenos data-linenos="238 "></span><span class=sd>    Raises:</span>
<span class=linenos data-linenos="239 "></span><span class=sd>        FedbiomedOptimizerError: If the input `state` dict has improper keys</span>
<span class=linenos data-linenos="240 "></span><span class=sd>            or fails to set up a declearn Optimizer and set back its state.</span>
<span class=linenos data-linenos="241 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="242 "></span>    <span class=k>try</span><span class=p>:</span>
<span class=linenos data-linenos="243 "></span>        <span class=n>optim</span> <span class=o>=</span> <span class=n>DeclearnOptimizer</span><span class=o>.</span><span class=n>from_config</span><span class=p>(</span><span class=n>state</span><span class=p>[</span><span class=s2>&quot;config&quot;</span><span class=p>])</span>
<span class=linenos data-linenos="244 "></span>        <span class=n>optim</span><span class=o>.</span><span class=n>set_state</span><span class=p>(</span><span class=n>state</span><span class=p>[</span><span class=s2>&quot;states&quot;</span><span class=p>])</span>
<span class=linenos data-linenos="245 "></span>    <span class=k>except</span> <span class=ne>KeyError</span> <span class=k>as</span> <span class=n>exc</span><span class=p>:</span>
<span class=linenos data-linenos="246 "></span>        <span class=k>raise</span> <span class=n>FedbiomedOptimizerError</span><span class=p>(</span>
<span class=linenos data-linenos="247 "></span>            <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>ErrorNumbers</span><span class=o>.</span><span class=n>FB621</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2>: Missing field in the breakpoints state: </span><span class=si>{</span><span class=n>exc</span><span class=si>}</span><span class=s2>&quot;</span>
<span class=linenos data-linenos="248 "></span>        <span class=p>)</span> <span class=kn>from</span> <span class=nn>exc</span>
<span class=linenos data-linenos="249 "></span>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>exc</span><span class=p>:</span>
<span class=linenos data-linenos="250 "></span>        <span class=k>raise</span> <span class=n>FedbiomedOptimizerError</span><span class=p>(</span>
<span class=linenos data-linenos="251 "></span>            <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>ErrorNumbers</span><span class=o>.</span><span class=n>FB621</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2>: `Optimizer.load_state`: </span><span class=si>{</span><span class=n>exc</span><span class=si>}</span><span class=s2>&quot;</span>
<span class=linenos data-linenos="252 "></span>        <span class=p>)</span> <span class=kn>from</span> <span class=nn>exc</span>
<span class=linenos data-linenos="253 "></span>    <span class=k>return</span> <span class=bp>cls</span><span class=p>(</span>
<span class=linenos data-linenos="254 "></span>        <span class=n>lr</span><span class=o>=</span><span class=n>optim</span><span class=o>.</span><span class=n>lrate</span><span class=p>,</span>
<span class=linenos data-linenos="255 "></span>        <span class=n>decay</span><span class=o>=</span><span class=n>optim</span><span class=o>.</span><span class=n>w_decay</span><span class=p>,</span>
<span class=linenos data-linenos="256 "></span>        <span class=n>modules</span><span class=o>=</span><span class=n>optim</span><span class=o>.</span><span class=n>modules</span><span class=p>,</span>
<span class=linenos data-linenos="257 "></span>        <span class=n>regularizers</span><span class=o>=</span><span class=n>optim</span><span class=o>.</span><span class=n>regularizers</span><span class=p>,</span>
<span class=linenos data-linenos="258 "></span>    <span class=p>)</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.Optimizer.set_aux class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">set_aux</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>set_aux</span><span class=p>(</span><span class=n>aux</span><span class=p>)</span>
</code></pre></div> <div class="doc doc-contents "> <p>Update plug-in modules based on received shared auxiliary variables.</p> <p><strong>Parameters:</strong></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>aux</code></td> <td> <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="typing.Any" href="https://docs.python.org/3/library/typing.html#typing.Any">Any</a>]]</code> </td> <td> <div class=doc-md-description> <p>Auxiliary variables received from the counterpart optimizer (on the other side of the node-researcher frontier), that are to be a <code>{module.name: module.collect_aux_var()}</code> <em>or</em> a <code>{module.name: {node: module.collect_aux_var()}}</code> dict (depending on which side this optimizer is on).</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <p><strong>Raises:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><a class="autorefs autorefs-internal" title="fedbiomed.common.exceptions.FedbiomedOptimizerError" href="../exceptions/#fedbiomed.common.exceptions.FedbiomedOptimizerError">FedbiomedOptimizerError</a></code> </td> <td> <div class=doc-md-description> <p>If a key from <code>aux_var</code> does not match the name of any module plugged in this optimizer (i.e. if received variables cannot be mapped to a destinatory module).</p> </div> </td> </tr> </tbody> </table> <div class="admonition info"> <p class=admonition-title>Note</p> <p>"Auxiliary variables" are information that is shared between the nodes and researcher between training rounds, to synchronize some optimizer plug-ins that work by pair. The inputs to this method are not simply stored by the Optimizer, but are processed into internal side effects; this method should therefore be called sparingly.</p> </div> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/optimizer.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="180 "></span><span class=k>def</span> <span class=nf>set_aux</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>aux</span><span class=p>:</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]])</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
<span class=linenos data-linenos="181 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Update plug-in modules based on received shared auxiliary variables.</span>
<span class=linenos data-linenos="182 "></span>
<span class=linenos data-linenos="183 "></span><span class=sd>    Args:</span>
<span class=linenos data-linenos="184 "></span><span class=sd>        aux: Auxiliary variables received from the counterpart optimizer</span>
<span class=linenos data-linenos="185 "></span><span class=sd>            (on the other side of the node-researcher frontier), that are</span>
<span class=linenos data-linenos="186 "></span><span class=sd>            to be a `{module.name: module.collect_aux_var()}` *or* a</span>
<span class=linenos data-linenos="187 "></span><span class=sd>            `{module.name: {node: module.collect_aux_var()}}` dict</span>
<span class=linenos data-linenos="188 "></span><span class=sd>            (depending on which side this optimizer is on).</span>
<span class=linenos data-linenos="189 "></span>
<span class=linenos data-linenos="190 "></span><span class=sd>    Raises:</span>
<span class=linenos data-linenos="191 "></span><span class=sd>        FedbiomedOptimizerError: If a key from `aux_var` does not match the</span>
<span class=linenos data-linenos="192 "></span><span class=sd>            name of any module plugged in this optimizer (i.e. if received</span>
<span class=linenos data-linenos="193 "></span><span class=sd>            variables cannot be mapped to a destinatory module).</span>
<span class=linenos data-linenos="194 "></span>
<span class=linenos data-linenos="195 "></span><span class=sd>    !!! info &quot;Note&quot;</span>
<span class=linenos data-linenos="196 "></span><span class=sd>        &quot;Auxiliary variables&quot; are information that is shared between the</span>
<span class=linenos data-linenos="197 "></span><span class=sd>        nodes and researcher between training rounds, to synchronize some</span>
<span class=linenos data-linenos="198 "></span><span class=sd>        optimizer plug-ins that work by pair. The inputs to this method are</span>
<span class=linenos data-linenos="199 "></span><span class=sd>        not simply stored by the Optimizer, but are processed into internal</span>
<span class=linenos data-linenos="200 "></span><span class=sd>        side effects; this method should therefore be called sparingly.</span>
<span class=linenos data-linenos="201 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="202 "></span>    <span class=k>try</span><span class=p>:</span>
<span class=linenos data-linenos="203 "></span>        <span class=bp>self</span><span class=o>.</span><span class=n>_optimizer</span><span class=o>.</span><span class=n>process_aux_var</span><span class=p>(</span><span class=n>aux</span><span class=p>)</span>
<span class=linenos data-linenos="204 "></span>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>exc</span><span class=p>:</span>
<span class=linenos data-linenos="205 "></span>        <span class=k>raise</span> <span class=n>FedbiomedOptimizerError</span><span class=p>(</span>
<span class=linenos data-linenos="206 "></span>            <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>ErrorNumbers</span><span class=o>.</span><span class=n>FB621</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2>: `Optimizer.set_aux`: </span><span class=si>{</span><span class=n>exc</span><span class=si>}</span><span class=s2>&quot;</span>
<span class=linenos data-linenos="207 "></span>        <span class=p>)</span> <span class=kn>from</span> <span class=nn>exc</span>
</code></pre></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h5 id=fedbiomed.common.optimizers.Optimizer.step class="doc doc-heading"> <span class="doc doc-object-name doc-function-name">step</span> </h5> <div class="doc-signature highlight"><pre><span></span><code><span class=n>step</span><span class=p>(</span><span class=n>grads</span><span class=p>,</span> <span class=n>weights</span><span class=p>)</span>
</code></pre></div> <div class="doc doc-contents "> <p>Run an optimization step to compute and return model weight updates.</p> <p>Use the pre-assigned <code>weights</code> and <code>grads</code> (set using the <code>set_weights</code> and <code>set_grads</code> methods) to compute weight updates, using the pipeline defined by this instance.</p> <p><strong>Parameters:</strong></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>grads</code></td> <td> <code><span title="declearn.model.api.Vector">Vector</span></code> </td> <td> <div class=doc-md-description> <p>Raw gradients based on which to compute weights updates, wrapped into a declearn Vector structure.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr> <td><code>weights</code></td> <td> <code><span title="declearn.model.api.Vector">Vector</span></code> </td> <td> <div class=doc-md-description> <p>Current values of the weights with respect to which the gradients were computed, wrapped into a declearn Vector with the same concrete type as <code>grads</code>.</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <p><strong>Returns:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <code><span title="declearn.model.api.Vector">Vector</span></code> </td> <td> <div class=doc-md-description> <p>Updates to be applied to the model weights, computed by: - running wrapped gradients and weights through the regularizer plug-ins (that add loss-regularization terms' derivatives); - running resulting gradients through the optimodule plug-ins (that perform any defined gradient-alteration operation); - adding a decoupled weight-decay term, if one is to be used; - scaling the updates by the base learning rate. The results are wrapped into a declearn Vector structure, the concrete type of which is same as input <code>grads</code> and <code>weights</code>.</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/optimizer.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="112 "></span><span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>grads</span><span class=p>:</span> <span class=n>Vector</span><span class=p>,</span> <span class=n>weights</span><span class=p>:</span> <span class=n>Vector</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Vector</span><span class=p>:</span>
<span class=linenos data-linenos="113 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Run an optimization step to compute and return model weight updates.</span>
<span class=linenos data-linenos="114 "></span>
<span class=linenos data-linenos="115 "></span><span class=sd>    Use the pre-assigned `weights` and `grads` (set using the `set_weights`</span>
<span class=linenos data-linenos="116 "></span><span class=sd>    and `set_grads` methods) to compute weight updates, using the pipeline</span>
<span class=linenos data-linenos="117 "></span><span class=sd>    defined by this instance.</span>
<span class=linenos data-linenos="118 "></span>
<span class=linenos data-linenos="119 "></span><span class=sd>    Args:</span>
<span class=linenos data-linenos="120 "></span><span class=sd>        grads: Raw gradients based on which to compute weights updates,</span>
<span class=linenos data-linenos="121 "></span><span class=sd>            wrapped into a declearn Vector structure.</span>
<span class=linenos data-linenos="122 "></span><span class=sd>        weights: Current values of the weights with respect to which the</span>
<span class=linenos data-linenos="123 "></span><span class=sd>            gradients were computed, wrapped into a declearn Vector with</span>
<span class=linenos data-linenos="124 "></span><span class=sd>            the same concrete type as `grads`.</span>
<span class=linenos data-linenos="125 "></span>
<span class=linenos data-linenos="126 "></span><span class=sd>    Returns:</span>
<span class=linenos data-linenos="127 "></span><span class=sd>        Updates to be applied to the model weights, computed by:</span>
<span class=linenos data-linenos="128 "></span><span class=sd>            - running wrapped gradients and weights through the regularizer</span>
<span class=linenos data-linenos="129 "></span><span class=sd>              plug-ins (that add loss-regularization terms&#39; derivatives);</span>
<span class=linenos data-linenos="130 "></span><span class=sd>            - running resulting gradients through the optimodule plug-ins</span>
<span class=linenos data-linenos="131 "></span><span class=sd>              (that perform any defined gradient-alteration operation);</span>
<span class=linenos data-linenos="132 "></span><span class=sd>            - adding a decoupled weight-decay term, if one is to be used;</span>
<span class=linenos data-linenos="133 "></span><span class=sd>            - scaling the updates by the base learning rate.</span>
<span class=linenos data-linenos="134 "></span><span class=sd>            The results are wrapped into a declearn Vector structure, the</span>
<span class=linenos data-linenos="135 "></span><span class=sd>            concrete type of which is same as input `grads` and `weights`.</span>
<span class=linenos data-linenos="136 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="137 "></span>    <span class=c1># This code mostly replicates that of</span>
<span class=linenos data-linenos="138 "></span>    <span class=c1># `declearn.optimizer.Optimizer.compute_updates_from_gradients`.</span>
<span class=linenos data-linenos="139 "></span>    <span class=k>try</span><span class=p>:</span>
<span class=linenos data-linenos="140 "></span>        <span class=c1># Add loss-regularization terms&#39; derivatives to the raw gradients.</span>
<span class=linenos data-linenos="141 "></span>        <span class=k>for</span> <span class=n>reg</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>_optimizer</span><span class=o>.</span><span class=n>regularizers</span><span class=p>:</span>
<span class=linenos data-linenos="142 "></span>            <span class=n>grads</span> <span class=o>=</span> <span class=n>reg</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>grads</span><span class=p>,</span> <span class=n>weights</span><span class=p>)</span>
<span class=linenos data-linenos="143 "></span>        <span class=c1># Iteratively refine updates by running them through the optimodules.</span>
<span class=linenos data-linenos="144 "></span>        <span class=k>for</span> <span class=n>mod</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>_optimizer</span><span class=o>.</span><span class=n>modules</span><span class=p>:</span>
<span class=linenos data-linenos="145 "></span>            <span class=n>grads</span> <span class=o>=</span> <span class=n>mod</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>grads</span><span class=p>)</span>
<span class=linenos data-linenos="146 "></span>        <span class=c1># Apply the base learning rate.</span>
<span class=linenos data-linenos="147 "></span>        <span class=n>updates</span> <span class=o>=</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>_optimizer</span><span class=o>.</span><span class=n>lrate</span> <span class=o>*</span> <span class=n>grads</span>
<span class=linenos data-linenos="148 "></span>        <span class=c1># Optionally add the decoupled weight decay term.</span>
<span class=linenos data-linenos="149 "></span>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_optimizer</span><span class=o>.</span><span class=n>w_decay</span><span class=p>:</span>
<span class=linenos data-linenos="150 "></span>            <span class=n>updates</span> <span class=o>-=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_optimizer</span><span class=o>.</span><span class=n>w_decay</span> <span class=o>*</span> <span class=n>weights</span>
<span class=linenos data-linenos="151 "></span>        <span class=c1># Return the model updates.</span>
<span class=linenos data-linenos="152 "></span>        <span class=k>return</span> <span class=n>updates</span>
<span class=linenos data-linenos="153 "></span>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>exc</span><span class=p>:</span>
<span class=linenos data-linenos="154 "></span>        <span class=k>raise</span> <span class=n>FedbiomedOptimizerError</span><span class=p>(</span>
<span class=linenos data-linenos="155 "></span>            <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>ErrorNumbers</span><span class=o>.</span><span class=n>FB621</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2>: error in &#39;step&#39;: </span><span class=si>{</span><span class=n>exc</span><span class=si>}</span><span class=s2>&quot;</span>
<span class=linenos data-linenos="156 "></span>        <span class=p>)</span> <span class=kn>from</span> <span class=nn>exc</span>
</code></pre></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h3 id=fedbiomed.common.optimizers.SklearnOptimizerProcessing class="doc doc-heading"> <span class="doc doc-object-name doc-class-name">SklearnOptimizerProcessing</span> </h3> <div class="doc-signature highlight"><pre><span></span><code><span class=n>SklearnOptimizerProcessing</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>disable_internal_optimizer</span><span class=p>)</span>
</code></pre></div> <div class="doc doc-contents "> <p>Context manager used for scikit-learn model, that checks if model parameter(s) has(ve) been changed when disabling scikit-learn internal optimizer - ie when calling <code>disable_internal_optimizer</code> method</p> <p><strong>Parameters:</strong></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>model</code></td> <td> <code><a class="autorefs autorefs-internal" title="fedbiomed.common.models.SkLearnModel" href="../models/#fedbiomed.common.models.SkLearnModel">SkLearnModel</a></code> </td> <td> <div class=doc-md-description> <p>a SkLearnModel that wraps a scikit-learn model</p> </div> </td> <td> <em>required</em> </td> </tr> <tr> <td><code>disable_internal_optimizer</code></td> <td> <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code> </td> <td> <div class=doc-md-description> <p>whether to disable scikit-learn model internal optimizer (True) in order to apply declearn one or to keep it (False)</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>fedbiomed/common/optimizers/generic_optimizers.py</code></summary> <div class=highlight><pre><span></span><code><span class=linenos data-linenos="34 "></span><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
<span class=linenos data-linenos="35 "></span>    <span class=bp>self</span><span class=p>,</span>
<span class=linenos data-linenos="36 "></span>    <span class=n>model</span><span class=p>:</span> <span class=n>SkLearnModel</span><span class=p>,</span>
<span class=linenos data-linenos="37 "></span>    <span class=n>disable_internal_optimizer</span><span class=p>:</span> <span class=nb>bool</span>
<span class=linenos data-linenos="38 "></span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
<span class=linenos data-linenos="39 "></span><span class=w>    </span><span class=sd>&quot;&quot;&quot;Constructor of the object. Sets internal variables</span>
<span class=linenos data-linenos="40 "></span>
<span class=linenos data-linenos="41 "></span><span class=sd>    Args:</span>
<span class=linenos data-linenos="42 "></span><span class=sd>        model: a SkLearnModel that wraps a scikit-learn model</span>
<span class=linenos data-linenos="43 "></span><span class=sd>        disable_internal_optimizer: whether to disable scikit-learn model internal optimizer (True) in order</span>
<span class=linenos data-linenos="44 "></span><span class=sd>            to apply declearn one or to keep it (False)</span>
<span class=linenos data-linenos="45 "></span><span class=sd>    &quot;&quot;&quot;</span>
<span class=linenos data-linenos="46 "></span>    <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=n>model</span>
<span class=linenos data-linenos="47 "></span>    <span class=bp>self</span><span class=o>.</span><span class=n>_disable_internal_optimizer</span> <span class=o>=</span> <span class=n>disable_internal_optimizer</span>
</code></pre></div> </details> <div class="doc doc-children"> </div> </div> </div> </div> </div> </div> </article> </main> </div> <div class=right-col> <div id=right-sidebar class=sidebar-right> <nav class=toc> <!-- Render item list --> <label class=toc-title for=__toc> <span class=toc-icon></span> </label> <ul class=toc-list data-md-component=toc data-md-scrollfix> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers-classes class=md-nav__link> Classes </a> <nav class=toc-nav aria-label=Classes> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.BaseOptimizer class=md-nav__link> BaseOptimizer </a> <nav class=toc-nav aria-label=BaseOptimizer> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.BaseOptimizer-attributes class=md-nav__link> Attributes </a> <nav class=toc-nav aria-label=Attributes> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.BaseOptimizer.optimizer class=md-nav__link> optimizer </a> </li> </ul> </nav> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.BaseOptimizer-functions class=md-nav__link> Functions </a> <nav class=toc-nav aria-label=Functions> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.BaseOptimizer.init_training class=md-nav__link> init_training </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.BaseOptimizer.load_state class=md-nav__link> load_state </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.BaseOptimizer.save_state class=md-nav__link> save_state </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.BaseOptimizer.step class=md-nav__link> step </a> </li> </ul> </nav> </li> </ul> </nav> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.DeclearnOptimizer class=md-nav__link> DeclearnOptimizer </a> <nav class=toc-nav aria-label=DeclearnOptimizer> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.DeclearnOptimizer-attributes class=md-nav__link> Attributes </a> <nav class=toc-nav aria-label=Attributes> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.DeclearnOptimizer.model class=md-nav__link> model </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.DeclearnOptimizer.optimizer class=md-nav__link> optimizer </a> </li> </ul> </nav> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.DeclearnOptimizer-functions class=md-nav__link> Functions </a> <nav class=toc-nav aria-label=Functions> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.DeclearnOptimizer.get_aux class=md-nav__link> get_aux </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.DeclearnOptimizer.load_state class=md-nav__link> load_state </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.DeclearnOptimizer.optimizer_processing class=md-nav__link> optimizer_processing </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.DeclearnOptimizer.save_state class=md-nav__link> save_state </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.DeclearnOptimizer.set_aux class=md-nav__link> set_aux </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.DeclearnOptimizer.step class=md-nav__link> step </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.DeclearnOptimizer.zero_grad class=md-nav__link> zero_grad </a> </li> </ul> </nav> </li> </ul> </nav> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.NativeSkLearnOptimizer class=md-nav__link> NativeSkLearnOptimizer </a> <nav class=toc-nav aria-label=NativeSkLearnOptimizer> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.NativeSkLearnOptimizer-functions class=md-nav__link> Functions </a> <nav class=toc-nav aria-label=Functions> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.NativeSkLearnOptimizer.optimizer_processing class=md-nav__link> optimizer_processing </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.NativeSkLearnOptimizer.step class=md-nav__link> step </a> </li> </ul> </nav> </li> </ul> </nav> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.NativeTorchOptimizer class=md-nav__link> NativeTorchOptimizer </a> <nav class=toc-nav aria-label=NativeTorchOptimizer> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.NativeTorchOptimizer-functions class=md-nav__link> Functions </a> <nav class=toc-nav aria-label=Functions> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.NativeTorchOptimizer.get_learning_rate class=md-nav__link> get_learning_rate </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.NativeTorchOptimizer.step class=md-nav__link> step </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.NativeTorchOptimizer.zero_grad class=md-nav__link> zero_grad </a> </li> </ul> </nav> </li> </ul> </nav> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.Optimizer class=md-nav__link> Optimizer </a> <nav class=toc-nav aria-label=Optimizer> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.Optimizer-functions class=md-nav__link> Functions </a> <nav class=toc-nav aria-label=Functions> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.Optimizer.from_declearn_optimizer class=md-nav__link> from_declearn_optimizer </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.Optimizer.get_aux class=md-nav__link> get_aux </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.Optimizer.get_state class=md-nav__link> get_state </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.Optimizer.init_round class=md-nav__link> init_round </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.Optimizer.load_state class=md-nav__link> load_state </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.Optimizer.set_aux class=md-nav__link> set_aux </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.Optimizer.step class=md-nav__link> step </a> </li> </ul> </nav> </li> </ul> </nav> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#fedbiomed.common.optimizers.SklearnOptimizerProcessing class=md-nav__link> SklearnOptimizerProcessing </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> </div> <!-- News Main Page--> </div> <footer> <div class=container-fluid> <div class=footer-first-inner> <div class=container> <div class=row> <div class=col-md-6> <div class=footer-contact> <strong>Address:</strong> <p>2004 Rte des Lucioles, 06902 Sophia Antipolis</p> <strong>E-mail:</strong> <p>fedbiomed _at_ inria _dot_ fr</p> </div> </div> <div class=col-md-6> <div class=footer-contact> <p>Fed-BioMed © 2022</p> </div> </div> </div> </div> </div> </div> </footer> <!-- JQuery --> <script src=https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js></script> <!-- Latest compiled and minified JavaScript --> <script src=https://stackpath.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js integrity=sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd crossorigin=anonymous></script> <script src=https://cdnjs.cloudflare.com/ajax/libs/tablesort/5.2.1/tablesort.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../../../assets/javascript/lunr.js></script> <script src=../../../../assets/javascript/theme.js></script> </body> </html>