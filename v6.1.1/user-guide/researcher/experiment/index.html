<!DOCTYPE html><html> <head><meta charset=UTF-8><meta name=viewport content="width=device-width, initial-scale=1"><meta name=robots context="index, archive"><!--  --><script>
      const versions_json = "../../../../versions.json";
      const search_worker_js = "../../../assets/javascript/search-worker.js";
      const search_index_json = "../../../search/search_index.json";
      const base_url = '../../..';
    </script><!-- Site title --><title>Experiment Class of Fed-BioMed - Fed-BioMed</title><link rel=icon type=image/x-icon href=../../../favicon.ico><!-- Page description --><meta name=description content="Fed-BioMed provides a model training process over multiple nodes where the datasets are stored and models get trained. The experiment is in charge of orchestrating the training process on available nodes."><!-- Page keywords --><meta name=keywords content="parameter aggregation,aggregation,federated average, Fed-BioMed experiment"><!-- Page author --><!-- Latest compiled and minified CSS --><link rel=stylesheet href=https://stackpath.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css integrity=sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu crossorigin=anonymous><!-- Bootstrap Icons --><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css><link href=../../../assets/_mkdocstrings.css rel=stylesheet><link href=../../../assets/css/style.css rel=stylesheet></head> <body> <header> <div class="container-fluid top-bar"> <div class=brand> <a href=/ > <img src=../../../assets/img/fedbiomed-logo-small.png> </a> </div> <nav class=top> <ul> <li class> <a href=../../..>Home</a> </li> <li class> <a href=../../../getting-started/what-is-fedbiomed/ >User Documentation</a> </li> <li class> <a href=../../../pages/about-us/ >About</a> </li> <li id=clicker class=has-sub> More <i class="bi bi-chevron-down"></i> <ul class=sub-nav-menu> <li class> <a href=../../../#funding>Funding</a> </li> <li class> <a href=../../../news/ >News</a> </li> <li class> <a href=../../../#contributors>Contributors</a> </li> <li class> <a href=../../../#users>Users</a> </li> <li class> <a href=../../../pages/roadmap/ >Roadmap</a> </li> <li class> <a href=../../../#citation>How to Cite Us</a> </li> <li class> <a href=../../../#contact-us>Contact Us</a> </li> </ul> </li> </ul> </nav> </div> <div class="container-fluid top-bar-mobile"> <div class=mobile-bar> <div class=brand> <a href=../../../ > <img src=../../../assets/img/fedbiomed-logo-small.png> </a> </div> <div class=hum-menu> <img class=open src=../../../assets/img/menu.svg> <img class=close style=display:none src=../../../assets/img/cancel.svg> </div> </div> <nav class=top-mobile> <ul> <li class> <a href=../../..>Home</a> </li> <li class> <a href=../../../getting-started/what-is-fedbiomed/ >User Documentation</a> </li> <li class> <a href=../../../pages/about-us/ >About</a> </li> <li id=clicker class=has-sub> More <i class="bi bi-chevron-down"></i> <ul class=sub-nav-menu> <li class> <a href=../../../#funding>Funding</a> </li> <li class> <a href=../../../news/ >News</a> </li> <li class> <a href=../../../#contributors>Contributors</a> </li> <li class> <a href=../../../#users>Users</a> </li> <li class> <a href=../../../pages/roadmap/ >Roadmap</a> </li> <li class> <a href=../../../#citation>How to Cite Us</a> </li> <li class> <a href=../../../#contact-us>Contact Us</a> </li> </ul> </li> </ul> </nav> </div> </header> <div class=main> <!-- Home page --> <div class=container-fluid> <div class=doc-row> <div class=left-col> <div class=sidebar-doc> <nav class=sidebar-inner> <ul class="sidebar-menu-left sub"> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Getting Started <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../getting-started/what-is-fedbiomed/ class>What's Fed-BioMed</a> </li> <li class> <a href=../../../getting-started/fedbiomed-architecture/ class>Fedbiomed Architecture</a> </li> <li class> <a href=../../../getting-started/fedbiomed-workflow/ class>Fedbiomed Workflow</a> </li> <li class> <a href=../../../getting-started/installation/ class>Installation</a> </li> <li class> <a href=../../../getting-started/basic-example/ class>Basic Example</a> </li> <li class> <a href=../../../getting-started/configuration/ class>Configuration</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Tutorials <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> PyTorch <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/ class>PyTorch MNIST Basic Example</a> </li> <li class> <a href=../../../tutorials/pytorch/02_Create_Your_Custom_Training_Plan/ class>How to Create Your Custom PyTorch Training Plan</a> </li> <li class> <a href=../../../tutorials/pytorch/03_PyTorch_Used_Cars_Dataset_Example/ class>PyTorch Used Cars Dataset Example</a> </li> <li class> <a href=../../../tutorials/pytorch/04-Aggregation_in_Fed-BioMed/ class>PyTorch aggregation methods in Fed-BioMed</a> </li> <li class> <a href=../../../tutorials/pytorch/05_Transfer-learning_tutorial_usingDenseNet-121/ class>Transfer-learning in Fed-BioMed tutorial</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> MONAI <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../tutorials/monai/01_monai-2d-image-classification/ class>Federated 2d image classification with MONAI</a> </li> <li class> <a href=../../../tutorials/monai/02_monai-2d-image-registration/ class>Federated 2d XRay registration with MONAI</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Scikit-Learn <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../tutorials/scikit-learn/01_sklearn_MNIST_classification_tutorial/ class>MNIST classification with Scikit-Learn Classifier (Perceptron)</a> </li> <li class> <a href=../../../tutorials/scikit-learn/02_sklearn_sgd_regressor_tutorial/ class>Fed-BioMed to train a federated SGD regressor model</a> </li> <li class> <a href=../../../tutorials/scikit-learn/03-other-scikit-learn-models/ class>Implementing other Scikit Learn models for Federated Learning</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Optimizers <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../tutorials/optimizers/01-fedopt-and-scaffold/ class>Advanced optimizers in Fed-BioMed</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> FLamby <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../tutorials/flamby/flamby/ class>General Concepts</a> </li> <li class> <a href=../../../tutorials/flamby/flamby-integration-into-fedbiomed/ class>FLamby integration in Fed-BioMed</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Advanced <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../tutorials/advanced/in-depth-experiment-configuration/ class>In Depth Experiment Configuration</a> </li> <li class> <a href=../../../tutorials/advanced/training-with-gpu/ class>PyTorch model training using a GPU</a> </li> <li class> <a href=../../../tutorials/advanced/breakpoints/ class>Breakpoints</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Security <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../tutorials/security/differential-privacy-with-opacus-on-fedbiomed/ class>Using Differential Privacy with OPACUS on Fed-BioMed</a> </li> <li class> <a href=../../../tutorials/security/non-private-local-central-dp-monai-2d-image-registration/ class>Local and Central DP with Fed-BioMed: MONAI 2d image registration</a> </li> <li class> <a href=../../../tutorials/security/training-with-approved-training-plans/ class>Training Process with Training Plan Management</a> </li> <li class> <a href=../../../tutorials/security/secure-aggregation/ class>Training with Secure Aggregation</a> </li> <li class> <a href=../../../tutorials/concrete-ml/concrete-ml/ class>End-to-end Privacy Preserving Training and Inference on Medical Data</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Biomedical data <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../tutorials/medical/medical-image-segmentation-unet-library/ class>Brain Segmentation</a> </li> </ul> </li> </ul> </li> <li data-adress=sub-1 class="current has-sub-side"> <div href class="parent-list current "> User Guide <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub active "> <li class> <a href=../../glossary/ class>Glossary</a> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Deployment <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../deployment/deployment/ class>Introduction</a> </li> <li class> <a href=../../deployment/deployment-vpn/ class>VPN Deployment</a> </li> <li class> <a href=../../deployment/matrix/ class>Network matrix</a> </li> <li class> <a href=../../deployment/security-model/ class>Security model</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Node <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../nodes/configuring-nodes/ class>Configuring Nodes</a> </li> <li class> <a href=../../nodes/deploying-datasets/ class>Deploying Datasets</a> </li> <li class> <a href=../../nodes/training-plan-security-manager/ class>Training Plan Management</a> </li> <li class> <a href=../../nodes/using-gpu/ class>Using GPU</a> </li> <li class> <a href=../../nodes/node-gui/ class>Node GUI</a> </li> </ul> </li> <li data-adress=sub-1 class="current has-sub-side"> <div href class="parent-list current "> Researcher <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub active "> <li class> <a href=../training-plan/ class>Training Plan</a> </li> <li class> <a href=../training-data/ class>Training Data</a> </li> <li class=current> <a href=./ class="link current">Experiment</a> </li> <li class> <a href=../aggregation/ class>Aggregation</a> </li> <li class> <a href=../listing-datasets-and-selecting-nodes/ class>Listing Datasets and Selecting Nodes</a> </li> <li class> <a href=../model-testing-during-federated-training/ class>Model Validation on the Node Side</a> </li> <li class> <a href=../tensorboard/ class>Tensorboard</a> </li> </ul> </li> <li class> <a href=../../advanced-optimization/ class>Optimization</a> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Secure Aggregation <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../secagg/introduction/ class>Introduction</a> </li> <li class> <a href=../../secagg/configuration/ class>Configuration</a> </li> <li class> <a href=../../secagg/researcher-interface/ class>Managing Secure Aggregation in Researcher</a> </li> </ul> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Developer <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> API Reference <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Common <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../developer/api/common/certificate_manager/ class>Certificate Manager</a> </li> <li class> <a href=../../../developer/api/common/cli/ class>CLI</a> </li> <li class> <a href=../../../developer/api/common/config/ class>Config</a> </li> <li class> <a href=../../../developer/api/common/constants/ class>Constants</a> </li> <li class> <a href=../../../developer/api/common/data/ class>Data</a> </li> <li class> <a href=../../../developer/api/common/db/ class>DB</a> </li> <li class> <a href=../../../developer/api/common/exceptions/ class>Exceptions</a> </li> <li class> <a href=../../../developer/api/common/ipython/ class>IPython</a> </li> <li class> <a href=../../../developer/api/common/json/ class>Json</a> </li> <li class> <a href=../../../developer/api/common/logger/ class>Logger</a> </li> <li class> <a href=../../../developer/api/common/message/ class>Message</a> </li> <li class> <a href=../../../developer/api/common/metrics/ class>Metrics</a> </li> <li class> <a href=../../../developer/api/common/models/ class>Model</a> </li> <li class> <a class href=../../../developer/api/common/mpc_controller.md>MPC controller</a> </li> <li class> <a href=../../../developer/api/common/optimizers/ class>Optimizers</a> </li> <li class> <a href=../../../developer/api/common/privacy/ class>Privacy</a> </li> <li class> <a href=../../../developer/api/common/secagg/ class>Secagg</a> </li> <li class> <a href=../../../developer/api/common/secagg_manager/ class>Secagg Manager</a> </li> <li class> <a href=../../../developer/api/common/serializer/ class>Serializer</a> </li> <li class> <a href=../../../developer/api/common/singleton/ class>Singleton</a> </li> <li class> <a href=../../../developer/api/common/synchro/ class>Synchro</a> </li> <li class> <a href=../../../developer/api/common/tasks_queue/ class>TasksQueue</a> </li> <li class> <a href=../../../developer/api/common/training_plans/ class>TrainingPlans</a> </li> <li class> <a href=../../../developer/api/common/training_args/ class>TrainingArgs</a> </li> <li class> <a href=../../../developer/api/common/utils/ class>Utils</a> </li> <li class> <a href=../../../developer/api/common/validator/ class>Validator</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Node <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../developer/api/node/cli/ class>CLI</a> </li> <li class> <a href=../../../developer/api/node/cli_utils/ class>CLI Utils</a> </li> <li class> <a href=../../../developer/api/node/config/ class>Config</a> </li> <li class> <a href=../../../developer/api/node/dataset_manager/ class>DatasetManager</a> </li> <li class> <a href=../../../developer/api/node/history_monitor/ class>HistoryMonitor</a> </li> <li class> <a href=../../../developer/api/node/node/ class>Node</a> </li> <li class> <a href=../../../developer/api/node/node_state_manager/ class>NodeStateManager</a> </li> <li class> <a href=../../../developer/api/node/requests/ class>Requests</a> </li> <li class> <a href=../../../developer/api/node/round/ class>Round</a> </li> <li class> <a href=../../../developer/api/node/secagg/ class>Secagg</a> </li> <li class> <a href=../../../developer/api/node/secagg_manager/ class>Secagg Manager</a> </li> <li class> <a href=../../../developer/api/node/training_plan_security_manager/ class>TrainingPlanSecurityManager</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Researcher <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../developer/api/researcher/aggregators/ class>Aggregators</a> </li> <li class> <a href=../../../developer/api/researcher/cli/ class>CLI</a> </li> <li class> <a href=../../../developer/api/researcher/config/ class>Config</a> </li> <li class> <a href=../../../developer/api/researcher/datasets/ class>Datasets</a> </li> <li class> <a href=../../../developer/api/researcher/federated_workflows/ class>Federated Workflows</a> </li> <li class> <a href=../../../developer/api/researcher/filetools/ class>Filetools</a> </li> <li class> <a href=../../../developer/api/researcher/jobs/ class>Jobs</a> </li> <li class> <a href=../../../developer/api/researcher/monitor/ class>Monitor</a> </li> <li class> <a href=../../../developer/api/researcher/node_state_agent/ class>NodeStateAgent</a> </li> <li class> <a href=../../../developer/api/researcher/requests/ class>Requests</a> </li> <li class> <a href=../../../developer/api/researcher/secagg/ class>Secagg</a> </li> <li class> <a href=../../../developer/api/researcher/strategies/ class>Strategies</a> </li> </ul> </li> <li data-adress=sub-1 class=has-sub-side> <div href class="parent-list "> Transport <i class="bi bi-chevron-down"></i> </div> <ul class="sub-sidebar-menu sub "> <li class> <a href=../../../developer/api/transport/client/ class>Client</a> </li> <li class> <a href=../../../developer/api/transport/controller/ class>Controller</a> </li> <li class> <a href=../../../developer/api/transport/node_agent/ class>NodeAgent</a> </li> <li class> <a href=../../../developer/api/transport/server/ class>Server</a> </li> </ul> </li> </ul> </li> <li class> <a href=../../../developer/usage_and_tools/ class>Usage and Tools</a> </li> <li class> <a href=../../../developer/ci/ class>Continuous Integration</a> </li> <li class> <a href=../../../developer/definition-of-done/ class>Definition of Done</a> </li> <li class> <a href=../../../developer/development-environment/ class>Development Environment</a> </li> <li class> <a href=../../../developer/testing-in-fedbiomed/ class>Testing in Fed-BioMed</a> </li> <li class> <a href=../../../developer/messaging/ class>RPC Protocol and Messages</a> </li> </ul> </li> <li class> <a href=../../../support/troubleshooting/ class>FAQ & Troubleshooting</a> </li> </ul> </nav> </div> </div> <div class=main-col> <main class=main-docs> <article> <h1 id=experiment-class-of-fed-biomed>Experiment Class of Fed-BioMed</h1> <h2 id=introduction>Introduction</h2> <p>The <code>Experiment</code> class in Fed-BioMed is in charge of orchestrating the federated learning process on available nodes. Specifically, it takes care of:</p> <ul> <li>Searching the datasets on active nodes, based on specific tags given by a researcher and used by the nodes to identify the dataset.</li> <li>Sending model, training plan and training arguments to the nodes.</li> <li>Tracking the training process on the nodes during all training rounds.</li> <li>Checking the nodes' responses to handle possible failures.</li> <li>Receiving the local model parameters after every round of training.</li> <li>Aggregating the local model parameters based on the specified federated approach.</li> <li>Sending the aggregated parameters to the selected nodes for the next round.</li> <li>Optimizing the global model (i.e. the aggregated model).</li> </ul> <p><img alt=ExperimentWorkFlow src=../../../assets/img/diagrams/ExperimentWorkFlow.jpg#img-centered-lr></p> <h2 id=defining-an-experiment>Defining an experiment</h2> <p>You may configure an <code>Experiment</code> by providing arguments to its constructor, as shown below.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>fedbiomed.researcher.federated_workflows</span> <span class=kn>import</span> <span class=n>Experiment</span>

<span class=n>exp</span> <span class=o>=</span> <span class=n>Experiment</span><span class=p>(</span><span class=n>tags</span><span class=o>=</span><span class=n>tags</span><span class=p>,</span>
                 <span class=n>nodes</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
                 <span class=n>training_plan_class</span><span class=o>=</span><span class=n>MyTrainingPlan</span><span class=p>,</span>
                 <span class=n>training_args</span><span class=o>=</span><span class=n>training_args</span><span class=p>,</span>
                 <span class=n>round_limit</span><span class=o>=</span><span class=n>rounds</span><span class=p>,</span>
                 <span class=n>aggregator</span><span class=o>=</span><span class=n>FedAverage</span><span class=p>(),</span>
                 <span class=n>node_selection_strategy</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</code></pre></div> <div class="admonition note"> <p>It is also possible to define an empty experiment and set the arguments afterwards, using the setters of the experiment object. Please visit the tutorial <a href=../../../tutorials/advanced/in-depth-experiment-configuration/ >In depth experiment configuration</a> to find out more about declaring an experiment step by step</p> </div> <p>Under the hood, the <code>Experiment</code> class takes care of a lot of heavy lifting for you. For example, when you initialize an experiment with the <code>tags</code> argument, it uses them to automatically create a <code>FederatedDataSet</code> by querying the federation. Afterwards, <code>Experiment</code> initializes several internal variables to manage federated training on all participating nodes. Finally, it also creates the strategy to select the nodes for each training round. When the <code>node_selection_strategy</code> is set to <code>None</code>, the experiment uses the default strategy which is <code>DefaultStrategy</code>.</p> <h3 id=setting-the-training-data>Setting the training data</h3> <h4 id=setting-the-training-data-by-setting-the-tags>Setting the training data by setting the tags</h4> <p>Each dataset deployed on the nodes is identified by tags. Tags allow researchers to select the same dataset registered under a given tag (or list of tags) on each node for the training.</p> <p>The argument <code>tags</code> of the experiment is used for dataset search request. It can be a list of tags which are of type <code>string</code>, or single tag of type <code>string</code>.</p> <div class=highlight><pre><span></span><code><span class=n>exp</span> <span class=o>=</span> <span class=n>Experiment</span><span class=p>()</span>
<span class=n>exp</span><span class=o>.</span><span class=n>set_tags</span><span class=p>(</span><span class=n>tags</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;#MNIST&#39;</span><span class=p>,</span> <span class=s1>&#39;#dataset&#39;</span><span class=p>])</span>
<span class=c1>#or</span>
<span class=n>exp</span><span class=o>.</span><span class=n>set_tags</span><span class=p>(</span><span class=n>tags</span><span class=o>=</span><span class=s1>&#39;#MNIST&#39;</span><span class=p>)</span>
</code></pre></div> <div class="admonition warning"> <p class=admonition-title>Setting tags also sets the <code>Experiment</code>'s training data</p> <p>Whenever the <code>set_tags</code> method is called, a query is <strong>always</strong> issued to identify all nodes in the federation that have datasets with matching tags. Consequently, the training data of <code>Experiment</code> is changed to match the results from the query.</p> </div> <p>You can check your tags in your experiment as follows:</p> <div class=highlight><pre><span></span><code><span class=n>tags</span> <span class=o>=</span> <span class=n>exp</span><span class=o>.</span><span class=n>tags</span><span class=p>()</span>
<span class=nb>print</span><span class=p>(</span><span class=n>tags</span><span class=p>)</span>
<span class=c1># &gt; OUTPUT:</span>
<span class=c1># &gt; [&#39;#MNIST&#39;, &#39;#dataset&#39;]</span>
</code></pre></div> <div class="admonition warning"> <p class=admonition-title>Tags matching multiple datasets</p> <p>An <code>Experiment</code> object must have <strong>one unique</strong> dataset per node. Object creation fails if this is not the case when trying to instantiate the <code>FederatedDataSet</code> object. This is done to ensure that training for an <code>Experiment</code> uses only a single dataset for each node.</p> </div> <p>As a consequence, <code>tags</code> specified for an <code>Experiment</code> should not be ambiguous, which means they cannot match multiple datasets on one node.</p> <p>For example if you instantiate <code>Experiment(tags='#dataset')</code> and a node has registered one dataset with tags <code>['#dataset', '#MNIST']</code> and another dataset with tags <code>['#dataset', '#foo']</code> then experiment creation fails.</p> <h4 id=setting-the-training-data-by-providing-the-metadata-directly>Setting the training data by providing the metadata directly</h4> <p>The dataset metadata can be provided directly using the <code>set_training_data</code> method. The metadata can be a <code>FederatedDataSet</code> object or a nested <code>dict</code> with format <code>{node_id: {metadata_key: metadata_value}}</code>.</p> <p>When you provide a metadata object directly, the <code>Experiment</code>'s tags attribute is set to <code>None</code>.</p> <h4 id=under-the-hood-consistency-with-all-members-of-experiment>Under-the-hood consistency with all members of <code>Experiment</code></h4> <p>When you change the training data (either through <code>set_tags</code> or <code>set_training_data</code>), the <code>Experiment</code> class performs a lot of operations to ensure that consistency is maintained for all of its attributes that use the training data. In particular, the <code>aggregator</code> and <code>node_state_agent</code> classes are updated with the new training data.</p> <h3 id=selecting-specific-nodes-for-the-training>Selecting specific Nodes for the training</h3> <p>The argument <code>nodes</code> stands for filtering the nodes that are going to be used for federated training. It is useful when there are too many nodes on the network, and you want to perform federated training on specific ones. <code>nodes</code> argument is a list that contains node ids. When it is set, the experiment queries only the nodes that are in the list for a dataset matching tags. You can visit <a href=../listing-datasets-and-selecting-nodes/ >listing datasets and selecting nodes</a> documentation to get more information about this feature.</p> <div class=highlight><pre><span></span><code><span class=n>nodes</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;node-id-1&#39;</span><span class=p>,</span> <span class=s1>&#39;node-id-2&#39;</span><span class=p>]</span>
<span class=n>exp</span><span class=o>.</span><span class=n>set_nodes</span><span class=p>(</span><span class=n>nodes</span><span class=o>=</span><span class=n>nodes</span><span class=p>)</span>
</code></pre></div> <p>By default, <code>nodes</code> argument is <code>None</code> which means that each node that has a registered dataset matching all the tags will be part of the federated training.</p> <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>set_nodes</span><span class=p>(</span><span class=n>nodes</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Node filtering happens at training time</p> <p>Setting nodes doesn't mean sending another dataset search request to the nodes. Node filtering happens dynamically each time a training request is sent to nodes. In other words, if you search again for datasets after setting <code>nodes</code> by running <code>exp.set_training_data(training_data=None, from_tags=True)</code> you select in your <code>FederatedDataset</code> the same nodes as with <code>nodes=None</code>.</p> </div> <h3 id=load-your-training-plan-training-plan-class>Load your Training Plan: Training Plan Class</h3> <p>The <code>training_plan_class</code> is the class where the model, training data and training step are defined. Although not required, optimizers and dependencies can also be defined in this class. The experiment will extract source of your training plan, save as a python module (script), and send the source code to the nodes every round of training. Thanks to that, each node can construct the model and perform the training.</p> <div class=highlight><pre><span></span><code><span class=k>class</span> <span class=nc>MyTrainingPlan</span><span class=p>(</span><span class=n>TorchTrainingPlan</span><span class=p>):</span>
    <span class=k>def</span> <span class=nf>init_model</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model_args</span><span class=p>):</span>
        <span class=c1># Builds the model and returns it</span>
        <span class=k>return</span> <span class=n>model</span>

    <span class=k>def</span> <span class=nf>init_optimizer</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>optimizer_args</span><span class=p>):</span>
        <span class=c1># Builds the optimizer and returns it</span>
        <span class=k>return</span> <span class=n>optimizer</span>

    <span class=k>def</span> <span class=nf>training_step</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=c1># Training step at each iteration of training</span>
        <span class=k>return</span> <span class=n>loss</span>

    <span class=k>def</span> <span class=nf>training_data</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=c1># Loads the dat and returns Fed-BioMed DataManager</span>
        <span class=k>return</span> <span class=n>DataManager</span><span class=p>()</span>

<span class=n>exp</span><span class=o>.</span><span class=n>set_training_plan_class</span><span class=p>(</span><span class=n>training_plan_class</span><span class=o>=</span><span class=n>MyTrainingPlan</span><span class=p>)</span>

<span class=c1># Retrieving training plan class from experiment object</span>
<span class=n>training_plan_class</span> <span class=o>=</span> <span class=n>exp</span><span class=o>.</span><span class=n>training_plan_class</span><span class=p>()</span>
</code></pre></div> <h3 id=model-arguments>Model Arguments</h3> <p>The <code>model_args</code> is a dictionary with the arguments related to the model (e.g. number of layers, layer arguments and dimensions, etc.). This will be passed to the <code>init_model</code> method during model setup. An example for passing the number of input adn output features for a model is shown below.</p> <div class=highlight><pre><span></span><code><span class=n>model_args</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s2>&quot;in_features&quot;</span>   <span class=p>:</span> <span class=mi>15</span><span class=p>,</span>
    <span class=s2>&quot;out_features&quot;</span>  <span class=p>:</span> <span class=mi>1</span>
<span class=p>}</span>
<span class=n>exp</span><span class=o>.</span><span class=n>set_model_args</span><span class=p>(</span><span class=n>model_args</span><span class=o>=</span><span class=n>model_args</span><span class=p>)</span>
</code></pre></div> <div class="admonition warning"> <p class=admonition-title>Incompatible <code>model_args</code></p> <p>If you try to set new <code>model_args</code> that are incompatible with the current model weights, the function will raise an exception and the <code>Experiment</code> class will be left in an <strong>inconsistent state</strong>. To rectify this, immediately re-execute <code>set_model_args</code> with additional keyword argument <code>keep_weights=False</code> as in the example below: <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>set_model_args</span><span class=p>(</span><span class=n>model_args</span><span class=p>,</span> <span class=n>keep_weights</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</code></pre></div></p> </div> <p>Model arguments can then be used within a <code>TrainingPlan</code> as in the example below,</p> <div class=highlight><pre><span></span><code><span class=k>class</span> <span class=nc>MyTrainingPlan</span><span class=p>(</span><span class=n>TorchTrainingPlan</span><span class=p>):</span>

    <span class=k>def</span> <span class=nf>init_model</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model_args</span><span class=p>):</span>
        <span class=n>model</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>Net</span><span class=p>(</span><span class=n>model_args</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>model</span>

    <span class=k>class</span> <span class=nc>Net</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
        <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model_args</span><span class=p>):</span>
            <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>in_features</span> <span class=o>=</span> <span class=n>model_args</span><span class=p>[</span><span class=s1>&#39;in_features&#39;</span><span class=p>]</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>out_features</span> <span class=o>=</span> <span class=n>model_args</span><span class=p>[</span><span class=s1>&#39;out_features&#39;</span><span class=p>]</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>in_features</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_features</span><span class=p>)</span>
</code></pre></div> <div class="admonition info"> <p class=admonition-title>Special model arguments for scikit-learn experiments</p> <p>In scikit-learn experiments, you are required to provide additional special arguments in the <code>model_args</code> dictionary. For classification tasks, you must provide <strong>both</strong> a <code>n_features</code> and an <code>n_classes</code> field, while for regression tasks you are only required to provide a <code>n_features</code> field.</p> <ul> <li><code>n_features</code>: an integer indicating the number of input features for your model</li> <li><code>n_classes</code>: an integer indicating the number of classes in your task</li> </ul> <p>Note that, as an additional requirement for classification tasks, <strong>classes must be identified by integers in the range <code>0..n_classes</code></strong></p> </div> <h3 id=training-arguments>Training Arguments</h3> <p><code>training_args</code> is a dictionary, containing the arguments for the training on the node side (e.g. data loader arguments, optimizer arguments, epochs, etc.). These arguments are <em>dynamic</em>, in the sense that you may change them between two rounds of the same experiment, and the updated changes will be taken into account (provided you also update the experiment using the <code>set_training_args</code> method).</p> <p>A list of valid arguments is given in the <a class="autorefs autorefs-internal" href="../../../developer/api/common/training_args/#fedbiomed.common.training_args.TrainingArgs.default_scheme">TrainingArgs.default_scheme</a> documentation.</p> <p>To set the training arguments you may either pass them to the <code>Experiment</code> constructor, or set them on an instance with the <code>set_training_arguments</code> method:</p> <p><div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>set_training_arguments</span><span class=p>(</span><span class=n>training_args</span><span class=o>=</span><span class=n>training_args</span><span class=p>)</span>
</code></pre></div> To get the current training arguments that are used for the experiment, you can write:</p> <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>training_args</span><span class=p>()</span>
</code></pre></div> <h4 id=controlling-the-number-of-training-loop-iterations>Controlling the number of training loop iterations</h4> <p>The preferred way is to set the <code>num_updates</code> training argument. This argument is equal to the number of iterations to be performed in the training loop. In mini-batch based scenarios, this corresponds to the number of updates to the model parameters, hence the name. In PyTorch notation, this is equivalent to the number of calls to <code>optimizer.step</code>.</p> <p>Another way to determine the number of training loop iterations is to set <code>epochs</code> in the training arguments. In this case, you may optionally set a <code>batch_maxnum</code> argument, in order to exit the training loop before a full epoch is completed. If <code>batch_maxnum</code> is set and is greater than 0, then only <code>batch_maxnum</code> iterations will be performed per epoch.</p> <div class="admonition warning"> <p class=admonition-title><code>num_updates</code> is the same for all nodes</p> <p>If you specify <code>num_updates</code> in your <code>training_args</code>, then every node will perform the same number of updates. Conversely, if you specify <code>epochs</code>, then each node may perform a different number of iterations if their local dataset sizes differ.</p> </div> <p>Note that if you set both <code>num_updates</code> and <code>epochs</code> by mistake, the value of <code>num_updates</code> takes precedence, and <code>epochs</code> will effectively be ignored.</p> <div class="admonition info"> <p class=admonition-title>Why num updates?</p> <p>In a federated scenario, different nodes may have datasets with very different sizes. By performing the same number of epochs on each node, we would be biasing the global model towards the larger nodes, because we would be performing more gradient updates based on their data. Instead, we fix the number of gradient updates regardless of the dataset size with the <code>num_updates</code> parameter.</p> </div> <h5 id=compatibility>Compatibility</h5> <p>Not all configurations are compatible with all types of aggregators and experiments. We list here the known constraints:</p> <ul> <li>the <a href=../aggregation/#scaffold>Scaffold</a> aggregator <strong>requires</strong> using <code>num_updates</code></li> </ul> <h4 id=batch-size-and-other-data-loader-arguments>Batch size and other data loader arguments</h4> <div class="admonition info"> <p class=admonition-title>Dataloader arguments are automatically injected through the <code>DataManager</code> class</p> <p>It is strongly recommended to always provide a <code>loader_args</code> key in your training arguments, with a dictionary as value containing at least the <code>batch_size</code> key.</p> </div> <p>Example of minimal loader arguments: <div class=highlight><pre><span></span><code><span class=n>training_args</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;loader_args&#39;</span><span class=p>:</span> <span class=p>{</span>
        <span class=s1>&#39;batch_size&#39;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
    <span class=p>},</span>
<span class=p>}</span>
</code></pre></div></p> <p>Note that the <code>loader_arguments</code>, as well as any additional keyword arguments that you will specify in your <code>DataManager</code> constructor, will be automatically injected in the definition of the data loader. Please refer to the <a href=../training-data/ ><code>training_data</code></a> method documentation for more details.</p> <h4 id=setting-a-random-seed-for-reproducibility>Setting a random seed for reproducibility</h4> <p>The <code>random_seed</code> argument allows to set a random seed at the beginning of each round.</p> <div class="admonition info"> <p class=admonition-title><code>random_seed</code> is set both on the node and the researcher</p> <p>The <code>random_seed</code> is set whenever the <code>TrainingPlan</code> is instantiated: both on the researcher side before sending the train command for a new round, and on the node side at the beginning of the configuration of the new training round.</p> </div> <p>Setting the <code>random_seed</code> affects:</p> <ul> <li>the random initialization of model parameters at the beginning of the experiment</li> <li>the random shuffling of data in the <code>DataLoader</code></li> <li>any other random effect on the node and researcher side.</li> </ul> <p>The same seed is used for the built-in <code>random</code> module, <code>numpy.random</code> and <code>torch.random</code>, effectively equivalent to: <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>random</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
<span class=kn>import</span> <span class=nn>torch</span>

<span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=n>training_args</span><span class=p>[</span><span class=s1>&#39;random_seed&#39;</span><span class=p>])</span>
<span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=n>training_args</span><span class=p>[</span><span class=s1>&#39;random_seed&#39;</span><span class=p>])</span>
<span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=n>training_args</span><span class=p>[</span><span class=s1>&#39;random_seed&#39;</span><span class=p>])</span>
</code></pre></div></p> <div class="admonition warning"> <p class=admonition-title><code>random_seed</code> is reset at every round</p> <p>The random_seed argument will be reset to the specified value at the beginning of every round.</p> </div> <p>Because of this, the same sequence will be used for random effects like shuffling the dataset for all the rounds within an <code>exp.run()</code> execution. This is not what the user typically wants. A simple workaround is to manually change the seed at every round and use <code>exp.run_once</code> instead:</p> <div class=highlight><pre><span></span><code><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_rounds</span><span class=p>):</span>
    <span class=n>training_args</span><span class=p>[</span><span class=s1>&#39;random_seed&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=mi>42</span> <span class=o>+</span> <span class=n>i</span>
    <span class=n>exp</span><span class=o>.</span><span class=n>set_training_args</span><span class=p>(</span><span class=n>training_args</span><span class=p>)</span>
    <span class=n>exp</span><span class=o>.</span><span class=n>run_once</span><span class=p>()</span>
</code></pre></div> <h4 id=sub-arguments-for-optimizer-and-differential-privacy>Sub-arguments for optimizer and differential privacy</h4> <p>In Pytorch experiments, you may include sub arguments such as <code>optimizer_args</code> and <code>dp_args</code>. Optimizer arguments represents the arguments that are going to be passed to <code>def init_optimizer(self, o_args)</code> method as dictionary and (<code>dp_args</code>) represents the arguments of <a href=../../../tutorials/security/non-private-local-central-dp-monai-2d-image-registration/ >differential privacy</a>.</p> <div class=note> <p> Optimizer arguments and differential privacy arguments are valid only in PyTorch base training plan. </p> </div> <div class=highlight><pre><span></span><code><span class=n>training_args</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;loader_args&#39;</span><span class=p>:</span> <span class=p>{</span>
        <span class=s1>&#39;batch_size&#39;</span><span class=p>:</span> <span class=mi>20</span><span class=p>,</span>
    <span class=p>},</span>
    <span class=s1>&#39;num_updates&#39;</span><span class=p>:</span> <span class=mi>100</span><span class=p>,</span>
    <span class=s1>&#39;optimizer_args&#39;</span><span class=p>:</span> <span class=p>{</span>
        <span class=s1>&#39;lr&#39;</span><span class=p>:</span> <span class=mf>1e-3</span><span class=p>,</span>
    <span class=p>},</span>
    <span class=s1>&#39;dp_args&#39;</span><span class=p>:</span> <span class=p>{</span>
        <span class=s1>&#39;type&#39;</span><span class=p>:</span> <span class=s1>&#39;local&#39;</span><span class=p>,</span>
        <span class=s1>&#39;sigma&#39;</span><span class=p>:</span> <span class=mf>0.4</span><span class=p>,</span>
        <span class=s1>&#39;clip&#39;</span><span class=p>:</span> <span class=mf>0.005</span>
    <span class=p>},</span>
    <span class=s1>&#39;dry_run&#39;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
<span class=p>}</span>
</code></pre></div> <h4 id=sharing-persistent-buffers>Sharing persistent buffers</h4> <p>In Pytorch experiments, you may include the argument <code>share_persistent_buffers</code>. When set to <code>True</code> (default), nodes will share the full <code>state_dict()</code> of the Pytorch module, which contains both the learnable parameters and the persistent buffers (defined as invariant in the network, like batchnormâ€™s <code>running_mean</code> and <code>running_var</code>). When set to <code>False</code>, nodes will only share learnable parameters.</p> <p>This argument will be ignored for scikit-learn experiments, as the notion of persistent buffers is specific to Pytorch.</p> <h3 id=aggregator>Aggregator</h3> <p>An aggregator is one of the required arguments for the experiment. It is used on the researcher for aggregating model parameters that are received from the nodes after every round. By default, when the experiment is initialized without passing any aggregator, it will automatically use the default <code>FedAverage</code> aggregator class. However, it is also possible to set a different aggregation algorithm with the method <code>set_aggregator</code>. Currently, Fed-BioMed has only <code>FedAverage</code> and <code>Scaffold</code> classes, but it is possible to create a custom aggregator class. You can see the current aggregator by running <code>exp.aggregator()</code>. It will return the aggregator object that will be used for aggregation.</p> <p>When you pass the aggregator argument as <code>None</code> it will use <code>FedAverage</code> aggregator (performing a Federated Averaging aggregation) by default.</p> <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>set_aggregator</span><span class=p>(</span><span class=n>aggregator</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</code></pre></div> <p>or you can directly pass an aggregator instance</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>fedbiomed.researcher.aggregators.fedavg</span> <span class=kn>import</span> <span class=n>FedAverage</span>
<span class=n>exp</span><span class=o>.</span><span class=n>set_aggregator</span><span class=p>(</span><span class=n>aggregator</span><span class=o>=</span><span class=n>FedAverage</span><span class=p>())</span>
</code></pre></div> <div class="admonition info"> <p>Custom aggregator classes should inherit from the base class <code>Aggregator</code> of Fed-BioMed. Please visit user guide for <a href=../aggregation/ >aggregators</a> for more information.</p> </div> <div class="admonition info"> <p class=admonition-title>About Scaffold Aggregator</p> <p><code>FedAverage</code> reflects only how local models sent back by <code>Nodes</code> are aggregated, whereas <code>Scaffold</code> also implement additional elements such as the <code>Optimizer</code> on <code>Researcher</code> side. Please note that currently only <code>FedAverage</code> is compatible with <a href=../../advanced-optimization><code>declearn</code>'s <code>Optimizers</code></a>.</p> </div> <h3 id=node-selection-strategy>Node Selection Strategy</h3> <p>Node selection Strategy is also one of the required arguments for the experiment. It is used for selecting nodes before each round of training. Since the strategy will be used for selecting nodes, thus, training data should be already set before setting any strategies. Then, strategy will be able to select among training nodes that are currently available regarding their dataset.</p> <p>By default, <code>set_strategy(node_selection_strategy=None)</code> will use the default <code>DefaultStrategy</code> strategy. It is the default strategy in Fed-BioMed that selects for the training all the nodes available regardless their datasets. However, it is also possible to set different strategies. Currently, Fed-BioMed only provides <code>DefaultStrategy</code> but you can create your custom strategy classes.</p> <h3 id=round-limit>Round Limit</h3> <p>The experiment should have a round limit that specifies the max number of training round. By default, it is <code>None</code>, and it needs to be created either declaring/building experiment class or using setter method for round limit. Setting round limit doesn't mean that it is going to be permanent. It can be changed after running the experiment once or more.</p> <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>set_round_limit</span><span class=p>(</span><span class=n>round_limit</span><span class=o>=</span><span class=mi>4</span><span class=p>)</span>
</code></pre></div> <p>To see current round limit of the experiment:</p> <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>round_limit</span><span class=p>()</span>
</code></pre></div> <p>You might also wonder how many rounds have been completed in the experiment. The method <code>round_current()</code> will return the last round that has been completed.</p> <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>round_currrent</span><span class=p>()</span>
</code></pre></div> <h3 id=displaying-training-loss-values-through-tensorboard>Displaying training loss values through Tensorboard</h3> <p>The argument <code>tensorboard</code> is of type boolean, and it is used for activating tensorboard during the training. When it is <code>True</code> the loss values received from each node will be written into tensorboard event files in order to display training loss function on the tensorboard interface.</p> <p>Tensorboard events are controlled by the class called <code>Monitor</code>. To enable tensorboard after the experiment has already been initialized, you can use the method <code>set_monitor()</code> of the experiment object.</p> <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>set_monitor</span><span class=p>(</span><span class=n>tensorboard</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <p>You can visit <a href=../tensorboard/ >tensorboard documentation</a> page to get more information about how to use tensorboard with Fed-BioMed</p> <h3 id=saving-breakpoints>Saving Breakpoints</h3> <p>Breakpoint is a researcher side function that saves an intermediate status and training results of an experiment to disk files. The argument <code>save_breakpoints</code> is of type boolean, and it indicates whether breakpoints of the experiment should be saved after each round of training or not. <code>save_breakpoints</code> can be declared while creating the experiment or after using its setter method.</p> <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>set_save_breakpoints</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <div class="admonition info"> <p class=admonition-title>Info</p> <p>Setting <code>save_breakpoints</code> to <code>True</code> after the experiment has performed several rounds of training will only save the breakpoints for remaining rounds.</p> </div> <p>Please visit the tutorial <a href=../../../tutorials/advanced/breakpoints/ >"Breakpoints (experiment saving facility)"</a> to find out more about breakpoints.</p> <h3 id=experimentation-folder>Experimentation Folder</h3> <p>Experimentation folder indicates the name of the folder in which all the experiment results will be stored/saved. By default, it will be <code>Experiment_XXXX</code>, and <code>XXXX</code> part stands for the auto increment (hence, first folder will be named <code>Experiment_0001</code>, the second one <code>Experiment_0002</code> and so on). However, you can also define your custom experimentation folder name.</p> <p>Passing experimentation folder while creating the experiment; <div class=highlight><pre><span></span><code><span class=n>exp</span> <span class=o>=</span> <span class=n>Experiment</span><span class=p>(</span>
    <span class=c1>#....</span>
    <span class=n>experimentation_folder</span><span class=o>=</span><span class=s1>&#39;MyExperiment&#39;</span>
    <span class=c1>#...</span>
<span class=p>)</span>
</code></pre></div></p> <p>Setting experimentation folder using setter; <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>set_experimentation_folder</span><span class=p>(</span><span class=n>experimentation_folder</span><span class=o>=</span><span class=s1>&#39;MyExperiment&#39;</span><span class=p>)</span>
</code></pre></div></p> <p>Using custom folder name for your experimentation might be useful for identifying different types of experiment. Experiment folders will be located at <code>${FEDBIOMED_DIR}/var/experiments</code>. However, you can always get exact path to your experiment folder using the getter method <code>experimentation_path()</code>. Below is presented a way to retrieve all the files from the folder using <code>os</code> builtin package.</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>os</span>

<span class=n>exp_path</span> <span class=o>=</span> <span class=n>exp</span><span class=o>.</span><span class=n>experimentation_path</span><span class=p>()</span>
<span class=n>os</span><span class=o>.</span><span class=n>listdir</span><span class=p>(</span><span class=n>exp_path</span><span class=p>)</span>
</code></pre></div> <h2 id=running-an-experiment>Running an Experiment</h2> <h3 id=train_request-and-train_reply-messages><code>train_request</code> and <code>train_reply</code> messages</h3> <p>Running an experiment means starting the training process by sending train request to nodes. It creates training request that are subscribed by each live node that has the dataset. After sending training commands it waits for the responses that will be sent by the nodes. The following code snippet represents an example of train request.</p> <div class=highlight><pre><span></span><code><span class=p>{</span>
<span class=w>  </span><span class=nt>&quot;researcher_id&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;researcher id that sends training command&quot;</span><span class=p>,</span>
<span class=w>  </span><span class=nt>&quot;experiment_id&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;created experiment id by experiment&quot;</span><span class=p>,</span>
<span class=w>  </span><span class=nt>&quot;state_id&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;state id for this round this experiment on this node&quot;</span><span class=p>,</span>
<span class=w>  </span><span class=nt>&quot;training_args&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
<span class=w>    </span><span class=nt>&quot;loader_args&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
<span class=w>      </span><span class=nt>&quot;batch_size&quot;</span><span class=p>:</span><span class=w> </span><span class=mi>32</span>
<span class=w>    </span><span class=p>},</span>
<span class=w>    </span><span class=nt>&quot;optimizer_args&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
<span class=w>      </span><span class=nt>&quot;lr&quot;</span><span class=p>:</span><span class=w> </span><span class=mf>0.001</span>
<span class=w>    </span><span class=p>},</span>
<span class=w>    </span><span class=nt>&quot;epochs&quot;</span><span class=p>:</span><span class=w> </span><span class=mi>1</span><span class=p>,</span>
<span class=w>    </span><span class=nt>&quot;dry_run&quot;</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=p>,</span>
<span class=w>    </span><span class=nt>&quot;batch_maxnum&quot;</span><span class=p>:</span><span class=w> </span><span class=mi>100</span>
<span class=w>  </span><span class=p>},</span>
<span class=w>  </span><span class=nt>&quot;dataset_id&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;id of the used dataset on this node&quot;</span><span class=p>,</span>
<span class=w>  </span><span class=nt>&quot;training&quot;</span><span class=p>:</span><span class=w> </span><span class=err>True</span><span class=p>,</span>
<span class=w>  </span><span class=nt>&quot;model_args&quot;</span><span class=p>:</span><span class=w> </span><span class=err>&lt;args&gt;</span><span class=p>,</span>
<span class=w>  </span><span class=nt>&quot;params&quot;</span><span class=p>:</span><span class=w> </span><span class=err>&lt;model</span><span class=w> </span><span class=err>weigh</span><span class=kc>ts</span><span class=err>&gt;</span><span class=p>,</span>
<span class=w>  </span><span class=nt>&quot;training_plan&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;&lt;training plan code&gt;&quot;</span><span class=p>,</span>
<span class=w>  </span><span class=nt>&quot;training_plan_class&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;MyTrainingPlan&quot;</span><span class=p>,</span>
<span class=w>  </span><span class=nt>&quot;command&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;train&quot;</span><span class=p>,</span>
<span class=w>  </span><span class=nt>&quot;round&quot;</span><span class=p>:</span><span class=w> </span><span class=err>&lt;rou</span><span class=kc>n</span><span class=err>d_</span><span class=kc>nu</span><span class=err>mber&gt;</span><span class=p>,</span>
<span class=w>  </span><span class=nt>&quot;aggregator_args&quot;</span><span class=p>:</span><span class=w> </span><span class=err>&lt;args&gt;</span><span class=p>,</span>
<span class=w>  </span><span class=nt>&quot;aux_vars&quot;</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=err>lis</span><span class=kc>t</span><span class=w> </span><span class=err>o</span><span class=kc>f</span><span class=w> </span><span class=err>auxiliary</span><span class=w> </span><span class=err>variables</span><span class=p>],</span>
<span class=w>  </span><span class=nt>&quot;secagg_arguments&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
<span class=w>    </span><span class=nt>&quot;secagg_servkey_id&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;secure aggregation server key id&quot;</span><span class=p>,</span>
<span class=w>    </span><span class=nt>&quot;secagg_random&quot;</span><span class=p>:</span><span class=w> </span><span class=err>&lt;ra</span><span class=kc>n</span><span class=err>dom</span><span class=w> </span><span class=kc>nu</span><span class=err>mber&gt;</span><span class=p>,</span>
<span class=w>    </span><span class=nt>&quot;secagg_clipping_range&quot;</span><span class=p>:</span><span class=w> </span><span class=mi>3</span>
<span class=w>      </span><span class=p>]</span>
<span class=w>  </span><span class=p>}</span>
<span class=w>  </span><span class=p>}</span>
<span class=p>}</span>
</code></pre></div> <p>After sending train requests, Experiment waits for the replies that are going to be published by each node once every round of training is completed. These replies are called training replies, and they include information about the training and the URL from which to download model parameters that have been uploaded by the nodes to the file repository. The following code snippet shows an example of <code>training_reply</code> from a node.</p> <p><div class=highlight><pre><span></span><code><span class=p>{</span>
   <span class=s2>&quot;researcher_id&quot;</span><span class=p>:</span><span class=s2>&quot;researcher id that sends the training command&quot;</span><span class=p>,</span>
   <span class=s2>&quot;experiment_id&quot;</span><span class=p>:</span><span class=s2>&quot;experiment id that creates training request&quot;</span><span class=p>,</span>
   <span class=s2>&quot;success&quot;</span><span class=p>:</span><span class=kc>True</span><span class=p>,</span>
   <span class=s2>&quot;node_id&quot;</span><span class=p>:</span><span class=s2>&quot;ID of the node that completes the training &quot;</span><span class=p>,</span>
   <span class=s2>&quot;dataset_id&quot;</span><span class=p>:</span><span class=s2>&quot;dataset_dcf88a68-7f66-4b60-9b65-db09c6d970ee&quot;</span><span class=p>,</span>
   <span class=s2>&quot;timing&quot;</span><span class=p>:{</span>
      <span class=s2>&quot;rtime_training&quot;</span><span class=p>:</span><span class=mf>87.74385611899197</span><span class=p>,</span>
      <span class=s2>&quot;ptime_training&quot;</span><span class=p>:</span><span class=mf>330.388954968</span>
   <span class=p>},</span>
   <span class=s2>&quot;msg&quot;</span><span class=p>:</span><span class=s2>&quot;&quot;</span><span class=p>,</span>
   <span class=s2>&quot;command&quot;</span><span class=p>:</span><span class=s2>&quot;train&quot;</span><span class=p>,</span>
  <span class=s2>&quot;state_id&quot;</span><span class=p>:</span> <span class=s2>&quot;state id for new round this experiment on this node&quot;</span><span class=p>,</span>
  <span class=s2>&quot;params&quot;</span><span class=p>:</span> <span class=o>&lt;</span><span class=n>model</span> <span class=n>weights</span><span class=o>&gt;</span><span class=p>,</span>
  <span class=o>...</span>
<span class=p>}</span>
</code></pre></div> <code>training_reply</code> always results of a <code>training_request</code> sent by the <code>Researcher</code> to the <code>Node</code>.</p> <p>To complete one round of training, the experiment waits until receiving each reply from nodes. At the end of the round, it downloads the model parameters that are indicated in the training replies. It aggregates the model parameters based on a given aggregation class/algorithm. This process is repeated until every round is completed. Please see Figure 1 to understand how federated training is performed between the nodes and the Researcher (<code>Experiment</code>) component.</p> <p><img alt="Federated training workflow" src=../../../assets/img/diagrams/fedbiomed-workflow.jpg#img-centered-xlr> <em>Figure 2 - Federated training workflow among the components of Fed-BioMed. It illustrates the messages exchanged between <code>Researcher</code> and 2 <code>Nodes</code> during a Federated Training</em></p> <h3 id=the-methods-runand-run_once>The Methods <code>run()</code>and <code>run_once()</code></h3> <p>In order to provide more control over the training rounds, <code>Experiment</code> class has two methods as <code>run</code> and <code>run_once</code> to run training rounds.</p> <ul> <li><code>run()</code> runs the experiment rounds from current round to round limit. If the round limit is reached it will indicate that the round limit has been reached. However, the method <code>run</code> takes 2 arguments as <code>rounds</code> and <code>increase</code>.<ul> <li><code>rounds</code> is an integer that indicates number of rounds that are going to be run. If the experiment is at round <code>0</code>, the round limit is <code>4</code>, and if you pass <code>rounds</code> as 3, it will run the experiment only for <code>3</code> rounds.</li> <li><code>increase</code> is a boolean that indicates whether round limit should be increased if the given <code>rounds</code> pass over the round limit. For example, if the current round is <code>3</code>, the round limit is <code>4</code>, and the <code>rounds</code> argument is <code>2</code>, the experiment will increase round limit to <code>5</code></li> </ul> </li> <li><code>run_once()</code> runs the experiment for single round of training. If the round limit is reached it will indicate that the round limit has been reached. This command is the same as <code>run(rounds=1, increase=False)</code>. However, if <code>run_once</code> is executed as <code>run_once(increase=True)</code>, then, when the round limit is reached, it increases the round limit for one extra round.</li> </ul> <p>To run your experiment until the round limit;</p> <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>run</span><span class=p>()</span>
</code></pre></div> <p>To run your experiment for given number of rounds (while not passing the round limit):</p> <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>rounds</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</code></pre></div> <p>To run your experiment for given number of rounds and increase the round limit accordingly if needed:</p> <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>rounds</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>increase</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <p>To run your experiment only once (while not passing the round limit):</p> <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>run_once</span><span class=p>()</span>
</code></pre></div> <p>To run your experiment only once even round limit is reached (and increase the round limit accordingly if needed):</p> <div class=highlight><pre><span></span><code><span class=n>exp</span><span class=o>.</span><span class=n>run_once</span><span class=p>(</span><span class=n>increase</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <div class="admonition info"> <p>Running experiment with both <code>run(rounds=rounds, increase=True)</code> and <code>run_once(increase=True)</code> will automatically increase/update round limit if it is exceeded.</p> </div> </article> </main> </div> <div class=right-col> <div id=right-sidebar class=sidebar-right> <nav class=toc> <!-- Render item list --> <label class=toc-title for=__toc> <span class=toc-icon></span> </label> <ul class=toc-list data-md-component=toc data-md-scrollfix> <!-- Table of contents item --> <li class=toc-item> <a href=#introduction class=md-nav__link> Introduction </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#defining-an-experiment class=md-nav__link> Defining an experiment </a> <nav class=toc-nav aria-label="Defining an experiment"> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#setting-the-training-data class=md-nav__link> Setting the training data </a> <nav class=toc-nav aria-label="Setting the training data"> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#setting-the-training-data-by-setting-the-tags class=md-nav__link> Setting the training data by setting the tags </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#setting-the-training-data-by-providing-the-metadata-directly class=md-nav__link> Setting the training data by providing the metadata directly </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#under-the-hood-consistency-with-all-members-of-experiment class=md-nav__link> Under-the-hood consistency with all members of Experiment </a> </li> </ul> </nav> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#selecting-specific-nodes-for-the-training class=md-nav__link> Selecting specific Nodes for the training </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#load-your-training-plan-training-plan-class class=md-nav__link> Load your Training Plan: Training Plan Class </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#model-arguments class=md-nav__link> Model Arguments </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#training-arguments class=md-nav__link> Training Arguments </a> <nav class=toc-nav aria-label="Training Arguments"> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#controlling-the-number-of-training-loop-iterations class=md-nav__link> Controlling the number of training loop iterations </a> <nav class=toc-nav aria-label="Controlling the number of training loop iterations"> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#compatibility class=md-nav__link> Compatibility </a> </li> </ul> </nav> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#batch-size-and-other-data-loader-arguments class=md-nav__link> Batch size and other data loader arguments </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#setting-a-random-seed-for-reproducibility class=md-nav__link> Setting a random seed for reproducibility </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#sub-arguments-for-optimizer-and-differential-privacy class=md-nav__link> Sub-arguments for optimizer and differential privacy </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#sharing-persistent-buffers class=md-nav__link> Sharing persistent buffers </a> </li> </ul> </nav> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#aggregator class=md-nav__link> Aggregator </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#node-selection-strategy class=md-nav__link> Node Selection Strategy </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#round-limit class=md-nav__link> Round Limit </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#displaying-training-loss-values-through-tensorboard class=md-nav__link> Displaying training loss values through Tensorboard </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#saving-breakpoints class=md-nav__link> Saving Breakpoints </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#experimentation-folder class=md-nav__link> Experimentation Folder </a> </li> </ul> </nav> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#running-an-experiment class=md-nav__link> Running an Experiment </a> <nav class=toc-nav aria-label="Running an Experiment"> <ul class=toc-list> <!-- Table of contents item --> <li class=toc-item> <a href=#train_request-and-train_reply-messages class=md-nav__link> train_request and train_reply messages </a> </li> <!-- Table of contents item --> <li class=toc-item> <a href=#the-methods-runand-run_once class=md-nav__link> The Methods run()and run_once() </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> </div> <!-- News Main Page--> </div> <footer> <div class=container-fluid> <div class=footer-first-inner> <div class=container> <div class=row> <div class=col-md-6> <div class=footer-contact> <strong>Address:</strong> <p>2004 Rte des Lucioles, 06902 Sophia Antipolis</p> <strong>E-mail:</strong> <p>fedbiomed _at_ inria _dot_ fr</p> </div> </div> <div class=col-md-6> <div class=footer-contact> <p>Fed-BioMed Â© 2022</p> </div> </div> </div> </div> </div> </div> </footer> <!-- JQuery --> <script src=https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js></script> <!-- Latest compiled and minified JavaScript --> <script src=https://stackpath.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js integrity=sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd crossorigin=anonymous></script> <script src=https://cdnjs.cloudflare.com/ajax/libs/tablesort/5.2.1/tablesort.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../../assets/javascript/lunr.js></script> <script src=../../../assets/javascript/theme.js></script> <!-- GitHub buttons --> <script async defer src=https://buttons.github.io/buttons.js></script> </body> </html>